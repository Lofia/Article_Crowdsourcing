Rating the valence of media content about electronic cigarettes using crowdsourcing: Testing rater instructions and estimating the optimal number of raters
Electronic cigarettes (e-cigarettes) are a controversial public health topic due to their increasing popularity among youth and the uncertainty about their risks and benefits. Researchers have started to assess the valence of media content about e-cigarette use, mostly using expert coding. The current study aims to offer a methodological framework and guideline when using crowdsourcing to rate the valence of e-cigarette media content. Specifically, we present 1) an experiment to determine rating instructions that would result in reliable valence ratings and 2) an analysis to identify the optimal number of raters needed to replicate these ratings. Specifically, we compared ratings produced by crowdsourced raters instructed to rate from several different perspectives (e.g., objective vs. subjective) and determined the instructions that led to reliable ratings. We then used bootstrapping methods and a set of criteria to identify the minimum number of raters needed to replicate these ratings. Results suggested that when rating e-cigarette valence, instructing raters to rate from their own subjective perspective produced reliable results, and nine raters were deemed the optimal number of raters. We expect these findings to inform future content analyses of e-cigarette valence. The study procedures can be applied to crowdsourced content analyses of other health-related media content to determine appropriate rating instructions and the number of raters.
Electronic cigarettes (e-cigarettes) are at the center of a public health debate. Current e-cigarette use among high school students increased from 1.5% in 2011 to 20.8% in 2018; among middle school students it increased from 0.6% in 2011 to 4.9% in 2018. The Surgeon General issued a report warning against youth e-cigarette use that voiced concerns about addiction and other health effects. On the other hand, since e-cigarettes are less toxic than traditional cigarettes, some propose that using e-cigarettes could reduce harm for adult smokers (; Warner, 2018). Amidst conflicting views, communication about e-cigarettes through news articles and unregulated e-cigarette advertising have the potential to shape public perceptions about e-cigarette use. This backdrop calls for close monitoring of the public communication environment about e-cigarettes.
In response, many studies have conducted analyses of e-cigarette content in newspapers and social media sources (e.g.,). However, most studies have used expert coding, where a pre-defined coding scheme is used to determine, for example, the sentiment or valence towards e-cigarettes. We propose that using crowdsourcing–utilizing a crowd of workers–(; Sheehan, 2017) to rate the valence of e-cigarette related media content can be particularly useful when aiming to predict public intentions from media content analysis, and given the complex nature of e-cigarette valence. In doing so, the objective of the current article is to outline and provide a methodological framework and guidelines to address two methodological issues–specifically, 1) which perspective to use for rating instructions; and 2) the most efficient number of workers needed to rate the valence of e-cigarette media texts.
Below, we first introduce the context of measuring the valence of e-cigarette texts and elaborate on why crowdsourced rating may be conducive to this context. Second, we present an experiment that manipulated the instructions to crowdworkers to change their perspectives when rating the valence of e-cigarette texts. We report the instruction that achieved the highest reliability. Third, we further analyze the experimental results to determine the minimum number of crowdsourced workers needed to replicate these results. Together we expect these findings to provide practical guidelines for future studies using crowdsourced methods for content analysis of e-cigarette valence, as well as other health communication topics.
Assessing valence perceptions of e-cigarette media content with crowdsourcing
The present study stems from a larger project that aimed to characterize the perceived valence of media content about e-cigarettes to examine how this valence content may affect population intentions to use e-cigarettes. Whether media coverage is mostly supportive of using e-cigarettes and the e-cigarette industry (i.e., pro-ecig valence) or mostly against using e-cigarettes and the e-cigarette industry (i.e., anti-ecig valence) may impact the public’s attitudes and intentions towards using e-cigarettes (Yang, Liu, Lochbuehler, & Hornik, 2017). E-cigarette use has become an increasingly salient public health issue. For example, youth use of e-cigarettes has increased at an alarming rate, but some propose that e-cigarettes can be used as a harm-reduction tool for adult smokers (Warner, 2018). As such, media content about e-cigarettes is expected to be complex and varied in valence.
Our decision to use crowdsourced raters (from Amazon Mechanical Turk [MTurk]) to assess the valence of e-cigarette media texts instead of traditional expert coding was shaped by both our aim to predict public intentions to use e-cigarettes from the valence of a large corpus of media coverage and our expectation of valence complexity. First, it made sense to use lay crowdsourced raters instead of expert coders since raters would more accurately represent a typical reader’s understanding of e-cigarette media texts. Compared to expert coders who may view e-cigarette media texts differently due to their knowledge and expertise on e-cigarettes, crowdsourced workers may relatively better characterize valence from the public’s point of view. Research shows that samples from MTurk reasonably represent the U.S. public in that their results are no different from Internet panels or nationally representative samples. Also, given our large corpus of media texts, we chose to task crowdsourced raters who are easier to recruit in large numbers and typically do not require extensive training. Second, valence of e-cigarette coverage is likely to be complex and may be better assessed with crowdsourced rating. Research has consistently shown that crowds produce evaluations of content similar to that of a smaller number of experts even when content is complex and latent.
This logic can also be applied to our study context of rating e-cigarette valence. The valence of e-cigarette texts is not easy to comprehensively describe through a codebook for expert coders because of multiple valence dimensions. For example, an article describing an event where e-cigarettes are banned may illustrate an anti-ecig valence while an author’s upset reaction to the ban may indicate pro-ecig sentiment. Instead, relying on the interpretation of the texts from a larger number of raters may yield a valence estimate more representative of typical readers’ perceptions, which may predict population opinion more accurately.
Maximizing the reliability of crowdsourced ratings: Perspective
The goal of measurement is validity, in this case assessing how well a measure of valence captures perceived valence that can accurately predict population intentions. While the test of validity is beyond the scope of this study, reliability, in turn, is measurable and a prerequisite for validity. We compare which crowdsourced rating procedures (meant to validly measure population-perceived valence) yield maximum reliability. The standards for this comparison must reflect the assumptions of how content is characterized. Traditionally, the goal to describe content objectively has led experts to code the valence of media content by treating it as manifest content. Manifest content is content that is “believed to have an objective pattern that all coders should uncover by sorting through symbols and recognizing the connections among them” (, p. 259). Previous tobacco-related content analyses relied on expert coding using detailed coding schemes that a priori defined valence in specific instances and contexts. For example, a codebook might specify that descriptions of tobacco regulations and secondhand smoke issues should be coded as anti-tobacco valence. The level of agreement across expert coders indicated whether the standard set by the codebook could be reliably reproduced.
On the other hand, when the goal is to capture a typical reader’s impression of valence to predict population intentions, crowdsourced ratings that treat valence as latent content may be more appropriate. Latent content is content in which researchers “put precedence with the coders’ judgments and believe that the elements in the content are symbols that require viewers to access their pre-existing mental schema in order to judge the meaning in the content” (, p. 259). The agreement across crowdsourced raters indicates that they are reliably converging on a shared norm or mental schema. In order to maximize the reliability of crowdsourced ratings, the task for researchers is to devise definitions and instructions that will get raters to access schemas that lead to high agreement.
With this standard in mind, it is reasonable to assume that the perspective raters take may influence their level of agreement. In the context of rating the sentiment toward political actors, only one study has examined the impact of perspectives on rating outcomes by asking raters for either “explicit evaluations”, “evaluation”, or to “take the perspective of the target of opinion”. However, to the best of our knowledge, there are no studies that examine the impact of perspectives in the domain of rating the valence of e-cigarette related media texts. In addition, there is limited research on how the level of agreement across raters is affected by differing rating perspectives.
Nonetheless, instructions for crowdsourced text analyses do vary, and, intended or not, seem to call for different perspectives. For instance, when asking crowdsourced workers to rate the ideological slant of news articles, Budak and colleagues (2016) asked: “Is the article generally positive, neutral, or negative toward members of the Democratic party?” The wording “is the article” implicitly suggests there is an objective, “true” slant that exists, independent of subjective interpretation. These instructions imply the perspective of manifest content. In contrast, when Benoit and colleagues had crowdsourced workers rate the ideological slant of policies, they asked: “If you believe the sentence expresses a centrist position on economic or social policy…click the ‘neither…nor…’ position on the scale.” Here the stem “if you believe” directs raters to use their own perspective and subjective judgment. These instructions reflect the perspective of latent content. Thus, while instructions for crowdsourced ratings vary in the perspectives requested, the question of which of these instructions can maximize rater agreement in the context of assessing valence perceptions of e-cigarette media content remains unaddressed.
The current study aims to fill this gap by experimentally varying perspectives in rating instructions and comparing the agreement levels produced by each instruction. Specifically, the study compared instructions implying an objective perspective such as those used by (Objective-text), and instructions requesting subjective judgments of the rater’s own opinion such as those used by (Subjective-own opinion). In addition, we considered two other subjective instructions that asked for ratings from the perspective of “most people” (Subjective-most people) and “someone who only knows a little about e-cigarettes” (Subjective-naïve people). We expected that taking the perspective of others might reduce individual biases and lead to more homogenous ratings.
RQ1: Among the four perspectives: Objective-text, Subjective-own opinion, Subjective-most people, Subjective-naïve people, which one(s) will yield the highest level of agreement across raters when rating the valence of e-cigarette media content?
Maximizing the efficiency of crowdsourced ratings: Optimal number of raters
Another important concern when using crowdsourced rating is maximizing efficiency. Sampling theory predicts that a large number of raters will converge on a more accurate estimate of valence compared to a small number of raters. However, a large number of raters requires more resources, and may therefore be less efficient. Therefore, we examined the optimal number of raters needed to reach accuracy. We define the optimal number as the minimum number of raters needed to closely reproduce the pattern of rating results produced by a larger set of raters (from the condition that was most successful in our instructions experiment). Prior research in educational and medical settings has used criteria such as the deviation of ratings produced by fewer raters from estimates produced by a large number of raters (e.g., smallest standard errors). examined the issue of optimal rater sample size to achieve accurate evaluation ratings of anti-tobacco messages. They used multiple criteria including Pearson correlation, deviation of ratings produced by fewer raters from the original data (i.e., mean difference), proportions of misspecification errors, and classification of messages into top vs. bottom rated groups.
Following this line of inquiry, the current study used two criteria used by, Pearson correlations and mean differences between the original sample and subsamples of fewer raters. These were used to gauge the extent to which subsamples of fewer raters can reproduce the percentage of raters who made the correct choice in the original sample. In addition, we propose two new metrics, percentage of “Any Mistakes” and “Big Mistakes” to maximally reduce rating errors. These metrics help mitigate against misspecification (particularly extreme deviation) of valence labels. We used these criteria to identify the minimum number of raters required.
Bootstrapping methods were used to draw subsamples of fewer raters and to empirically estimate the four metrics. Bootstrapping is a nonparametric simulation-based approach that is most useful when strong assumptions of normal sampling distributions cannot be made or are unknown, or when sample sizes are relatively small. The key underlying logic for performing bootstrapping in our study context is to treat the rater sample as the population, and to iteratively conduct random resampling (with replacement) from the sample to mimic the original sampling process. The new subsamples generated by the resampling processes form empirical sampling distributions that allow for calculations of confidence intervals and hypothesis testing. Bootstrapping is particularly appropriate for comparing mean consistency across different rater sizes because rating data is by nature skewed (in fact, the higher the consistency, the more skewed the distribution), and our sample sizes (both for texts and raters) are relatively small. Applying bootstrapping procedures, we aimed to identify an efficient rater size by comparing the Pearson correlations and mean differences between the original sample and the new subsamples and by calculating the percentage of “Any Mistakes” and “Big Mistakes” in subsamples.
RQ2: What is the minimum number of raters necessary to provide valence ratings that sufficiently reproduce the results from a larger set of raters and avoids misspecification errors when rating e-cigarette media content?
Method
Study design
Raters (N=526) were recruited from Amazon Mechanical Turk’s platform to participate in a preliminary qualification task in June 2016. Raters who were located in the US; had a Human Intelligence Task (HIT) approval rate greater than 95%; and had completed more than 100 HITs were invited. Raters had a mean age of 35 (SD=12); almost half had a college degree or more; and 78% were White, 8% Black, and 6% Hispanic. The qualification task familiarized raters with the condition-specific definitions of e-cigarette valence (see Table 1), provided them with example texts, and asked them to rate texts judged by the investigators to have a clear valencei using the rating instructions to which they were randomly assigned (one of four). This step trained raters on the task and allowed us to exclude low-quality raters who were inattentive or unable to follow instructions carefully. Only those who passed the qualification were given the opportunity to participate in the main experiment that took place within a week of the qualification task.ii
In the main experiment, 20 raters in each condition from the pool of qualified raters entered the same conditions to which they were originally assigned (n=20 per condition; N=80) on a first come basis. The main experiment asked raters to rate the valence of 34 texts randomly selected from a pool of media articles (media articles published between May 2014–December 2015; Associated Press [n=6], newspapers [n=6], websites [n=12], and broadcast TV news [n=10]). We required each article to have at least three e-cigarette relevant terms to ensure that some portion of the article was centrally about e-cigarettes, and not just a passing mention of them.iii The sample size of 34 texts was determined by following the formula for judging consistency of ratings, which is a function of the smallest estimated proportion of a certain valence label in the population of texts (.33; assuming equal proportions across the three valence categories: anti-ecig, pro-ecig, and not applicable), the smallest acceptable Krippendorff’s alpha (.667), and the desired level of significance (.05).
Main experiment procedure
The main experiment was hosted on the Qualtrics survey platform. Raters were first reminded of the definition for e-cigarettes and condition-specific definitions of the valence categories: anti-ecig, pro-ecig, and not applicable. They then rated 34 e-cigarette texts, one text at a time.iv To reduce rating fatigue, e-cigarette-relevant words in the text were highlighted.
Conditions.
The four conditions of the qualification task and main experiment manipulated the perspective from which raters were asked to make a judgment (i.e., Objective-text, Subjective-own opinion, Subjective-most people, Subjective-naïve people). Condition-specific definitions and question wordings are listed in Table 1.
Measures.
The majority of texts in the corpus were about e-cigarettes, but due to the automated nature of text retrieval, a few were not. Therefore, before rating the valence of each given text, raters were first asked whether some part of the text was relevant to e-cigarettes. Those who answered yes were then asked whether the e-cigarette relevant content was either pro-ecig, anti-ecig, or not applicable. It is important to note that the latter question about valence was framed differently depending on the condition to which raters were assigned. For example, for raters in Condition 1 (Objective-text), the question was framed as: “For the content that is ecig-relevant, is it mostly anti-ecig, pro-ecig or not applicable?” The phrase, “is it” was used to consistently instruct the rater to rate from the objective perspective of the text. In contrast, the question for Condition 2 (Subjective-own opinion) reads: “For the content that is ecig-relevant, would you understand it to be mostly anti-ecig, pro-ecig or not applicable?” The phrase, “would you understand it to be” was used to instruct raters to judge from their own perspective or opinion.
Analysis: Instructions for reliable ratings
First, to assess the level of rating consistency across raters by condition, 95% and 83% confidence intervals for Krippendorff’s alpha were calculated for each condition using bootstrapping. Krippendorff’s alpha was chosen as the primary metric as it provides the degree of agreement across multiple raters within a condition while adjusting for chance agreement. Eighty three percent confidence intervals were computed because they allow direct comparison between estimates; if the 83% CIs of two alpha estimates do not overlap, we infer that the alphas are not equal at the p<.05 level, two-tailed. Our second measure of consistency re-coded raters’ valence labels according to how consistent they were with the within-condition rating consensus for each text.v For example, if the consensus for a text in Condition 1 was anti-ecig, then anti-ecig ratings were assigned a score of 3 (most consistent), NA ratings a score of 2, and pro-ecig ratings a score of 1 (least consistent).vi We conducted a multilevel regression analysis with these 3-level consistency scores as the dependent variable and condition as a categorical independent variable, controlling for random variations of rater and text differences. We used Bonferroni-corrected post-hoc pair-wise comparisons to examine differences in consistency scores across all pairs of conditions.
Analysis: Optimal number of raters
The rating results produced by the 20 raters in the condition that achieved the highest agreement were deemed as reliable and were used to determine the minimum number of raters needed to replicate ratings by the full sample of raters. Specifically, four steps were involved.
Step 1: Determine “majority score” of texts.
First, the “majority” valence label was determined for each of the 34 texts based on the consensus across the 20 raters. We used the label of the mode category as the “majority” label for the text unless the mode percentage was below 50%. In that case, we labeled the text NA instead since in such cases the percentages of raters who chose pro-ecig and anti-ecig labels were relatively equal, thus satisfying the definition of NA (see Table 1). For each text, the percentage of raters who chose the “majority” label category was named the “majority score”, reflecting the extent to which raters were able to converge on the “majority” label.
Step 2: Draw bootstrap samples.
After calculating the “majority scores” for all 34 texts, our next step was to estimate rating outcomes produced by fewer raters. Following prior practices, bootstrapping was used to draw multiple random subsamples with replacement from the pool of 20 raters, with rater sample sizes ranging from one to 20 with an increment of one; 500 bootstrap samples were drawn for each sample size.
Step 3: Calculate “sample score” for each bootstrap sample.
Next, for each bootstrap sample, we calculated the percentage of raters who chose the “majority” label for each of the 34 texts, which we called the “sample score”. This step produced 500 “sample scores” for each text at each sample size (i.e., for a given text, each of the 500 bootstrap samples drawn for five raters yielded one “sample score” for this text). These sample scores were used in the next stage for comparisons with the original data (i.e., “majority score”) using various criteria.
Step 4: Compare ratings with multiple criteria.
First, two criteria were used to compare the rating estimates from smaller samples (“sample scores”) with the ratings from the original data (“majority scores”). The first criterion was 1) Pearson Correlation Coefficients. Correlations between all “sample scores” derived from the bootstrap samples and the corresponding “majority scores” for the 34 texts at each sample size were calculated. Then the average correlation across 500 bootstrap samples with the same rater sample size was calculated (Figure 1, Panel A). This procedure yielded 20 average correlation coefficients (i.e., one for each sample size aggregating the results of the 500 bootstrap samples) each indicating the conformity of smaller sample size ratings with the full sample ratings. Higher correlation coefficients indicate that the bootstrap samples reproduce the original data better.
The second criterion was the 2) Aggregated Mean Absolute Differences between the “sample scores” and the “majority scores” at each sample size. Similar to the first criterion, for each of the bootstrap samples, the absolute difference between the “sample score” and the “majority score” was calculated for each text, and then summed across 34 texts to get an overall estimation of how much that particular bootstrap sample deviated from the original data. We then averaged the difference scores over 500 bootstrap samples within each sample size to estimate how much deviation from the original data the raters at each sample size would produce (Figure 1, Panel B). The smaller the absolute differences, the better the bootstrap samples reproduced the original rating results. These two criteria provide quantifiable metrics to identify a reasonable threshold in sample sizes–i.e., a point where adding an additional rater would not result in either much higher correlations or much smaller difference scores.
In addition to these two criteria, we also considered the best number of raters to avoid rating errors, particularly ones that deviate too far from the “majority” label. Therefore, two additional criteria, 3) Percentage of “Any Mistakes” and 4) Percentage of “Big Mistakes” were proposed. “Any mistakes” refer to instances where the percentage of raters in a bootstrap sample who chose the “majority” label was less than 50% (e.g., 40% chose pro-ecig, when the “majority” label is pro-ecig); “big mistakes” refer to instances where 50% or more of the raters in a bootstrap sample chose the opposite valence category relative to the “majority” label (e.g., 60% chose anti-ecig, when the “majority” label is pro-ecig). The fourth criterion was applied only to texts that had either pro-ecig or anti-ecig “majority” labels (n = 26). At each sample size, we calculated the average percentage of “any mistakes” and “big mistakes” across the total number of texts (34 or 26) and across the 500 bootstrap samples (Figure 1, Panels C and D). The smaller the average percentage of mistakes, the better the bootstrap samples reproduced the original rating results.
Considering that we have four criteria that each represent different aspects of rating, implications of all criteria need to be considered simultaneously to determine the optimal rater size. Therefore, we considered a rater size that 1) does not violate conclusions based on any of the criteria, 2) is determined by the largest minimum number required by each of the criteria; and 3) represents the smallest rater size possible from a cost-benefit perspective.
Results: Instructions for reliable ratings
Table 2 presents Krippendorff’s alpha estimates and their 95% and 83% confidence intervals by condition. Condition 2 (Subjective-own opinion) yielded the highest estimate (.77) indicating consensus across the 20 raters was highest when rating the texts based on their own understanding. However, it is important to note that the confidence intervals overlapped across conditions. Table 3 presents the results from a multilevel regression model predicting the 3-level consistency scores from the four conditions. Table 4 provides post-hoc comparisons across each pair of conditions. The results suggest that rating from raters’ own perspective (Condition 2) and rating from the perspective of most people (Condition 3) led to more consistent ratings compared to rating from the perspective of the objective text (Condition 1). Taken together, results indicate that ratings based on one’s own understanding may achieve the most homogenous ratings of e-cigarette valence in media texts.
Results: Optimal number of raters
The four criteria were examined together to determine the optimal number of raters needed. Figure 2 plots the correlation coefficients between the “majority scores” and aggregated “sample scores” for each rater sample size, and the growth rates which reflect the percent increase in correlation coefficients when comparing rater size n to rater size n - 1  As demonstrated in Figure 2, while the magnitude of correlations gradually increased with each additional rater, after about seven raters, the growth rate started to reach a saturation point. Particularly, adding a tenth rater to the sample of nine only minimally increased (1.22%) the strength of correlations, indicating that increasing the sample size beyond nine raters would not increase efficiency.
The mean absolute differences between the “sample scores” and “majority scores” were examined next. We plotted the average absolute differences at each sample size as well as the decrease rates  with each additional rater in Figure 3. The absolute difference values demonstrated a gradually decreasing trend as the sample size increased. Each additional rater substantially reduced the absolute differences between sample scores and “majority scores”, until reaching a saturation point of around nine raters. Although there were fluctuations in absolute differences with 10-20 raters, on average changes after this point were small.
The last two criteria were related to the likelihood of wrong decisions on sample labels compared to the “majority” labels. As shown in Figure 4, estimates of “any mistakes” decreased in general as the number of raters increased. However, even numbers of raters, relative to odd numbers of raters, tended to produce higher percentages of “any mistakes”. The chances of having more than one mode (i.e., most frequently selected valence category) are usually higher with an even number of raters, thus making it more difficult to have a single dominant category (≥50% according to our definition). In contrast, odd numbers of raters are less likely to have tied ratings. Therefore, to achieve consensus, odd numbers of raters are preferable given our method of determining the “majority” text labels.
As shown in Figure 5, the estimated average percentages of “big mistakes” were relatively low across all sample sizes, ranging from 0% to 4%. The biggest jump was observed with five raters, indicating that with at least five raters, the percentage of extreme misspecification occurring (i.e., choosing an opposite valence label) could be effectively lowered to 1% or less.
Finally, the results from all four criteria were considered together to determine the optimal number of raters for this task. Criterion #1 indicated that the correlation with the “majority score” reached a saturation point at around seven raters; criterion #2 found the saturation point to be around nine raters; criterion #3 suggested that an odd number of raters should be considered; and criterion #4 recommended that at least five raters are needed to minimize the likelihood of “big mistakes”. Therefore, based on the three principles laid out earlier, we determined that nine is the minimum number that 1) does not violate any of the four criteria, 2) is the largest minimum number among the requirements of each criterion, and 3) represents the smallest rater sample size possible (i.e., any odd number that is larger than nine can produce estimates close enough to the “majority score” but is not as cost-effective).
Conclusions and Discussion
Summary of Results
The present study found that rating from raters’ own perspectives (Condition 2: Subjective-own opinion) resulted in both the highest estimate of Krippendorff’s alpha (.77) and the most consistent 3-level consistency score compared to rating from an objective perspective (Condition 1: Objective-text). Taken together, in the context of rating the valence towards e-cigarettes with the objective to predict public response from ratings, we recommend instructing raters to rate from their own perspective to elicit higher agreement across raters. The minimum number of raters needed to replicate the proportion of raters who agreed on the “majority” valence label of e-cigarette texts from the full rater sample (from Condition 2) and to avoid making mistakes on judgments of text valence was nine. The procedures and methods used to determine appropriate instructions and number of raters can be applied to future studies in different contexts and domains of health communication.
Implications
Most surprisingly, instructing raters to rate the valence of e-cigarette media texts from their own perspective yielded the most homogenous ratings particularly in comparison to an instruction frequently used in previous research where raters were asked to rate with an objective perspective. While the study results cannot explain why this was so, one can speculate about potential mechanisms. Rating from raters’ own understanding may have relieved them of the burden of carrying out a somewhat ambiguous task when rating complex, latent content. For example, asking raters “Is the text pro-, anti- or NA?” (Condition 1 Objective-text) could have increased raters’ cognitive burden by implying that there is an objectively right or wrong answer. Rating from another group’s perspective (i.e., Condition 4 Subjective-Naïve people and Condition 3 Subjective-Most people) could have been difficult for raters because they needed to think from other people’s perspectives. These cognitive burdens may have led to less agreement in those conditions, as well as potentially biased ratings depending on raters’ assumptions about naïve people and most people’s attitudes towards e-cigarettes. Of note, appropriate instructions may differ depending on the nature of the content analysis task and the health topic of interest. For example, an objective perspective may yield more homogenous ratings when rating more manifest content such as well-defined themes (e.g., mentions about youth substance use). Or health topics with social connotations (e.g., stigmatized media representations of obesity) may benefit from instructing participants to rate with most people in mind. These of course are empirical questions that can be addressed with methods utilized in the current study.
Regarding the issue of determining the minimum number of crowdsourced raters needed, it is important to note that there is no single metric for making this choice. Decisions about how many raters are needed will be heavily dependent on the ambiguity of the health communication context, the rule for deciding upon the “majority” label of texts, and the criteria considered. As illustrated in this study, a single criterion does not suffice to make the most informed decision. We chose the optimal number of raters by taking into consideration two dimensions: close reproduction of the original data and minimum likelihood of rating errors. It is worth noting that our two new criteria, “any mistakes” and “big mistakes”, led us to choose an odd number of raters, no smaller than five, in order to maximally avoid misspecification of valence labels, particularly the opposite ones. Minimizing errors was especially important in this study because these ratings will be used for media effects analyses where accurately assessing the direction of valence is crucial for predicting population-level effects. Thus, we recommend that future studies carefully evaluate or perhaps modify the metrics used here based on their rating goals and specific health behavior contexts, to ensure that they serve their rating purpose.
The study has some limitations. When examining the effect of different instructions on levels of rater agreement, confidence intervals for the four Krippendorff’s alphas overlapped and the highest estimate was .77. Considering that it is often more difficult to achieve high reliability for non-expert ratings of a complex latent construct, .77 may be adequate, but not as high as desired. One reason for lower reliability could be that rating the valence of texts about e-cigarettes, may have been difficult for raters since e-cigarettes are relatively new products. Thus, the context of the health topic should be considered when interpreting reliability data.
While the bootstrapping methods employed in the current study provide a helpful methodological framework for future efforts to determine empirically an optimal rater sample size, it is important to point out two caveats. First, the performance of bootstrapped samples is always bounded by the performance of the full sample with which researchers start. Namely, determining an optimal rater size can be achieved in a relatively rather than absolutely. Therefore, it is important that ratings of the full sample are of the highest quality possible. Second, while the use of multiple criteria can help make informed decisions by accounting for various dimensions of accuracy, it is at the researchers’ discretion to decide which dimensions to include, and to construct rules and specify cutoff points for each criterion. Researcher judgment also weighs heavily in how to inspect and interpret resampling results when formal statistical tests are not viable. Therefore, it is important to fully acknowledge that subjective decisions may exist in this process, and reasonable justifications for each step are necessary.
Finally, the current study used a sample of Mturk raters. While there is research suggesting that similar study conclusions can be drawn from Mturk populations compared to larger probability samples or in-lab participants, if crowdworkers are generally homogenous on characteristics that might affect their judgment (e.g., education), there is a risk that the estimates will not represent how the texts will be read by the population as a whole. Thus, future studies may benefit from striving to acquire a more diverse crowdworker sample to avoid bias.
Despite these limitations, this study is important because it discusses standards of reliability and efficiency when using crowdsourced ratings in the context of assessing valence perceptions of e-cigarette media texts and presents methods and procedures for evaluating these standards. We expect the results will inform more reliable and efficient future research using crowdsourced ratings in health communication.
The qualification task included 6 texts (e-cig irrelevant [n=1], pro-ecig [n=2], anti-ecig [n=2], or not applicable [n=1]). We qualified raters based on whether they correctly answered valence or relevance for the 5 texts that were clear in valence/relevance (i.e., e-cig irrelevant, pro-ecig and anti-ecig texts).
The proportion of qualified participants was 49% on average and did not differ by condition (χ2 (3) = 2.93, p = .40).
To identify e-cigarette related texts, we compiled a list of e-cigarette keyword rules, which were developed through expert consensus and an iterative process of adding and dropping keywords with a training set of broadly tobacco-related texts based on precision and recall. Final precision for e-cigarette texts was >.97 for all sources and recall was 1.0. The final set of keywords including e(−)cig, e(−)cigarette, electronic cigarette, e(−)hookah, hookah pen, vape, vaping, blu, njoy, ego, etc., were used to retrieve e-cigarette related articles and determine whether an article was mostly about e-cigarettes. The three mentions cutoff demonstrated precision of .85 and recall of .80 in distinguishing between more than passing and passing mention texts across different types of media.
We acknowledge that raters can rate smaller units of text such as sentences or paragraphs. The present study however focuses on the entire text as the unit of analysis because 1) it allows raters to rate in a more naturalistic setting (i.e., readers are expected to understand entire texts as a whole vs. interpreting disconnected chunks of text individually); and 2) because it allows assessment of the general impression of the text which dovetails with the overall project goal to predict the public’s intentions from analyzed content.
Determining the within-condition consensus for each text followed the rule used to determine the “majority valence” label of texts described next in the analysis of number of raters needed.
When the consensus for a certain text within a certain condition was NA, for those who rated the text as either pro-ecig or anti-ecig, the proportion of raters choosing either pro-ecig or anti-ecig was used to judge the consistency score for these raters. For example, if the proportion of those choosing pro-ecig was larger than anti-ecig, those who rated the text as pro-ecig received a score of 2 while those who rated it as anti-ecig were assigned a score of 1.
References
Campaigns and counter campaigns: Reactions on Twitter to e-cigarette education
Crowd-sourced text analysis: Reproducible and agile production of political data
Evaluating online labor markets for experimental research: Amazon.com’s Mechanical Turk
2 x 2 Kappa Coefficients: Measures of Agreement or Association
Fair and balanced?: Quantifying media bias through crowdsourced content analysis
Amazon’s Mechanical Turk: A new source of inexpensive, yet high-quality, data?
Separate but equal? A comparison of participants and data gathered via Amazon’s MTurk, social media, and face-to-face behavioral testing
Nonnaïveté among Amazon Mechanical Turk workers: Consequences and solutions for behavioral researchers
Assessing electronic cigarette-related tweets for sentiment and content using supervised machine learning
Notes from the field: Use of electronic cigarettes and any tobacco product among middle and high school students — United States, 2011–2018
Tobacco in the news: An analysis of newspaper coverage of tobacco issues in australia, 2001
Wanna know about vaping? Patterns of message exposure, seeking and sharing information about e-cigarettes across media platforms
A randomized trial of the effect of e-cigarette TV advertisements on intentions to use e-cigarettes
Sentiment analysis of political communication: Combining a dictionary approach with crowdcoding
Answering the call for a standard reliability measure for coding data
A method of automated nonparametric content analysis for social science
Can the online crowd match real expert judgments? How task complexity and coder location affect the validity of crowd-coded data
The rise of crowdsourcing
How Many Raters Should be Used for Establishing Cutoff Scores with the Angoff Method? A Generalizability Theory Study
An efficient message evaluation protocol: Two empirical analyses on positional effects and optimal sample size
Crowdsourced data collection for public health: A comparison with nationally representative, population tobacco use data
Issues and best practices in content analysis
Potential deaths averted in USA by replacing cigarettes with e-cigarettes
Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs
How many raters are needed for a reliable diagnosis?
Reduced harm or another gateway to smoking? Source, message, and information characteristics of e-cigarette videos on youtube
Inside the turk: Understanding Mechanical Turk as a participant pool
Overlapping confidence intervals or standard error intervals: What do they mean in terms of statistical significance?
Effects of advertisements on smokers’ interest in trying e-cigarettes: The roles of product comparison and visual cues
Rethinking validity and reliability in content analysis
Crowdsourcing research: Data collection with Amazon’s Mechanical Turk
Exposure to advertisements and electronic cigarette use among US middle and high school students
Still a burning issue: Trends in the volume, content and population reach of newspaper coverage about tobacco issues
How to think – not feel – about tobacco harm reduction
Does seeking e-cigarette information lead to vaping? Evidence from a national longitudinal survey of youth and young adults
A content analysis of electronic cigarette portrayal in newspapers
Four metrics for comparing results from bootstrap samples with the original “majority scores” (maj. score): (A) Pearson correlation with “sample scores”; (B) Absolute difference of “sample score” from “majority scores”; (C) Proportion of “Any mistakes”; (D) Proportion of “Big mistakes”.
Pearson correlations between “majority score” and “sample score” at each rater sample size and growth rates at each increment of sample size. Each estimate is calculated from 500 bootstrap samples.
Absolute differences between “majority score” and “sample score” at each rater sample size and decrease rates at each increment of sample size. Each estimate is calculated from 500 bootstrap samples.
Average percentage of making “any mistakes” at each rater sample size. Each estimate is calculated from 500 bootstrap samples.
Average percentage of making “big mistakes” at each rater sample size. Each estimate is calculated from 500 bootstrap samples.
Definitions of Valence Labels (Pro-ecig, Anti-ecig, and Not applicable) and Rating Questions for Four Conditions
Condition	Definitions of Valence Labels	Rating Question	 	Condition 1: Objective-Text	Pro-ecig: The text is mostly supportive of e-cigarette use OR supportive of the e-cigarette industry – as a whole, it encourages using e-cigarettes.	For the content that is ecig-relevant, is it mostly anti-ecig, pro-ecig or not applicable?	 	Anti-ecig: The text is mostly against e-cigarette use OR against the e-cigarette industry – as a whole, it discourages using e-cigarettes.	 	Not applicable: The text does not have enough information to judge if it is mostly pro-ecig or mostly anti-ecig.	 	Condition 2: Subjective-Own opinion	Pro-ecig: You understand the text to be mostly supportive of e-cigarette use OR supportive of the e-cigarette industry – as a whole, it encourages using e-cigarettes.	For the content that is ecig-relevant, would you understand it to be mostly anti-ecig, pro-ecig or not applicable?	 	Anti-ecig: You understand the text to be mostly against e-cigarette use OR against the e-cigarette industry – as a whole, it discourages using e-cigarettes.	 	Not applicable: You understand the text to not have enough information to judge if it is mostly pro-ecig or mostly anti-ecig.	 	Condition 3: Subjective-Most people	Pro-ecig: Most people would understand the text to be mostly supportive of e-cigarette use OR supportive of the e-cigarette industry – they would see it as encouraging the use of e-cigarettes.	For the content that is ecig-relevant, would most people understand it to be mostly anti-ecig, pro-ecig or not applicable?	 	Anti-ecig: Most people would understand the text to be mostly against e-cigarette use OR against the e-cigarette industry – they would see it as discouraging the use of e-cigarettes.	 	Not applicable: Most people would understand the text to not have enough information to judge if it is mostly pro-ecig or mostly anti-ecig.	 	Condition 4: Subjective-Naïve people	Pro-ecig: Someone who only knows a little about e-cigarettes would understand the text to be mostly against e-cigarette use OR against the e-cigarette industry – they would see it as discouraging the use of e-cigarettes.	For the content that is ecig-relevant, would someone who only knows a little about e-cigarettes understand it to be mostly anti-ecig, pro-ecig or not applicable?	 	Anti-ecig: Someone who only knows a little about e-cigarettes would understand the text to be mostly supportive of e-cigarette use OR supportive of the e-cigarette industry – they would see it as encouraging the use of e-cigarettes.	 	Not applicable: Someone who only knows a little about e-cigarettes would understand the text to not have enough information to judge if it is mostly pro-ecig or mostly anti-ecig.
Note. The “Not applicable” label could include judgments that texts do not have any valence (i.e., neutral), but also those that had mixed valence (having both pro- and anti-ecig content) since there is not enough information to judge whether texts were mostly pro- or anti-ecig.
Krippendorff’s Alphas (95% and 83% Confidence Intervals) by Condition
Condition	Krippendorff’s Alpha	[95% CI]	[83% CI]	 	Condition 1: Objective-Text	0.56	[0.34, 0.77]	[0.41, 0.70]	 	Condition 2: Subjective-Own opinion	0.77	[0.60, 0.93]	[0.66, 0.90]	 	Condition 3: Subjective-Most people	0.67	[0.47, 0.84]	[0.54, 0.80]	 	Condition 4: Subjective-Naïve people	0.52	[0.22, 0.78]	[0.31, 0.70]
Note. N = 20 raters of 34 texts per condition.
Multilevel Regression Model Predicting Consistency Scores from Study Conditions
Fixed effects variables	B (SE)	 	Condition 1: Objective-Text	Reference	 	Condition 2: Subjective-Own opinion	.11** (.04)	 	Condition 3: Subjective-Most people	.11** (.04)	 	Condition 4: Subjective-Naïve people	.06 (.04)	 	Intercept	2.58 (.06)	 		 	Random effects variables		 		 	Texts	.30* (.04)	 	Raters	.11* (.02)
Note. N = 80 raters of 34 texts.
p < .01;
p < .05
Bonferroni Pair-wise Comparisons of Consistency Scores across Conditions
Pairs	Contrast [95% CI]	 	Condition 2 vs. Condition 1	.11 [.02, .20]	 	Condition 3 vs. Condition 1	.11 [.02, .21]	 	Condition 4 vs. Condition 1	.06 [−.04, .15]	 	Condition 3 vs. Condition 2	.00 [−.09, .10]	 	Condition 4 vs. Condition 2	−.05 [−.15, .04]	 	Condition 4 vs. Condition 3	−.06 [−.15, .04]
Note. N = 20 raters of 34 texts per condition. Condition 1 = Objective-Text; Condition 2 = Subjective-Own opinion; Condition 3 = Subjective-Most people; Condition 4 = Subjective-Naïve people.
