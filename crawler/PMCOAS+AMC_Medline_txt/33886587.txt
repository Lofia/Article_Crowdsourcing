Crowdsourcing airway annotations in chest computed tomography images
Measuring airways in chest computed tomography (CT) scans is important for characterizing diseases such as cystic fibrosis, yet very time-consuming to perform manually. Machine learning algorithms offer an alternative, but need large sets of annotated scans for good performance. We investigate whether crowdsourcing can be used to gather airway annotations. We generate image slices at known locations of airways in 24 subjects and request the crowd workers to outline the airway lumen and airway wall. After combining multiple crowd workers, we compare the measurements to those made by the experts in the original scans. Similar to our preliminary study, a large portion of the annotations were excluded, possibly due to workers misunderstanding the instructions. After excluding such annotations, moderate to strong correlations with the expert can be observed, although these correlations are slightly lower than inter-expert correlations. Furthermore, the results across subjects in this study are quite variable. Although the crowd has potential in annotating airways, further development is needed for it to be robust enough for gathering annotations in practice. For reproducibility, data and code are available online: http://github.com/adriapr/crowdairway.git.
Introduction
Chest computed tomography (CT) can be used to quantify structural abnormalities in the lungs, such as bronchiectasis, air trapping and emphysema, which in turn can be used for diagnostic or prognostic purposes. For example, the airway-to-artery ratio (AAR) is an objective measurement of bronchiectasis which is sensitive to detect early lung disease. Other promising measurements are the wall-area percentage (WAP) and the wall thickness ratio (WTR) which characterize the ratio of the airway wall to the airway lumen. Unfortunately, manual measurements of the airways and vessels suffer from intra- and inter-observer variation and are time-consuming (8-16 hours per chest CT). Machine learning techniques such as can be an alternative, but may require a large amount of annotated data to be able to generalize to all situations.
In various applications, crowdsourcing has been proposed as an alternative for tasks where annotated data is scarce. Crowdsourcing refers to outsourcing tasks (often referred to as human intelligence tasks or HITs) to a group of online users (often referred to as knowledge workers or KWs). This strategy has also been quite effective in medical image analysis— surveys over 50 papers where results have been mostly positive. One of these papers is our earlier study where we described our experiences with crowdsourcing airway measurements. We found that 67.8% of the collected results were not valid, i.e. the airway measurements could not be extracted. However, after filtering out such results, strong correlations between the crowd and expert were observed. Although these experiences were encouraging, they only concerned a single chest CT image, and it was unclear whether they could be generalized to other scans.
Does the crowd create valid results?
What is the quality of the crowd compared to a trained expert, after combining different results per task?
Can we predict the quality of the crowd results, given a particular scan?
In this paper we describe crowdsourced airway measurements collected shortly thereafter for a larger set of 24 chest CT images, and with a slightly updated crowdsourcing procedure. With this follow-up study we aim to answer the following questions:
Materials and methods
Chest CT scans
We used inspiratory pediatric CT scans from a cohort of 24 subjects, collected at the Erasmus MC—Sophia Children’s Hospital. These scans have been collected and anonymized for previous studies, and approved to be used for further research. The anonymized scans were shared with us, including the age and sex of the subject, whether the subject had cystic fibrosis, and several measures related to lung capacity and airway size.
The voxel size was 0.5508 × 0.5508 × 0.6000 mm. Each scan contained a number of airways of different generations from the 2nd to the 14th. The most common generations were the 6th (23.7%), 7th (20.0%), 5th (17.7%) and 8th (13.9%).
In each scan, a number of airways were annotated by an expert. The expert localized an airway, outlined the airway lumen (inner airway boundary) and airway wall (outer airway boundary) in a plane approximately perpendicular to the airway center line, and recorded the measurements of the areas.
Generating airway images
Fig 1 shows a global overview of our method. The first step is to create a crowdsourcing task for each airway, which requires extracting 2D image slices from a 3D volume. This requires having a 3D location and orientation of the airway. Normally this localization would be done by the expert, however in this study we assume that localization was already done, and focus only on outlining the airway in the image.
Overview of the method.
A 3D image is annotated by experts. The locations and orientations of the airways are then used to generate 2D slices of the airways, which are then annotated by the workers.
More specifically, we used 3D voxel coordinates, at which experts have previously outlined airways using the Myrian™ software. We generated 2D slices of 50 × 50 voxels.
The slides were reviewed by one observer (APR) to retain only the images with a visible airway that was cut approximately perpendicularly. There were 1026 such images, which are further analysed here. We used cubic interpolation and an intensity range between -950 and 550 Hounsfield units for better contrast, as recommended by the experts. Each image slice was rescaled to 500 × 500 pixels for annotation purposes.
Annotating airway images
Each of the generated airway images is a crowdsourcing task. A worker assigned to a task creates a result, consisting of one or more annotations (outlines) placed in the image.
To gather these results, we used Amazon Mechanical Turk. All decisions regarding Amazon Mechanical Turk (MTurk) were based on consultation with colleagues who had used MTurk in the past. Apart from updating the instructions to workers, we used the same settings as in our preliminary study, which we repeat here for completeness. All results were collected in 2016.
The annotation interface was integrated into the platform by supplying a dynamic webpage, built with HTML5 and Javascript. This custom-made interface had an ellipse tool, which resembled the tool used by the experts more closely than the default annotation tools available on MTurk. The details of our HIT, which the workers could see when searching for HITs, are shown in Table 1. The workers were instructed to draw two ellipses outlining the airway lumen and the airway wall, or to place a small circle in the top right corner of the image, if no airway is visible. Following our experiences in the preliminary experiments, we revised our instructions, placing more emphasis on the need to draw two ellipses. A screenshot is shown in Fig 2.
First set of instructions provided to the workers.
One more set of instructions was available, but this was the set that all workers would see.
Details of HIT on Amazon Mechanical Turk.
Parameter	Value	 	Title	Save lives by annotating airways!	 	Description	Draw two contours to annotate an airway (dark circle or ellipse) in image from a lung scan	 	Keywords	image, annotation, contour, draw, drawing, segmentation, medical
We randomly created HITs with 10 images per HIT. A worker could request a HIT, annotate 10 images, and then submit the HIT. The workers were paid $0.10 per completed HIT. Only workers who had previously done at least 100 HITs with an acceptance rate of 90% could request the HITs. We collected 20 results per image, because with 10 results per image as in the preliminary experiment, some images did not have valid annotations.
The data collection was done in 2016, shortly after our preliminary study. Workers consented to doing the task when accepting it on Amazon MT. Due to the non-graphic nature and anonymous character of the airway slices, no other specific approval was obtained. For each result, we recorded an anonymized ID of the worker and the coordinates of the annotations. No other information about the workers was recorded.
Measuring crowd annotations
number of ellipses not equal to 2
not resized ellipses (default size of the tool is a circle)
not overlapping ellipses
We applied a simple filtering step to filter out invalid results. The following results were excluded:
After filtering, we measured the areas of the inner (ai) and outer (ao) ellipse, and calculated the wall thickness ratio (WTR) and wall area percentage (WAP). The WTR is the wall thickness divided by the outer diameter:  where do is the diameter of the outer ellipse and di is the diameter of the inner ellipse, and the wall thickness WT is defined as:
The WAP is the percentage of the total airway area that is airway wall:
Note that both the workers and the experts place ellipses in slices with roughly perpendicular airways, resulting in almost-circular ellipses. For the workers we have both the major and minor diameters available, but the experts only record the area. For comparisons between the two, we therefore had to assume circular airways.
Quality of crowd measurements
Taking the inner/outer areas of all valid results, and combining them with the median function. WAP and WTR are then calculated based on these median values. This is the strategy used in our preliminary study.
Selecting a random valid result per task. This gives an indication of how good the crowd could be, if each task was assigned to only one worker, and gives a pessimistically biased indication of how good the crowd could be.
Taking the valid result that is closest to the expert measurement, based on the inner and outer measurements. This is an optimistically biased indication of how good the crowd could be, if we only selected the best workers.
Before measuring how good the crowd is on each task, we need to combine the results per task. We used three different strategies for this:
Additionally, we can choose to exclude tasks that have less than v valid results. This will reduce the number of tasks for which a combined result is available, but will presumably increase the quality of the result.
After combining the results per task, we use the Pearson’s correlation coefficient, ρ, between the crowd measurement and the expert measurement. Correlation coefficients are interpreted as follows: weak correlation for 0 ≤ ρ < 0.3, moderate correlation for 0.3 ≤ ρ < 0.5, strong correlation for 0.5 ≤ ρ < 1. Note that, if a task has had no valid results, it will be excluded from the analysis.
Predicting crowd quality
Lastly, we investigate whether any factors contribute to the crowd’s performance across different scans in our data. We use the inner airway after median combining as a proxy for the quality.
Whether or not the subject has cystic fibrosis (CF)
Forced expiration volume in 1 second (FEV1), which measures how much air a participant can exhale in 1 second.
Forced vital capacity (FVC), which measures the total volume of air a participant can exhale.
Number of airways as indicated by the expert.
Average airway generation, which indicates the number of bifurcations between the current branch and the trachea. Higher generations correspond to smaller airways and vice versa.
We then look at the relationship between the quality and the following characteristics:
We use the Spearman correlation to investigate the relationship of these characteristics, because we cannot assume a linear relationship between them (in particular, the CF status variable is binary). We report the correlation coefficient and the p-value from a two-sided hypothesis test, where the null hypothesis is that the characteristics are not correlated. We use a significance threshold of 0.05. Since we perform five comparisons in total, after adjusting for multiple comparisons the threshold becomes 0.01.
Results
Validity of crowdsourced annotations
In total we collected 20520 results for 1026 tasks. A few typical examples are shown in Fig 3. Of these 11742 results (57.2%) were classified as invalid, and 624 (3.0%) contained multiple pairs of ellipses per image, which we excluded to simplify the analysis.
Example results acquired for the same task: Valid result with two annotations, and two invalid results: A worker who indicates not seeing an airway, and a worker who detects the airway but does not outline it.
Of the 11742 invalid results, 8809 tasks only had one annotation. This could indicate not seeing an airway, which was the case for 2641 of the results. A further 2933 results had signs of the worker trying to annotate the image (placing ellipses on top of airways), but not following the instructions of outlining two ellipses.
We visually examined several cases where all or most workers indicated not seeing an airway. These appeared to be difficult cases due to low contrast and/or only part of the airway wall being visible (but not so much the size of the airway). While a trained observer would identify these as airways, it is not unexpected that the workers were not able to do so.
The results were created by 577 workers in total, who made as little as 1 or as many as 2313 results. Similar to the observations in, most workers only created a few results, and a few workers were responsible for a lot of the results, as shown in Fig 4(a). Fig 4(b) shows the number of valid and invalid results made by each worker. Overall there is a tendency for workers to create more valid than invalid results. However, there are a few workers who have created a lot of results overall, and who tend to create more invalid results. They contribute to 57.2% of the invalid results. Finally, there are no workers that created only invalid results.
(a) Cumulative results made by the workers. Many workers contribute a few results, and a few workers contribute the most results. (b) Valid results vs invalid results by each worker. The blue line shows the fit to the data, and that workers create tend to create more valid results in general.
Quality of airway measurements
When considering each result independently (without combining the results per task), there is a correlation of 0.803 for the inner airway and 0.697 for the outer airway.
Additionally, we found moderate correlations for the ratio based measures, 0.426 for the WAP and 0.424 for the WTR.
The airway measurements and correlations after combining the results across workers are shown in Table 2, as well as Figs 5–8. Combining improves all correlations, and for the ratios the correlations can be categorized as strong for median and “best” combining. “Best” combining gives the highest correlations, although the difference with median combining is rather small for the inner airway, WAP and WTR. For the outer airway, the difference is more pronounced (0.769 vs 0.896), suggesting that the task is more difficult, leading to more variation in the crowd.
Measurements of the inner airway, comparing expert 1 (x-axis) and to three combining methods and expert 2 (y-axis).
Measurements of the outer airway, comparing expert 1 (x-axis) and to three combining methods and expert 2 (y-axis).
Measurements of the WTR, comparing expert 1 (x-axis) and to three combining methods and expert 2 (y-axis).
Measurements of the WAP, comparing expert 1 (x-axis) and to three combining methods and expert 2 (y-axis).
Pearson correlations between the expert and the crowd with different combining methods, and between two experts.
Method	vs Expert 1	vs Expert 2	 		inner	outer	wap	wtr	inner	outer	wap	wtr	 	None	0.803	0.697	0.426	0.424	0.808	0.686	0.469	0.466	 	Random	0.803	0.701	0.421	0.418	0.813	0.679	0.489	0.481	 	Median	0.844	0.769	0.572	0.565	0.850	0.746	0.661	0.649	 	Best	0.858	0.896	0.585	0.590	0.858	0.842	0.583	0.570	 	Expert 1					0.964	0.925	0.701	0.687
Overall, since the “best” combining method is optimistically biased due to access to ground truth, our results suggest median combining is a good choice for this data.
Median combining simply combines all (between 1 and 20) the valid results available for a particular task. To understand how the number of valid results affects the correlations, we investigated combining only for tasks where at least a certain number of valid results must be available.
The correlations are shown in Fig 9. There is almost no effect on the correlations for the inner and outer airway, and the correlations for WAP and WTR steadily improve as more valid results are combined. This could also indicate that the tasks with more valid results, are in general easier images to annotate.
Correlation of all four measures between the expert and crowd (median combining), for tasks with at least a specific number of valid results.
To summarize, the crowd can create good annotations, and combining annotations using the median helps to improve the quality, although not to the quality of the expert. For median combining strong correlations for the inner and outer airways (0.844, 0.769), but moderate to strong correlations for the ratios are observed (0.572, 0.565). It is important to note that a similar trend is noticeable in the expert-to-expert correlations: the correlations for the airway dimensions are much higher (0.964, 0.925) than correlations of WAP and WTR (0.701, 0.687).
Predicting crowdsourcing quality
Next, we look at the correlations per subject, and whether this correlation can be predicted based on subject characteristics. The individual subject characteristics and correlations (between expert and median combining) are shown in Table 3. Overall we can see high variability across subjects. Correlations range between 0.64 and 1.00 for inner airway, 0.60 and 0.98 for outer airway, 0.07 and 0.75 for WAP, and 0.14 and 0.68 for WTR. Note that these correlations are based on smaller (and different) numbers of tasks (column “n”).
Characteristics of the subject: ID (not used in modeling), whether a subject has CF (1 = yes), FVC1, FEV (as percentage of predicted value), number of airways (n), and correlations between the crowd (median combining) and the expert.
Horizontal lines inserted for legibility.
Subject characteristics	Correlation crowd-expert	 	Index	CF	FVC1	FEV	n	inner	outer	WAP	WTR	 	0	1	109.20	119.20	77	0.93	0.97	0.52	0.42	 	1	0	111.50	95.40	89	0.93	0.98	0.33	0.36	 	2	1	112.20	99.00	60	0.64	0.92	0.07	0.14	 	3	0	82.70	78.60	168	0.78	0.94	0.35	0.36	 	4	0	110.00	105.10	73	0.90	0.90	0.61	0.59	 	5	1	118.60	94.50	68	0.76	0.71	0.75	0.75	 	6	0	95.60	85.40	173	0.90	0.87	0.45	0.43	 	7	1	94.20	73.40	82	0.91	0.95	0.45	0.43	 	8	0	99.40	73.20	143	0.75	0.97	0.39	0.37	 	9	0	120.30	123.80	122	0.93	0.97	0.60	0.61	 	10	1	70.60	75.30	75	0.82	0.86	0.22	0.21	 	11	0	104.20	100.10	134	0.77	0.90	0.50	0.51	 	12	1	73.40	56.10	47	0.91	0.91	0.51	0.44	 	13	1	70.60	78.10	18	0.65	0.60	0.58	0.59	 	14	0	98.90	95.90	145	0.93	0.92	0.56	0.56	 	15	0	73.40	66.70	278	0.87	0.93	0.43	0.39	 	16	1	128.60	80.60	66	0.81	0.98	0.42	0.45	 	17	1	91.90	79.10	40	0.86	0.98	0.37	0.35	 	18	1	96.90	91.10	27	1.00	0.89	0.48	0.53	 	19	1	109.00	105.80	104	0.94	0.98	0.67	0.68	 	20	0	110.10	104.10	32	0.88	0.98	0.54	0.55	 	21	1	64.30	69.60	64	0.87	0.94	0.24	0.26	 	22	0	109.60	82.00	151	0.86	0.93	0.42	0.44	 	23	0	77.00	71.20	144	0.91	0.94	0.63	0.61
Lastly we looked at the relationship between the quality of the crowd (here represented by the inner airway correlation) and five subject characteristics. The Spearman correlations and corresponding p-values are shown in Table 4. There is a weak negative correlation between the subject having CF and the crowd quality, however this correlation is not significant for the adjusted alpha level of 0.01. The other characteristics show almost no correlation with the crowd quality.
Spearman correlation between the crowd quality (measured by the crowd-expert correlation of the inner airway) and five subject characteristics.
Characteristic	Correlation	p-value	 	Has CF	-0.265	0.211	 	Generation	0.003	0.987	 	FVC	-0.038	0.859	 	FEV1	0.090	0.677	 	Number of airways:	-0.038	0.861
These results suggest that other factors, not investigated here, are more important. We suspect that these factors are related to the difficulty of individual tasks (for example depending on size, shape, and contrast of the airway and its proximity to vessels or other structures), and/or assignment of workers to different tasks.
Discussion
This paper describes a follow-up study of. In that study we concluded that workers try to annotate airways in the images but often do not create valid results. After filtering out the invalid results, the correlations between the crowd and the expert were 0.69 for the inner and 0.75 for the outer airway, and could be further improved by combining the results. As follow-up steps, we revised our instructions to the crowd, increased the number of workers from 10 to 20 per slice, and collected annotations for all 24 subjects in the cohort.
The current study (increased from 1 to 24 subjects) shows that despite revising the instructions, the number of invalid annotations is still high (57.2%). This can happen when a worker does not see an airway, sees an airway but annotates it incorrectly, or due to spam (workers who submit random results just to get the reward). Our analysis shows that most workers create both valid and invalid results. An improvement to increase the number of valid results would be to perform checks (such as requiring one ellipse to be inside the other) inside the annotation interface.
After removing the invalid results, we examined the correlations between the crowd and the expert. Without combining, the correlations were strong for the inner and outer airway, and moderate for WAP and WTR. Combining results across tasks improved correlations, so that all four measures had strong correlations. However, all correlations were still lower than correlations between two experts (see Table 2).
We used simple combining methods to combine the results of the crowd. Here many alternatives are possible, such as weighting the workers by their estimated quality. Instead we tried to estimate an “upper limit” for the crowd with a method which selected the best available result for each task. This method did indeed lead to the highest correlations, but simple median combining was a close second. We conclude that median combining is suitable for this data.
Overall we conclude that the crowd is capable of producing good-quality, but not expert-quality, results. As such, in its current form the proposed method is not robust enough for gathering measurements “in the wild”. In our experience this is primarily due to the difficulty of converting a clinical problem into a crowdsourcing problem, such as figuring out how to display parts of a 3D image in a 2D interface, explaining the task to the workers, and dealing with the constraints of the crowdsourcing platform.
Our study was inspired by the lack of annotated datasets for machine learning, however, we focused on evaluating the quality of the annotations alone. There are indications that lower quality labels may still be useful for training machine learning algorithms. For example, combinations of expert and crowdsourced labels have shown to be more effective in cases where the crowdsourced labels alone were not sufficient. More generally, weakly-supervised learning with approximate annotations such as bounding boxes shows that labels do not need to be precise to add value during training.
There are a number of important lessons from this study, which could be valuable for other researchers doing similar studies. Firstly, our interface was custom built by a crowdsourcing start-up, in the context of a pilot for academic groups. This allowed us to use an ellipse tool that is similar to the tool used by the experts. A disadvantage of this approach is that we could not easily access the interface after the pilot ended, and thus would not be able to collect additional data.
Secondly, although we did a test run of the task (collecting results described in), we did not gather feedback from the workers about their experiences. This is possible through various online groups such as https://www.turkernation.com, and could have reduced possible misunderstandings of the instructions. Furthermore, we used crowd qualifications (such as acceptance rate) and rewards that are outdated by today’s standards, so we would recommend other researchers to consult the latest crowdsourcing literature before setting up such a study.
For reproducibility of the results and any follow-up analyses, we made the airway images, crowd results and our code available via http://github.com/adriapr/crowdairway.git.
Conclusions
We conducted a follow-up study of crowdsourcing airway measurements in slices of chest CT scans. In a previous, preliminary study we examined airways of one subject and found moderate to strong correlations with the expert, motivating this follow-up study. Overall we observed similar or better results, with strong correlations for the inner and outer airway dimension measurements, and moderate to strong correlations for ratios of these structures. Combining results across different workers improved the correlations, but correlations were still lower than between two experts. We conclude that with appropriate processing, the crowd has potential in measuring airways, but that our method is not yet robust enough for use in practice.
References
Cystic fibrosis lung disease starts in the small airways: can we treat it more effectively?
Assessment of early bronchiectasis in young children with cystic fibrosis is dependent on lung volume
Automatic airway-artery analysis on lung CT to quantify airway wall thickening and bronchiectasis
Assessment of CF lung disease using motion corrected PROPELLER MRI: a comparison with CT
Vessel-guided airway tree segmentation: A voxel classification approach
Qin Y, Chen M, Zheng H, Gu Y, Shen M, Yang J, et al. AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer; 2019. p. 212–220.
Ørting S, Doyle A, van Hilten A, Hirth M, Inel O, Madan CR, et al. A survey of crowdsourcing in medical image analysis. arXiv preprint arXiv:190209159. 2019.
Cheplygina V, Perez-Rovira A, Kuo W, Tiddens H, de Bruijne M. Early experiences with crowdsourcing airway annotations in chest CT. In: Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS); 2016. p. 209–218.
Perez-Rovira A, Kuo W, Petersen J, Tiddens HA, de Bruijne M. Automated quantification of bronchiectasis, airway wall thickening and lumen tapering in chest CT. In: ECR 2015-European Congress of Radiology; 2015.
Kuo W, et al. Assessment of bronchiectasis in children with cystic fibrosis by comparing airway and artery dimensions to normal controls on inspiratory and expiratory spirometer guided chest computed tomography. European Congress of Radiology 2015; 2015.
Opportunities for crowdsourcing research on Amazon Mechanical Turk
Crowd science user contribution patterns and their implications
AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images
Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis
10.1371/journal.pone.0249580.r001
Decision Letter 0
Bagci
Ulas
This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
5 Feb 2021
PONE-D-20-37113
Crowdsourcing Airway Annotations in Chest Computed Tomography Images
PLOS ONE
Dear Dr. Cheplygina,
Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.
Please submit your revised manuscript by Mar 22 2021 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at plosone@plos.org. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.
Please include the following items when submitting your revised manuscript:
A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.
A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.
An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.
If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.
If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols
We look forward to receiving your revised manuscript.
Kind regards,
Ulas Bagci, Ph.D.
Academic Editor
PLOS ONE
Additional Editor Comments (if provided):
The paper has a considerably interest in the field, easy to follow, with some minor concerns raised by the reviewers.
Journal Requirements:
When submitting your revision, we need you to address these additional requirements.
1) Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at
https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and
https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf
2) Please provide additional details regarding participant consent. In the Methods section, please ensure that you have specified how consent was obtained and how the study met relevant personal data and privacy laws. If data were collected anonymously, please include this information.
3) Please include a copy of Tables 1 & 2 which you refer to in your text.
[Note: HTML markup is below. Please do not edit.]
Reviewers' comments:
Reviewer's Responses to Questions
Comments to the Author
1. Is the manuscript technically sound, and do the data support the conclusions?
The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.
Reviewer #1: Yes
Reviewer #2: Yes
**********
2. Has the statistical analysis been performed appropriately and rigorously?
Reviewer #1: Yes
Reviewer #2: Yes
**********
3. Have the authors made all data underlying the findings in their manuscript fully available?
The PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.
Reviewer #1: Yes
Reviewer #2: Yes
**********
4. Is the manuscript presented in an intelligible fashion and written in standard English?
PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.
Reviewer #1: Yes
Reviewer #2: Yes
**********
5. Review Comments to the Author
Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)
Reviewer #1: This paper presents an experiment of crowdsourcing for medical image annotation, specifically annotating airway and airway wall from 2D resampled CT scans. I would say this is a valid study and whether crowdsourcing can be used to for medical applications is an interesting topic. Overall the manuscript is clear and easy to follow.
Some details that need to be considered in revision:
- “is our earlier study [2] where we described our experiences with crowdsourcing airway measurements.” [2] does not seem to be the correct citation, guess it should be [9]?
- What is the voxel size of the CT images? What is the physical length of 50x50 voxels?
- The 1026 image patches, how are they distributed along the airway tree? Which generation of branch do they belong to? It will help to understand the approximate size of the airways, by comparing with the dimensions from the last question.
- Inner and outer diameter, since they are ellipses, which “diameter” is used? Major? Minor? Average of major and minor? or average of all?
- Just to confirm, considering the “not seeing an airway” cases, for the input image patches, airway is always visible right? Is there a case that everyone marks as “not seeing”?
- Table 2, Expert 1 and 2 seems to have fairly good agreement, still, it may be helpful to also give the coor number for three combination against Expert 2 (and maybe further against the average number of 1&2)
- Maybe out-of-scope for this paper, but it would be interesting to see how good a SOTA automated algorithm can achieve on the same samples.
Reviewer #2: The authors facilitated Amazon Mechanical Turk to crowdsource airway annotations from chest CT slices. Annotations from multiple workers were analyzed against the annotations provided by two experts. Almost half of the annotations from workers were identified as invalid due to insufficient instructions and/or issues in experimental design. These issues are discussed in the manuscript and the manuscript provides enough information to design a robust crowdsourcing platform for chest CT airway annotations.
The authors used crowdsourcing to annotate datasets to machine learning algorithms, as indicated in the abstract. However, experiments analyzing the effect of using annotated dataset generated by workers in ML algorithms were not performed or discussed. It is not clear if nonexpert annotated “good-quality” dataset can be of any use in ML model development or not. Please elucidate.
Page 7 2nd paragraph of the Discussion Section. Please add the “%” sign after 57.2
The manuscript is easy to follow, with issues on the Figure numbers which could be mitigated by using the Figure number as a filename.
**********
6. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.
If you choose “no”, your identity will remain anonymous but your review may still be made public.
Do you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.
Reviewer #1: No
Reviewer #2: No
[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]
While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at figures@plos.org. Please note that Supporting Information files do not need this step.
10.1371/journal.pone.0249580.r002
Author response to Decision Letter 0
12 Feb 2021
Thank you for taking the time to review our manuscript. Please view response_letter.pdf for our point by point response.
10.1371/journal.pone.0249580.r003
Decision Letter 1
Bagci
Ulas
This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
22 Mar 2021
Crowdsourcing Airway Annotations in Chest Computed Tomography Images
PONE-D-20-37113R1
Dear Dr. Cheplygina,
We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.
Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.
An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at http://www.editorialmanager.com/pone/, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at authorbilling@plos.org.
If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.
Kind regards,
Ulas Bagci, Ph.D.
Academic Editor
PLOS ONE
Additional Editor Comments (optional):
Authors successfully answered the comments.
Reviewers' comments:
Reviewer's Responses to Questions
Comments to the Author
1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.
Reviewer #1: All comments have been addressed
**********
2. Is the manuscript technically sound, and do the data support the conclusions?
The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.
Reviewer #1: Yes
**********
3. Has the statistical analysis been performed appropriately and rigorously?
Reviewer #1: Yes
**********
4. Have the authors made all data underlying the findings in their manuscript fully available?
The PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.
Reviewer #1: Yes
**********
5. Is the manuscript presented in an intelligible fashion and written in standard English?
PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.
Reviewer #1: Yes
**********
6. Review Comments to the Author
Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)
Reviewer #1: (No Response)
**********
7. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.
If you choose “no”, your identity will remain anonymous but your review may still be made public.
Do you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.
Reviewer #1: No
10.1371/journal.pone.0249580.r004
Acceptance letter
Bagci
Ulas
This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
31 Mar 2021
PONE-D-20-37113R1
Crowdsourcing Airway Annotations in Chest Computed Tomography Images
Dear Dr. Cheplygina:
I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department.
If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact onepress@plos.org.
If we can help with anything else, please email us at plosone@plos.org.
Thank you for submitting your work to PLOS ONE and supporting open access.
Kind regards,
PLOS ONE Editorial Office Staff
on behalf of
Dr. Ulas Bagci
Academic Editor
PLOS ONE
