<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210429</date><key>pmc.key</key><document><id>7301713</id><infon key="license">author_manuscript</infon><passage><infon key="article-id_doi">10.1109/tg.2018.2877325</infon><infon key="article-id_manuscript">NIHMS1048165</infon><infon key="article-id_pmc">7301713</infon><infon key="article-id_pmid">32551410</infon><infon key="fpage">213</infon><infon key="issue">2</infon><infon key="kwd">autism emotion crowdsourcing mobile</infon><infon key="license">
          This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
        </infon><infon key="lpage">218</infon><infon key="name_0">surname:Kalantarian;given-names:Haik</infon><infon key="name_1">surname:Jedoui;given-names:Khaled</infon><infon key="name_2">surname:Washington;given-names:Peter</infon><infon key="name_3">surname:Wall;given-names:Dennis P.</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">12</infon><infon key="year">2020</infon><offset>0</offset><text>A Mobile Game for Automatic Emotion-Labeling of Images</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>55</offset><text>In this paper, we describe challenges in the development of a mobile charades-style game for delivery of social training to children with Autism Spectrum Disorder (ASD). Providing real-time feedback and adapting game difficulty in response to the child’s performance necessitates the integration of emotion classifiers into the system. Due to the limited performance of existing emotion recognition platforms for children with ASD, we propose a novel technique to automatically extract emotion-labeled frames from video acquired from game sessions, which we hypothesize can be used to train new emotion classifiers to overcome these limitations. Our technique, which uses probability scores from three different classifiers and meta information from game sessions, correctly identified 83% of frames compared to a baseline of 51.6% from the best emotion classification API evaluated in our work.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>953</offset><text>INTRODUCTION</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>966</offset><text>Autism Spectrum Disorder (ASD) is a developmental disorder characterized by deficits in social communication and the presence of repetitive behaviors and interests. The prevalence of this condition has increased in recent years, rising from an estimated 1-in-68 in 2010 to 1-in-59 in 2014. Though there is no cure for autism, multiple studies have demonstrated that Applied Behavioral Analysis (ABA) therapy can improve developmental progress and social acuity if applied consistently from a young age. The application of ABA therapy is customized to suit the child’s deficits and needs, often including a method of teaching called Discrete Trial Training (DTT).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1631</offset><text>A discrete trial is a unit of instruction delivered by the teacher to the child that lasts between five and twenty seconds, consisting of a prompt, response, reinforcement, and brief pause before the next trial. Two areas in which DTT has been shown to be effective include 1) teaching new discriminations: the recognition of various cues often presented using flashcards, and 2) imitation: the ability to provide a response identical to the queue. As difficulties in emotion recognition and expression are hallmarks of Autism Spectrum Disorder,, discrete trial training can be an integral component of a treatment program designed to address these deficits.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2290</offset><text>Caring for a child with autism can pose a significant financial burden on families. This is in part due to interventions that require long hours of 1-on-1 therapy administered by trained specialists in increasingly short supply due to substantial growth in the incidence of this condition. Alternatives that can ameliorate some of these challenges could be derived from digital and mobile tools. We are developing Guess What?: a mobile charades-style game that can potentially deliver a form of discrete trial training to children at home through a social activity shared between the child, who must interpret and act out various emotive prompts shown on the screen, and the parent, who is tasked with guessing the emotion.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3014</offset><text>Presently, the caregiver is tasked with fulfilling the reinforcement step of discrete trial training based on the fidelity of the child’s imitation. Automatic image-based emotion recognition algorithms can supplement the reinforcement process by detecting if the child is emoting the correct prompt. This functionality can facilitate the development of additional game features that are integral aspects of ABA therapy: (1) adapting the prompts to target the child’s unique deficits, and (2) providing the appropriate visual feedback to guide the child toward the correct behavior without diminishing the challenge.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3634</offset><text>Most commercial emotion classifiers are trained on large databases of labeled images such as the CIFAR-100, ImageNet, Cohn-Kanade Database and Belfast-Induced Natural Emotion Databases. While these datasets contain thousands of images, children are significantly underrepresented in these sources. Thus, classifiers trained on these databases are not optimized for vision-based autism research. This motivates the development of new approaches for scalable aggregation of emotive frames from children that can be used to design future classifiers and augment existing ones.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4208</offset><text>The primary contributions of this paper are as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4264</offset><text>We present a preliminary version of our mobile charades-style game, Guess What?, as it is developed into a form of discrete trial training for teaching emotion recognition and expression.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4452</offset><text>We describe the repurposing of the game for aggregation of emotive egocentric video for autism research.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4557</offset><text>We propose and evaluate two automatic labeling algorithms, which use probability scores from existing emotion classifiers and contextual meta-information to extract labeled frames from videos derived from these game sessions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4783</offset><text>This paper is organized as follows. In Section II, we briefly cover related work in this area. In Section III, we describe the game design. In Section IV, we present our algorithms. In Section V, we describe our experimental methods, followed by results in Section VI and concluding remarks in Section VII.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5090</offset><text>RELATED WORK</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5103</offset><text>Meta classifiers are systems which consider the output of multiple classifiers as features, used to build another classifier for final class label assignment. This technique has been applied to a variety of problem domains, such as grammar correction, news video classification,, and autism identification. In, Chaibi et al. propose an ensembleclassification approach to detect emotions. Similarly, Perikos et al. propose an ensemble-based method to detect emotion from textual data using bagging and boosting techniques. Our approach leverages the success of these previous ensemble- based emotion recognition techniques by using classification scores from three classifiers combined with meta information from the game session to tag each frame with an emotion with a higher accuracy than any individual classifier could achieve.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5935</offset><text>In, Burmania et al. evaluate the ability of raters to correctly label data based on the inclusion of reference sets within the database with a pre-determined ground truth. Sessions in which raters are performing poorly are therefore paused, which increases the accuracy of aggregated frames. This idea has also been explored in earlier works such as that of Le et al.. These approaches are conceptually similar to our method, though we do not rely on human raters and instead characterize the systematic biases of individual classifiers in the determination of the final class label assignment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6530</offset><text>In, the authors use a deep-learning architecture to evaluate several manual labeling techniques to develop a framework in which scores from ten raters can be combined to generate a final label with highest accuracy. Similarly, Yu et al. demonstrate that an ensemble of deep learning classifiers can significantly outperform a single classifier for facial emotion recognition. In contrast with these previous approaches, our method fuses classification confidence scores with game meta information rather than selecting the label with the maximum probability. By considering both per-class probabilities and a priori knowledge about the prompt shown at the time, labeling accuracy is significantly improved.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7237</offset><text>A novel approach for crowdsourcing labeled expressions is described in. The authors propose an iPad puzzle game in which players are recorded through the device’s front camera while periodically instructed to make various expressions. This work bears similarity to our approach, though we target a younger audience and feature a social interplay between caregiver and child. Another crowdsourcing approach by Tuite et al. is the Meme Quiz: a game in which users are tasked with making an expression which the system attempts to recognize based on a continuously-expanding training dataset. Results demonstrated statistically significant increases in classification accuracy over time despite an increasing numbers of classes. Unlike this platform, our system does not contain any online learning functionality in its current form. However, repeated game sessions will provide larger datasets that can be used to train a more robust emotion classifier offline.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8199</offset><text>Aside from crowdsourcing data, various educational and therapeutic games have been proposed in recent years. An example is Recovery Rapids: a gamified implementation of Constraint-Induced Movement Therapy for stroke rehabilitation in which the authors adapt therapy to the individual user in real-time. A similar work within the domain of autism research is described by Harrold et al. in. Rather than acquiring labeled images, the authors propose an educational iPad game which targets deficits in emotion recognition and mimicry in children with developmental delay. During game-play, the child is given a voice-instruction to imitate a face shown on the screen, which is presented beside the child’s face acquired from the devices front camera. Our game, while similar, requires a caregiver to drive the experience due to the targeted age of participants.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>9060</offset><text>GAME DESIGN</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9072</offset><text>Guess What? is a mobile game available for Android and iOS platforms designed to be a shared experience between the child, who attempts to enact the prompt shown on the screen through gestures and facial expressions, and the parent, who is tasked with guessing the word associated with the prompt during the 90 second game session. During each session, the parent holds the phone with the screen directed outward toward the child, who is recorded with the phone’s front camera. This interplay, generally structured around the inversion-problem game described in, has the potential to provide a social, engaging, and educational experience for the child while providing structured video for researchers to develop a dataset of semi-labeled emotion data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9827</offset><text>While several categories of prompt are supported, the two most germane to emotion recognition and expression are emoji, showing exaggerated cartoon representations of emotive faces, and faces, which displays real photos of children. Examples of the main game screen when these two prompts are shown can be seen in Figure 1, along with the main deck selection screen that allows users to select any combination of prompts to be shown during the 90-second game session.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10295</offset><text>When the child acknowledges the correct guess, or when the parent makes the determination that the prompt has been represented correctly based on a priori knowledge about the image shown, parents can change the prompt by tilting the phone forward to awards a point. By tilting the phone backward, the prompt is skipped without awarding a point. Immediately thereafter, a new prompt is randomly selected until the ninety seconds have elapsed. The parent can determine the correct prompt by rotating the screen laterally to peek at the screen; the prompt only changes if a tilt in the longitudinal direction is detected. To reduce the likelihood of points being awarded accidentally, tilt detection is disabled for the first two seconds following the display of a new prompt.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11069</offset><text>After the game session, parents can review the footage and elect to share the data by uploading the video to an IRB-approved secure Amazon S3 bucket fully compliant with the Stanford University’s High-Risk Application security standards. Meta information is included with the video, which describes the prompts shown, timing data, and the number of points awarded. Guess What? is available for download online at: guesswhat.stanford.edu.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11509</offset><text>ALGORITHMS</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11520</offset><text>The development of an emotion classifier that generalizes appropriately to children with ASD requires a substantial database of labeled images from this population. While videos derived from emotion-centric Guess What? game sessions are a natural choice to extract these frames, children are rarely able to consistently enact the correct prompts. Therefore, video segments in which images of a particular emotion are displayed cannot be used as a label for the frames contained therein. For example, preliminary results indicated that less than 25% of frames within regions of the video in which sad emotion were shown matched the prompt. As manual frame-by-frame analysis is a tedious and error-prone process requiring trained raters, it is desirable to develop an automatic labeling approach that leverages meta information from the game, but that is not solely reliant on it. Therefore, an additional filtering step is necessary to remove invalid frames from video segments associated with a particular emotion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12535</offset><text>Ensemble Learning for Emotion Classification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12580</offset><text>Commercial emotion recognition APIs such as Azure Emotion API, Amazon Rekognition and Sighthound are convenient platforms for emotion recognition applications, and can be used to filter out frames within a region that are discordant with the prompt. However, these platforms showed remarkably poor performance on our datasets: no classifier identified over a third of frames in categories manually labeled as sad, disgusted, or angry. By combining the classification scores from multiple classifiers, we are able to effectively average out the nuances associated with each classifier, increasing the robustness of our classification. Two such approaches for combining classification confidence scores to label frames derived from game videos are shown in Figure 2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13345</offset><text>Figure 2a shows how three emotion classifiers can be used to obtain three sets of classification confidence scores for each emotion. For example, Sighthound may report a 80% chance that the frame is happy with a 20% chance that it is sad, while AWS results may indicate 70%/30% probabilities. In the reduction-based architecture, classification confidence scores for each emotion are normalized and combined in three different ways: min, max, and average. A final classification layer then predicts the emotion associated with the image based on a reduced feature set consisting of a single probability score per emotion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13967</offset><text>Figure 2b shows the aggregation-based, approach, in which probability scores from each emotion are not combined. Rather, probabilities are concatenated into a feature vector and all are used for prediction. In this approach, the final classification layer determines the best method to integrate duplicated probabilities from multiple classifiers into a final label assignment. This approach provided generally higher performance compared to the reduction-based, method; detailed results are presented in Section VI.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14484</offset><text>Last-Layer Classification for Label Assignment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14531</offset><text>In the final classification layer, the feature set generated from the outputs of three emotion classifiers is supplemented by the emotion of the prompt shown to the child at the time the frame was extracted to provide contextual meta information. This is based on the intuition that a frame is more likely to be happy if the prompt shown to the child at the time of the frame is related to a happy emotion. An additional classifier leverages this feature vector to assign a final emotion label to the frame. Random Forest is a supervised ensemble learning method for classification that operates by training multiple decision tree classifiers on the dataset and predicting a class based on the number of votes by each decision tree. An advantage of Random Forest over other techniques is the use of bootstrap aggregation to improve predictive accuracy and control over-fitting.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15409</offset><text>Our classifier is trained for 7-class classification based on the maximum overlapping subset of the Ekman universal emotions shared across the three classifiers: neutral, happy, sad, surprised, disgusted, scared and angry. Two samples were designated as the minimum requirement to split an internal node, with at least 1 sample required for a leaf node. √N features were considered when searching for the best split. For hyperparameters tuning, grid-search Cross Validation was used to determine the number of trees in each forest, as well as the maximum depth of each tree. The regression inputs were binned based on the techniques described in. A total of 250 trees were used per forest, each with a maximum depth of 92. For maximum performance, final classification results are based on Leave One Out cross-validation (LOOCV): K-fold Cross Validation with K equal to the dataset size.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>16299</offset><text>EXPERIMENTAL METHODS</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16320</offset><text>In this section, we describe the methods used to collect videos from Guess What?, manually label them to establish our ground truth, and leverage this dataset to validate our automatic labeling algorithms.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16526</offset><text>Data Collection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16542</offset><text>To generate a dataset of Guess What? videos for algorithm evaluation, eight children with a diagnosis of ASD each played several Guess What? games in a single session administered by a member of our research staff on a Google Pixel phone running Android 8.1. The average age of participating children was 4.4 years ± .54. Due to the non-uniform incidence of autism between genders - and small sample size, all participants in this study were boys. The session consisted of up to five games with the following decks in: emoji, faces, animals, sports, and jobs. However, we focus this study on the category most strongly correlated with facial affect, faces, which yielded a total of 8100 frames subsampled from 30 frames per second (FPS) to 5 FPS.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17290</offset><text>DATA PROCESSING</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17306</offset><text>To establish a ground truth, two raters manually assigned emotion labels to each frame in the selected videos based on the six Ekman universal emotions with the addition of a neutral class. In cases when no face could be located within the frame, or the frame was too blurry to discern, reviewers did not assign a label and the frame was excluded. To reduce the burden of manual annotation, the originally 30 frame-per- second videos were subsampled to five FPS. From the selected videos within the faces category, a total of 1350 frames were manually labeled by the two raters. Frames were discarded in cases when the raters disagreed. This produced a total of 1080 frames from the original 1350. A confusion matrix showing the distribution of the rater’s assignments can be seen in Figure 3. The Cohen’s Kappa statistic for inter-rater reliability, a metric which accounts for agreements due to chance, was 0.9. This indicates a high level of reliability between the two manual raters.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>18298</offset><text>RESULTS</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18306</offset><text>In this section, we demonstrate our algorithm’s performance in automatic labeled frame extraction based on the ground truth established by two manual raters.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>18466</offset><text>Baseline Labeling Accuracy</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18493</offset><text>To establish a baseline to compare performance, we ran the 1080 derived frames through three Emotion Classification APIs using no meta information and selected the class with the highest probability as the final label assignment. As shown in Table I, results were poor; the highest accuracy was achieved by Azure Emotion at 51.6%. The Sighthound API produced the lowest accuracy at 9.2%, followed by AWS Rekognition at 10.1%. Note that the weighted F1-scores are significantly higher than accuracy based on total percentage of correctly classified instances. This indicates that these platforms are tuned to correctly recognize the most common emotions such as happy and neutral to the detriment of other less common emotions. These results preclude the integration of these emotion classifiers into Guess What? and necessitate novel methods to automatically label frames using ensemble-techniques and game context.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>19409</offset><text>Ensemble-Classifier Results: No Meta Information</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19458</offset><text>The performance of the ensemble-labeling technique with no meta information is shown in Table II. Specifically, this approach is based on Random-Forest classification of a feature set of confidence scores derived from three emotion classifiers, but does not include the prompt shown at the time the frame was extracted as a feature. These results indicate that the aggregation-based technique shown in Figure 2b is a better labeling approach than the reduction-based techniques of 2a for overall classification accuracy. Furthermore, we can conclude that both aggregation and reduction-based techniques outperform all three commercial emotion recognition APIs even without the inclusion of game context.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>20162</offset><text>Ensemble-Classifier Results: With Meta Information</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>20213</offset><text>Results for ensemble-based approaches that use probability scores from all three classifiers in addition to game meta information are shown in Table III. The aggregation approach, which treats scores from each classifier as separate features, was associated with the highest accuracy, at 83.4%: a significant improvement over the best emotion classification API’s 62.6% accuracy. While all reduction-based approaches had lower accuracy, performance of the max-probability and average-probability approaches were similar at 78.3% and 77.6% respectively. As before, the minimum-probability approach had the lowest accuracy at 61.0%.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>20846</offset><text>Comparison of Methods</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>20868</offset><text>Table IV shows the best possible accuracy achieved by the baseline classifier with no meta information (Azure), the ensemble technique with no meta information, and the technique with the highest accuracy: the ensemble classifier that includes game-meta information, which correctly identified 83.4% of frames.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21179</offset><text>Emotion-Specific Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21204</offset><text>Figure 4 shows the percentage of frames within each region that were correctly identified by the best commercial API evaluated (Azure), the best reduction technique (max), and the aggregation-based technique. Specifically, the reported accuracy for each emotion represents the percentage of identified frames of that class within game periods in which the image associated with that emotion were shown. It should be noted that the Azure classifier operates solely on the data frame and does not take into consideration any meta information from the game session.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21767</offset><text>Several conclusions can be drawn from this data. First, per- class results indicate that the Azure API performance was highly inconsistent between different emotions: the system performed well on neutral and happy frames but performed poorly on others. Secondly, results show that the aggregation-based technique does not outperform the max-reduction algorithm for every emotion: max-reduction identified a greater percentage of frames in disgust and surprise categories. Lastly, even the best of the two labeling techniques still could not correctly identify a majority of frames associated with angry and scared classes; this may be a consequence of a limited training set and will be addressed in future work.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>22480</offset><text>CONCLUSION</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>22491</offset><text>In this paper, we have presented a mobile charades-style game in active development, designed to deliver emotion- recognition training to children with Autism Spectrum Disorder. We describe how this platform can be used to derive emotion-rich egocentric video for processing with automatic labeling algorithms that fuse game-meta information with probability scores to predict emotion with a higher accuracy than commercial emotion recognition APIs. In future work, the labeled frames extracted from these videos will be used to train an emotion classifier that generalizes to children with ASD, which will be integrated into the game to provide reinforcement as social deficits are addressed through gameplay via at-home discrete trial training.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>23238</offset><text>References</text></passage><passage><infon key="fpage">1</infon><infon key="issue">6</infon><infon key="name_0">surname:Baio;given-names:J</infon><infon key="name_1">surname:Wiggins;given-names:L</infon><infon key="name_2">surname:Christensen;given-names:DL</infon><infon key="name_3">surname:Maenner;given-names:MJ</infon><infon key="name_4">surname:Daniels;given-names:J</infon><infon key="name_5">surname:Warren;given-names:Z</infon><infon key="name_6">surname:Kurzius-Spencer;given-names:M</infon><infon key="name_7">surname:Zahorodny;given-names:W</infon><infon key="name_8">surname:Rosenberg;given-names:CR</infon><infon key="name_9">surname:White;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">MMWR Surveillance Summaries</infon><infon key="type">ref</infon><infon key="volume">67</infon><infon key="year">2018</infon><offset>23249</offset><text>Prevalence of autism spectrum disorder among children aged 8 years</text></passage><passage><infon key="fpage">821</infon><infon key="issue">4</infon><infon key="lpage">834</infon><infon key="name_0">surname:Foxx;given-names:RM</infon><infon key="section_type">REF</infon><infon key="source">Child and Adolescent Psychiatric Clinics</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2008</infon><offset>23316</offset><text>Applied behavior analysis treatment of autism: The state of the art</text></passage><passage><infon key="name_0">surname:Council;given-names:NR</infon><infon key="section_type">REF</infon><infon key="source">Educating children with autism</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>23384</offset></passage><passage><infon key="fpage">86</infon><infon key="issue">2</infon><infon key="lpage">92</infon><infon key="name_0">surname:Smith;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Focus on autism and other developmental disabilities</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2001</infon><offset>23385</offset><text>Discrete trial training in the treatment of autism</text></passage><passage><infon key="fpage">290</infon><infon key="issue">3</infon><infon key="lpage">322</infon><infon key="name_0">surname:Harms;given-names:MB</infon><infon key="name_1">surname:Martin;given-names:A</infon><infon key="name_2">surname:Wallace;given-names:GL</infon><infon key="pub-id_pmid">20809200</infon><infon key="section_type">REF</infon><infon key="source">Neuropsychology review</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2010</infon><offset>23436</offset><text>Facial emotion recognition in autism spectrum disorders: a review of behavioral and neuroimaging studies</text></passage><passage><infon key="fpage">865</infon><infon key="issue">6</infon><infon key="lpage">877</infon><infon key="name_0">surname:Macdonald;given-names:H</infon><infon key="name_1">surname:Rutter;given-names:M</infon><infon key="name_2">surname:Howlin;given-names:P</infon><infon key="name_3">surname:Rios;given-names:P</infon><infon key="name_4">surname:Conteur;given-names:AL</infon><infon key="name_5">surname:Evered;given-names:C</infon><infon key="name_6">surname:Folstein;given-names:S</infon><infon key="pub-id_pmid">2592470</infon><infon key="section_type">REF</infon><infon key="source">Journal of Child Psychology and Psychiatry</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">1989</infon><offset>23541</offset><text>Recognition and expression of emotional cues by autistic and normal adults</text></passage><passage><infon key="comment">e106552</infon><infon key="issue">9</infon><infon key="name_0">surname:Horlin;given-names:C</infon><infon key="name_1">surname:Falkmer;given-names:M</infon><infon key="name_2">surname:Parsons;given-names:R</infon><infon key="name_3">surname:Albrecht;given-names:MA</infon><infon key="name_4">surname:Falkmer;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">PloS one</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>23616</offset><text>The cost of autism spectrum disorders</text></passage><passage><infon key="fpage">770</infon><infon key="lpage">778</infon><infon key="name_0">surname:He;given-names:K</infon><infon key="name_1">surname:Zhang;given-names:X</infon><infon key="name_2">surname:Ren;given-names:S</infon><infon key="name_3">surname:Sun;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Deep residual learning for image recognition</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>23654</offset></passage><passage><infon key="name_0">surname:Cohn;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Cohn-kanade au-coded facial expression database</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>23655</offset></passage><passage><infon key="name_0">surname:Douglas-Cowie;given-names:E</infon><infon key="name_1">surname:Cowie;given-names:R</infon><infon key="name_2">surname:Schröder;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">A new emotion database: considerations, sources and scope</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>23656</offset></passage><passage><infon key="comment">guesswhat.stanford.edu</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>23657</offset></passage><passage><infon key="fpage">323</infon><infon key="lpage">326</infon><infon key="name_0">surname:Lin;given-names:W-H</infon><infon key="name_1">surname:Hauptmann;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">News video classification using svm- based multimodal classifiers and combination strategies</infon><infon key="type">ref</infon><infon key="year">2002</infon><offset>23658</offset></passage><passage><infon key="fpage">163</infon><infon key="lpage">171</infon><infon key="name_0">surname:Gamon;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Using mostly native data to correct errors in learners’ writing: a meta-classifier approach</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>23659</offset></passage><passage><infon key="fpage">3798</infon><infon key="issue">12</infon><infon key="lpage">3809</infon><infon key="name_0">surname:Papageorgiou;given-names:EI</infon><infon key="name_1">surname:Kannappan;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Applied Soft Computing</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2012</infon><offset>23660</offset><text>Fuzzy cognitive map ensemble learning paradigm to solve classification problems: Application to autism identification</text></passage><passage><infon key="fpage">99</infon><infon key="lpage">108</infon><infon key="name_0">surname:Chaibi;given-names:MW</infon><infon key="section_type">REF</infon><infon key="source">An ensemble classifiers approach for emotion classification</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>23778</offset></passage><passage><infon key="fpage">363</infon><infon key="issue">1</infon><infon key="lpage">370</infon><infon key="name_0">surname:Perikos;given-names:I</infon><infon key="name_1">surname:Hatzilygeroudis;given-names:I</infon><infon key="section_type">REF</infon><infon key="source">WEBIST</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>23779</offset><text>A classifier ensemble approach to detect emotions polarity in social media</text></passage><passage><infon key="fpage">374</infon><infon key="lpage">388</infon><infon key="name_0">surname:Burmania;given-names:A</infon><infon key="name_1">surname:Parthasarathy;given-names:S</infon><infon key="name_2">surname:Busso;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Increasing the reliability of crowdsourcing evaluations using online quality assessment</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2016</infon><offset>23854</offset></passage><passage><infon key="name_0">surname:Le;given-names:J</infon><infon key="name_1">surname:Edmonds;given-names:A</infon><infon key="name_2">surname:Hester;given-names:V</infon><infon key="name_3">surname:Biewald;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution</infon><infon key="type">ref</infon><infon key="volume">2126</infon><infon key="year">2010</infon><offset>23855</offset></passage><passage><infon key="fpage">279</infon><infon key="lpage">283</infon><infon key="name_0">surname:Barsoum;given-names:E</infon><infon key="name_1">surname:Zhang;given-names:C</infon><infon key="name_2">surname:Ferrer;given-names:CC</infon><infon key="name_3">surname:Zhang;given-names:Z</infon><infon key="section_type">REF</infon><infon key="source">Training deep networks for facial expression recognition with crowd-sourced label distribution</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>23856</offset></passage><passage><infon key="fpage">435</infon><infon key="lpage">442</infon><infon key="name_0">surname:Yu;given-names:Z</infon><infon key="name_1">surname:Zhang;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Image based static facial expression recognition with multiple deep network learning</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>23857</offset></passage><passage><infon key="fpage">491</infon><infon key="lpage">494</infon><infon key="name_0">surname:Tan;given-names:CT</infon><infon key="name_1">surname:Sapkota;given-names:H</infon><infon key="name_2">surname:Rosser;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Befaced: a casual game to crowdsource facial expressions in the wild</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>23858</offset></passage><passage><infon key="name_0">surname:Tuite;given-names:K</infon><infon key="name_1">surname:Kemelmacher;given-names:I</infon><infon key="section_type">REF</infon><infon key="source">FDG</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>23859</offset><text>The meme quiz: A facial expression game combining human agency and machine involvement</text></passage><passage><infon key="name_0">surname:Maung;given-names:D</infon><infon key="name_1">surname:Crawfis;given-names:R</infon><infon key="name_2">surname:Gauthier;given-names:LV</infon><infon key="name_3">surname:Worthen-Chaudhari;given-names:L</infon><infon key="name_4">surname:Lowes;given-names:LP</infon><infon key="name_5">surname:Borstad;given-names:A</infon><infon key="name_6">surname:McPherson;given-names:RJ</infon><infon key="name_7">surname:Grealy;given-names:J</infon><infon key="name_8">surname:Adams;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">FDG</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>23946</offset><text>Development of recovery rapids-a game for cost effective stroke therapy</text></passage><passage><infon key="fpage">33</infon><infon key="lpage">37</infon><infon key="name_0">surname:Harrold;given-names:N</infon><infon key="name_1">surname:Tan;given-names:CT</infon><infon key="name_2">surname:Rosser;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Towards an expression recognition game to assist the emotional development of children with autism spectrum disorders</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>24018</offset></passage><passage><infon key="fpage">58</infon><infon key="issue">8</infon><infon key="lpage">67</infon><infon key="name_0">surname:Von Ahn;given-names:L</infon><infon key="name_1">surname:Dabbish;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">Communications of the ACM</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2008</infon><offset>24019</offset><text>Designing games with a purpose</text></passage><passage><infon key="comment">https://azure.microsoft.com/en-us/services/cognitive-services/emotion/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>24050</offset></passage><passage><infon key="comment">https://aws.amazon.com/rekognition/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>24051</offset></passage><passage><infon key="comment">https://www.sighthound.com/products/cloud</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>24052</offset></passage><passage><infon key="comment">[Online].</infon><infon key="fpage">5</infon><infon key="issue">1</infon><infon key="lpage">32</infon><infon key="name_0">surname:Breiman;given-names:L</infon><infon key="pub-id_doi">10.1023/A:1010933404324</infon><infon key="section_type">REF</infon><infon key="source">Machine Learning</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2001</infon><offset>24053</offset><text>Random forests</text></passage><passage><infon key="fpage">712</infon><infon key="issue">4</infon><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="name_1">surname:Friesen;given-names:WV</infon><infon key="name_2">surname:O’sullivan;given-names:M</infon><infon key="name_3">surname:Chan;given-names:A</infon><infon key="name_4">surname:Diacoyanni- Tarlatzis;given-names:I</infon><infon key="name_5">surname:Heider;given-names:K</infon><infon key="name_6">surname:Krause;given-names:R</infon><infon key="name_7">surname:LeCompte;given-names:WA</infon><infon key="name_8">surname:Pitcairn;given-names:T</infon><infon key="name_9">surname:Ricci-Bitti;given-names:PE</infon><infon key="pub-id_pmid">3681648</infon><infon key="section_type">REF</infon><infon key="source">Journal of personality and social psychology</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">1987</infon><offset>24068</offset><text>Universals and cultural differences in the judgments of facial expressions of emotion</text></passage><passage><infon key="name_0">surname:Friedman;given-names:J</infon><infon key="name_1">surname:Hastie;given-names:T</infon><infon key="name_2">surname:Tibshirani;given-names:R</infon><infon key="section_type">REF</infon><infon key="source">The elements of statistical learning</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2001</infon><offset>24154</offset></passage><passage><infon key="comment">http://www.autism-society.org/what-is/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>24155</offset></passage><passage><infon key="fpage">775</infon><infon key="issue">3</infon><infon key="lpage">803</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="pub-id_pmid">18606031</infon><infon key="section_type">REF</infon><infon key="source">Development and psychopathology</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2008</infon><offset>24156</offset><text>Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder</text></passage><passage><infon key="fpage">1455</infon><infon key="issue">4</infon><infon key="lpage">1472</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="name_1">surname:Bernier;given-names:R</infon><infon key="pub-id_pmid">24342850</infon><infon key="section_type">REF</infon><infon key="source">Development and psychopathology</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2013</infon><offset>24252</offset><text>A quarter century of progress on the early detection and treatment of autism spectrum disorder</text></passage><passage><infon key="file">nihms-1048165-f0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24347</offset><text>In this mobile charades-style game, various prompts to the child during a 90-second game session. The parent attempts to guess the prompt as the child acts it out.</text></passage><passage><infon key="file">nihms-1048165-f0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24511</offset><text>(a) In the reduction-based feature processing algorithm, probability scores from each classifier are combined on a per-class basis before the final classification layer using min, max, or average functions. (b) In the aggregation-based feature processing algorithm, redundant probability scores from different classifiers are tagged as separate features.</text></passage><passage><infon key="file">nihms-1048165-f0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24866</offset><text>The confusion matrix of the two raters assignments of frames into emotion categories. The abbreviations are: happy, sad, angry, disgust, neutral, scared, and surprised.</text></passage><passage><infon key="file">nihms-1048165-f0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25035</offset><text>A comparison of Azure Emotion API’s performance with our max-reduction and aggregation techniques on a per-class basis shows that our algorithm can correctly label the majority of frames in every category except scared and angry, and our aggregation-based method outperforms the best of the three commercial emotion recognition APIs in every category.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25389</offset><text>Baseline accuracy with existing platforms</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Technique&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weighted F1-Score&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Azure Emotion&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.6%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.62&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AWS Rekognition&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.1%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.12&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sighthound&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.2%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.11&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>25431</offset><text>Technique	Accuracy	Weighted F1-Score	 	Azure Emotion	51.6%	0.62	 	AWS Rekognition	10.1%	0.12	 	Sighthound	9.2%	0.11	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25550</offset><text>Ensemble classification without meta information</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Technique&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Features&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weighted F1-Score&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Minimum probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.5%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.36&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Maximum probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.8%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.64&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Average probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.4%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.64&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All probabilities&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.6%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.75&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>25599</offset><text>Technique	Features	Accuracy	Weighted F1-Score	 	Minimum probability	7	47.5%	0.36	 	Maximum probability	7	66.8%	0.64	 	Average probability	7	66.4%	0.64	 	All probabilities	21	76.6%	0.75	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25787</offset><text>Ensemble classification with meta information</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Technique&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Features&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weighted F1-Score&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Minimum probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.0%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.54&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Maximum probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.3%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.77&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Average probability&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.5%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.77&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All probabilities&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.4%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>25833</offset><text>Technique	Features	Accuracy	Weighted F1-Score	 	Minimum probability	8	61.0%	0.54	 	Maximum probability	8	78.3%	0.77	 	Average probability	8	77.5%	0.77	 	All probabilities	22	83.4%	0.84	 	</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>26021</offset><text>Overall accuracy comparison</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;center&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Technique&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;
            &lt;th align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weighted F1-Score&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Baseline (best)&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.6%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.62&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ensemble without meta (best)&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.6%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.75&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ensemble with meta (best)&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.4%&lt;/td&gt;
            &lt;td align=&quot;center&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>26049</offset><text>Technique	Accuracy	Weighted F1-Score	 	Baseline (best)	51.6%	0.62	 	Ensemble without meta (best)	76.6%	0.75	 	Ensemble with meta (best)	83.4%	0.84	 	</text></passage></document></collection>
