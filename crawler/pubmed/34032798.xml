<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210609</date><key>pmc.key</key><document><id>8148353</id><infon key="license">CC BY</infon><passage><infon key="alt-title">Utilizing a responsive web portal for studying disc tracing agreement in retinal images</infon><infon key="article-id_doi">10.1371/journal.pone.0251703</infon><infon key="article-id_pmc">8148353</infon><infon key="article-id_pmid">34032798</infon><infon key="article-id_publisher-id">PONE-D-20-22689</infon><infon key="elocation-id">e0251703</infon><infon key="issue">5</infon><infon key="license">This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</infon><infon key="name_0">surname:Sarhan;given-names:Abdullah</infon><infon key="name_1">surname:Swift;given-names:Andrew</infon><infon key="name_2">surname:Gorner;given-names:Adam</infon><infon key="name_3">surname:Rokne;given-names:Jon</infon><infon key="name_4">surname:Alhajj;given-names:Reda</infon><infon key="name_5">surname:Docherty;given-names:Gavin</infon><infon key="name_6">surname:Crichton;given-names:Andrew</infon><infon key="name_7">surname:Qiu;given-names:Yuchen</infon><infon key="name_8">surname:Sarhan;given-names:Abdullah</infon><infon key="notes">All relevant data required to reproduce the results of this study can be accessed at the following link: https://data.mendeley.com/datasets/7xv5rzxgrh/1. Additional data can be accessed using the following link: http://www.aisarhan.com/fundusportal/login. To obtain credentials to access this second portal, please contact the Corresponding Author at asarhan@ucalgary.ca.</infon><infon key="section_type">TITLE</infon><infon key="title">Data Availability</infon><infon key="type">front</infon><infon key="volume">16</infon><infon key="year">2021</infon><offset>0</offset><text>Utilizing a responsive web portal for studying disc tracing agreement in retinal images</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>88</offset><text>Glaucoma is a leading cause of blindness worldwide whose detection is based on multiple factors, including measuring the cup to disc ratio, retinal nerve fiber layer and visual field defects. Advances in image processing and machine learning have allowed the development of automated approached for segmenting objects from fundus images. However, to build a robust system, a reliable ground truth dataset is required for proper training and validation of the model. In this study, we investigate the level of agreement in properly detecting the retinal disc in fundus images using an online portal built for such purposes. Two Doctors of Optometry independently traced the discs for 159 fundus images obtained from publicly available datasets using a purpose-built online portal. Additionally, we studied the effectiveness of ellipse fitting in handling misalignments in tracing. We measured tracing precision, interobserver variability, and average boundary distance between the results provided by ophthalmologists, and optometrist tracing. We also studied whether ellipse fitting has a positive or negative impact on properly detecting disc boundaries. The overall agreement between the optometrists in terms of locating the disc region in these images was 0.87. However, we found that there was a fair agreement on the disc border with kappa = 0.21. Disagreements were mainly in fundus images obtained from glaucomatous patients. The resulting dataset was deemed to be an acceptable ground truth dataset for training a validation of models for automatic detection of objects in fundus images.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1685</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1698</offset><text>A study by Bourne et al. indicates that, as of 2015, there were approximately 444 million people living with visual impairments worldwide; 39 million of these were blind, 216 million had moderate to severe visual impairment, and 189 million had mild visual impairment. A major contribution to these statistics comes from vision loss to glaucoma. It is the world’s second leading cause of irreversible vision loss after cataracts, accounting for 12% of cases of blindness annually. It is estimated that the number of people between the ages of 40–80 affected by glaucoma will increase from the present of 80 million to 111.8 million by 2040. Furthermore, 2.4% of all individuals and 4.7% of those over 70 are at risk of developing this condition.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2448</offset><text>The term glaucoma refers to a condition caused by a group of diseases that leads to the degeneration of retinal ganglion cells (RGCs). The death of RGCs leads to (i) structural changes to the optic nerve head and the nerve fiber layer and (ii) simultaneous functional constriction of the visual field. These two effects of glaucoma cause peripheral vision loss and, if left untreated, can eventually lead to blindness. One possible indicator of glaucoma is increased intraocular pressure (IOP), which can damage the RGCs, resulting in nerve fibre layer atrophy and thus structural changes to the optic nerve head.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3062</offset><text>Current approaches for glaucoma detection rely on manual interpretation of fundus images by optometrists and ophthalmologists. First patients would visit an optometrist who will then be referred to an ophthalmologist or not based on the interpretation of the fundus image. This interpretation is both critical and time-consuming and relies on the experience of the ophthalmologist/optometrist. in particular, they have difficulties in detecting the initial stages of glaucoma. As a result, about 80% of early-onset glaucoma cases may go undiagnosed. Thus, there is a critical need for automated tools to accurately detect glaucoma which will then have the potential to decrease blindness due to glaucoma.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3767</offset><text>The advent of machine learning has opened up new possibilities for the automatic analysis of medical images and in particular for analyzing and segmenting fundus images. for the detection of various eye conditions, such as glaucoma and diabetic retinopathy.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4025</offset><text>With the rise of deep learning comes the potential for achieving high performance when automating the segmentation of various objects from retinal images such as disc segmentation. However, a common requirement for proper training for these models is the availability of a ground truth dataset. Creating such datasets with reliable ground truth labeling can be both subjective and time-consuming. Furthermore, issues of subjectivity in the labeling by experts add to the difficulty of developing the dataset. To handle the time issue researchers have been using crowdsourcing to annotate and label a large number of images, especially in telemedicine. Unfortunately, the application of this approach has not yet demonstrated that the results are reliable enough to be used as ground truths.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4816</offset><text>In the context reliable is defined in terms of several experts making the same identification of an object or objects in a fundus image. While this is a somewhat imprecise definition it reflects the underlying problem of making an absolute definition of objects in a fundus image. This problem also occurs in other deep learning approaches where it is overcome by also increasing the sizes of the datasets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5223</offset><text>Most crowdsourcing approaches in the retinal field have focused on comparing the classification accuracy of non ophthalmologists/optometrists undergoing a brief training instead of comparing the tracing done for various anatomical retinal objects. Mitry et al., conducted a study utilizing the crowdsourcing technique that focuses on discriminating between normal and glaucomatous discs. Amazon’s Mechanical Turk platform was used to recruit participants known as knowledge workers (KW) from the online community with the KWs that perform well being denoted as “masters”. The study obtained 2,540 classifications for 127 color fundus images within 24 hours and the performance of KWs with previous experience in performing such a task was compared to those without. The average area under curve (AUC)-a performance measure for binary classifier where a value closer to 100 indicates greater performance-achieved was 62.75%, with no significant difference between the two groups. However, specificity was very low (38.9%). Further, they did not measure the agreement between KWs. Mitry et al., performed another crowdsourcing study as an extension to the previous one they conducted. In this study, they found an overall sensitivity of 71% and specificity of 87% for all classifications and a masters-only group achieved the highest performance, which was 10% superior to that of the non-masters.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6625</offset><text>The study conducted by Son et al.,, measured the accuracy of KWs recruited using Amazon Mechanical Turk in correctly localizing abnormalities in 109,985 images. However, they neither localized the disc boundaries nor the peripapillary atrophy (PPA), which are indicators of glaucoma. Another study measures the performance of experienced ophthalmologists instead of KWs. On average, three experienced raters agreed that an abnormality was present for 46.4% of the images, whereas two raters were in agreement for 69.9% of the images. The agreement rate when all three raters participated ranged from 5.7% to 43.3%. However, none of these approaches works when attempting to trace a disc.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7313</offset><text>None of the studies discussed above focused on comparing the tracing performed for the disc in retinal fundus images, nor did they investigate participants’ ability to discriminate between the disc border and the PPA, which is important when diagnosing glaucoma. Rather, these studies mainly focused on image classification and/or identifying regions with abnormalities and did not measure the level of agreement between different groups of participants.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7770</offset><text>In this study, we investigate the tracing done by two doctors of optometry OD1 and OD2, for the retinal disc by utilizing a responsive web portal built for such purposes. This portal can also be used for the identification of other objects in the retina. Unlike other desktop applications, such as MS-Paint that work on some devices with a specific operating system, the portal can also be used on any device with access to a browser and internet, including mobile phones, tablets, laptops, and desktops. Doing so allows users to perform tracing for fundus images regardless of the type of device they are using. Additionally, we compare the tracing performed by the two optometrists with previous ones performed by ophthalmologists which are provided by the publishers of these datasets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8559</offset><text>Exact tracing can be time-consuming, but it is required to develop a ground truth that can be used to train machine learning models for consistent and proper segmentation of retinal objects. For instance, it is important to be able to discriminate between disc boundary and PPA region when diagnosing glaucoma. Hence, the goal of this study is to investigate and compare the tracing of the disc performed by the two optometrists through the responsive online portal that we developed. To our knowledge, there is no publicly available portal similar to the one developed in this study. Our contributions can be listed as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9188</offset><text>We demonstrated the level of agreement/disagreement between optometrists for disc tracing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9279</offset><text>We demonstrated the level agreement/disagreement between ophthalmologists and optometrists.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9371</offset><text>We showed that using ellipse fitting for adjusting misalignments in the traced disc does not always perform well especially in the case of glaucoma.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9520</offset><text>We published the new tracings performed by the optometrists so that researchers can use the data as a ground truth dataset when developing their models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9673</offset><text>We developed an online portal that can be used for annotating discs by multiple contributors. This portal can be expanded to other retinal object and even be used for educational purposes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>9862</offset><text>Materials and methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9884</offset><text>The doctors of optometry OD1 and OD2 and one ophthalmologist specializing in glaucoma were involved in this study. The two optometry doctors were responsible for tracing the disc and the glaucoma specialist investigated the images that had a high disagreement. Each optometry doctor traced the discs in retinal images independently, using their own laptops/tablets. They used the built-in features of the web portal to perform the tracing. This section discusses the datasets used, together with how the tracing was done and the statistical analysis conducted.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>10445</offset><text>Datasets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10454</offset><text>The publicly available datasets Drishti,, and ARIA,, were used in this study. A total of 159 images were used from these datasets. One of the reasons for using these datasets is that we wished to evaluate tracing variability when different retinal conditions were presented. Additionally, these datasets are the most commonly used datasets by researchers working in the field of ophthalmology and machine learning. Of the images, 69 depicted cases of glaucoma, 53 diabetic retinopathy, and 37 depicted normal cases. The normal and glaucomatous images were obtained from the Drishti dataset, while those depicting diabetic retinopathy (DR) were obtained from the ARIA dataset. The images in the ARIA dataset are taken from a 50-degree field of view and stored in “.tif” format, with dimensions of 768 × 576 pixels, while those from the Drishti dataset are taken from a 30-degree field of view and stored in “.png” format, with dimensions of 2,896 × 1,944 pixels.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11426</offset><text>Each image obtained from these datasets had its own disc ground truth provided by a specialist ophthalmologist. Hence, in our analysis, we considered the ground truth provided in these datasets as our reference if the two doctors of optometry properly trace the disc or not. When high disagreement occurred the glaucoma specialist would investigate the reason behind this disagreement. All tracings generated in this study are available online for researchers to use for further investigation. Note that we did not collect any new retinal images and hence we just used those publicly available datasets and compared the tracing performed in our study to the ground truth provided in these datasets. We did not need to obtain ethics approval as this was already performed by the researchers who collected the images and made them available.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12266</offset><text>Study design</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12279</offset><text>Two optometry doctors were involved in this study; their primary role was to trace the discs in the images obtained from the Drishti and ARIA datasets. These two datasets provided ground truth for discs obtained from the experienced ophthalmologists. Each optometrist involved in this study was assigned the sets of images (159 images in total) through the web portal, where they also traced the discs. The tracing was done independently and without consulting with each other. Fig 1 summarizes the overall process by which this stage of the research was conducted.</text></passage><passage><infon key="file">pone.0251703.g001.jpg</infon><infon key="id">pone.0251703.g001</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>12845</offset><text>Tracing and data collection flow adopted in this study.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12901</offset><text>Once the pop-up model appears, the user can start tracing. An erase option is presented should the user wish to erase any of the tracings. We keep track of the time when the users start tracing and when this tracing is finished. Once the tracing is done, the user can click on the submit button, which will allow the storage of tracing information on a dedicated server. Users have the option either to trace the whole disc at once or in steps. Upon successful submission of the tracing, the traced image will be eliminated from the list of images on the tracing page and the number of untraced images at the top of the page, Fig 2, will also be decreased. Once the data had been stored, the traced images could be compared, and the ophthalmologist could investigate the reasons behind any significant disagreement mainly between optometrists.</text></passage><passage><infon key="file">pone.0251703.g002.jpg</infon><infon key="id">pone.0251703.g002</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>13745</offset><text>Web portal showing images assigned to the optometrists along with the pop up dialogue displayed once an image is pressed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13867</offset><text>Statistical analysis</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13888</offset><text>below 0 ⇒ less than chance agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13927</offset><text>0.01-0.2 ⇒ poor agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13955</offset><text>0.21-0.40 ⇒ fair agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13984</offset><text>0.41-0.60 ⇒ moderate agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14017</offset><text>0.61-0.80 ⇒ substantial agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14053</offset><text>0.81-0.99 ⇒ almost perfect agreement</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14092</offset><text>Three statistical evaluation techniques are adopted in this study, namely the Cohen’s kappa statistic (to measure inter-agreement between the optometrists and between optometrists and available ground truth), the dice coefficient (to measure the precision of locating of the disc region) and average boundary distance (to measure precision in detecting disc border). The kappa calculation was calculated using Eq 1 where Po is the observed proportionate agreement and Pe is the probability of both optometrists saying either yes or no concerning whether a specific pixel should be considered as being in the disc region or not. When calculating the kappa value, we only included the area surrounding the disc. To do this we retrieved the lowest x, lowest y, highest x, and highest y values among the disc ground truth and tracing done by optometrists. Then we drew a bounding box which was used to calculate the kappa. There are various interpretations of kappa values; however, in this study, we adopted the most commonly used interpretation. The cut off values are as follows:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15175</offset><text>For the dice coefficient (DC) calculation, we used Eq 2, where A is the ground truth area and B is the area traced by the optometrist. A DC value of 1 means that the tracing of the optometrist agreed with the ground truth. However, DC does not indicate how precise the optometrists were in identifying disc boundary. Hence, we used another evaluation technique for this task known as the average boundary distance (μd). This evaluation method compares the disc ground truth with the tracing done by each optometrist. We used Eq 3, where  and  are the distance from the ground truth’s tracing centroid to the intersection point with the expert’s and optometrist’s tracings respectively at a given angle k. We calculated the distance in 12 directions namely 0°, 20°, 60°, 90°, 120°, 150°, 180°, 210°, 240°, 270°, 300°, and 330°. At each angle, a line is drawn from the centroid of the disc (Cg). Then the intersection points between this line and ground truth, OD1, and OD2 tracing is detected at each of these 12 directions. An overview of the centroid and the 12 directions is shown in Fig 3. Moreover, we also show the distance difference between  and .  </text></passage><passage><infon key="file">pone.0251703.g003.jpg</infon><infon key="id">pone.0251703.g003</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>16351</offset><text>Retinal images showing lines draw from the centroid to expert (green) and optometrist (blue) traced boundary in different directions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16485</offset><text>While performing the analysis, we also noticed a calibration issue in that some of the tracings were misaligned. These misalignments were due to tracings deviating from their intended paths. While this issue could be fixed by erasing the misalignment, this process was fairly time-consuming. We hence used an approach called ellipse fitting to fix these misalignments by producing perfectly aligned ellipses; we then compared the tracings before and after the application of ellipse fitting. Another reason for using ellipse fitting is that various approaches claim that the discs tend to have an elliptic shape.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17098</offset><text>The least-square fitting approach identifies the smallest circle that can traverse all given traced pixels; specifically, we adopted the algorithm developed by Halir and Flusser. Given Eq 4, where A,B,C,D,E, and F are the coefficients of the ellipse and B2 − 4AC &lt; 0, generate an ellipse by minimizing the square of the algebraic distance of the points to the ellipsoid plane. This can be achieved by first calculating the design matrices as shown in matrices 5 and 6. These design matrices are then utilized to calculate the scattered plots as shown in Eq 7. The scatter plots are then reduced using the constrained coefficient in matrix 8 to produce the reduced matrix using Eq 9 where I and T represent the identity and transpose of the related matrices. The reduced scattered matrix is utilized to calculate eigenvectors and then extract the ellipse coefficients namely the center, width and height of the ellipse, and rotation angle. Additional information about the effectiveness of this approach with respect to other ones can be found in.      </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18153</offset><text>Once the preceding steps were completed, we then measured the level of agreement between the two optometrists concerning the identification of the disc region both with and without ellipse fitting. We also investigated whether the level of agreement between the images taken from the Drishti and ARIA databases depended on the fundus status (i.e., normal, glaucoma, and diabetic retinopathy). Moreover, we investigated the level of precision with respect to the available ground truth for properly detecting the disc regions between the two datasets and among different retinal image statuses. Further, we study the level of agreement with regard to correctly detecting the disc boundary and how precise the optometrists were in doing so. All data generated in this study is publicly available for use by researchers.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>18971</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18979</offset><text>In this section, we discuss various analysis techniques applied on the tracing performed by OD1 and OD2 along with comparing them with ground truth. A summary of the results before and after applying ellipse fitting can be seen in Tables 1 and 2 respectively. Additionally, Figs 4 and 5 demonstrate the performance of ellipse fitting in fixing the misalignment in the tracing of the optometrists. We realized that applying ellipse fitting does not always improve the precision in tracing. For instance, in Fig 4 when applying ellipse fitting the DC has increased 96.8% to 98.1% for the Drishti image while for the Aria image, the DC has increased from 95.6% to 96.1%. However, this was not the case in Fig 4 where the DC for the Drishti after applying ellipse fitting decreased from 90.59% to 89.35% and from 86.28% to 84.73% for the Aria image. We should emphasize that the Drishti dataset is mainly for glaucoma while the Aria dataset is for patients with diabetic retinopathy.</text></passage><passage><infon key="file">pone.0251703.g004.jpg</infon><infon key="id">pone.0251703.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>19959</offset><text>Images obtained from the Drishti and ARIA datasets showing improved disc tracing when ellipse fitting is applied.</text></passage><passage><infon key="file">pone.0251703.g004.jpg</infon><infon key="id">pone.0251703.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>20073</offset><text>The first row represents an image from the Drishti dataset before and after applying ellipse fitting. The second row is similar to the first one but for an image obtained from the Aria dataset. The green tracing represents the ground truth and the red represents the tracing performed by the optometrist. The application of Ellipse fitting led to an increase of DC from 96.80% to 98.10% for the Drishti image from 95.60% to 96.10% for the Aria image.</text></passage><passage><infon key="file">pone.0251703.g005.jpg</infon><infon key="id">pone.0251703.g005</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>20524</offset><text>Images obtained from the Drishti and ARIA datasets showing ellipse fitting lead to lower precision in the traced disc region.</text></passage><passage><infon key="file">pone.0251703.g005.jpg</infon><infon key="id">pone.0251703.g005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>20650</offset><text>The first row represents an image from the Drishti dataset before and after applying ellipse fitting. The second row is similar to the first one but for an image obtained from the Aria dataset. The green tracing represents the ground truth and the red represents the tracing performed by the optometrist. The application of Ellipse fitting led to a decrease of DC from 90.59% to 89.35% for the Drishti image from 86.28% to 84.73% for the Aria image.</text></passage><passage><infon key="file">pone.0251703.t001.xml</infon><infon key="id">pone.0251703.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>21100</offset><text>The values for DC, kappa, and boundary when comparing OD1 with OD2, OD1 with ground truth (G), and OD2 with ground truth (G) before applying ellipse fitting.</text></passage><passage><infon key="file">pone.0251703.t001.xml</infon><infon key="id">pone.0251703.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot; border=&quot;0&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Dice Coefficient&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Kappa (region)&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Kappa (border)&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Boundary (pixels)&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 1 vs OD 2&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.09&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.15&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.60&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 1 vs G&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.92&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.80&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.015&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.09&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.90&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 2 vs G&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.93&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.80&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.02&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.05&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.70&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.40&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21258</offset><text>	Dice Coefficient	Kappa (region)	Kappa (border)	Boundary (pixels)	 		Drishti	Aria	Drishti	Aria	Drishti	Aria	Drishti	Aria	 	OD 1 vs OD 2	-	-	0.86	0.84	0.09	0.15	6.60	1.80	 	OD 1 vs G	0.95	0.92	0.86	0.80	0.015	0.09	6.90	3.20	 	OD 2 vs G	0.95	0.93	0.85	0.80	0.02	0.05	7.70	3.40	 	</text></passage><passage><infon key="file">pone.0251703.t002.xml</infon><infon key="id">pone.0251703.t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>21536</offset><text>This table summarizes the values for DC, kappa, and boundary when comparing OD1 with OD2, OD1 with ground truth (G), and OD2 with ground truth (G) after applying ellipse fitting.</text></passage><passage><infon key="file">pone.0251703.t002.xml</infon><infon key="id">pone.0251703.t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot; border=&quot;0&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Dice Coefficient&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Kappa (region)&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Kappa (border)&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Boundary (pixels)&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drishti&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aria&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 1 Vs OD 2&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.88&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.18&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.28&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.70&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 1 vs G&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.92&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.79&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.13&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.102&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.90&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;OD 2 vs G&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.93&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.79&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.02&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.06&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.70&lt;/td&gt;&lt;td align=&quot;char&quot; char=&quot;.&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.40&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21715</offset><text>	Dice Coefficient	Kappa (region)	Kappa (border)	Boundary (pixels)	 		Drishti	Aria	Drishti	Aria	Drishti	Aria	Drishti	Aria	 	OD 1 Vs OD 2	-	-	0.88	0.86	0.18	0.28	5.70	1.30	 	OD 1 vs G	0.95	0.92	0.85	0.79	0.13	0.102	6.90	3.20	 	OD 2 vs G	0.95	0.93	0.84	0.79	0.02	0.06	7.70	3.40	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21993</offset><text>Tracing agreement</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22011</offset><text>To measure the level of agreement between the two optometrists OD1 and OD2, we calculated the kappa value for the level of agreement between the optometrists in disc tracing. We calculated the level of agreement in terms of locating the disc region in each dataset. Before the application of ellipse fitting, the level of agreement for the Drishti dataset was 0.86, while that for the ARIA dataset 0.84, which were both almost perfect. After applying ellipse fitting, the level of the agreement changed slightly, with the level for Drishti being 0.88 and for ARIA 0.86, as shown in Fig 6. As can be seen in this figure, after applying ellipse fitting, we realized there was an improvement in some images in the Drishti dataset, as their kappa values increased from 0.94 to 0.96; meanwhile, the kappa values for some images from the ARIA set increased from 0.92 to 0.95. We also realized the kappa value for the image with the lowest agreement score in the Drishti dataset improved from 0.55 to 0.59. While this was not a significant improvement, it shows the effect of misalignment in tracing and how ellipse fitting can be used to address this issue in some images but not to all.</text></passage><passage><infon key="file">pone.0251703.g006.jpg</infon><infon key="id">pone.0251703.g006</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>23193</offset><text>The level of agreement between the optometrists when identifying disc region.</text></passage><passage><infon key="file">pone.0251703.g006.jpg</infon><infon key="id">pone.0251703.g006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23271</offset><text>Figures A and B Show the Level of Agreement in each Dataset. Figures C and D show the Level of Agreement in Regard to Retinal Condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23408</offset><text>We took the analysis one step further by investigating whether there was a difference in determining the disc region as a result of retinal status. We measured the level of agreement with respect to the presence of retinal conditions, namely glaucoma, diabetic retinopathy (DR), and normal eyes (see Fig 6A and 6B). We compared the results with and without ellipse fitting. The levels of agreement before applying ellipse fitting for glaucoma, DR, and normal were 0.87, 0.84, and 0.84, respectively, which are slightly lower than the levels achieved after applying ellipse fitting, which were 0.91, 0.86, and 0.85, respectively. The major improvement has occurred in the images related to glaucoma. It is also clear how the distribution of kappa values for the images changed before and after ellipse fitting, as can be seen in Fig 6C and 6D. This change is reflected by the fact that the image with the lowest level of agreement in the Drishti sample has a better level of agreement after the application of ellipse fitting. Such improvement in the level of agreement is mainly because one of the ODs encountered calibration issues while tracing the disc. However, the application of ellipse fitting did not always lead to an improvement in results, check Fig 5, and highly dependant on how much miss alignment was produced by the optometrist when tracing the disc.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24775</offset><text>Measuring the level of agreement with regard to detecting the disc regions was found not reliable in determining the level of agreement in detecting disc borders. We also measured the level of agreement between OD1 and OD2 in locating the disc border. We first checked the level of agreement based on the dataset and then based on retinal condition, as shown in Fig 7A and 7B. The levels of agreement between the two optometrists before applying ellipse fitting to the Drishti and ARIA datasets were 0.09 and 0.15 respectively, which are lower than the levels of agreement after applying ellipse fitting, which were 0.18 and 0.28, respectively. Thus, the level of agreement after applying ellipse fitting improved from poor to fair for the ARIA dataset but remained poor for the Drishti. We also found that the level of agreement with regard to the ARIA dataset was slightly better than that for the Drishti dataset before applying ellipse fitting; this is mainly because of the presence of PPA in glaucomatous images. The levels of agreement for some images in the Drishti and ARIA datasets improved from moderate to substantial agreement, which is considered a significant improvement. However, in some images, the level of agreement decreased to zero; this likely occurred due to misalignment towards the disc center and/or because ellipse fitting takes the minimal point, meaning that the disc boundary will change.</text></passage><passage><infon key="file">pone.0251703.g007.jpg</infon><infon key="id">pone.0251703.g007</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>26195</offset><text>The level of agreement between the optometrists when identifying disc region.</text></passage><passage><infon key="file">pone.0251703.g007.jpg</infon><infon key="id">pone.0251703.g007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26273</offset><text>Figures A and B Show the Level of Agreement in each Dataset. Figures C and D show the Level of Agreement in Regard to Retinal Condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26410</offset><text>When we measured the level of agreement in detecting the disc region, we also measured the level of agreement in detecting the disc border based on retinal status (see Fig 7C and 7D). The levels of agreement for glaucoma, DR, and normal retinal images before applying ellipse fitting were 0.09, 0.15, and 0.08, respectively, which are considered to be very poor values and are lower than the levels of agreement achieved after the application of ellipse fitting (0.19, 0.3, and 0.2 for glaucoma, DR, and normal images, respectively, with these values being considered to represent fair levels of agreement). Applying ellipse fitting helped to improve the level of agreement from poor to fair in boundary identification, particularly for images related to glaucoma. However, this level of agreement is still low and may have a negative impact on patient referrals, especially for those with glaucoma. We can also realize that glaucomatous images have the lowest level of agreement among other images.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>27410</offset><text>Tracing precision</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27428</offset><text>Measuring inter-agreement between the optometrists made it possible to obtain a value for the level of agreement, but it did not indicate how precise the optometrists were. To measure the level of precision in detecting the disc region, we calculated the dice coefficient using Eq 2. We calculated the dice coefficient for OD1 and OD2 with respect to the available ground truth. The dice coefficient is useful when measuring precision as dependent on the volume of a traced object but not when measuring precision in detecting an object’s boundary. For this task, we used another approach called average distance boundary as discussed in section 1. We applied Eq 3 to each image for the tracing done by each optometrist.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28151</offset><text>We first measured the level of precision in detecting the disc region in each dataset, regardless of the retina status, before and after applying ellipse fitting, as shown in Fig 8A and 8B. The DC values obtained by OD1 were 0.95 for the Drishti dataset and 0.92 for the ARIA dataset, while OD2 achieved 0.95 for the Drishti dataset and 0.93 for the ARIA dataset. The level of precision did not improve significantly after the application of ellipse fitting, as it stayed almost the same (see Fig 8). However, images with low levels of tracing precision improved after the application of ellipse fitting. Both optometrists performed almost the same in identifying disc region.</text></passage><passage><infon key="file">pone.0251703.g008.jpg</infon><infon key="id">pone.0251703.g008</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>28828</offset><text>The level of precision between the optometrists when identifying disc region.</text></passage><passage><infon key="file">pone.0251703.g008.jpg</infon><infon key="id">pone.0251703.g008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28906</offset><text>Figures A and B Show the Level of Precision in each Dataset. Figures C and D show the Level of Precision in Regards to Retinal Condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29044</offset><text>We also investigated whether the level of precision in terms of detecting the disc region was affected by retinal status, both before and after the application of ellipse fitting, as shown in Fig 8C and 8D. The levels of precision for OD1 before applying ellipse fitting were 0.95, 0.92, and 0.94 for glaucoma, DR, and normal retinal images, respectively, while those for OD2 were 0.95, 0.92, and 0.95. However, ellipse fitting did not lead to improvement in the precision, as the values remained roughly the same. We can also realize that ellipse fitting helped to slightly improve the tracing precision for OD2. However, for some images, ellipse fitting has a negative effect when being compared with the ground truth. This is depending on where the misalignment occurred, and how frequently it happened. Overall, we can see improvement in the lower and upper bounds in Fig 9 before and after applying ellipse fitting. Moreover, the levels of precision in glaucoma and normal images for both optometrists remained almost the same after the application of ellipse fitting, but we can realize improvements in the upper and lower bounds along with the median value.</text></passage><passage><infon key="file">pone.0251703.g009.jpg</infon><infon key="id">pone.0251703.g009</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>30209</offset><text>The level of precision between the optometrists when identifying disc border.</text></passage><passage><infon key="file">pone.0251703.g009.jpg</infon><infon key="id">pone.0251703.g009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30287</offset><text>Figures A and B Shows the Level of Precision in each Dataset. Figures C and D show the Level of Precision in Regards to Retinal Condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30426</offset><text>In addition to measuring the level of agreement in detecting the disc border, we also measured the level of precision in correctly identifying the disc boundary by OD1 and OD2 both before and after applying ellipse fitting to each dataset, as shown in Fig 9A and 9B. Instead of using the dice coefficient, we used the average boundary distance measure described earlier to measure boundary tracing precision with respect to the available ground truth. The average boundary distance before applying ellipse fitting for OD1 were 6.9 and 3.2 pixels for the Drishti and ARIA datasets, respectively, while those for OD2 were 7.7 and 3.4 pixels. These results revealed some information that could not be discovered using only DC values. DC values showed perfect precision in identifying disc region, but this was not the case when trying to check for the boundary. Moreover, in some cases, the difference between ground truth and optometrist tracing distance reached 53 pixels. It can thus be concluded that the optometrists were unable to correctly detect the disc boundary mainly in the Drishti dataset. This can also be correlated to a poor level of agreement achieved in detecting disc boundary. The application of ellipse fitting did not affect the overall results for μd. Moreover, the results showed poor tracing precision in the glaucomatous dataset (Drishti). OD1 performed better than OD2 and this can be clearly seen in Fig 9. Such observations can not be achieved without using evaluations similar to the boundary distance one.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31961</offset><text>A similar measurement was applied to gauge the average boundary distance in terms of determining retinal image status for the two optometrists before and after applying ellipse fitting (Fig 9C and 9D). The levels of boundary distance for OD1 before applying ellipse fitting were 6.6, 3.2, and 7.4 for glaucoma, DR, and normal retinal images, respectively, while those for OD2 were 7.3, 3.4, and 8.6. We did not observe significant improvement in the overall results after applying ellipse fitting. Actually, ellipse fitting negatively affected the tracing done for some images especially for those traced by OD2. The overall performance in terms of correctly detecting the disc border in all retinal conditions was low; indeed, in some cases, it was as low as zero. Images with DR had the highest level of precision while those with glaucoma had the lowest as can be observed from high average boundary distance. This is mainly related to the level of subjectivity when trying to distinguish the disc border from PPA.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>32979</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32990</offset><text>This study evaluated the level of agreement and precision for tracing the disc in fundus images. This involved the development of a customized web portal for disc annotation. We found that optometrist tracings showed high levels of agreement for identifying the disc region. However, there was poor agreement for correctly identifying the disc boundary. The application of ellipse fitting helped in adjusting for misalignment in some images.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33432</offset><text>Two optometrists OD1 and OD2 produced a set of disc and disc-boundary traces of fundus images. A glaucoma specialist went over the tracings and found that the two optometrists had a great variance in detecting the disc boundary and distinguishing it from PPA in the inferior and superior regions of the disc in various images. One of the reasons for this variance is that the disc border was not very clear in certain images, and hence the bend of blood vessels was used instead, which does not allow the actual disc border to be identified correctly in all cases. Another reason was the calibration challenge faced by optometrists when tracing the whole disc at once. Although an eraser was provided with which to remove incorrect tracing, this feature did not prevent this issue from occurring. We used ellipse fitting to address misalignments in tracing, which helped to improve the levels of agreement and precision for some images. The low resolution and poor disc visibility in some of the images were additional reasons for the poor performance of the optometrists. In some of the images obtained from the ARIA dataset, the disc border was not very clear, and it was occasionally blurred by background illumination. However, optometrists performed better in locating disc boundary in images from the ARIA dataset as compared with the Drishti dataset.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34790</offset><text>On average, OD1 took 80 seconds to completely trace the disc, which was longer than OD2’s 30 seconds. In general, OD1 performed better than OD2 in tracing the disc, which is reflected in a higher DC values and lower average boundary distance achieved than that of OD2, particularly with regard to the disc border. Both optometrists achieved high kappa and DC in identifying disc regions, but this was not the case when locating the disc boundary especially in the images from the glaucomatous dataset.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35294</offset><text>In general, the portal is a great tool for disc tracing instead of using a desktop application. Users can use any device with an internet browser that has access to the internet. Once access is established they can start tracing the images assigned to them and share their results. We also showed that applying ellipse fitting to tracings is not always helpful. The effectiveness of ellipse fitting is highly dependent on the misalignment of tracing caused by the user and in some cases the shape of the disc is not close to elliptic. One of the limitations of this study is that, for some of the images, the optometrists found it difficult to use the pencil and eraser, which led to more time being spent on specific images. Future work could be done using a customized object with a rotation feature, which can be used to compare participants’ results. Another limitation is that the system does not currently provide live feedback, which is a feature that we intend to add. We would like to expand this system to include annotation of other retinal anatomical objects, such as the fovea and PPA. Additionally, we would like to develop a semi-supervised approached that could help in speeding up the tracing process. While the system has the potential to be applied in any field that involves tracing, our research mainly focuses on the field of ophthalmology, in particular glaucoma.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36683</offset><text>In this study, the levels of precision and agreement were very low when it comes to correctly detect the disc border especially when working with glaucomatous retinal images. Hence, developing an automated approach that could assist in correctly detecting the disc border in a precise, consistent, and rapid fashion is very important in this field. Such an approach would have the potential to be helpful in detecting glaucomatous discs, influencing decisions concerning treatment, offering appropriate referrals, as well as in other areas. The developed portal in this study could serve as a model for generating sufficient ground truth to develop an automated system for identifying cup to disc ratio and helping to improve the identification of individuals who may have glaucoma, allowing for more timely referrals and management. We also demonstrated that there is a degree of subjectivity in tracing the disc. Using the average boundary distance evaluation approaches revealed information related to optometrists’ precision in disc boundary tracing that was not revealed when using DC. Using kappa and average boundary distance identified where disagreement is happening which we then investigated.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>37889</offset><text>References</text></passage><passage><infon key="fpage">e888</infon><infon key="issue">9</infon><infon key="lpage">e897</infon><infon key="name_0">surname:Bourne;given-names:RR</infon><infon key="name_1">surname:Flaxman;given-names:SR</infon><infon key="name_2">surname:Braithwaite;given-names:T</infon><infon key="name_3">surname:Cicinelli;given-names:MV</infon><infon key="name_4">surname:Das;given-names:A</infon><infon key="name_5">surname:Jonas;given-names:JB</infon><infon key="pub-id_doi">10.1016/S2214-109X(17)30293-0</infon><infon key="pub-id_pmid">28779882</infon><infon key="section_type">REF</infon><infon key="source">The Lancet Global Health</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>37900</offset><text>Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis</text></passage><passage><infon key="fpage">1930</infon><infon key="issue">9</infon><infon key="lpage">1938</infon><infon key="name_0">surname:Fu;given-names:H</infon><infon key="name_1">surname:Xu;given-names:Y</infon><infon key="name_2">surname:Lin;given-names:S</infon><infon key="name_3">surname:Zhang;given-names:X</infon><infon key="name_4">surname:Wong;given-names:DWK</infon><infon key="name_5">surname:Liu;given-names:J</infon><infon key="pub-id_doi">10.1109/TMI.2017.2703147</infon><infon key="pub-id_pmid">28499992</infon><infon key="section_type">REF</infon><infon key="source">IEEE transactions on medical imaging</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2017</infon><offset>38061</offset><text>Segmentation and quantification for angle-closure glaucoma assessment in anterior segment OCT</text></passage><passage><infon key="fpage">2081</infon><infon key="issue">11</infon><infon key="lpage">2090</infon><infon key="name_0">surname:Tham;given-names:YC</infon><infon key="name_1">surname:Li;given-names:X</infon><infon key="name_2">surname:Wong;given-names:TY</infon><infon key="name_3">surname:Quigley;given-names:HA</infon><infon key="name_4">surname:Aung;given-names:T</infon><infon key="name_5">surname:Cheng;given-names:CY</infon><infon key="pub-id_doi">10.1016/j.ophtha.2014.05.013</infon><infon key="pub-id_pmid">24974815</infon><infon key="section_type">REF</infon><infon key="source">Ophthalmology</infon><infon key="type">ref</infon><infon key="volume">121</infon><infon key="year">2014</infon><offset>38155</offset><text>Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis</text></passage><passage><infon key="fpage">73</infon><infon key="lpage">82</infon><infon key="name_0">surname:Mookiah;given-names:MRK</infon><infon key="name_1">surname:Acharya;given-names:UR</infon><infon key="name_2">surname:Lim;given-names:CM</infon><infon key="name_3">surname:Petznick;given-names:A</infon><infon key="name_4">surname:Suri;given-names:JS</infon><infon key="pub-id_doi">10.1016/j.knosys.2012.02.010</infon><infon key="section_type">REF</infon><infon key="source">Knowledge-Based Systems</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2012</infon><offset>38272</offset><text>Data mining technique for automated diagnosis of glaucoma using higher order spectra and wavelet energy features</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>38385</offset><text>Kumar BN, Chauhan R, Dahiya N. Detection of Glaucoma using image processing techniques: A review. In: 2016 International Conference on Microelectronics, Computing and Communications (MicroCom). IEEE; 2016. p. 1–6.</text></passage><passage><infon key="fpage">101657</infon><infon key="name_0">surname:Sarhan;given-names:A</infon><infon key="name_1">surname:Rokne;given-names:J</infon><infon key="name_2">surname:Alhajj;given-names:R</infon><infon key="pub-id_pmid">31675645</infon><infon key="section_type">REF</infon><infon key="source">Computerized Medical Imaging and Graphics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>38601</offset><text>Glaucoma detection using image processing techniques: A literature review</text></passage><passage><infon key="name_0">surname:Thienes;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Canadian association of optometrists pre-budget submission</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>38675</offset></passage><passage><infon key="issue">22</infon><infon key="name_0">surname:Park;given-names:SH</infon><infon key="name_1">surname:Kressel;given-names:HY</infon><infon key="pub-id_doi">10.3346/jkms.2018.33.e152</infon><infon key="section_type">REF</infon><infon key="source">Journal of Korean Medical Science</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2018</infon><offset>38676</offset><text>Connecting technological innovation in artificial intelligence to real-world medical practice through rigorous clinical validation: what peer-reviewed medical journals could do</text></passage><passage><infon key="fpage">643</infon><infon key="issue">4</infon><infon key="lpage">655</infon><infon key="name_0">surname:Akram;given-names:MU</infon><infon key="name_1">surname:Tariq;given-names:A</infon><infon key="name_2">surname:Khalid;given-names:S</infon><infon key="name_3">surname:Javed;given-names:MY</infon><infon key="name_4">surname:Abbas;given-names:S</infon><infon key="name_5">surname:Yasin;given-names:UU</infon><infon key="pub-id_doi">10.1007/s13246-015-0377-y</infon><infon key="pub-id_pmid">26399880</infon><infon key="section_type">REF</infon><infon key="source">Australasian physical &amp; engineering sciences in medicine</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2015</infon><offset>38853</offset><text>Glaucoma detection using novel optic disc localization, hybrid feature set and classification techniques</text></passage><passage><infon key="fpage">27</infon><infon key="lpage">35</infon><infon key="name_0">surname:De La Fuente-Arriaga;given-names:JA</infon><infon key="name_1">surname:Felipe-Riverón;given-names:EM</infon><infon key="name_2">surname:Garduño-Calderón;given-names:E</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2014.01.005</infon><infon key="pub-id_pmid">24530536</infon><infon key="section_type">REF</infon><infon key="source">Computers in biology and medicine</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2014</infon><offset>38958</offset><text>Application of vascular bundle displacement in the optic disc for glaucoma detection using fundus images</text></passage><passage><infon key="fpage">466</infon><infon key="issue">3</infon><infon key="lpage">476</infon><infon key="name_0">surname:Panda;given-names:R</infon><infon key="name_1">surname:Puhan;given-names:N</infon><infon key="name_2">surname:Panda;given-names:G</infon><infon key="pub-id_doi">10.1016/j.bbe.2017.05.008</infon><infon key="section_type">REF</infon><infon key="source">Biocybernetics and Biomedical Engineering</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2017</infon><offset>39063</offset><text>Robust and accurate optic disk localization using vessel symmetry line measure in fundus images</text></passage><passage><infon key="fpage">280</infon><infon key="lpage">296</infon><infon key="name_0">surname:Cheplygina;given-names:V</infon><infon key="name_1">surname:de Bruijne;given-names:M</infon><infon key="name_2">surname:Pluim;given-names:JP</infon><infon key="pub-id_doi">10.1016/j.media.2019.03.009</infon><infon key="pub-id_pmid">30959445</infon><infon key="section_type">REF</infon><infon key="source">Medical image analysis</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2019</infon><offset>39159</offset><text>Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis</text></passage><passage><infon key="fpage">60</infon><infon key="lpage">88</infon><infon key="name_0">surname:Litjens;given-names:G</infon><infon key="name_1">surname:Kooi;given-names:T</infon><infon key="name_2">surname:Bejnordi;given-names:BE</infon><infon key="name_3">surname:Setio;given-names:AAA</infon><infon key="name_4">surname:Ciompi;given-names:F</infon><infon key="name_5">surname:Ghafoorian;given-names:M</infon><infon key="pub-id_doi">10.1016/j.media.2017.07.005</infon><infon key="pub-id_pmid">28778026</infon><infon key="section_type">REF</infon><infon key="source">Medical image analysis</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2017</infon><offset>39271</offset><text>A survey on deep learning in medical image analysis</text></passage><passage><infon key="fpage">1</infon><infon key="issue">6</infon><infon key="lpage">4</infon><infon key="name_0">surname:Howe;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Wired magazine</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2006</infon><offset>39323</offset><text>The rise of crowdsourcing</text></passage><passage><infon key="issue">2</infon><infon key="name_0">surname:Mitry;given-names:D</infon><infon key="name_1">surname:Peto;given-names:T</infon><infon key="name_2">surname:Hayat;given-names:S</infon><infon key="name_3">surname:Blows;given-names:P</infon><infon key="name_4">surname:Morgan;given-names:J</infon><infon key="name_5">surname:Khaw;given-names:KT</infon><infon key="pub-id_doi">10.1371/journal.pone.0117401</infon><infon key="pub-id_pmid">25692287</infon><infon key="section_type">REF</infon><infon key="source">PloS one</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2015</infon><offset>39349</offset><text>Crowdsourcing as a screening tool to detect clinical features of glaucomatous optic neuropathy from digital photography</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>39469</offset><text>Amazon Mechanical Turk;. https://www.mturk.com/.</text></passage><passage><infon key="fpage">6</infon><infon key="issue">5</infon><infon key="lpage">6</infon><infon key="name_0">surname:Mitry;given-names:D</infon><infon key="name_1">surname:Zutis;given-names:K</infon><infon key="name_2">surname:Dhillon;given-names:B</infon><infon key="name_3">surname:Peto;given-names:T</infon><infon key="name_4">surname:Hayat;given-names:S</infon><infon key="name_5">surname:Khaw;given-names:KT</infon><infon key="pub-id_doi">10.1167/tvst.5.5.6</infon><infon key="pub-id_pmid">27668130</infon><infon key="section_type">REF</infon><infon key="source">Translational vision science &amp; technology</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2016</infon><offset>39518</offset><text>The accuracy and reliability of crowdsource annotations of digital retinal images</text></passage><passage><infon key="fpage">95</infon><infon key="lpage">104</infon><infon key="name_0">surname:Son;given-names:J</infon><infon key="name_1">surname:Kim;given-names:S</infon><infon key="name_2">surname:Park;given-names:SJ</infon><infon key="name_3">surname:Jung;given-names:KH</infon><infon key="section_type">REF</infon><infon key="source">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39600</offset></passage><passage><infon key="fpage">3103</infon><infon key="issue">7</infon><infon key="lpage">3111</infon><infon key="name_0">surname:Vianna;given-names:JR</infon><infon key="name_1">surname:Malik;given-names:R</infon><infon key="name_2">surname:Danthurebandara;given-names:VM</infon><infon key="name_3">surname:Sharpe;given-names:GP</infon><infon key="name_4">surname:Belliveau;given-names:AC</infon><infon key="name_5">surname:Shuba;given-names:LM</infon><infon key="pub-id_doi">10.1167/iovs.16-19646</infon><infon key="pub-id_pmid">27294804</infon><infon key="section_type">REF</infon><infon key="source">Investigative ophthalmology &amp; visual science</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2016</infon><offset>39601</offset><text>Beta and gamma peripapillary atrophy in myopic eyes with and without glaucoma</text></passage><passage><infon key="issue">43</infon><infon key="name_0">surname:Park;given-names:SJ</infon><infon key="name_1">surname:Shin;given-names:JY</infon><infon key="name_2">surname:Kim;given-names:S</infon><infon key="name_3">surname:Son;given-names:J</infon><infon key="name_4">surname:Jung;given-names:KH</infon><infon key="name_5">surname:Park;given-names:KH</infon><infon key="pub-id_doi">10.3346/jkms.2018.33.e239</infon><infon key="pub-id_pmid">30344460</infon><infon key="section_type">REF</infon><infon key="source">Journal of Korean medical science</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2018</infon><offset>39679</offset><text>A novel fundus image reading tool for efficient generation of a multi-dimensional categorical image database for machine learning algorithm training</text></passage><passage><infon key="fpage">53</infon><infon key="lpage">56</infon><infon key="name_0">surname:Sivaswamy;given-names:J</infon><infon key="name_1">surname:Krishnadas;given-names:S</infon><infon key="name_2">surname:Joshi;given-names:GD</infon><infon key="name_3">surname:Jain;given-names:M</infon><infon key="name_4">surname:Tabish;given-names:AUS</infon><infon key="section_type">REF</infon><infon key="source">2014 IEEE 11th international symposium on biomedical imaging (ISBI)</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>39828</offset></passage><passage><infon key="issue">3</infon><infon key="name_0">surname:Bankhead;given-names:P</infon><infon key="name_1">surname:Scholfield;given-names:CN</infon><infon key="name_2">surname:McGeown;given-names:JG</infon><infon key="name_3">surname:Curtis;given-names:TM</infon><infon key="pub-id_doi">10.1371/journal.pone.0032435</infon><infon key="pub-id_pmid">22427837</infon><infon key="section_type">REF</infon><infon key="source">PloS one</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2012</infon><offset>39829</offset><text>Fast retinal vessel detection and measurement using wavelets and edge location refinement</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>39919</offset><text>Tracing Done By Optometrists;. https://data.mendeley.com/datasets/7xv5rzxgrh/1.</text></passage><passage><infon key="fpage">323</infon><infon key="issue">5</infon><infon key="name_0">surname:Fleiss;given-names:JL</infon><infon key="name_1">surname:Cohen;given-names:J</infon><infon key="name_2">surname:Everitt;given-names:BS</infon><infon key="pub-id_doi">10.1037/h0028106</infon><infon key="section_type">REF</infon><infon key="source">Psychological bulletin</infon><infon key="type">ref</infon><infon key="volume">72</infon><infon key="year">1969</infon><offset>39999</offset><text>Large sample standard errors of kappa and weighted kappa</text></passage><passage><infon key="fpage">360</infon><infon key="issue">5</infon><infon key="lpage">363</infon><infon key="name_0">surname:Viera;given-names:AJ</infon><infon key="name_1">surname:Garrett;given-names:JM</infon><infon key="pub-id_pmid">15883903</infon><infon key="section_type">REF</infon><infon key="source">Fam med</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2005</infon><offset>40056</offset><text>Understanding interobserver agreement: the kappa statistic</text></passage><passage><infon key="fpage">1192</infon><infon key="issue">6</infon><infon key="lpage">1205</infon><infon key="name_0">surname:Joshi;given-names:GD</infon><infon key="name_1">surname:Sivaswamy;given-names:J</infon><infon key="name_2">surname:Krishnadas;given-names:S</infon><infon key="pub-id_doi">10.1109/TMI.2011.2106509</infon><infon key="pub-id_pmid">21536531</infon><infon key="section_type">REF</infon><infon key="source">IEEE transactions on medical imaging</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2011</infon><offset>40115</offset><text>Optic disk and cup segmentation from monocular color retinal images for glaucoma assessment</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>40207</offset><text>Halır R, Flusser J. Numerically stable direct least squares fitting of ellipses. In: Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG. vol. 98. Citeseer; 1998. p. 125–132.</text></passage></document></collection>
