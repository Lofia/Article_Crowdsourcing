<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220708</date><key>pmc.key</key><document><id>8940264</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1007/s11042-022-12900-5</infon><infon key="article-id_pmc">8940264</infon><infon key="article-id_pmid">35342329</infon><infon key="article-id_publisher-id">12900</infon><infon key="fpage">25029</infon><infon key="issue">18</infon><infon key="kwd">Federated learning Blockchain Privacy preserving Decentralized machine learning Data as a service Society 5.0</infon><infon key="license">This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</infon><infon key="lpage">25050</infon><infon key="name_0">surname:Peyvandi;given-names:Amirhossein</infon><infon key="name_1">surname:Majidi;given-names:Babak</infon><infon key="name_2">surname:Peyvandi;given-names:Soodeh</infon><infon key="name_3">surname:Patra;given-names:Jagdish C.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">81</infon><infon key="year">2022</infon><offset>0</offset><text>Privacy-preserving federated learning for scalable and high data quality computational-intelligence-as-a-service in Society 5.0</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>128</offset><text>Training supervised machine learning models like deep learning requires high-quality labelled datasets that contain enough samples from various categories and specific cases. The Data as a Service (DaaS) can provide this high-quality data for training efficient machine learning models. However, the issue of privacy can minimize the participation of the data owners in DaaS provision. In this paper, a blockchain-based decentralized federated learning framework for secure, scalable, and privacy-preserving computational intelligence, called Decentralized Computational Intelligence as a Service (DCIaaS), is proposed. The proposed framework is able to improve data quality, computational intelligence quality, data equality, and computational intelligence equality for complex machine learning tasks. The proposed framework uses the blockchain network for secure decentralized transfer and sharing of data and machine learning models on the cloud. As a case study for multimedia applications, the performance of DCIaaS framework for biomedical image classification and hazardous litter management is analysed. Experimental results show an increase in the accuracy of the models trained using the proposed framework compared to decentralized training. The proposed framework addresses the issue of privacy-preserving in DaaS using the distributed ledger technology and acts as a platform for crowdsourcing the training process of machine learning models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1584</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1597</offset><text>The issue of the privacy is one the most important issues that should be addressed in Society 5.0. It can be estimated that 2.5 quintillion bytes (25 × 105 TB) of data is generated and accessed daily. Although all of this data is not useful for machine learning, if only 1% of this data is valuable and can be utilized in smart environment modelling and applications, it would amount to 25,000 TB of usable data generated daily. Using this data to train machine learning models allows researchers to enhance computational intelligence solutions for different industries, healthcare, management, business, smart environments, and many other applications in Society 5.0. However, an essential issue for using this data is preserving the privacy of people who generate or own the data. There has been a significant discussion regarding the issue of preserving the privacy of data providers. A solution for this problem is decentralized transmission and storage of data as opposed to the centralized approach in which data is stored in a server resulting in open opportunities for various attacks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2697</offset><text>A decentralized solution for training machine learning models is federated learning. This method trains the model across multiple nodes or servers that hold local data samples. However, this method does not necessarily protect the privacy of data providers. Blockchain technology can be used as a solution to address this issue. In this paper, a blockchain-based federated learning framework for secure, scalable, and privacy-preserving machine learning, which combines blockchain-based Data as a Service (DaaS) and Machine Learning as a Service (MLaaS), called Decentralized Computational Intelligence as a Service (DCIaaS), is proposed. In the proposed framework, the models will be trained off-chain on the data provider’s side, and only the learned model parameters and weights will be shared on the blockchain. The decentralized DaaS allows the data providers to make the data available to the machine learning specialists based on their demand. The provision of the data is irrespective of geospatial or other relations of the data provider and data consumer. In the proposed framework, each data point is coupled with its owner, and data remains distributed and private.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3877</offset><text>Training supervised machine learning models like deep learning requires high-quality labelled datasets which contain enough samples from each category and specific cases. The proposed framework helps in the creation of better machine learning datasets by increasing samples of minority classes. The main contributions of this manuscript are as follows: 1. A computationally inexpensive framework that combines blockchain and federated learning allowing privacy-preserving MLaaS; 2. The proposed framework can increase the accuracy of machine learning models in comparison with decentralized training; 3. Two practical multimedia case studies that demonstrate the applications of the proposed framework for Society 5.0.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4596</offset><text>The rest of the paper is organized as follows: In Section 2, a review of recent literature related to federated learning, privacy-preserving, and machine learning on the blockchain is presented. In Section 3, the proposed DCIaaS framework is presented and detailed. In Section 4, experimental results and practical applications of the proposed framework are discussed. Finally, Section 5 concludes the paper and provides future directions for the research.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5053</offset><text>Related works</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5067</offset><text>Recently, federated learning has been widely investigated for various applications in the Internet of Things (IoT), Internet of Medical Things (IoMT), and Industry 4.0. During the COVID-19 pandemic, it has been proposed for improved COVID-19 detection. It has also been utilized for the secure classification of COVID-19 chest X-ray images. Moreover, blockchain technology is also proposed as a reliable tool for secure Biomedical Data as a Service (BDaaS) for epidemic management. Furthermore, federated learning has been utilized for training models on distributed data located in different medical institutions . Rajendran et al. proposed cloud-based federated learning. This method uses a centralized approach and therefore, even though a 3% increase in performance of the trained models is reported, it can lead to exposure of sensitive medical records. In most federated learning frameworks, the aggregation of trained models takes place on a centralized server, and the shared weights and models are open to attacks. Ge et al. propose a privacy-preserving medical framework which utilizes federated learning for health purposes. In this research, the sharing of trained models and weights does not take place on a secure channel.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6304</offset><text>One of the main issues with federated learning is that it is not secure. For example, the participants may behave maliciously during gradient collection or parameter updating process, and the server may act maliciously as well. The worst-case scenario is when federated learning is utilized in a centralized setting, i.e., storing all of the data and parameters in a single server, which multiplies risk factors. The decentralized federated learning is also vulnerable because even one single malicious server can pose a threat to the data and models. There are researches which prove that the intermediate gradients can be used to infer important information about the training data. Hitaj et al. demonstrate that a federated deep learning approach does not protect the training data. They developed an attack which exploits the real-time nature of the learning process and allows the adversary to train a Generative Adversarial Network (GAN), which generates prototypical samples of the targeted training set that was meant to be private. Moreover, they demonstrate that record-level differential privacy applied to the shared parameters of the model is ineffective. Moreover, other researchers questioned the security of federated learning. There are computationally expensive solutions proposed to address this problem. Phong et al. presented a privacy-preserving deep learning system in which different learning participants perform deep learning over a combined dataset without revealing the participants’ local data to a central server. In this work, the authors connect deep learning and cryptography and utilize asynchronous stochastic gradient descent in combination with homomorphic encryption.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8012</offset><text>In order to solve the problems of federated learning, machine learning on a blockchain can be used. DeepChain is a distributed and secure deep learning framework which aims to solve the aforementioned problems. DeepChain provides a value-driven incentive mechanism based on blockchain in order to make the participants behave correctly. Moreover, their proposed framework guarantees the privacy of participants and data providers during the training process. This framework preserves the privacy of local gradients and guarantees the auditability of the training process. In DeepChain, two smart contracts are utilized. One contract is dedicated to the management of data providers, while the other contract controls “workers” who train the models. By utilizing blockchain, the authors ensure that no malicious activity can happen. Goel et al. proposed that by utilizing a cryptographic hash, as well as symmetric/asymmetric encryption and decryption algorithms, security will be ensured without any centralized authority. The authors proposed DeepRing, which utilizes the learned parameters of a standard deep neural network model and is secured from external adversaries by cryptography and blockchain technology. Their proposed framework transforms each layer of the deep neural network into a block and handles them accordingly. Baldominos et al. proposed a blockchain-based system named “Coin.AI,” in which the mining arrangement requires training deep learning models, and a block is only mined when the performance of a model exceeds a threshold. The distributed system allows the blockchain nodes to verify the models delivered by miners, determining when a block is to be generated. Moreover, the authors introduced a proof-of-storage scheme for rewarding users that provide storage for the deep learning models. Fadaeddini et al. proposed a secure decentralized peer-to-peer framework in order to train deep neural network models on distributed ledger technology on Stellar blockchain. A Deep Learning Coin (DLC) is proposed for blockchain compensation. In order to address the issue of data sharing for platforms that depend on a Trusted Third Party (TTP), Naz et al. proposed a blockchain-based secure data and file sharing platform by utilizing IPFS and smart contract technology. In this method, the owner first uploads metadata, which is then divided into n secret shares. Furthermore, customers can review files and comment on them. Addressing the issue of privacy at the data level is another solution for privacy-preserving, which is investigated by Hajiabbasi et al..</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10607</offset><text>Comparison of the proposed DCIaaS framework with the above-mentioned machine learning on blockchain proposals shows that the proposed DCIaaS framework does not have the vulnerabilities of federated learning-based approaches such as, that may expose sensitive and personal data during their training process. On the other hand, although works such as aim to eliminate problems of federated learning by utilizing blockchain technology since they are built around distributed systems and the training data is still shared between miners or other individuals, this distribution of models and data can still prove to be harmful. However, the proposed DCIaaS framework solves this issue as data holders are not required to share their data. In DCIaaS, models are trained on the data owner’s end of the blockchain and only the trained weights are shared in a secure manner. Furthermore, the proposed framework does not require the expensive computations of cryptographic-based methods. Further comparison of both results and the advantages of the proposed framework is discussed in the experimental results section of the manuscript.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>11736</offset><text>Decentralized computational-intelligence-as-a-service</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11790</offset><text>The proposed DCIaaS uses decentralized DaaS, in which the data remains distributed on the data owners’ nodes, and combined with blockchain and federated learning, it remains anonymised. This is opposed to the centralized approach in which data is aggregated on a central server, and at best, the data can be pseudonymized. It should be noted that training models on blockchain infrastructure is hard and consumes a significant amount of time, money, and resources. Therefore, in the proposed framework, the actual training process is performed off-chain. In the proposed DCIaaS framework, there are three groups of primary nodes. The first nodes are Data Providers such as governmental bodies, organizations, companies, hospitals, medical centres, citizens, and other researchers. This group is not required to share their data. Instead, they are only required to offer a sample of the data (i.e., a few records of their dataset), plus a description of the dataset and its features. Moreover, if this sample contains any sensitive information, the data providers can either anonymise the sample by completely removing such attributes and only provide a detailed description of them or use pseudonymization techniques. The data providers are the clients in the federated learning algorithm.</text></passage><passage><infon key="file">11042_2022_12900_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>13082</offset><text>DCIaaS for privacy preserving federated learning</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13131</offset><text>The second nodes are referred to as Applicants. This group will not have direct access to sensitive data and will only share their algorithms and codes with the Data Providers. They can implement their algorithms and codes by analysing the shared sample and its description. Moreover, when signing a contract with data providers, they are required to send their overall proposal to the data providers. This proposal consists of a summary of what their algorithm is going to do, what programming language and which frameworks and libraries are used, and required resources, such as CPU, TPU, and GPU. The final node is the Smart Contract. In the proposed framework, one smart contract, called TrainingModel, is utilized. This smart contract is used to control the contracts signed between a Data Provider and an Applicant. Furthermore, it is responsible for performing federated learning (applying federated learning algorithm to model weights) on the blockchain. The model for the relationship between Data Providers (clients) and Applicants is presented in Fig. 1. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14199</offset><text>In this section, first, the issue of privacy-preserving in decentralized machine learning is discussed, and then the federated learning algorithm used in DCIaaS is presented. Then, the details of the implementation of DCIaaS on the blockchain are presented.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>14457</offset><text>Preservation of privacy for decentralized machine learning</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14516</offset><text>Input privacy, which aims to preserve data privacy during training or inference. This requirement is needed when the data is sent to an external, non-trusted party (a cloud server) that performs the computation;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14728</offset><text>Output privacy, that ensures the non-revelation of private information about the data from the products of the training (i.e., the model) or inference (i.e., the output predictions);</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14911</offset><text>Model privacy is the property ensuring the non-revelation of the attributes that define a model, such as architecture and weights.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15042</offset><text>Due to the high computational and resource cost of deep learning algorithms, data scientists often rely upon MLaaS to outsource the computational load onto third-party servers. However, outsourcing the computation raises privacy concerns when dealing with sensitive information, such as medical records. Furthermore, privacy regulations like the European GDPR, limit the collection, distribution, and use of sensitive data and information. Recent advances in privacy-preserving techniques, such as Homomorphic Encryption (HE), federated learning, and Differential Privacy (DP), have enabled model training and inference over protected data. These data privacy techniques aim at reducing the amount of sensitive information that data carry. Overall, MLaaS relies on three different types of privacy requirements: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15855</offset><text>There are three main solutions for addressing these issues. The first solution is HE that is an encryption scheme. Homomorphism is a mathematical concept whereby the structure is preserved throughout a computation. Since only certain mathematical operations, such as addition and multiplication, are homomorphic, the application of HE to neural networks requires the procedures defined within the algorithm to conform to these limitations. In order to implement HE and encrypt the models’ weights, the ncryption scheme can be utilized. This method takes the secret key with large noise as input and outputs unencrypted data of the same input with a fixed amount of noise. Let R be the unencrypted matrix data of the mini-batch dataset with the size of N × M. Before the encryption of a tensor, a private key matrix φ with size N × N as: is created. This key is only accessed by the participants who are authorized and shared the mini-batch dataset, with ℝ(N) being the plaintext space: where R(j) shows the vector data of the jth node of the ledger. The ⊗ operator shows the product between two ciphertext: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16980</offset><text>The second solution for addressing the issue of privacy is DP. DP mechanisms often rely on adding noise to the data, which ends up reducing its expressiveness. A differentially private mechanism acting on very similar datasets will return results which are statistically indiscernible. Given privacy mechanism M, which maps inputs from domain D to outputs in the range R, by multiplicative factor ϵ, regardless of the presence or absence of a single individual in two neighbouring datasets d and d′ drawn from D, it is probable that for any subset of outputs S ⊆ R: where d and d′ are correspondent with the same output. This method protects individuals from being identified within the dataset. DP is an example of a perturbative privacy-preserving method, as the privacy guarantee is achieved by the addition of noise to the true output. This noise is usually drawn from a Laplacian distribution, but it can also be drawn from an exponential distribution or via the novel staircase mechanism that provides greater utility compared to Laplacian noise for the same ϵ. The aforementioned description of differential privacy is often known as ϵ-differential privacy (ϵ-DP). The amount of noise needed for ϵ-DP is controlled by ϵ, and the sensitivity of the function Q defined by: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18295</offset><text>This maximum is evaluated over all neighbouring datasets in the set D. The output of the mechanism using noise drawn from the Laplacian distribution L is: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18451</offset><text>Moreover, a moderate version of DP known as (ϵ, δ)-DP provides greater flexibility in designing privacy preserving mechanisms and greater resistance to attacks: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18623</offset><text>The Gaussian mechanism is commonly used to add noise to satisfy (ϵ, δ)-DP, but instead of the l1 norm, the noise is scaled to the l2 norm: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18773</offset><text>Given ϵ, δ ∈ (0, 1), the following mechanism satisfies (ϵ, δ)-DP: </text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>18860</offset><text>Comparison of federated learning with HE and DP</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot;&gt;Method&lt;/th&gt;&lt;th colspan=&quot;4&quot;&gt;Privacy Goal&lt;/th&gt;&lt;th rowspan=&quot;2&quot;&gt;Strengths&lt;/th&gt;&lt;th rowspan=&quot;2&quot;&gt;Weaknesses&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Identity&lt;/th&gt;&lt;th&gt;Dataset&lt;/th&gt;&lt;th&gt;Model&lt;/th&gt;&lt;th&gt;Input&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;DP&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;Provable guarantee of privacy&lt;/td&gt;&lt;td&gt;Compromising accuracy of DL model&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;HE&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;Computing on encrypted data&lt;/td&gt;&lt;td&gt;High computation costs, works on numerical data&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FL&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;Decentralized training&lt;/td&gt;&lt;td&gt;High communication cost, high availability&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>18908</offset><text>Method	Privacy Goal	Strengths	Weaknesses	 	Identity	Dataset	Model	Input	 	DP	✓	✓	✓	×	Provable guarantee of privacy	Compromising accuracy of DL model	 	HE	✓	✓	×	✓	Computing on encrypted data	High computation costs, works on numerical data	 	FL	✓	✓	✓	×	Decentralized training	High communication cost, high availability	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19249</offset><text>In this paper, federated learning combined with blockchain is used to address the issue of privacy preservation in MLaaS. Federated learning is a machine learning framework in which multiple parties upload local gradients to a server or multiple servers, and these servers update model parameters with the collected gradients. Moreover, federated learning allows a model to be collaboratively trained using local data from distributed entities without revealing it to the other parties. The clients use their local data to train a local version of the model to compute the updates. Next, these updates are sent back to a central server, which aggregates them into a global model. Table 1 shows the strengths and weaknesses of federated learning in comparison with HE and DP. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20025</offset><text>By itself, federated learning suffers from security issues since the generated model and gradients are shared, and they may be abused to breach privacy. While federated learning is flexible and resolves data governance and ownership issues, it does not guarantee security and confidentiality by itself unless combined with other methods. A lack of encryption can enable attackers to steal sensitive identifiable information directly from the nodes or interfere with the communication process. The required secure communication can be expensive for large machine learning models or large data volumes. Therefore, federated learning is often combined with other techniques such as HE or DP to preserve input and output privacy. However, these methods are computationally expensive.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20805</offset><text>In the proposed DCIaaS, the blockchain is utilized for the aggregation part of federated learning. In this case, a smart contract plays the role of “central server” and the privacy-preserving and security are highly guaranteed. Moreover, communication through transactions of the blockchain offers a safe and secure communication channel for sharing weights between different clients and data owners. In the proposed framework, an Ethereum-based smart contract is utilized. The hash function plays the fundamental role in security structure of blockchain over Ethereum network. The hash function used in Ethereum is Keccak-256. The hash functions compress the volume of data with arbitrary size to the fixed-length. In (10), let H(x) be a hash function (one way – {0, 1}∗ → {0, 1}n) in which x is a random finite length bit-string that produces output Y with fixed length size: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21702</offset><text>Preimage resistance: Given output Y, it is computationally impractical to find input x;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21790</offset><text>Second preimage resistance: Given input x1 which holds Y = H(x1), it is difficult to find x2 such that it yields (x1) = H(x2);</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21925</offset><text>Collision resistance: Given two different inputs x1 and x2 (x1 ≠ x2), it is difficult to get the same output Y: H(x1) ≠ H(x2).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22064</offset><text>The cryptographic hash function has three key properties: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22123</offset><text>Therefore, it is difficult to interfere with blocks and transactions in this network. The base layer of Ethereum is its Peer-to-Peer (P2P) network architecture. In a P2P network, each workstation or node has the same privileges and responsibilities for sharing, maintaining, and utilizing resources. Furthermore, each workstation can have restrictions upon itself and control privacy and anonymity. The P2P has no dedicated central server, thus making the network decentralized. It has a flat topology, and each node can serve as both server and client at the same time. In Ethereum, the architecture is an overlay network in which nodes logically connect through the Internet. Ethereum uses DevP2P multiprotocol P2P network and extended it by Whisper protocol in order to provide P2P secure communication and Swarm protocol to provide distributed storage.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22980</offset><text>The blockchain based federated learning for DCIaaS</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23031</offset><text>The implemented federated learning in this paper is based on McMahan federated learning. Assume that there is a fixed set of K clients, each with a fixed local dataset. At the beginning of each round, a random fraction C of clients is selected, and the server, which in DCIaaS is the Ethereum-based smart contract, sends the current global algorithm state (weights and the current model parameters) to each of these clients. Each client then performs local computation based on the global state and its local dataset and sends an update to the smart contract. The smart contract then applies these updates to its global state, and the process repeats. The algorithm is applicable to any finite-sum objective of the form: in which for a deep learning problem, we take fi(w) = ℓ(xi, yi; w), meaning that loss of the prediction on example (xi, yi) calculates the model parameter w. Assume that there are K clients over which the data is partitioned, with  being the set of indexes of data points on client K, with . Therefore, the previously discussed objective can be rewritten in the form of: </text></passage><passage><infon key="file">11042_2022_12900_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24137</offset><text>FA (FedAvg) Algorithm</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24159</offset><text>If the partition  were formed by distributing the training examples over the clients uniformly at random, we would have . The above equation presents the Federated Averaging (FA) algorithm (the commonly used algorithm for federated learning), which is utilized in this paper. Here, the weight parameters for each client based on the loss values recorded across every data point are being estimated. The FA Algorithm (FedAvg) is illustrated in Fig. 2. </text></passage><passage><infon key="file">11042_2022_12900_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24612</offset><text>The illustration of LI and GI of the proposed DCIaaS framework</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24675</offset><text>This algorithm is consisted of two main loops. Figure 3 shows the implementation of this algorithm using smart contracts in DCIaaS. The red dashed lines in Fig. 3 show which steps are performed using the smart contract in a secure manner. After compiling and getting the initial weights of the designed model, the applicant shares these weights via smart contract. The IPFS can also be utilized if the weights are stored as a file and not a list of numbers or a tensor. Then, the first client will receive the initial global weights and set its local model’s weights to the global weights. Next, this client will train the model on its local data, scale the acquired local weights, and adds it to a list. This process will continue for each participant, and a list of aggregated scaled weight will be generated. This is the end of a single Local Iteration (LI). Next, this list of scaled weights of one LI will be sent to the smart contract in order to perform the final step of federated averaging and update the global model. This will be the end of a Global Iteration (GI). In the end, the applicant will receive these finalized weights. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25820</offset><text>If we assume a scenario in which an Applicant wants to train a model on data available from Data Providers (clients), first, data providers are required to provide a sample of their dataset, along with its description. This provides enough information such as its type, format, and size for the node that wants to train a model on the dataset. Next, an applicant connects to the smart contract (TrainingModel contract) via the DCIaaS web application. Then, in order to sign a contract with a data provider and send a request for training his model on their dataset, the applicant must provide a proposal of what he intends to do with the data and which programming language, library, frameworks, and resources are needed in order to train his model.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26570</offset><text>After that, the data provider will receive the request, and after accepting, the applicant will be notified. If the request is accepted, the applicant must upload his code and algorithm on IPFS. Then, he will be given a hash to this file, and the hash will be shared via the smart contract, and the data provider will receive it. At this step, the initial weights of the global model should also be shared with clients. Then, data providers (clients) will train the model, and after the model is trained, the data provider will upload all files and checkpoints on IPFS and send the hash to the applicant. The applicant will receive this hash and download the model file(s).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27244</offset><text>The last part of the proposed DCIaaS is the participants’ compensation mechanism. Although in the proposed framework, the data remains private, as the training process consumes computing resources, some clients may not be willing to participate. By introducing an encouragement mechanism in which the participants and data providers are rewarded based on their contributions, the participation rate can be improved, and more data providers might be encouraged to join the framework. This can be achieved by combining the Multi-KRUM and the reputation-based incentive protocols, in which an encouragement mechanism is designed which prevents the poisoning attack and also rewards participants properly.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27948</offset><text>In our scenario, after the local model’s weights and updates are sent to the smart contract, verifiers calculate the reputation using the Multi-KRUM algorithm and eliminate dubious updates. The verifiers, which are selected based on the VRF from miners, will remove malicious updates by executing the Multi-KRUM algorithm on updates in the received pool and accept the top majority of the updates received every GI. The verifier will add up Euclidean distances of each client c’s updates to the closest R − f − 2 updates and denote the sum as each client c’s score S(c), where R is the number of updates, and f is the number of Byzantine clients: where ∆W is the model update, and c → k indicate that Δwk belongs to the R − f − 2 nearest updates to Δwc. The R − f clients who gets the lowest scores will be chosen while rejecting the rest of the clients. The value of the reward is proportional to the client’s reputation, meaning if a client’s update is accepted by verifiers, the value of reputation is increased by one, and otherwise, it is decreased by one. Each participant is assigned with an initial reputation value γ. The γ is an integer selected form the set(0, 1, …, γmax), where γmax indicates the highest reputation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29234</offset><text>Let h denote the average reputation of all clients. If a miner verifies that a solution is correct and provides a positive evaluation, the reputation of the current client will be increased and stored on the blockchain. If a denotes the evaluation function’s output, then a = H indicates a high evaluation, while a = L indicates a low result. The update rule of the reputation γ is as follows: where h is the threshold of the selected social strategy. If a client’s reputation is h and receives an L (low) feedback after the evaluation, this client’s reputation will be decreased to 0, and the status of the reputation will be stored on the blockchain.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>29902</offset><text>DCIaaS software implementation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29933</offset><text>The smart contract (TrainingModel) manages contracts between data providers and applicants. However, before connecting to the smart contract, data providers and applicants are required to set up a crypto wallet or blockchain browser. In doing so, they will be given a unique address, which will be used to identify them in the smart contract. Moreover, in order to connect to this address in other applications, browsers, or wallets, they are given a private key or mnemonic phrase, and as long as they keep this key or phrase safe and do not share it with anyone else, the stakeholders will be safe.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30534</offset><text>In this paper, a gateway to blockchain called MetaMask browser extension is used for this task and connecting to the smart contract. MetaMask allows connectivity to the distributed web, and instead of running the full Ethereum node, it runs Ethereum decentralized applications in the browser. It should be mentioned that no additional personal information is required, and no information will be stored on a blockchain, and therefore the anonymity of users will be preserved. As for the private records, by controlling access and permissions in the smart contracts, no one other than the data provider will have access to these records.</text></passage><passage><infon key="file">11042_2022_12900_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>31171</offset><text>Overall development connection</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31202</offset><text>The address given to the applicant is unique and can only be used by the applicant. For creating the Ethereum smart contracts, Solidity ^0.5.0 programming language is used. For compiling the smart contracts, Truffle Suite is utilized, and for migrating smart contracts for development on a local blockchain and further evaluation and tests, Ganache was used and the MetaMask is connected to this blockchain. The connection between UI and blockchain is handled by an Ethereum JavaScript API named web3.js. Figure 4 shows the overall connection and relations in the deployment and test phases. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31795</offset><text>For registration in the DCIaaS, applicants can access the application and register with their unique address given by MetaMask or any other crypto wallet, and then they can see available datasets. When registering, they must choose the “Applicant” role. Data providers must register as “Data Provider.” Moreover, data providers can offer further information about themselves. For example, agencies and organization can register their name and title. After registering, they can inform visitors on their website that they are using the proposed framework, and to avoid any possible abuse, they can share their address so that applicants can be sure whom they are going to work with.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32485</offset><text>As mentioned before, access control and permissions have been handled in the smart contract. Two functions (accessPermited and accessRevoked) are utilized to handle access and permissions. These functions utilize the implicitly available msg.sender from global variable msg, which contains the address of the applicant/data provider sending the transaction. By holding this address, the IPFS hash will only be available for the person who uploaded it (Data Provider) and the person who requested it (Applicant).</text></passage><passage><infon key="file">11042_2022_12900_Fig5_HTML.jpg</infon><infon key="id">Fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32997</offset><text>The web application for applicant to view available datasets that were previously added to the smart contract</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33107</offset><text>After the smart contract was tested and its functionality evaluated, it was deployed on an online test blockchain named Kovan Testnet Network. Moreover, for uploading files on IPFS and later downloading them, an IPFS API named Infura was utilized. Figure 5 shows the web application connected to the smart contract and blockchain with two datasets added to it. By clicking on “Request,” the applicant will be redirected to a new page in which the applicant must provide a proposal of what he wants to do with the datasets and fill other requirements. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33663</offset><text>The final issue which should be considered in the implementation of the DCIaaS is the security of training the machine learning models. The security of the training process of machine learning models can be compromised as there are methods that determine whether an entity was used in the training set. For example, adversarial attacks called Member Inference, and Model Inversion can reconstruct raw input data given the model’s output. In theory, reconstructing a standard neural network to exploit input data seems unrealistic. In practice, however, there is always some real-world context which can be used to trace back the model to the input data. Publicly available datasets can also be linked to the original private and sensitive data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34410</offset><text>In order to solve this problem, different solutions exist for training machine learning models while taking the privacy of the input data in mind. An effective method is using the TensorFlow Privacy (TFP). In order to use TFP, compared to standard TensorFlow, no changes to the model architectures and training procedures are required. Instead, to train models that protect the privacy of the training data, the hyper-parameters relevant to privacy, such as optimizers, are changed. Using a TFP optimizer that clips gradients according to a defined magnitude and adds noise of a defined size, the privacy of the training data can be protected. The TFP optimizers wrap the original TensorFlow optimizers. For example, when using Adam optimizer, TFP wraps it with its differential private counterpart (DPAdamOptimizer).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>35228</offset><text>Experimental results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>35249</offset><text>DCIaaS for lung cancer classification using histopathological images</text></passage><passage><infon key="file">11042_2022_12900_Fig6_HTML.jpg</infon><infon key="id">Fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35318</offset><text>Samples of the lung cancer Histopathological images dataset</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35378</offset><text>In the first section of the experiments, the proposed framework is evaluated for medical applications. Since early diagnosis of cancer is crucial for treatment, as the first case study the proposed DCIaaS is used for lung cancer detection. The performance of the DCIaaS is compared with standard Stochastic Gradient Descent (SGD) method for training a CNN-based model on the lung cancer Histopathological images dataset. For the training, 80% of the dataset was used, and the remaining 20% were selected for the test. One sample of some of the classes of the dataset is presented in Fig. 6. </text></passage><passage><infon key="file">11042_2022_12900_Fig7_HTML.jpg</infon><infon key="id">Fig7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35971</offset><text>EfficientNetB7 architecture</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35999</offset><text>In real-world scenarios of federated learning, each federated member (Client) will own its data locally. However, in this experiment, the entire dataset is stored in one place. In this simulation, five clients were considered, and therefore, the training data was randomly batched and split into five fragments, one for each client. For this case study, the EfficientNetB7 neural network architecture, illustrated in Fig. 7, which is based on a weighted Bi-directional Feature Pyramid Network (BiFPN), was utilized for the classification task of the lung dataset. The top layers of the network were frozen, and the default initial weights were used. However, the output of the network was first fed to a GlobalAveragePooling2D layer followed by a Flatten layer, two fully connected layers of 128 and 64 neurons respectively, each activated by the ReLU function, and finally, an fully connected layer activated by Softmax acting as the final output of the architecture, classifies the dataset. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36994</offset><text>For training the models, SGD optimizer with a learning rate of 0.01 and accuracy as the metric was utilized. The global model was trained for 10 GI (epochs), meaning that the FedAvg algorithm (Fig. 2) is executed 10 times. The variable called w0 is used to hold the initial weights, which comes from the weights of the global model. Next, a list of clients with correspondent addresses and data fragments of each participant is stored in a list called St. Then the first LI is executed. A local model Keras object for the current client is created and compiled, and the local model’s weights are set to that of global weights of the ongoing GI. In the next step, the local model will train only for one epoch on the client’s data, and the acquired weights are added to a list, and the LI will be complete. For each GI, the LI is executed five times since five participants were considered for this simulation, and each client trains the model for one epoch per local iteration. After the clients trained their models locally for the current GI, it sums up all the weights acquired from the clients, takes the average, and sets the global model’s weights to this average. Finally, the performance of the current model will be tested, and the current GI will be completed.</text></passage><passage><infon key="file">11042_2022_12900_Fig8_HTML.jpg</infon><infon key="id">Fig8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38271</offset><text>Performance comparison of SGD vs DCIaaS trained models for lung cancer classification. a Accuracy, b Loss</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38377</offset><text>The performance of the federated learning-trained models is compared with a standard SGD trained model. As previously mentioned, the federated learning models were trained for 10 global epochs in total. Overall, the DCIaaS trained model for the classification of lung cancer showed better performance compared to that of the SGD trained model, which was also trained for 10 epochs. The DCIaaS trained model offered an accuracy of 96.52% on the test set, while the SGD-trained model has 95.0.% accuracy. Furthermore, the loss of the DCIaaS based trained model was 0.6012, which is lower than that of the SGD model at 0.6327. The comparison of the performance of these models trained for lung cancer classification is presented in Fig. 8. </text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>39116</offset><text>Comparison with existing works</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Reference&lt;/th&gt;&lt;th&gt;Blockchain&lt;/th&gt;&lt;th&gt;Federated Learning&lt;/th&gt;&lt;th&gt;IPFS&lt;/th&gt;&lt;th&gt;Access Control&lt;/th&gt;&lt;th&gt;Decentralized&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;DCIaaS&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Tsung-Ting et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR37&quot;&gt;37&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sheller et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR55&quot;&gt;55&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Dias et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR15&quot;&gt;15&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Zhang et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR68&quot;&gt;68&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Naz et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR47&quot;&gt;47&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Rajendran et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR53&quot;&gt;53&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Liu et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR39&quot;&gt;39&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Hasan et al. &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR24&quot;&gt;24&lt;/xref&gt;&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>39147</offset><text>Reference	Blockchain	Federated Learning	IPFS	Access Control	Decentralized	 	DCIaaS	✓	✓	✓	✓	✓	 	Tsung-Ting et al. 	✓	×	×	✓	✓	 	Sheller et al. 	×	✓	×	✓	×	 	Dias et al. 	✓	×	×	✓	×	 	Zhang et al. 	×	✓	×	×	×	 	Naz et al. 	✓	×	✓	✓	✓	 	Rajendran et al. 	×	✓	×	×	✓	 	Liu et al. 	×	✓	×	×	×	 	Hasan et al. 	✓	×	✓	×	×	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39524</offset><text>Overall, the results suggest that the DCIaaS leads to better accuracy in training this model. In order to present the advantages and contributions of this research, the characteristics of the proposed framework are compared with recently proposed methods in Table 2. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39792</offset><text>In some of these researches, the aggregation of the trained models takes place on a centralized server, which makes results vulnerable to malicious activities. In other research, the sharing of trained models and weights does not take place on a secure channel. The DCIaaS framework does not have the vulnerabilities of a federated learning-based method that may expose sensitive and personal data during their training process. Other researches that aim to eliminate problems of federated learning by utilizing blockchain technology are built around distributed systems, and the training data is still shared between miners or other individuals. This distribution of models and data can prove to be harmful to privacy. The proposed DCIaaS framework solves this issue as data holders are not required to share their data, models are trained on the data owner’s end of the blockchain, and only the trained weights are shared in a secure manner.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40738</offset><text>Modelling and management of the smart environments in Society 5.0 require a vast amount of data. The proposed DCIaaS framework has many applications for the participation of the data owners in training machine learning models for smart environments. As currently, one of the significant issues faced by the communities around the world is the COVID-19 pandemic; in this research, the application of the DCIaaS framework for training models required for management of discarded face masks which proved to be a major environmental and health hazard faced by governments during the pandemic is investigated. Previously, the applications of DCIaaS for computer aided diagnosis in COVID-19 pandemic is also investigated.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>41454</offset><text>DCIaaS for smart city management in pandemic conditions</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41510</offset><text>Training accurate machine learning models for smart city management in pandemic conditions requires the rapid and large-scale collection of data. The issue of privacy reduces the willingness of the crowd to share their data with academia and governmental agencies. In this case study, the application of DCIaaS for autonomous visual detection of littered face masks, which can act as an agent for the spread of the virus, is demonstrated.</text></passage><passage><infon key="file">11042_2022_12900_Fig9_HTML.jpg</infon><infon key="id">Fig9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>41949</offset><text>The sequence diagram of the proposed DCIaaS framework</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42003</offset><text>Litter management is one of the significant tasks that should be addressed in smart environments. The discarded face masks can lead to the possible spread of the virus through intermediary agents. In order to investigate this problem, we collected a new dataset called MaskNet (https://github.com/Tenebris97/MaskNet), in Austria and Iran during July 2020. The dataset was collected daily for seven days in cities like Steyr, Linz, Wels, and Tehran, during different times of day from 6 A.M. up to 6 P.M. from different environments such as streets, parks, riverbanks, inside buildings, and offices. The dataset consists of 1058 surgical mask images that are littered on the streets and other urban areas. We assume that there is a researcher (applicant) who wants to use this MaskNet dataset to train an object detection model which can detect masks in different environments. We assume that due to legal and privacy concerns, the dataset cannot be shared online. However, the applicant can use the proposed DCIaaS framework to train the required deep learning model without being concerned about legal and privacy issues. Using DCIaaS, the smart city management can train various required machine learning models on datasets created by citizens. Figure 9 demonstrates this scenario. </text></passage><passage><infon key="file">11042_2022_12900_Fig10_HTML.jpg</infon><infon key="id">Fig10</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>43288</offset><text>Testing the face mask detection model on Google Images</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43343</offset><text>In this scenario, we play the role of the data provider, and an applicant wants to train an object detection model using the MaskNet dataset. After the applicant sends his request and we accept it, he uploads his code on IPFS, and the hash will be stored on the smart contract after it is validated (by the smart contract). Then, we (the data provider) will receive the hash, download the file, and train the object detection model. After the training process is finished, we upload the final checkpoint and the model itself (protobuf or pb file, for example) on IPFS. Moreover, the rest of the necessary files, such as the pipeline config file, will be uploaded by us on IPFS, and their respective hash will be shared via the smart contract. Then, the applicant will receive the hashes and download the files. Now, the applicant can test the object detection model on any images that are not in dataset, as shown in Fig. 10. </text></passage><passage><infon key="file">11042_2022_12900_Fig11_HTML.jpg</infon><infon key="id">Fig11</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>44271</offset><text>Performance comparison of SGD vs DCIaaS models trained on the MaskNet dataset. a Accuracy, b Loss</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44369</offset><text>The experiments in this scenario are performed using the previously-discussed EfficientNetB7 architecture. For experimental results, the architecture was retrained using TensorFlow Object Detection API for 18,000 steps. In order to train a model on the MaskNet dataset, both using the vanilla SGD and DCIaaS based federated learning, transfer learning using the EfficientNetB7 architecture was utilized. This architecture, with approximately 66 million parameters, has presented finer results compared to other widely used architectures in the literature such as Inception, Xception, DenseNet, NASNet, and ResNet, when trained on the ImageNet. By utilizing the proposed DCIaaS framework, a federated learning model was trained on the MaskNet dataset, and its performance was compared with a standard SGD model. The DCIaaS based model was trained for 50 global epochs, 10 per client, and it offered a better performance compared to that of the SGD-trained model, also trained for 50 epochs. For the SGD trained model, the training accuracy of 94.39% was achieved. The DCIaaS trained model on the MaskNet dataset offered an accuracy of 95.83% on the test set. Moreover, the loss of the DCIaaS trained model (0.0831) was notably lower than that of the SGD trained model (1.4188). The test accuracy and test loss of the DCIaaS and SGD trained models on the MaskNet dataset are presented in Fig. 11. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45766</offset><text>It should be considered that in real-life and more complex scenarios, these contrast between the results can be higher as federated data held by distributed clients can keep samples of minority classes which allows the final data used for training the machine learning models to contain enough samples from various categories and specific cases. Another issue that should be considered is implementation of DCIaaS on the IoT. Software Defined Networks (SDN) can be utilized for management of data collection and performing distributed and decentralized machine learning.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>46337</offset><text>Formal verification of the efficiency of DCIaaS</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46385</offset><text>As the proposed framework is a software framework, the analysis of the applicability and performance of the DCIaaS is in the domain of software performance evaluation and verification of software dependability. As the proposed framework is a combination of software entities such as smart contracts on the blockchain, the formal verification of these software components can prove the dependability of the DCIaaS framework. Even though blockchain provides a secure and trusted environment for executing and storing smart contracts, these contracts are still vulnerable, and possible attacks and bugs might exploit them. Therefore, in order to solve potential issues in smart contracts, they should be verified before being deployed on a blockchain network. This verification ensures that the smart contracts will be executed according to the intended parameters.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>47248</offset><text>The formal verification of smart contracts is investigated and proven using various methods. Bai et al. introduced a formal modelling and verification method to verify the properties of smart contracts using SPIN, a model checker tool. By utilizing this tool, the authors were able to verify the correctness and necessary properties of smart contracts. For the Ethereum based smart contracts similar to the smart contracts used in DCIaaS, Yang and Lei proposed a formal symbolic process for verifying the reliability and security of Ethereum smart contracts using a formal proof management tool named Coq proof assistant. Abdellatif et al. proposed formal modelling and verification of smart contracts based on the users’ behaviour on a blockchain network, which verifies a smart contract’s behaviours in its execution environment. Beillahi et al. proposed an automated method for verifying smart contracts based on the functional properties of a smart contract. Even a standard runtime verification approach can be utilized to support the dependability and correctness of smart contracts during their runtime.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48363</offset><text>In the Solidity verification tool, which is used in DCIaaS, a Satisfiability Modulo Theories (SMT) based formal verification module is used for verification of the smart contracts. This verification tool is integrated into the Solidity compiler, and during compilation, warns for potential fails. Similarly, Solv-verify is a source-level verification tool that is built on top of the Solidity compiler and automates Solidity smart contracts’ verification based on SMT solvers. For the general verifiability of the smart contracts, by utilizing the Markov decision process (MDP) and game theory, it is proven that smart contracts are verifiable.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>49010</offset><text>Conclusions and future work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49038</offset><text>In this paper, a blockchain-based federated learning framework for privacy-preserving machine learning, called DCIaaS, is proposed. In the proposed decentralized blockchain-based framework, the models will be trained on the data provider’s side and off-chain using federated learning, and only the learned model parameters and weights will be shared on the blockchain and using the smart contracts. Experimental results show an increase in accuracy of the models trained using the DCIaaS framework compared to decentralized training. As a case study, the DCIaaS framework is utilized for medical and smart city applications related to Society 5.0. For the future work of the proposed framework, decentralized agent-based modelling will be implemented. Current simulation models for autonomous vehicles, drones, and robots extensively rely on centralized models. However, such an approach can target security and privacy. A blockchain-based and agent-based simulator for smart cities, which considers the communication between agents through smart contracts, can address this issue. This decentralized agent-based model can use privacy-preserved data to model complex scenarios more accurately. The further expansion of DCIaaS can include agent-based modelling using a decentralized blockchain network.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>50342</offset><text>Publisher’s note</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>50361</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title</infon><offset>50480</offset><text>Declarations</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>50493</offset><text>Conflict of interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>50514</offset><text>The authors declare that they have no conflict of interest.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>50574</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50585</offset><text>Abbasi MH et al (2019) Deep visual privacy preserving for internet of robotic things. In: 2019 5th conference on knowledge based engineering and innovation (KBEI). IEEE</text></passage><passage><infon key="name_0">surname:Abdellatif;given-names:T</infon><infon key="name_1">surname:Brousmiche;given-names:K-L</infon><infon key="section_type">REF</infon><infon key="source">2018 9th IFIP international conference on new technologies, mobility and security (NTMS)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>50754</offset><text>Formal verification of smart contracts based on users and blockchain behaviors models</text></passage><passage><infon key="fpage">140699</infon><infon key="lpage">140725</infon><infon key="name_0">surname:Aledhari;given-names:M</infon><infon key="name_1">surname:Razzak;given-names:R</infon><infon key="name_2">surname:Parizi;given-names:RM</infon><infon key="name_3">surname:Saeed;given-names:F</infon><infon key="pub-id_doi">10.1109/ACCESS.2020.3013541</infon><infon key="pub-id_pmid">32999795</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>50840</offset><text>Federated learning: a survey on enabling technologies, protocols, and applications</text></passage><passage><infon key="name_0">surname:Alt;given-names:L</infon><infon key="name_1">surname:Reitwießner;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">International symposium on leveraging applications of formal methods</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>50923</offset><text>SMT-based verification of solidity smart contracts</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50974</offset><text>Bai X et al 2018 Formal modeling and verification of smart contracts. In: Proceedings of the 2018 7th International Conference on Software and Computer Applications</text></passage><passage><infon key="fpage">723</infon><infon key="issue">8</infon><infon key="name_0">surname:Baldominos;given-names:A</infon><infon key="name_1">surname:Saez;given-names:Y</infon><infon key="pub-id_doi">10.3390/e21080723</infon><infon key="section_type">REF</infon><infon key="source">Entropy</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>51139</offset><text>Coin. AI: A proof-of-useful-work scheme for blockchain-based distributed deep learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51226</offset><text>Beillahi SM et al (2020) Behavioral simulation for smart contracts. In: Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</text></passage><passage><infon key="fpage">142</infon><infon key="lpage">161</infon><infon key="name_0">surname:Bigi;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">Programming languages with applications to biology and security</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>51395</offset><text>Validation of decentralised smart contracts through game theory and formal methods</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51478</offset><text>Blanchard P et al (2017) Machine learning with adversaries: Byzantine tolerant gradient descent. In: Proceedings of the 31st International Conference on Neural Information Processing Systems</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51669</offset><text>Borkowski AA et al (2019) Lung and colon cancer histopathological image dataset (lc25000). arXiv preprint arXiv:1912.12142</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="lpage">36</infon><infon key="name_0">surname:Brakerski;given-names:Z</infon><infon key="name_1">surname:Gentry;given-names:C</infon><infon key="name_2">surname:Vaikuntanathan;given-names:V</infon><infon key="pub-id_doi">10.1145/2633600</infon><infon key="section_type">REF</infon><infon key="source">ACM Trans Comput Theory</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2014</infon><offset>51792</offset><text>(Leveled) fully homomorphic encryption without bootstrapping</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51853</offset><text>Briggs C, Fan Z, Andras P (2021) A review of privacy-preserving federated learning for the internet-of-things. Federated Learning Systems, p 21–50</text></passage><passage><infon key="fpage">59</infon><infon key="lpage">67</infon><infon key="name_0">surname:Brisimi;given-names:TS</infon><infon key="name_1">surname:Chen;given-names:R</infon><infon key="name_2">surname:Mela;given-names:T</infon><infon key="name_3">surname:Olshevsky;given-names:A</infon><infon key="name_4">surname:Paschalidis;given-names:IC</infon><infon key="name_5">surname:Shi;given-names:W</infon><infon key="pub-id_doi">10.1016/j.ijmedinf.2018.01.007</infon><infon key="pub-id_pmid">29500022</infon><infon key="section_type">REF</infon><infon key="source">Int J Med Inform</infon><infon key="type">ref</infon><infon key="volume">112</infon><infon key="year">2018</infon><offset>52002</offset><text>Federated learning of predictive models from federated electronic health records</text></passage><passage><infon key="fpage">139</infon><infon key="lpage">162</infon><infon key="name_0">surname:Cabrero-Holgueras;given-names:J</infon><infon key="name_1">surname:Pastrana;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">Proc Priv Enh Technol</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2021</infon><offset>52083</offset><text>SoK: privacy-preserving computation techniques for deep learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52148</offset><text>Dias JP et al (2018) Blockchain for access control in e-health scenarios. arXiv preprint arXiv:1805.12267</text></passage><passage><infon key="fpage">211</infon><infon key="issue">3–4</infon><infon key="lpage">407</infon><infon key="name_0">surname:Dwork;given-names:C</infon><infon key="name_1">surname:Roth;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Found Trends Theor Comput Sci</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>52254</offset><text>The algorithmic foundations of differential privacy</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52306</offset><text>Entriken W. Introduction to smart contracts. [cited 2020 November 30]; Available from: https://ethereum.org/en/developers/docs/smart-contracts/.</text></passage><passage><infon key="fpage">10354</infon><infon key="lpage">10368</infon><infon key="name_0">surname:Fadaeddini;given-names:A</infon><infon key="name_1">surname:Majidi;given-names:B</infon><infon key="name_2">surname:Eshghi;given-names:M</infon><infon key="pub-id_doi">10.1007/s11227-020-03251-9</infon><infon key="section_type">REF</infon><infon key="source">J Supercomput</infon><infon key="type">ref</infon><infon key="volume">76</infon><infon key="year">2020</infon><offset>52451</offset><text>Secure decentralized peer-to-peer training of deep neural networks based on distributed ledger technology</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52557</offset><text>Ganache. [cited 2020 December 17]; Available from: https://www.trufflesuite.com/ganache.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52646</offset><text>Ge S et al (2020) Fedner: Privacy-preserving medical named entity recognition with federated learning. arXiv preprint arXiv:2003.09288</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52781</offset><text>Gilad Y et al ( 2017) Algorand: scaling byzantine agreements for cryptocurrencies. In: Proceedings of the 26th symposium on operating systems principles</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52934</offset><text>Goel A et al (2019) DeepRing: protecting deep neural network with blockchain. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops</text></passage><passage><infon key="name_0">surname:Hajdu;given-names:Á</infon><infon key="name_1">surname:Jovanović;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Working conference on verified software: theories, tools, and experiments</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>53104</offset><text>Solc-verify: a modular verifier for solidity smart contracts</text></passage><passage><infon key="fpage">65439</infon><infon key="lpage">65448</infon><infon key="name_0">surname:Hasan;given-names:HR</infon><infon key="name_1">surname:Salah;given-names:K</infon><infon key="pub-id_doi">10.1109/ACCESS.2018.2876971</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2018</infon><offset>53165</offset><text>Proof of delivery of digital assets using blockchain and smart contracts</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53238</offset><text>Hitaj B, Ateniese G, Perez-Cruz F (2017) Deep models under the GAN: information leakage from collaborative deep learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53450</offset><text>Infura. [cited 2020 December 15]; Available from: https://infura.io/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53520</offset><text>Kim JMJ (2014) Stellar. Available from: https://www.stellar.org/</text></passage><passage><infon key="fpage">305</infon><infon key="issue">6</infon><infon key="lpage">311</infon><infon key="name_0">surname:Kaissis;given-names:GA</infon><infon key="name_1">surname:Makowski;given-names:MR</infon><infon key="name_2">surname:Rückert;given-names:D</infon><infon key="name_3">surname:Braren;given-names:RF</infon><infon key="pub-id_doi">10.1038/s42256-020-0186-1</infon><infon key="section_type">REF</infon><infon key="source">Nat Mach Intell</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2020</infon><offset>53585</offset><text>Secure, privacy-preserving and federated machine learning in medical imaging</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53662</offset><text>Karki D. Can you guess how much data is generated every day? [cited 2021 Januray 7]; Available from: https://www.takeo.ai/can-you-guess-how-much-data-is-generated-every-day/</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53836</offset><text>Kasiviswanathan SP, Smith A (2014) On the'semantics' of differential privacy: a bayesian formulation. J Priv Confid 6(1)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53957</offset><text>Keccak. Keccak Team. [cited 2021 August 28th]; Available from: https://keccak.team/keccak.html.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54053</offset><text>Keydana. RStudio AI Blog: Hacking deep learning: model inversion attack by example. [cited 2020 December 14]; Available from: https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/.</text></passage><passage><infon key="fpage">1759</infon><infon key="issue">3</infon><infon key="lpage">1799</infon><infon key="name_0">surname:Khan;given-names:LU</infon><infon key="name_1">surname:Saad;given-names:W</infon><infon key="name_2">surname:Han;given-names:Z</infon><infon key="name_3">surname:Hossain;given-names:E</infon><infon key="name_4">surname:Hong;given-names:CS</infon><infon key="pub-id_doi">10.1109/COMST.2021.3090430</infon><infon key="section_type">REF</infon><infon key="source">IEEE Commun Surv Tutor</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2021</infon><offset>54259</offset><text>Federated learning for internet of things: recent advances, taxonomy, and open challenges</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54349</offset><text>Konečný J et al (2016) Federated optimization: distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54487</offset><text>Kovan Testnet. [cited 2020 December 15]; Available from: https://kovan-testnet.github.io/website/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54586</offset><text>Kumar R et al (2021) Blockchain based privacy-preserved federated learning for medical images: a case study of COVID-19 CT scans</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54715</offset><text>Kuo T-T, Ohno-Machado L (2018) Modelchain: decentralized privacy-preserving healthcare predictive modeling framework on private blockchain networks. arXiv preprint arXiv:1802.01746</text></passage><passage><infon key="fpage">50</infon><infon key="issue">3</infon><infon key="lpage">60</infon><infon key="name_0">surname:Li;given-names:T</infon><infon key="name_1">surname:Sahu;given-names:AK</infon><infon key="name_2">surname:Talwalkar;given-names:A</infon><infon key="name_3">surname:Smith;given-names:V</infon><infon key="pub-id_doi">10.1109/MSP.2020.2975749</infon><infon key="section_type">REF</infon><infon key="source">IEEE Signal Process Mag</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2020</infon><offset>54896</offset><text>Federated learning: challenges, methods, and future directions</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54959</offset><text>Liu B et al (2020) Experiments of federated learning for covid-19 chest x-ray images. arXiv preprint arXiv:2007.05592</text></passage><passage><infon key="fpage">471</infon><infon key="lpage">491</infon><infon key="name_0">surname:Majidi;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Enabling AI applications in data science</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>55077</offset><text>Geo-spatiotemporal intelligence for smart agricultural and environmental eco-cyber-physical systems</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55177</offset><text>Mallaki M, Majidi B, Peyvandi A, Movaghar A (2021) Off-chain management and state-tracking of smart programs on blockchain for secure and efficient decentralized computation. Int J Comput Appl:1–8</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55376</offset><text>McMahan B et al (2017) Communication-efficient learning of deep networks from decentralized data. In artificial intelligence and statistics. PMLR</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55522</offset><text>McMahan HB et al (2018) A general approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55662</offset><text>Melis L et al (2018) Inference attacks against collaborative learning. arXiv preprint arXiv:1805.04049 13</text></passage><passage><infon key="name_0">surname:Menezes;given-names:AJ</infon><infon key="name_1">surname:Van Oorschot;given-names:PC</infon><infon key="name_2">surname:Vanstone;given-names:SA</infon><infon key="section_type">REF</infon><infon key="source">Handbook of applied cryptography</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>55768</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55769</offset><text>MetaMask. [cited 2020 December 17]; Available from: https://metamask.io/.</text></passage><passage><infon key="fpage">7054</infon><infon key="issue">24</infon><infon key="name_0">surname:Naz;given-names:M</infon><infon key="name_1">surname:al-zahrani;given-names:FA</infon><infon key="name_2">surname:Khalid;given-names:R</infon><infon key="name_3">surname:Javaid;given-names:N</infon><infon key="name_4">surname:Qamar;given-names:AM</infon><infon key="name_5">surname:Afzal;given-names:MK</infon><infon key="name_6">surname:Shafiq;given-names:M</infon><infon key="pub-id_doi">10.3390/su11247054</infon><infon key="section_type">REF</infon><infon key="source">Sustainability</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2019</infon><offset>55843</offset><text>A secure data sharing platform using blockchain and interplanetary file system</text></passage><passage><infon key="fpage">201</infon><infon key="issue">1</infon><infon key="lpage">218</infon><infon key="name_0">surname:Nguyen;given-names:HT</infon><infon key="name_1">surname:Sehwag;given-names:V</infon><infon key="name_2">surname:Hosseinalipour;given-names:S</infon><infon key="name_3">surname:Brinton;given-names:CG</infon><infon key="name_4">surname:Chiang;given-names:M</infon><infon key="name_5">surname:Vincent Poor;given-names:H</infon><infon key="pub-id_doi">10.1109/JSAC.2020.3036952</infon><infon key="section_type">REF</infon><infon key="source">IEEE J Sel Areas Commun</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2021</infon><offset>55922</offset><text>Fast-convergent federated learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55957</offset><text>Norouzi A, Majidi B, Movaghar A (2018) Reliable and energy-efficient routing for green software defined networking. In: 2018 9th international symposium on telecommunications (IST). IEEE</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56144</offset><text>Peyvandi A et al (2021) Computer-aided-diagnosis as a service on decentralized medical cloud for efficient and rapid emergency response intelligence. N Gener Comput:1–24</text></passage><passage><infon key="fpage">405</infon><infon key="lpage">424</infon><infon key="name_0">surname:Peyvandi;given-names:A</infon><infon key="name_1">surname:Majidi;given-names:B</infon><infon key="name_2">surname:Peyvandi;given-names:S</infon><infon key="name_3">surname:Kose;given-names:U</infon><infon key="section_type">REF</infon><infon key="source">Computational intelligence for covid-19 and future pandemics: emerging applications and strategies</infon><infon key="type">ref</infon><infon key="year">2022</infon><offset>56316</offset><text>Blockchain-based secure biomedical data-as-a-service for effective internet of health things enabled epidemic management</text></passage><passage><infon key="fpage">1333</infon><infon key="issue">5</infon><infon key="lpage">1345</infon><infon key="name_0">surname:Phong;given-names:LT</infon><infon key="name_1">surname:Aono;given-names:Y</infon><infon key="name_2">surname:Hayashi;given-names:T</infon><infon key="name_3">surname:Wang;given-names:L</infon><infon key="name_4">surname:Moriai;given-names:S</infon><infon key="pub-id_doi">10.1109/TIFS.2017.2787987</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Inf Forensics Secur</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2018</infon><offset>56437</offset><text>Privacy-preserving deep learning via additively homomorphic encryption</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">11</infon><infon key="name_0">surname:Rajendran;given-names:S</infon><infon key="name_1">surname:Obeid;given-names:JS</infon><infon key="name_2">surname:Binol;given-names:H</infon><infon key="name_3">surname:D Agostino R Jr</infon><infon key="name_4">surname:Foley;given-names:K</infon><infon key="name_5">surname:Zhang;given-names:W</infon><infon key="name_6">surname:Austin;given-names:P</infon><infon key="name_7">surname:Brakefield;given-names:J</infon><infon key="name_8">surname:Gurcan;given-names:MN</infon><infon key="name_9">surname:Topaloglu;given-names:U</infon><infon key="pub-id_doi">10.1200/CCI.20.00060</infon><infon key="pub-id_pmid">33411624</infon><infon key="section_type">REF</infon><infon key="source">JCO Clin Cancer Inform</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2021</infon><offset>56508</offset><text>Cloud-based federated learning implementation across medical centers</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56577</offset><text>Shayan M et al (2018) Biscotti: A ledger for private and secure peer-to-peer machine learning. arXiv preprint arXiv:1811.09904</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:Sheller;given-names:MJ</infon><infon key="name_1">surname:Edwards;given-names:B</infon><infon key="name_10">surname:Bakas;given-names:S</infon><infon key="name_2">surname:Reina;given-names:GA</infon><infon key="name_3">surname:Martin;given-names:J</infon><infon key="name_4">surname:Pati;given-names:S</infon><infon key="name_5">surname:Kotrotsou;given-names:A</infon><infon key="name_6">surname:Milchenko;given-names:M</infon><infon key="name_7">surname:Xu;given-names:W</infon><infon key="name_8">surname:Marcus;given-names:D</infon><infon key="name_9">surname:Colen;given-names:RR</infon><infon key="pub-id_doi">10.1038/s41598-020-69250-1</infon><infon key="pub-id_pmid">31913322</infon><infon key="section_type">REF</infon><infon key="source">Sci Rep</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2020</infon><offset>56704</offset><text>Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56813</offset><text>Shokri R, Shmatikov V (2015) Privacy-preserving deep learning. In: Proceedings of the 22nd ACM SIGSAC conference on computer and communications security</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56966</offset><text>Shokri R et al (2017) Membership inference attacks against machine learning models. In: 2017 IEEE symposium on security and privacy (SP). IEEE</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57109</offset><text>Solidity. [cited 2020 December 17]; Available from: https://docs.soliditylang.org/en/v0.5.0/resources.html.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57217</offset><text>Song C, Ristenpart T, Shmatikov V (2017) Machine learning models that remember too much. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57396</offset><text>Tan M, Le Q (2019) Efficientnet: rethinking model scaling for convolutional neural networks. In: International conference on machine learning. PMLR</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57544</offset><text>Truffle Suite. [cited 2020 December 17]; Available from: https://www.trufflesuite.com/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57632</offset><text>web3.js. [cited 2020 December 17]; Available from: https://web3js.readthedocs.io/en/v1.3.0/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57725</offset><text>Weng J et al (2019) Deepchain: Auditable and privacy-preserving deep learning with blockchain-based incentive. IEEE Trans Dependable Secure Comput</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57872</offset><text>Yang Z, Lei H (2018) Formal process virtual machine for smart contracts verification. arXiv preprint arXiv:1805.00808</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57990</offset><text>Zhang Y, Van der Schaar M (2012) Reputation-based incentive protocols in crowdsourcing applications. In: 2012 proceedings IEEE INFOCOM. IEEE</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>58131</offset><text>Zhang Y et al (2020) The secret revealer: generative model-inversion attacks against deep neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</text></passage><passage><infon key="fpage">106775</infon><infon key="name_0">surname:Zhang;given-names:C</infon><infon key="name_1">surname:Xie;given-names:Y</infon><infon key="name_2">surname:Bai;given-names:H</infon><infon key="name_3">surname:Yu;given-names:B</infon><infon key="name_4">surname:Li;given-names:W</infon><infon key="name_5">surname:Gao;given-names:Y</infon><infon key="pub-id_doi">10.1016/j.knosys.2021.106775</infon><infon key="section_type">REF</infon><infon key="source">Knowl-Based Syst</infon><infon key="type">ref</infon><infon key="volume">216</infon><infon key="year">2021</infon><offset>58324</offset><text>A survey on federated learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>58355</offset><text>Zhang W, Zhou T, Lu Q, Wang X, Zhu C, Sun H, Wang Z, Lo SK, Wang FY (2021) Dynamic Fusion-based Federated Learning for COVID-19 Detection. IEEE Internet Things J:1</text></passage><passage><infon key="fpage">1817</infon><infon key="issue">3</infon><infon key="lpage">1829</infon><infon key="name_0">surname:Zhao;given-names:Y</infon><infon key="name_1">surname:Zhao;given-names:J</infon><infon key="name_2">surname:Jiang;given-names:L</infon><infon key="name_3">surname:Tan;given-names:R</infon><infon key="name_4">surname:Niyato;given-names:D</infon><infon key="name_5">surname:Li;given-names:Z</infon><infon key="name_6">surname:Lyu;given-names:L</infon><infon key="name_7">surname:Liu;given-names:Y</infon><infon key="pub-id_doi">10.1109/JIOT.2020.3017377</infon><infon key="section_type">REF</infon><infon key="source">IEEE Internet Things J</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>58519</offset><text>Privacy-preserving blockchain-based federated learning for IoT devices</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>58590</offset><text>Zhou J et al (2021) A survey on federated learning and its applications for accelerating industrial internet of things. arXiv preprint arXiv:2104.10501</text></passage><passage><infon key="fpage">7</infon><infon key="lpage">16</infon><infon key="name_0">surname:Zhu;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Differential privacy and applications</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>58742</offset><text>Preliminary of differential privacy</text></passage></document></collection>
