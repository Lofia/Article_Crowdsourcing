<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220405</date><key>pmc.key</key><document><id>8967349</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/frai.2022.830299</infon><infon key="article-id_pmc">8967349</infon><infon key="article-id_pmid">35372829</infon><infon key="elocation-id">830299</infon><infon key="kwd">map knowledge graph crowdsourcing class-level attributes common sense knowledge acquisition</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Welty;given-names:Chris</infon><infon key="name_1">surname:Aroyo;given-names:Lora</infon><infon key="name_2">surname:Korn;given-names:Flip</infon><infon key="name_3">surname:McCarthy;given-names:Sara M.</infon><infon key="name_4">surname:Zhao;given-names:Shubin</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">5</infon><infon key="year">2022</infon><offset>0</offset><text>Addressing Label Sparsity With Class-Level Common Sense for Google Maps</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>72</offset><text>Successful knowledge graphs (KGs) solved the historical knowledge acquisition bottleneck by supplanting the previous expert focus with a simple, crowd-friendly one: KG nodes represent popular people, places, organizations, etc., and the graph arcs represent common sense relations like affiliations, locations, etc. Techniques for more general, categorical, KG curation do not seem to have made the same transition: the KG research community is still largely focused on logic-based methods that belie the common-sense characteristics of successful KGs. In this paper, we propose a simple yet novel three-tier crowd approach to acquiring class-level attributes that represent broad common sense associations between categories, and can be used with the classic knowledge-base default &amp; override technique, to address the early label sparsity problem faced by machine learning systems for problems that lack data for training. We demonstrate the effectiveness of our acquisition and reasoning approach on a pair of very real industrial-scale problems: how to augment an existing KG of places and offerings (e.g. stores and products, restaurants and dishes) with associations between them indicating the availability of the offerings at those places. Label sparsity is a general problem, and not specific to these use cases, that prevents modern AI and machine learning techniques from applying to many applications for which labeled data is not readily available. As a result, the study of how to acquire the knowledge and data needed for AI to work is as much a problem today as it was in the 1970s and 80s during the advent of expert systems. Our approach was a critical part of enabling a worldwide local search capability on Google Maps, with which users can find products and dishes that are available in most places on earth.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1902</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1918</offset><text>From the outset, knowledge graphs (KGs) have prominently used crowdsourcing for knowledge acquisition, both from the perspective of scaling out graph creation and long-term maintenance, solving the historical knowledge acquisition bottleneck by revisiting the expert systems assumption that knowledge should be acquired from experts. As a result, popular KGs like Freebase (Bollacker et al.,)—now Google's Knowledge Graph—and WikiData (Vrandečić and Krötzsch,) are composed primarily of popular “common sense” entities and relations in the world that people are exposed to regularly and that can be acquired from and validated by the crowd.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2570</offset><text>Similarly, today Google Maps overlays data on maps about the different places or establishments (stores, restaurants, hospitals, etc.) worldwide, and crowdsourcing plays a central role in the acquisition and maintenance of this information, as discussed in Lagos et al.. Users contribute opening hours, locations, reviews, etc., as well as categorical information about places such as whether it is a supermarket, department store, etc., which makes KGs a natural representation for this information.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3071</offset><text>Despite such heavy and widespread success of KGs for representing entities in the world and their properties, Taylor points out that there has not been much attention paid in the research community to class-level attributes in KGs: graph edges between nodes that represent categorical terms, what they might mean and how to acquire them. For the purposes of this paper we use the words type, category, class interchangeably, as well as attribute, property, relation. Practical and industrial KG edges remain almost exclusively at the instance level (e.g., McDonalds serves Big Mac), and a few KGs may encode class-level domain/range constraints (e.g., Restaurants serve Food), but no KG includes attributes of classes that represent our common-sense knowledge about them (e.g., Burger Joints serve burgers). There has certainly been a lot of research published in the sub-fields of Knowledge Representation on axiomatic knowledge acquisition, for example Ji et al., but these methods are not well-suited for crowdsourcing and have not made the transition to any industrial KG settings.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4157</offset><text>In this paper we explore the question of acquiring common sense class-level attributes from the crowd and applying those attributes effectively with other sources of information to solve a knowledge-base completion (KBC) problem, as defined in Bordes et al., where success is measured by the precision and recall of graph edges. We take a particular problem, that of understanding what is offered at each establishment on earth. Such a KG could be used to answer questions like, “Where can I buy an umbrella nearby?” (see Figure 1), “Where can I eat lamyun?”, or “Where can I get a flu shot?”, etc. We call this problem local offerings and it is one that is of interest to search engines like Google.1</text></passage><passage><infon key="file">frai-05-830299-g0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>4871</offset><text>Google Maps local shopping search results for umbrellas in NYC shows stores that sell them.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4963</offset><text>Local offerings, compared to on-line, poses a significant practical knowledge acquisition problem because real-world transactions do not occur on-line or the data is heavily siloed, and therefore data about what products are being sold at what stores, or what dishes are served at what restaurants, is not broadly available; it is a sort of “dark matter” of the web—we know it's there but can't directly observe it. Although it may seem familiar to us—e.g., brick and mortar shops that support on-line ordering and in-store pickup—such exceptions are actually quite rare, by the numbers. Less than 30% of stores worldwide having a website and even fewer that include a product catalog.2</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5660</offset><text>Indeed, our data shows that web pages and merchant feeds account for less than 0.001% of the total matrix of products at stores. To address this shortage of web information, we harness the crowd in three tiers: users around the world who have visited stores and voluntarily provide instance-level product availability (e.g., Ajay Mittal Dairy sells Milk); a much smaller set of paid raters who curate class-level attributes connecting common sense store and product categories (e.g., Grocery Stores sell Milk); and a very small set of paid operators who call stores to confirm the instance-level associations as evaluation ground-truth labels. The intuition behind this combination is that a lot of the instance-level associations are obviously true or false at the categorical level, and that acquiring knowledge at that level can jump-start the instance-level acquisition and help it be more productive: don't waste a user's efforts answering about milk or asphalt at an individual grocery store when simple common sense tells us the answer. Due to the prominence of common sense curation in our approach, we call the project CrowdSense (CS).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6805</offset><text>To our knowledge, acquiring class-level attributes from the crowd in order to jump-start a KBC problem has not been attempted before, and there are very few examples of KBC problems at this scale (tens of millions of stores wordwide and more than 10k products). The project and approach led to a successful worldwide launch of local shopping results overlaid on Google Maps, and involved many complexities beyond the scope of this paper, including more than 2 years of data collection at a worldwide scale. Due to this complexity and scope, we focus here on the real-world knowledge acquisition aspect of the work, and present a few simplified experiments that demonstrate how the acquired knowledge can be used for KBC. The contributions of this paper are primarily:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7573</offset><text>To demonstrate that class-level bipartite knowledge acquisition can be effective in approximating instance-level knowledge (Section 5.5) as a solution to label sparsity;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7743</offset><text>A crowdsourcing approach to acquire such class-level knowledge for the local shopping problem (Section 5.4);</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7852</offset><text>Experimental results that show the effective combination of class- and instance- level knowledge from various sources used in the launched system (Section 6.3).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8013</offset><text>The approach has generalized to other bipartite relations between places and types of entities that are organized in a taxonomy, such as dishes at restaurants, services at professional offices, etc., as well as a wide range of other bipartite graph problems where common sense or categorical knowledge prevails as defaults, such as ingredients for dishes, linnean taxonomies of living creatures, etc.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>8414</offset><text>2. Formalization</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8431</offset><text>We start with an initial knowledge graph , where  forms the set of all categories, partitioned into place  and offering  categories (e.g., hardware-store, power-tools, resp.), and  the set of all place instances (i.e., the establishments such as stores and restaurants themselves). The edges of the graph are the class/instance (also known as type) relation between place instances and place categories , and the subclass relation  with a disjointness constraint</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8894</offset><text>so that the relation is only defined over pairs of categories belonging to the same type. Lastly each of these primitive sets are disjoint , making  tripartite. As usual,  forms a partial order within each (place and offering) category partition, and is transitive over the subcategory relation so that . This is meant to capture a traditional kind of knowledge-graph scenario.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9272</offset><text>Problem 1. The local offerings problem is the extension of  to  through the addition of the class-level offering availability relation  and the instance-level offering availability relation .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9464</offset><text>The place instances  represent individual physical places like Trader Joe's at 142 14th St. (TJ142), each of which is typed with some number of place categories  like Supermarket. The offering categories  represent the types of offerings at all places, such as Milk or Dairy, so that  means that particular Trader Joe's sells Milk. Note that a more complete definition of the local shopping problem would include the extension of  to instances (i.e., place inventory), but we do not have access to that data, and use this definition as a simplification that serves to answer most local offering queries. A simple example is shown in Figure 2, showing four categorical graph nodes and one instance node, with each of the relation types shown as edges.</text></passage><passage><infon key="file">frai-05-830299-g0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>10215</offset><text>Example subset of graph  with a place instance ip, a place category cp, its parent category , a offering category co, its parent  and the class- and instance- level offering availability relations between them.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10426</offset><text>This simplification is best understood as a matrix  representing , where Ri,j are observations (or predictions) that place i offers j. With enough observed Ri,j, collaborative filtering methods (e.g., matrix factorization) can be exploited to predict unobserved values from observed ones. Moving between matrix and graph representation can be done in a variety of ways, such as thresholding matrix values into discrete edges in , or using a graph formalism that supports confidence values on edges, as described in Noy et al..</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10953</offset><text>We argue that the real world grounding of the  association in people's everyday experience allows us to exploit meaningful common sense categorical knowledge for the problem of acquiring the edges in , and use simple defeasable methods to then infer the edges in the graph for the relation .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>11245</offset><text>3. Vocabulary</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11259</offset><text>The local offerings system and all the experiments described in this paper use the open Google My Business (GMB) categories3,4 for place categories () and Google Product Taxonomy5,6 for the offering categories (). Each set comes with a taxonomic structure that we encode as the  relation, every category has at least one parent category with the exception of the top-level (most general) categories, and a few categories have multiple parents.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11703</offset><text>This project began with shopping and was extended to dining by adding a number of dishes to . These dishes are from Google's KG, and most of them can be found in Freebase under the type /food/dish. The restaurant categories are already part of the GMB set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11960</offset><text>There are roughly 15k products categories in , that are similar in semantics to UPCs (Universal Product Code, the bar codes on most packaged products), grounding out in 19 top-level categories. There are roughly 10k dishes in , that are similar to menu items, with very little taxonomic structure. The GMB categories that comprise in  include many that are unrelated to local shopping or dining, so we restrict () to those below store and restaurant, resulting in roughly 3k with those two roots.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12457</offset><text>These taxonomies have different graphical structure: the product taxonomy is fairly deep, and the place taxonomy is fairly shallow, yet they align surprisingly well. For example, there is a deep taxonomy of products under “Grocery,” and a store category “Grocery Store.” There are a few misalignments, for example “Batteries” are under “Electronics” but are sold at “Drugstores.” A few of these misalignments are ameliorated by hybrid categories like “Household products,” which is an additional ancestor for “Batteries.” The food taxonomy we used from Freebase is nearly flat, making for an interesting comparison on the usefulness of a good taxonomy. Note that we do not change the taxonomies or memberships; as defined in Section 2, we treat the initial graph  as given.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13259</offset><text>Finally, Google Maps has tens of millions of establishments worldwide that form the set of places ; each has a category label which is displayed in the maps UI under the place name and user rating, giving us the edges in . A large part of these labels are assigned by merchants, some by users, some by operators and others by machine automation. These labels are generally high quality, with precision over 0.8. The largest source of inaccuracies are store labels that are more general than they need to be, when a more appropriate category exists. The labeling infrastructure requires a single “primary” category, while many places could be categorized in several ways. A Glossary of terms defined in this paper has been provided in Table 1.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>14006</offset><text>Glossary of terms.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Terms&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Place&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;An establishment (store or restaurant) on Google Maps&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Offering&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;A product or dish available at a place&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;KBC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Knowledge Base Completion&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMB&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google my Business (source store categories)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GPT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google Product Taxonomy (source product categories)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;UGC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User Generated Content–user responses to yes/no questions&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CS&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crowd Sense, our approach&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WebIE&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Information extraction of offering names from place web pages&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WALS&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Matrix factorization using WALS to predict 〈&lt;italic&gt;i&lt;sub&gt;p&lt;/sub&gt;, c&lt;sub&gt;o&lt;/sub&gt;&lt;/italic&gt;〉 pairs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Knowledge graph&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M42&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set of place instances&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M43&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set of place categories&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M44&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set of offering categories&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M45&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;〈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;′&lt;/mml:mo&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;〉&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Place subclass/superclass relation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M46&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;〈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mo&gt;′&lt;/mml:mo&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;〉&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Offering subclass/superclass relation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M47&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;〈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;〉&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Place instance/class type relation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M48&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;〈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;〉&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Class-level offering @ place availability relation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M49&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;〈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;〉&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Instance-level offering @ place availability relation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M50&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;G&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;′&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Base KG of place/offering classes and place instances&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;
&lt;mml:math id=&quot;M51&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;G&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;
&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;inline-formula&gt;&lt;mml:math id=&quot;M52&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;G&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;′&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt; extended with &lt;inline-formula&gt;&lt;mml:math id=&quot;M53&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;mml:math id=&quot;M54&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;R&lt;sub&gt;&lt;italic&gt;i,j&lt;/italic&gt;&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Likelihood that place instance &lt;italic&gt;i&lt;/italic&gt; sells offering class &lt;italic&gt;j&lt;/italic&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Crowd task&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;w&lt;/italic&gt;
&lt;sub&gt;
&lt;italic&gt;x,o&lt;/italic&gt;
&lt;/sub&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rater score for place (class or instance) x and offering class o&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;α&lt;sub&gt;&lt;italic&gt;c,o&lt;/italic&gt;&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of “always” answers for class-level pair 〈&lt;italic&gt;c, o&lt;/italic&gt;〉&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ν&lt;sub&gt;&lt;italic&gt;c,o&lt;/italic&gt;&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of “never answers for class-level pair 〈&lt;italic&gt;c, o&lt;/italic&gt;〉&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;y&lt;/italic&gt;
&lt;sub&gt;
&lt;italic&gt;i,o&lt;/italic&gt;
&lt;/sub&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of “yes” answers for instance-level pair 〈&lt;italic&gt;i, o&lt;/italic&gt;〉&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;n&lt;/italic&gt;
&lt;sub&gt;
&lt;italic&gt;i,o&lt;/italic&gt;
&lt;/sub&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of “no” answers for instance-level pair 〈&lt;italic&gt;i, o&lt;/italic&gt;〉&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>14025</offset><text>Terms	 	Place	An establishment (store or restaurant) on Google Maps	 	Offering	A product or dish available at a place	 	KBC	Knowledge Base Completion	 	GMB	Google my Business (source store categories)	 	GPT	Google Product Taxonomy (source product categories)	 	UGC	User Generated Content–user responses to yes/no questions	 	CS	Crowd Sense, our approach	 	WebIE	Information extraction of offering names from place web pages	 	WALS	Matrix factorization using WALS to predict 〈ip, co〉 pairs	 	Knowledge graph	 		Set of place instances	 		Set of place categories	 		Set of offering categories	 		Place subclass/superclass relation	 		Offering subclass/superclass relation	 		Place instance/class type relation	 		Class-level offering @ place availability relation	 		Instance-level offering @ place availability relation	 		Base KG of place/offering classes and place instances	 		 extended with  and 	 	Ri,j	Likelihood that place instance i sells offering class j	 	Crowd task	 	wx,o	Rater score for place (class or instance) x and offering class o	 	αc,o	Number of “always” answers for class-level pair 〈c, o〉	 	νc,o	Number of “never answers for class-level pair 〈c, o〉	 	yi,o	Number of “yes” answers for instance-level pair 〈i, o〉	 	ni,o	Number of “no” answers for instance-level pair 〈i, o〉	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>15358</offset><text>4. A Three-Tiered crowd</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15382</offset><text>The system for which we performed the crowdsourcing described in this paper is quite large and complex, and is launched and available to users worldwide through search. It uses a DNN model to predict  pairs from many signals that include information extraction (IE) from store web pages, direct merchant feeds, store type, and dozens of other features that include a significant amount of user-generated content (UGC).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15801</offset><text>The well-known bipartite problems that have been solved by machine learning have the advantage that the organizations that solved them had a lot of labeled data for those problems. For example, Netflix has millions of 〈user, movie〉 pairs, and can use this massive data to seed big machine learning systems to better predict what movies a user make like. A vast number of practical bipartite problems, however, have very little data, resulting in label sparsity.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16267</offset><text>Label sparsity means that machine learning systems don't have enough data to make reasonable predictions, and the only way to move forward is to acquire it. Acquiring the data needed to seed large scale AI systems is as much a problem today as it was during the bygone era of expert systems, where, according to Shortliffe and Buchanan and many others, the bulk of the research focus was on algorithmic solutions to rule-based reasoning problems, but the bulk of the difficulty and work was in knoweldge acquisition. This history continues to repeat itself; Sambasivan et al. point out that knowledge acquisition is viewed as less glamorous than inventing new neural algorithms and architectures. As noted above, for the local shopping and dining problems, existing sources gave us less than 0.001% of the total matrix R, leaving a huge knowledge acquisition problem. We developed a novel three-tiered crowd to gather the data discussed in this paper:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17219</offset><text>CrowdSense (CS): We collected 25k class-level  pairs for shopping and 20k for dining, from a pool of paid raters. Though a relatively small crowd effort, this ends up being the largest source of instance-level  pairs through default inference (full details in Section 5), yielding billions of instance-level pairs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17534</offset><text>UGC: Google Maps provides the facility for users to voluntarily add reviews, photos, venue categorization, and attributes (e.g., “has Wi-Fi”) to places they've visited. Through the UGC framework, users answer yes/no questions about product and dish availability at places they've visited, shown in Figure 3. While Google's deployed local search system does use all the UGC data, including reviews and photos, etc., in this paper we only describe and analyze the impact of the yes/no questions, which comprise the largest crowdsourcing element of the system, at millions of answers per day. Each user is given a set of  pairs to answer, giving us a distribution of yes and no answers for each pair. In the experiments shown in Section 6, we show the growth in coverage over time as more answers are collected, yielding hundreds of millions of instance-level pairs over the course of this study (2 years for shopping and 15 months for dining).7</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18481</offset><text>Gold: We collected 40k gold standard  pairs for shopping, and 20k for dining, by having paid operators call each place ip and ask them if they sold or served co. The places were selected from among more than 50 countries with the top-5 countries being US (20%), JP (5%), IN (5%), GB (5%), BR (4%); places within each country were sampled uniformly to provide a microcosm of representative demographics. Clearly the highest fidelity and most expensive data, it is by far the smallest.</text></passage><passage><infon key="file">frai-05-830299-g0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18965</offset><text>Example question used to gather UGC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19002</offset><text>One of the critical obstacles to gathering this data from people in all tiers is the class imbalance: less than 4% of the possible store-offering pairs are positive. Gathering 96% negative results is a waste of human labeling resources and, far more critical, makes for an unsatisfactory user experience—users want to feel helpful and answering 9/10 negative questions is frustrating. Moreover, particularly obvious negative questions, like fish heads at a hardware store, confuse some users into saying they are unsure—the questions are so obvious they feel they must be missing something. Finally, a few of these obvious negatives end up on social media as jokes, which is embarassing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19694</offset><text>Active learning (AL) is a known method for dealing with class imbalance—sampling near the classifier boundary typically yields a good balance between positives and negatives and, thus, provides utility for training the model. Unfortunately for problems where there is also label sparsity, there is not enough data to train a model and so nothing to base AL on. Our class-level approach offers a solution to this problem as well. As described in more detail in Section 5, we gather a distribution of judgements on class-level pairs, and the resuling pairs fall into three categories: obviously available (e.g., grocery store, milk), obviously unavailable (e.g., hardware store, fish heads), and possible (e.g., hardware store, 9 inch nails). The possible category of class-level pairs captures products that are available at some, but not necessarily all, stores in the class, and provide excellent guidance for selecting instance-level pairs to ask users.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20652</offset><text>Even after we'd acquired enough data to begin training a model and use AL, the possible category offered an additional benefit. In the early stages of acquiring training data, known as the explore (vs. exploit) stage,  pairs with enough evidence to be close to the classifier boundary are very likely to be positive, so much so that the class balance of margin sampling was 80% positive. Clearly a 50% class balance could then be achieved by up-sampling pairs that are further below the classifier boundary, however such an approach is very likely to choose these problematic obvious negatives discussed above. A mix of possibles with margin sampling was able to achieve a 50% class balance with high utility and no embarassment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21382</offset><text>For the Gold data, class imbalance presents as much of a problem as for UGC, however since this data set is used to measure the quality of the CS data, we did not want to bias our evaluations by using CS as a guide. Instead, to achieve better class balance, the WebIE baseline data (q.v. below) was used to guide the collection toward pairs that had an increased chance of being true; for example, if a places's webpage mentioned an offering we would try to call places of the same type and ask about that offering. We enforced a positive/negative class balance of 50%, and targeted a stratification of the sampling that preserved the 30/70 balance of places with and without websites.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>22068</offset><text>5. Crowd Sense</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22083</offset><text>The obvious way to gather the edges in  would be to use store inventory or transaction records. The problem with this approach is that local offerings is still mostly an off-line or highly siloed process worldwide, and we did not have access to transactional data that gives us these observations. Google provides merchants a free way to share their menus or inventory on-line, but much fewer than 1% of places worldwide had made use of it. Our data showed that web pages and merchant feeds together accounted for less than 0.001% of the space of the matrix R, giving us the label sparsity problem. Filling the cells of matrix R means acquiring the edges in , and we propose to accomplish this by starting with the acquisition of edges in , the class level attributes, and inferring those values as defaults for .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22897</offset><text>5.1. Crowd Hypothesis</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22919</offset><text>The intuition driving our approach is that the crowd can provide the class-level knowledge () by appealing to their common sense experience; everybody knows that, e.g., “All supermarkets sell milk.” Reality is more complicated, and since the problem space is sparse, the class-level data is also dominated by what offerings are obviously not available. Far behind the obvious negatives are, as discussed above, the possibles—offerings that are usually, but not always, available at some type of establishment. Wasabi Peas, while they are found almost exclusively in grocery stores, are not found in all of them. What we really aim for the crowd to provide is a distribution of the offerings available at places of a given type. This is where a lot of existing knowledge graph methods fail, especially at the class-level, as they rely on an assumption of discreteness.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23793</offset><text>It may seem that we could ask individual people to answer a question like, “What percent of stores of type cp sell product co?” However, research in human computation such as Surowiecki has shown that individuals cannot reliably answer such questions. Using (Welty et al.,; Aroyo and Welty,) as a starting point, we hypothesized:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24127</offset><text>Hypothesis 1. Asking multiple raters about the same categorical pairs would produce a distribution of answers that approximate the real world distribution of .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24287</offset><text>In other words, if 70% of raters say that oat milk is sold at grocery stores, then 70% of grocery stores will sell oat milk.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24412</offset><text>Before testing our hypothesis, we ran numerous pilots to tune the hyper-parameters of the crowd task in the shopping domain, asking raters questions about 11k  pairs from 154 store types and 3600 products in five countries. We experimented with: the number of raters per pair, testing between 5 and 25 raters per pair; the size of the rater pool, ranging from 100 to 500; the question phrasing; and the answer options. Based on manual analysis of the cost and quality, we settled on these task hyper-parameters: five raters per pair, randomly selected from a pool of 130 raters in six countries, sourced from contracted operators through an in-house crowdsourcing platform, and the question, “Would you expect to find co products in stores of the category cp?” with four answer options (“Always Available,” “Sometimes Available,” “Never Available,” “I don't know”). For dishes, the question was rephrased, “Would you expect to find co dishes in restaurants in the category cp?”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25414</offset><text>Under these settings, our final PRODCAT task (see below) gathered 25k class-level () pairs with 5 labels per country, that through inference (q.v. Section 6.1) resulted in billions of  pairs, 99% of which were negative. It took 6 weeks to run and analyze the pilots, and 2 weeks to run the final task. For dishes, the MATRIX task collected 15k class level pairs from 5 raters per pair in 2 weeks, resulting in billions of instance-level pairs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25858</offset><text>Raters were supplied by a set of contractors who are obligated to follow Google's Code of Conduct, and were managed by an administrator outside our group. The MATRIX and PRODCAT task designs (q.v. below) grouped between 200 and 400 pairs in a single matrix, raters were assigned a matrix by the administrator based primarily on availability. Many raters were assigned multiple matrices over time, but in our analysis we did not account for individual characteristics of raters (such as expertise), even though we know from Aroyo and Welty this can yield improvements.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>26426</offset><text>5.2. Data Collection Tasks</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26453</offset><text>Another way to state our hypothesis is that the categorical crowd disagreement should reflect the real world distribution, but disagreement can have many causes that are not related to the desired distribution. The various pilot tasks we ran represented a gradual refinement of the data and task descriptions to eliminate disagreement from other causes. We report here on four different approaches for the shopping domain:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>26876</offset><text>5.2.1. RANDOM</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26890</offset><text>To confirm the sparsity of , we randomly and independently selected category pairs from , weighing the selection from  proportionally to the number of stores belonging to each category (i.e., larger categories are more likely to be selected). Pairs were presented to 5 raters from the same country. This RANDOM task confirmed that the vast majority of pairs are “obvious” negatives (asphalt at grocery stores, cars at violin shops, etc.), as more than 95% of the pairs resulted in 5 “Never” ratings.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>27398</offset><text>5.2.2. SINGLETON</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27415</offset><text>To address the sparsity shown in RANDOM, we leveraged web signals (see Section 6.1) to select pairs with more likelihood to be available at places within a given category, and presented one pair at a time to 5 raters from the same country. This resulted in a distribution of rating scores ranging from all-5 “Always Available” to all-5 “Never Available” skewing toward the positive (always) side. The SINGLETON task results showed disagreement from other causes, described in Section 5.3.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>27912</offset><text>5.2.3. MATRIX</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27926</offset><text>To address the disagreement due to ambiguity (Section 5.3), we designed a novel matrix presentation of class-level pairs, with four  as the columns and a set of 100–200  as the rows, depending on our ability to match offerings to the place categories using web signals. Figure 4 shows the matrix presentation (with data sampled through the PRODCAT method below). The advantage of this presentation is that raters familiarized themselves with a category and answered many questions related to it, rather than having to understand one pair at a time. This approach still produced some unwanted disagreements due to difficulty understanding some of the products, esp. very specific ones, and we were concerned that the web signals were biasing our sample toward availability patterns of online places, rather than our target class of establishments without web pages. Most importantly, the amount of time the raters spent per  dropped by 50%.</text></passage><passage><infon key="file">frai-05-830299-g0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28868</offset><text>Partial view of the PRODCAT data collection template with example answers from one rater.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>28958</offset><text>5.2.4. PRODCAT</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28973</offset><text>The final crowdsourcing task used the MATRIX presentation but changed to a dynamic method that sampled the  pairs starting at the top of the product taxonomy, and working down the  relation from most general to most specific. It was not useful to treat the store taxonomy this way, as it is very shallow, and we did not have a dish taxonomy. When a pair was given an overall negative label, we did not sample any subcategories of co and inferred a negative label for all descendents. For example, since Auto parts stores do not sell Grocery and , we did not ask 〈Auto parts stores, Dairy〉.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29567</offset><text>The product taxonomy is not a strict tree, but a DAG, and when reconciling conflicting ratings from multiple parents, we retained the most positive rating. Electronics are not sold at Pharmacies, whereas HouseholdProducts are sometimes sold there, and Batteries are a subcategory of both Electronics and HouseholdProducts, so we do ask about 〈Batteries, Pharmacies〉.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29938</offset><text>This top-down taxonomic pruning eliminated any need for the web signals, and accounted for the sparsity at a very high level, since (by accident or ontology) the store and product categories were well aligned: e.g., Auto parts stores sell Auto parts and do not sell Groceries. Higher level categories also made a lot more sense to raters when presented with a sub-category, e.g., Sports and Outdoor Electronics with Fitness Trackers, and since our rater pool did not vary much, they became familiar with the taxonomic distinctions as they progressed down the taxonomy, which was evidenced by a reduction in visits to the taxonomy element descriptions over time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>30600</offset><text>5.2.5. Dish MATRIX</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30619</offset><text>To gather the class-level pairs  for the dining domain, we were not able to fully reuse the PRODCAT method, since the dishes in our KG did not have taxonomic organization, which was the key to the improvements of PRODCAT over MATRIX. Instead, we used the MATRIX method, presenting the class-level pairs in a matrix, selected by their popularity in web signals. As with singleton, this approach favored positive pairs, indeed our raters appear to have been overly positive in their answers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>31109</offset><text>5.3. Ambiguity</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31124</offset><text>In the pilot experiments run for shopping we observed disagreement in the results that did not support our crowd hypothesis, but were caused by ambiguity such as:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31287</offset><text>product is a material, substance (e.g., plastic, starch, arugula) or some product aspect (e.g., color, size)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31396</offset><text>product is a brand (e.g., Avian, Kleenex) or contains a brand name (e.g., Nike Sneakers, Todd's boots)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31499</offset><text>place or offering is too specific (e.g., duck sauce, goat meat, vanilla orchids, banner store)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31594</offset><text>place or offering is too generic (e.g., gift, organic food, chicken, restaurant)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31675</offset><text>offering is regional (e.g., Harissa, Jajangmyeon)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31725</offset><text>offering is seasonal (e.g., christmas trees, flip-flops)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31782</offset><text>offering is polysemous in a way that is resolved by the store type, e.g., “fish” in a grocery store vs. a pet store</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31902</offset><text>flashy menu item (e.g., nacho fries bellgrande, del monde delux).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31968</offset><text>In MATRIX and SINGLETON, for example, raters seem more willing and able to answer the question, “Is milk sold here?” compared to “Is dairy sold here?” In the latter case, there is uncertainty over what minimum set of dairy items (milk, cheese, butter, yogurt, etc.) would be needed for “sells dairy” to be true, yet the equally rich sub-categories of milk (whole milk, skim milk, organic milk, etc.) did not cause the same uncertainty. When presented with the categories in a top-down fashion, raters first dealt with their uncertainty about “dairy” and applied it to the subcategories as well, and this handled most of the general and specific ambiguity, and for many store types, raters were willing to give definite answers about the other sub-types in subsequent tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32758</offset><text>We specifically addressed the material, aspect and brand problems by removing them from the product set, their treatment is the subject of future work. We instructed the raters to treat seasonal products as “year round,” after confirming that users are less likely to search for such products out of season. We updated the task design to allow raters to explore the two taxonomies to help with polysemy, but we found that grouping store categories by taxonomic (sibling and parent) relations in PRODCAT obviated this exploration.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33292</offset><text>Regional products produced disagreement esp. across countries, where for the final tasks we sourced raters in six countries (US, IN, BR, FR, JP, IN). Often this showed up merely as “I Don't Know” answers which were not used in predicting , but do show up in IRR. More interesting cases included when a product had a slightly different meaning, or was sold in different types of stores, in different regions. For example, “syrup” in France is sold in drug stores, and raters in other countries did not agree. This is because in France “syrup” is cough syrup, and this association did not exist elsewhere that we tested. We had many expectations for the role of, and differences between, raters in different countries, described in more detail in Section 5.6. Despite these anectdotal examples, class-level ratings from one country were generally worse at predicting instance-level availability within the same country, and better at predicting other countries. In the final system, we ignored the country of the class-level ratings, treating all raters as equal.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34366</offset><text>Flashy menu items, in which superlatives and other postive-sentiment modifiers are added to dish names, were an additional problem in dining that we did not observe in the shopping domain. This is in part due to the taxonomy curation of the shopping data, in which such modifiers had been removed to create a fairly neutral set of categories. For dining, which lacked the taxonomy, some raters were able to identify the superlatives as meaningless, or were familiar with the dish names because they came from well-known chains, while other raters didn't have that knowledge and would answer either negatively or uncertainly. Our scoring method effectively neutralizes such dish names (see Section 5.5), as the disagreement moves the score close to zero, and we did not choose to address it otherwise. Our current work seeks to address this problem through the automatic development of a taxonomy.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>35263</offset><text>5.4. PRODCAT Data Collection Task</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35297</offset><text>The final design of the PRODCAT task, which was only used in the shopping domain, presented a matrix of  pairs to raters in six countries, five raters per country, and consisted of several elements:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35496</offset><text>a list of store categories, </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35525</offset><text>a list of product categories, </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35556</offset><text>cp, co pairs presented in an n × 4 matrix, where each cp is a row and each co is a column; n ranged from 40 to 200 depending on our ability to find suitable products</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35723</offset><text>the matrix was prefaced with: “Would you expect to find in country the products (in the columns) in stores of the types (in the rows)?”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35863</offset><text>each cell in the matrix connected one pair with four possible answers: “Always available,” “Sometimes available,” “Never available,” and “I Don't Know”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36031</offset><text>the row and column headers co and cp included links to an image, a short description, and the position in the respective taxonomy</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36161</offset><text>raters were encouraged to explore the taxonomies in order to better understand categories</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36251</offset><text>The column product types were chosen such that three were taxonomy-related (sibling or more-specific child) and one was not, e.g., “aspirin,” “notebooks,” “paper supplies,” and “lined paper.”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36459</offset><text>The final matrix PRODCAT crowd template is shown in Figure 4 with an example of answers provided by one rater. Based on rater feedback and metrics shown in Section 5.5, this presentation helped resolve many forms of polysemy mentioned in Section 5.3.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>36710</offset><text>5.5. Error of Class-Level Ratings</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36744</offset><text>Table 2 shows a small sample of the CS task results for  pairs; we have intentionally downsampled the “5-never” pairs to show a mixture of different vote ratios.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>36910</offset><text>Example CrowdSense ratings on  pairs.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Category&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Product&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Always&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Some&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Never&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Auto parts store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pita&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bakery&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Longline Vests&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Beauty supply store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aromatherapy&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bicycle store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Home furnishings&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Butcher shop&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Quicklime&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Chinaware store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Watches&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clothing store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Women's shirts&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clothing store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Petite negligee&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clothing store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Truck tailgate caps&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clothing store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Chameleon&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clothing store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Typewriter ribbon&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Coffee store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Instant coffee&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cosmetics store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Non-dairy milk&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drugstore&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;tarragon&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Electronics store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Canister vacuums&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feed store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;cybex&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fresh food market&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Work dresses&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fruits and vegetables&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Turkey sausage&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Furniture store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Canopy beds&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Furniture store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Box springs&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Grocery store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Smart light bulbs&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Grocery store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frozen clams&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Grocery store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Soy nuts&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Home goods store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Storage baskets&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>36948</offset><text>Category	Product	Always	Some	Never	 	Auto parts store	Pita	0	0	5	 	Bakery	Longline Vests	0	0	5	 	Beauty supply store	Aromatherapy	5	0	0	 	Bicycle store	Home furnishings	0	0	5	 	Butcher shop	Quicklime	0	0	5	 	Chinaware store	Watches	0	0	5	 	Clothing store	Women's shirts	5	0	0	 	Clothing store	Petite negligee	5	0	0	 	Clothing store	Truck tailgate caps	0	0	5	 	Clothing store	Chameleon	0	0	5	 	Clothing store	Typewriter ribbon	0	0	5	 	Coffee store	Instant coffee	4	0	1	 	Cosmetics store	Non-dairy milk	0	0	5	 	Drugstore	tarragon	0	0	5	 	Electronics store	Canister vacuums	5	0	0	 	Feed store	cybex	0	0	5	 	Fresh food market	Work dresses	0	0	5	 	Fruits and vegetables	Turkey sausage	0	1	4	 	Furniture store	Canopy beds	4	1	0	 	Furniture store	Box springs	4	0	1	 	Grocery store	Smart light bulbs	0	0	5	 	Grocery store	Frozen clams	5	0	0	 	Grocery store	Soy nuts	4	1	0	 	Home goods store	Storage baskets	4	1	0	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>37856</offset><text>In Welty et al. we showed that inter-rater reliability (IRR) cannot reflect the quality of ratings where disagreement is the desired result, so we report the error of different  pairs in predicting the distribution of  pairs, by comparing ratings-based scores on  pairs against UGC scores on  pairs obtained from users (see Section 4). Each class and instance level pair has a score:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38240</offset><text>where αx,o is the number of “always” answers for class-level pairs 〈x, o〉, σx,o the number of “sometimes,” and νx,o the number of “never” answers; and yx,o is the number of “yes” answers for store instance-level pairs 〈x, o〉 and nx,o the number of “no” answers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38533</offset><text>Next let  be the instances of place category c under RT. The mean absolute error of class-level pair 〈c, o〉 is:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38649</offset><text>The idea is that if the class-level scores (wc,o) are an accurate prediction of the availability distribution at the instance level, then they should model user observations at individual stores (wi,o), averaged over the size of the store category (). Figure 5 shows the distribution of MAE scores per category pairs for the three shopping and one dining data collection tasks. Despite PRODCAT being a harder task for raters due to the sampled pairs, it performs much better than the other shopping tasks, with nearly half of its categories scoring in the lowest error range, clearly supporting our crowd hypothesis: the disagreement on  pairs approximates the distribution of  when , according to user observations. For Dining, we only ran the MATRIX task, to replicate as much as possible the results from Shopping. As expected, the MAE is lower than for PRODCAT on shopping, but considerably better than MATRIX for shopping. One explanation for this is that our raters were more familiar with dining around the world than shopping, and there was less disagreement caused by not understanding the pair.</text></passage><passage><infon key="file">frai-05-830299-g0005.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>39754</offset><text>Histogram of Normalized-MAE on CrowdSense pairs for three shopping and one dining (Section 5.5) class-level crowd task designs. Bins to the left indicate the relative number of pairs with lower error, making Shopping-PRODCAT the clear leader. Dining-MATRIX performs better than shopping MATRIX.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>40049</offset><text>5.6. Error of International Ratings</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>40085</offset><text>Another hypothesis we formed early on was that raters in our class-level rating pool, which was international, would know their own countries better than other countries, and the initial design of the system called for increasing the weight of in-country class-level ratings over out-of-country ratings when calculating wx,o (see above). In our analysis of CrowdSense errors in the pilot studies, we certainly saw examples of raters misunderstanding dishes and products from other countries (see Section 5.3).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>40595</offset><text>This hypothesis was mostly supported by our analysis of the shopping data, but it turned out to be largely false for dining, to our great surprise, as shown in Figure 6; as with Figure 5, the charts show the distribution of the normalized MAE from CrowdSense predictions, but in each chart we've restricted the actual restaurants to those within the indicated country, and calculated the wx,o scores for CrowdSense for raters in the country (solid blue bars) and for raters not in the country (hashed red bars). With the exception of Japan, outside raters have a lower error rate, as their distributions are shifted significantly to the left. In Brazil, the effect is small, in the US it is large and in India the largest. In Japan, the expected effect is dramatic—Japanese CrowdSense raters were far better at predicting the distribution of dishes at Japanese restaurants than non-Japanese raters. We ran the experiment for Germany and Indonesia (not shown) with similar results as the US and India.</text></passage><passage><infon key="file">frai-05-830299-g0006.jpg</infon><infon key="id">F6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>41598</offset><text>Distribution of CrowdSense errors (Normalized-MAE) for ratings in four countries, comparing CrowdSense predictions from raters in each country to raters outside that country. A shift of scores to the left indicates lower overall error; surprisingly, for all countries except Japan, out-of-country CrowdSense raters are more accurate than those within the country.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>41962</offset><text>For the US, this may be explained by the fact that there are far more chain restaurants that dominate the numbers when calculating the MAE, and many of these chains are familiar abroad, so while US raters are making their decisions based on a broader perspective of chains and non-chains, non-US raters are making their decisions based only on chains, and these capture a larger piece of the US restaurant landscape. In addition, the US has far more restaurants serving international cuisines than any other country, making it possible for international raters to know something about more US restaurants. For Japan, more than any other country, there are many restaurants that serve only a very specific kind of food, and this is well known in Japan and not as much outside it. A possible explanation for the counter-intuitive results in the other countries is that the restaurant taxonomy does not cover those regions very well, leaving more restaurants mis-categorized.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>42935</offset><text>6. Instance-Level Prediction Experiments</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>42976</offset><text>6.1. Data Sources</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42994</offset><text>We compare and contrast several approaches for acquiring and predicting the relations in :</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43085</offset><text>CrowdSense (CS): Class-level associations  and an associated score for each pair wcp,co, collected through PRODCAT (as described above) for shopping, and MATRIX for dining. In our experiments, we treated the CS data as a static set, although in practice it could grow or change over time like UGC. We collected 25k pairs in the shopping domain and 20k for dining.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43449</offset><text>User Responses (UGC): As described in Section 4, we collected more than 100M instance-level pairs for shopping from volunteer users around the world over a 2 year period, and roughly half that amount over a 15-month period for dining. Most of the UGC pairs have a distribution of yes and no answers, and more sophisticated processing of the answers is possible, but for simplicity we use the majority vote as the label in the experiments below, where we break the data into sets representing the first n ∈ [1, 24] months of collection, to illustrate the growth of the data over time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>44035</offset><text>Web baseline (WebIE): The baseline approach to supporting local queries is the Web: using product or dish names mentioned on each place's registered web site as part of an inverted index that are matched to search queries for those products. As discussed above, this approach for local shopping is limited by the coverage of local (aka brick and mortar) stores and restaurants on the web, which was under 30% (60% for the US) at the start of this project in 2017, and has not increased substantially in the years hence. We used a named entity recognizer to extract instance-level pairs () for places with a web site that mention offerings on any of the site's pages, and used the extraction confidence probability threshold yielding 80% precision. WebIE is only able to obtain positive labels, leaving negatives to be inferred from the complement. We chose the 80% precision threshold as this is roughly the precision of the CS inferred data (see Figures 7, 8), which we compare to this and other data sources. While other Web sources (user reviews, coupons, photos, search keyword click-throughs, etc.) and more advanced entity extraction techniques such as Wang et al. might improve the recall, for most places this information simply is not available. We treated the Web as a single unchanging dataset; for our experiments, the change over time was not significant enough to measure.</text></passage><passage><infon key="file">frai-05-830299-g0007.jpg</infon><infon key="id">F7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>45422</offset><text>Precision, Recall, and F-measure for different ways of predicting  for shopping.</text></passage><passage><infon key="file">frai-05-830299-g0008.jpg</infon><infon key="id">F8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>45503</offset><text>Precision, Recall, and F-measure for different ways of predicting  for dining.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45582</offset><text>WALS(UGC): Since predictions of the instance-level pairs form a matrix, R, an obvious approach is to use matrix factorization on the matrix formed from data gathered using the above methods. We used an off-the-shelf WALS implementation based on Koren et al. trained on the UGC scores discussed below. Since WALS does not use “features,” but rather a matrix of real values, we did not include other inputs to WALS in Figure 7 or Figure 8.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>46024</offset><text>6.2. Evaluation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46040</offset><text>Ultimately our goal is to enable offering queries like, “where can i buy a raincoat?” or “where can i get sesame chicken?” to return nearby places on maps as well as (web) search results; however, direct application impact metrics from our system, which launched in mid-2020, are proprietary. Here we focus on the knowledge acquisition part of the system using metrics of knowledge-based completion, see for example (McNamee and Dang,; Welty et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46500</offset><text>We collected 40k gold standard  pairs for shopping, and 20k for dining, by having paid operators call each place ip and ask them if they sold or served co (see Section 4). We used these pairs as a test set in the experiments below. When evaluating against the gold standard, any instance-level pairs that are present in the gold set but missing in the evaluated data are counted as false negatives toward recall. Table 3 shows a small sample of the shopping gold standard pairs, and Figures 7, 8 show the results on 24 and 15 months of UGC data, resp. Note that since WebIE was used to guide the collection of the gold standard, it has a slight advantage in the evaluation.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>47174</offset><text>Example gold standard  pairs.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Store&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Category&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;loc&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Product&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Available&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7-Eleven&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Convenience store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Distilled water&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FALSE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ALDI&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Grocery store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fruitcake&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AURORA MKT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Men's Gloves&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FALSE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Adams Pharmacy&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pharmacy&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Kool aid&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ag construcciones&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Building materials&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PY&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Blinds&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Alanyurt Gıda&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;General store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TR&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Razor blades&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Amorino&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ice cream shop&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FR&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Meat&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FALSE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Barnes and Noble&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Book store&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Blankets&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FALSE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Barstow Buick&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Car dealer&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crown victoria&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Barstow Buick&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Car dealer&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;US&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gears&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TRUE&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bazar&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;bazar&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BR&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mary kay&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FALSE&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>47204</offset><text>Store	Category	loc	Product	Available	 	7-Eleven	Convenience store	US	Distilled water	FALSE	 	ALDI	Grocery store	US	Fruitcake	TRUE	 	AURORA MKT	Store	US	Men's Gloves	FALSE	 	Adams Pharmacy	Pharmacy	US	Kool aid	TRUE	 	Ag construcciones	Building materials	PY	Blinds	TRUE	 	Alanyurt Gıda	General store	TR	Razor blades	TRUE	 	Amorino	Ice cream shop	FR	Meat	FALSE	 	Barnes and Noble	Book store	US	Blankets	FALSE	 	Barstow Buick	Car dealer	US	Crown victoria	TRUE	 	Barstow Buick	Car dealer	US	Gears	TRUE	 	Bazar	bazar	BR	Mary kay	FALSE	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>47737</offset><text>6.3. Results</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>47750</offset><text>6.3.1. WebIE</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>47763</offset><text>Since the values on the WebIE data for each  are fractional in [0, 1], we determined the lowest threshold with at least 0.80 precision and computed recall based on that, resulting in a recall of 0.136 at 0.80 precision for shopping, and a near-identical 0.139 for dining. This recall reflects the fraction of places with web pages, the fraction of offerings (products or dishes) mentioned on those pages, and the recall of the named entity recognition. We did not independently measure these other factors, as Web performance was merely a baseline. WALS on WebIE data was not able to show very significant improvement, and the results are not shown.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>48413</offset><text>6.3.2. CS</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48423</offset><text>The primary hypothesis of this paper is that the acquisition of class-level associations in  from the crowd is an effective way of rapidly jump-starting instance-level associations in . As described in Section 5, we acquired 25k class-level pairs from a paid crowd for shopping and 20k for dining, each with a score wx,o (see Section 5.5), and chose the following simple procedure to infer the instance level pairs:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48839</offset><text>In other words, for class level pair 〈x,o〉, if x is a place category, and the majority of raters (wx,o &gt; 0.5) answered that you can find o at places of that type, add a class-level edge to , and an instance-level edge to every instance of place category x.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>49100</offset><text>We then measured the effectiveness of the CS by comparison of the inferred edges in  to the Gold set, achieving a recall of 0.238 with a precision of 0.788 for shopping, and 0.214 with a precision of 0.788 for dining. While this shows a distinct improvement over WebIE, of interest is the combination, which improves recall to 0.351—near perfect complementarity—while slightly losing precision at 0.782 (for simplicity we do not show this in Figure 7 or Figure 8). The combination uses the WebIE or CS signal if the other is not present, and the CS signal if they are both present, since the CS data includes negatives and WebIE does not. (WALS inference was ineffective here; see below).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>49793</offset><text>6.3.3. UGC</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>49804</offset><text>The UGC dataset grows over time as more users visit places and answer questions, while we treat the Web and CS data as constant (see above). We expect that, given enough time, UGC will overtake CS and WebIE in recall, so an important question is how much time the CS data is worth compared to UGC, and whether it continues to show value. In Figures 7, 8, the blue line shows the precision, recall, and F1 score of the UGC data using the majority vote as the label, and the red line shows the CS performance, which, as noted above, doesn't change. In both shopping and dining, the UGC line crosses the CS line at around 11 months, indicating that CS is worth about 11 months of UGC collection in both domains.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>50513</offset><text>6.3.4. WALS(UGC)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>50530</offset><text>We populated the matrix Rp,o from UGC wp,o scores, factorized R using WALS, and measured the resulting dot-products against the Gold Standard dataset, shown in Figures 7, 8 in green. Since WALS produces real-valued predictions, we chose the 0.8 prec. threshold, the comparable precision of the CS and UGC methods, and measured the recall at that threshold with increasing UGC over time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>50917</offset><text>Note that some of the 〈p, o〉 pairs in the Gold set were in the training set, however the labels used in the training matrix may be different than Gold, making it a fair comparison. As in the previous experiments we broke the dataset into sets representing the first n ∈ [1, 24] months of collected user responses. WALS clearly improves over UGC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>51269</offset><text>6.3.5. CS+UGC</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>51283</offset><text>While 11 months is the intersection point of the metric values for CS and UGC independently, the CS data is supposed to complement as well as jump-start the knowledge acquisition. We tested the role of CS over time using a simple “CS as default” combination, shown in Figures 7, 8 as CS+UGC, in which the UGC label is used if present, and the CS label is used if not. This line tracks the improvement in recall over time from UGC collection, while jump starting at the recall of CS. This is a clear demonstration of our core research hypothesis.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>51833</offset><text>Of particular interest is the comparison of WALS(UGC) with CS+UGC. The former does eventually surpass the latter for shopping after roughly 18 m (Figure 7), but the CS+UGC combination is a strong contender from an extremely simple method. This is again clear evidence of our core hypothesis. However, for dining the story is not so clear, as the WALS(UGC) very quickly reaches near-parity with CS+UGCafter only 5 months, and starts to improve over it in the 11th month of UGCcollection (Figure 8). The reason for this is not entirely clear, the dining matrix is smaller than shopping—the number of restaurants and the number of dishes are both smaller—meaning the same amount of data collection is a higher part of the total matrix. There may be something slightly easier about the restaurant problem as well—restaurant menus tend to be much smaller than the number of products sold in most stores. Perhaps most importantly, for the early part of gathering shopping UGC, we did not have the crowd sense data to guide the collecting, that was available after 6 months, whereas for dining we collected the crowd sense data first and it guided the collection from the start.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53011</offset><text>Other ways of filling the initial training matrix R by combining CS, UGC, and WebIE signals in various ways were tried but not included as they do not outperform WALS(UGC). Of note is that the CS signal does not work well with WALS, since it effectively does what WALS itself should do with enough data - filling in giant portions of the matrix with default values. Other machine learning approaches are certainly possible, indeed the launched local search system uses a deep neural network with many more features that are beyond the scope of this paper, and measured at the scale of the web. The three signals reported here are very signifant features of that system, and the full system improves significantly over search alone.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>53743</offset><text>7. Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53759</offset><text>The core of this work is overcoming a knowledge acquisition bottleneck in acquiring data reflecting the availability of products at millions of brick and mortar stores worldwide. The approach of harnessing class-level knowledge to infer instance-level knowledge is based on a long standing idea in knowledge engineering, dating back at least as far as Minsky. Other methods in the formal knowledge representation (KR) field have never scaled to the level necessary for our problem, nor have they considered the problem of how to acquire distributions instead of discrete facts.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>54337</offset><text>Information Extraction (IE) methods perform knowledge acquisition of real-world entities from web text, and are discussed in Zang et al.. Martínez-Rodríguez et al. present a survey of IE techniques for populating semantic structures, e.g., entity extraction and linking. In the context of shopping, research has mainly focused on product information extraction, e.g., crawling the Web for offers to maintain product catalogs as in Nguyen et al. and Qiu et al., extracting product specifications and attributes as with Kannan et al., Qiu et al., Zheng et al., and Wang et al., and IE methods for building product knowledge graphs such as Dong and Xu et al.. Our paper defines a method for linking these already defined entities similar to Dong, incorporating product and store taxonomy knowledge.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>55135</offset><text>Knowledge Base Completion (KBC) is the problem of inferring missing entities and/or relations in an existing knowledge graph based on existing ones, such as via link prediction as in Bordes et al. or from a combination of sources such as Riedel et al.. Our product × store category matrix (Figure 4) is inspired by the item-based collaborative filtering matrix introduced in recommender systems found in Sarwar et al. and Ekstrand et al., and we leverage a well-known collaborative filtering approach introduced in Koren et al. for KBC to demonstrate the additional power of inference on our knowledge graph.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>55745</offset><text>We use a knowledge graph as the basic representation and, like most well known KGs, employ no general-purpose reasoning; hence, any inference we do must be defeasible. The most relevant KR area would be reasoning with defaults (e.g., Reiter,; Lang,), as our CS+UGC baseline mechanism for combining  with  pairs treats the first as a default and the second as an override. Beyond this simple combination strategy, which was first proposed in Quillian, more sophisticated combinations of CS+UGC with other forms of evidence are done using optimizations from machine learning. The full local shopping system uses many signals, of which we've described only three, that are combined using a deep neural network that optimizes the prediction of observed labels for many billions of  pairs. While we exploit the taxonomies in  and especially  to optimize the selection of class-level pairs to acquire from workers as discussed in Lees et al., taxonomy-based reasoning was only used for negative associations. This negative inheritance was first observed by Deng et al..</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>56809</offset><text>IE and KBC techniques have advanced the state-of-the-art in capturing human knowledge in machine-readable form, but there is still the need for human curation and crowdsourcing. Important milestones for crowdsourcing knowledge acquisition at scale are Wikidata (Bollacker et al.,) and Freebase (Vrandečić and Krötzsch,), where the crowd defines or curates real world entities and some relationships between them, typically driven by Wikipedia. With respect to KBC, Revenko et al. propose a method for crowdsourcing categorical common sense knowlegde from nonexperts for adding new relationships between nodes in the graph and ensuring consistencey with existing relations. However in all these sources, Taylor has pointed to the sparsity of graph edges expressing relations between the class-level nodes. Our work focuses directly on that problem by acquiring both class-level and instance level graph edges, and scaling the latter from the former.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>57761</offset><text>The crowdsourcing approach we propose in this paper is grounded in the theoretical framework of Aroyo and Welty and Aroyo and Welty, which breaks the constraints of typical methodologies for collecting ground truth, showing disagreement is a necessary characteristic of annotated data; when interpreted correctly, Dumitrache showed it can make evaluation of machine learning models more attuned to real-world data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>58176</offset><text>The immense body of research on common sense and crowdsourcing has directly influenced our work. The UGC and Crowd Sense tasks drew on our knowledge of Games-with-a-purpose such as Verbosity for collecting common sense facts (von Ahn et al.,), Common Consensus for gathering common sense goals (Lieberman et al.,), GECKA for common sense knowledge acquisition (Cambria et al.,), Concept Game for verifying common sense knowledge assertions (Herdagdelen and Baroni,), the FACTory Game for facts verification (Lenat and Guha,) and many others. Rodosthenous and Michael refer to common sense as “knowledge about the world&quot; and propose a hybrid (machine and human tasks) workflow to gather general common sense knowledge rules.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>58902</offset><text>Active learning investigates efficiency for acquisition and learning when acquiring training data for ML models. In essence, the early stages of KG acquisition strongly represent the exploration side of the exploration vs. exploitation tradeoff introduced by Bondu et al.. ML models during exploration do not have enough knowledge of the space to be able to offer reliable judgements as to which items (in this case,  pairs) to acquire labels for. As noted in the Section 6.1, class-level pairs can serve as a guide for recognizing obvious  pairs that likely do not need labels, and conversely, high-disagreement pairs are very likely to have instances that do. Thus the  pairs can serve to stratify the  space, and make the job of active learning easier by narrowing down their targets. In Section 4 we discussed using these possible class-level pairs to guide sampling for UGC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>59782</offset><text>The problem of mining “interesting” negative statements from Wikidata was investigated in Karagiannis et al., Arnaout et al., and Arnaout et al., which in principle could be used to supplement our active learning strategies for selecting difficult training examples to improve the model. Specifically, these could be combined with the obvious (positive and negative) class-level pairs to find exceptions at individual stores, e.g., a grocery store that does not sell milk or that sells certain tools. Our approach would be slow to find such exceptions, since we don't ask users and would need other sources of evidence used by the larger production syste (e.g., a web page, a user review, etc.). Peer-based detection, which compares triples with other triples that share entities in the same category, is similar in spirit to collaborative filtering (CF) though they did not compare experimentally against a CF method such as WALS. Pattern-based detection, presented in Karagiannis et al. and Arnaout et al. seems better suited for mining (negative) trivia than for product availability, since it is unlikely many online users write about e.g., why supermarkets don't sell asphalt.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>60968</offset><text>Perhaps the most similar crowdsourcing work to ours studies the problem of approximating aggregation queries presented in Trushkowsky et al., such as “How many restaurants in San Francisco serve scallops?” While this approach works well for estimating counts, clearly it does not scale for KBC.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>61267</offset><text>8. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>61282</offset><text>The CrowdSense approach was an integral part of a successful worldwide launch of local search results to queries for products or dishes, overlaid on Google Maps, as shown in Figure 1. Due to the complexity and scope of the deployed project, we focused on the real-world knowledge acquisition aspect of the work, and presented a few simplified experiments that demonstrate how the acquired class-level knowledge can be used for KBC at the instance level. These experiments may seem over-simplified, but they accurately capture the impact of the three-tiered crowdsourcing approach on the deployed product, in particular the rapid jump-start of the place-offering edges in the knowledge graph.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>61974</offset><text>To achieve these results, we augmented an existing knowledge graph of most stores and restaurants on earth, their categories, dishes and a product taxonomy, by adding place to product and place to dish edges. We combined web-based information extraction (WebIE) and direct user observations collected over 2 years (UGC) with a novel collection of class-level 〈store, offering〉 pairs from the crowd (CS), which were inferred to the instance-level based on class membership. In 2 weeks of data collection we achieved a recall of 0.24 at 0.80 precision against gold standard instance-level labels for shopping, and 0.21 for dining. The class-level data for shopping combined with WebIE to achieve 0.35 recall, which was the recall of a WALS model with 18 months of UGC input. For dining the same combination also produced 0.34 recall, which was the WALs recall for 11 months of UGC. We conclude that the Crowd Sense approach uses human common sense knowledge to rapidly jump start the kind of generalization that ML systems are good at with a lot of data. This has implications for practical ML and Human Computation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>63093</offset><text>Our class-level crowdsourcing results show that the disagreement in categorical knowledge collected from the crowd can indicate the distribution of that knowledge at the instance level, rather than assuming the class-level associations are universally true: in other words, if 80% of raters say “Grocery stores sell oat milk,” then ~80% of grocery stores sell oat milk. These results held also for dishes at restaurants.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>63518</offset><text>The taxonomy of products was used to guide the sampling of class-level pairs in a way that helped us address the sparsity of the  space, and only the negative class-level attributes were accurate when inferred to more specific categories, as in Deng et al., as opposed to the more traditional view that positive attributes are “inherited.”</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>63862</offset><text>We found the categorical pairs which were rapidly acquired were extremely useful in guiding the collection of instance-level labels, since we did not have to ask users about obviously available or unavailable products—this has implications for active learning, and held also for dining.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>64151</offset><text>We expected the class-level ratings we acquired from a small, international, pool of paid raters, to show bias toward ratings coming from the same country of a restaurant. In other words, we expected class-level ratings from Indian raters to have lower error for restaurants in India than class-level ratings from raters in other countries. This turned out to only be true for Japan, and for all other countries it was the opposite. This may tell us something about the way the place categories model the real world, more investigation is required.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>64700</offset><text>We believe Crowd Sense is a general technique for knowledge acquisition that can provide a rapid jump-start to the process by acquiring more general, common-sense defaults as a first step, while more precise but time-consuming acquisition (i.e., at the instance level) proceeds over time. We have shown that the original local shopping idea, first presented in Welty et al., can generalize to other establishment domains with similar gains, in this case dining, and we have considered many other bipartite problems that meet the basic requirement that there is a strong, common-sense understanding of the relation at the categorical level, for example:</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>65353</offset><text>Dish contains ingredient. Dishes have associated recipes and a strong notion of taxonomy8, and many ingredient associations are ridiculous at a class level, such as Apple Pie and Curry.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>65539</offset><text>Cuisine includes dish. Dishes are also associated with cuisines, a pairing that could be useful for recipe datasets, and understanding menus. Many cuisines are regional, introducing a different kind of partial order (containment rather than generalization, see Guarino and Welty,) on one side of the bipartite relation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>65859</offset><text>Wildlife inhabiting a region. Several NGOs track wildlife populations through remote cameras and citizen science collection of photos, and identify animals using automatic methods.9 Such methods would benefit from large scale understanding of obvious negatives (tigers are not found in Africa). Like cuisines, this involves treating locations as a partial order based on containment, and the Linnaean taxonomy for animals is well established.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>66302</offset><text>Animal has body part. In the early days of AI, much ink was spilled on modeling defaults and exceptions such as “Elephants have trunks” and “Humans have two legs.” This work was summarized nicely in Brachman. Modern AI systems do not use this information and rely on the formation of embeddings that bely human understanding, but such systems have been shown in Aroyo and Paritosh to make “silly” categorical mistakes. An approach that forces large models to form meaningful intermediate representations such as parts of the body, as described by Hinton, could avoid silly mistakes with this form of common sense curation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>66937</offset><text>Company owns patent. Finding patents is a difficult search task that continues to be a focus of AI systems. While these systems do not generally lack data, they do often suffer from silly mistakes, as image understanding systems do, which reflect a lack of common sense. Adding categorical associations such as, “Tech companies do not own pharmaceutical patents” would eliminate some of these mistakes.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>67344</offset><text>To see CrowdSense at work, type the name of a product or dish into Google Maps (or Google Search). Results that say “Sold here: product” come from the data we published (see Figure 9, as opposed to “In stock” (merchant feeds) and “Webpage says.” Anyone with a Google account can participate in UGC (user generated content) acquisition. Users with location tracking turned on (so that maps knows what places the user has visited10) can navigate to the “contribute” tab that allows them to rate and leave reviews, as well as review facts and answer the yes/no questions regarding locations they have visited.</text></passage><passage><infon key="file">frai-05-830299-g0009.jpg</infon><infon key="id">F9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>67967</offset><text>CrowdSense search results in NYC for knapsacks.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>68015</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>68043</offset><text>The original contributions presented in the study are included in the article/supplementary materials, further inquiries can be directed to the corresponding author/s. The categories of places and products are available and included in the article, and the rest of the data discussed in this paper is not, as it is proprietary data that drives Google Maps.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>68400</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>68421</offset><text>CW led the project. CW and FK wrote most of the article. All authors contributed experimental results and background research.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>68548</offset><text>Conflict of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>68569</offset><text>All authors were employed by Google Research.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>68615</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>68632</offset><text>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>68979</offset><text> 1 https://support.google.com/merchants/answer/9825611?hl=en </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69041</offset><text> 2 https://www.forbes.com/sites/jiawertz/2018/05/17/how-brick-and-mortar-stores-can-compete-with-e-commerce-giants/#2019f5a23cc0 </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69171</offset><text> 3 https://support.google.com/business/answer/3038177/#categories </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69238</offset><text> 4 https://bayareawebsitedesigner.com/gmb-categories/ </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69293</offset><text> 5 https://www.google.com/basepages/producttype/taxonomy.en-US.txt </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69361</offset><text> 6 https://feedonomics.com/google_shopping_categories.html </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69421</offset><text>7Collection continues, these windows were used for this paper.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69484</offset><text>8e.g., https://www.wikidata.org/wiki/Wikidata:WikiProject_Food/Taxonomy.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69557</offset><text>9Examples include wildlifeinsights.org and inaturalist.org.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>69617</offset><text>10See https://support.google.com/local-guides/answer/6225846.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>69679</offset><text>Supplementary Material</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>69702</offset><text>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frai.2022.830299/full#supplementary-material</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>69860</offset><text>References</text></passage><passage><infon key="name_0">surname:Arnaout;given-names:H.</infon><infon key="name_1">surname:Razniewski;given-names:S.</infon><infon key="name_2">surname:Weikum;given-names:G.</infon><infon key="name_3">surname:Das;given-names:D.</infon><infon key="name_4">surname:Hajishirzi;given-names:H.</infon><infon key="name_5">surname:McCallum;given-names:A.</infon><infon key="name_6">surname:Singh;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>69871</offset><text>Enriching knowledge bases with interesting negative statements</text></passage><passage><infon key="fpage">544</infon><infon key="lpage">551</infon><infon key="name_0">surname:Arnaout;given-names:H.</infon><infon key="name_1">surname:Razniewski;given-names:S.</infon><infon key="name_2">surname:Weikum;given-names:G.</infon><infon key="name_3">surname:Pan;given-names:J. Z.</infon><infon key="name_4">surname:Leskovec;given-names:J.</infon><infon key="name_5">surname:Grobelnik;given-names:M.</infon><infon key="name_6">surname:Najork;given-names:M.</infon><infon key="name_7">surname:Tang;given-names:J.</infon><infon key="name_8">surname:Zia;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">Companion of The Web Conference 2021, Virtual Event/Ljubljana, Slovenia, April 19–23, 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>69934</offset><text>Negative knowledge for open-world wikidata</text></passage><passage><infon key="name_0">surname:Aroyo;given-names:L.</infon><infon key="name_1">surname:Paritosh;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">Uncovering Unknown Unknowns in Machine Learning. Google AI Blog</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>69977</offset></passage><passage><infon key="name_0">surname:Aroyo;given-names:L.</infon><infon key="name_1">surname:Welty;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 5th Annual ACM Web Science Conference, WebSci '13</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>69978</offset><text>Crowd truth: harnessing disagreement in crowdsourcing a relation extraction gold standard</text></passage><passage><infon key="fpage">31</infon><infon key="lpage">44</infon><infon key="name_0">surname:Aroyo;given-names:L.</infon><infon key="name_1">surname:Welty;given-names:C.</infon><infon key="pub-id_doi">10.15346/hc.v1i1.3</infon><infon key="section_type">REF</infon><infon key="source">Hum. Comput</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2014</infon><offset>70068</offset><text>The three sides of crowdtruth</text></passage><passage><infon key="fpage">15</infon><infon key="lpage">24</infon><infon key="name_0">surname:Aroyo;given-names:L.</infon><infon key="name_1">surname:Welty;given-names:C.</infon><infon key="pub-id_doi">10.1609/aimag.v36i1.2564</infon><infon key="section_type">REF</infon><infon key="source">AI Mag</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2015</infon><offset>70098</offset><text>Truth is a lie: Crowd Truth and the seven myths of human annotation</text></passage><passage><infon key="fpage">1247</infon><infon key="lpage">1250</infon><infon key="name_0">surname:Bollacker;given-names:K.</infon><infon key="name_1">surname:Evans;given-names:C.</infon><infon key="name_2">surname:Paritosh;given-names:P.</infon><infon key="name_3">surname:Sturge;given-names:T.</infon><infon key="name_4">surname:Taylor;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of SIGMOD International Conference on Management of Data</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>70166</offset><text>Freebase: a collaboratively created graph database for structuring human knowledge</text></passage><passage><infon key="name_0">surname:Bondu;given-names:A.</infon><infon key="name_1">surname:Lemaire;given-names:V.</infon><infon key="name_2">surname:Boullé;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">The 2010 International Joint Conference on Neural Networks (IJCNN)</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>70249</offset><text>Exploration vs. exploitation in active learning : a bayesian approach</text></passage><passage><infon key="fpage">2787</infon><infon key="lpage">2795</infon><infon key="name_0">surname:Bordes;given-names:A.</infon><infon key="name_1">surname:Usunier;given-names:N.</infon><infon key="name_2">surname:García-Durán;given-names:A.</infon><infon key="name_3">surname:Weston;given-names:J.</infon><infon key="name_4">surname:Yakhnenko;given-names:O.</infon><infon key="section_type">REF</infon><infon key="source">NIPS 2013</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>70319</offset><text>Translating embeddings for modeling multi-relational data</text></passage><passage><infon key="fpage">80</infon><infon key="name_0">surname:Brachman;given-names:R. J.</infon><infon key="section_type">REF</infon><infon key="source">AI Mag</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">1985</infon><offset>70377</offset><text>I lied about the trees, or, defaults and definitions in knowledge representation</text></passage><passage><infon key="name_0">surname:Cambria;given-names:E.</infon><infon key="name_1">surname:Nguyen;given-names:T. V.</infon><infon key="name_2">surname:Cheng;given-names:B.</infon><infon key="name_3">surname:Kwok;given-names:K.</infon><infon key="name_4">surname:Sepulveda;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">CoRR-2016</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>70458</offset><text>Gecka3d: A 3d game engine for commonsense knowledge acquisition</text></passage><passage><infon key="name_0">surname:Deng;given-names:J.</infon><infon key="name_1">surname:Russakovsky;given-names:O.</infon><infon key="name_2">surname:Krause;given-names:J.</infon><infon key="name_3">surname:Bernstein;given-names:M.</infon><infon key="name_4">surname:Berg;given-names:A.</infon><infon key="name_5">surname:Fei-Fei;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of CHI 2014</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>70522</offset><text>Scalable multi-label annotation</text></passage><passage><infon key="fpage">2724</infon><infon key="lpage">2734</infon><infon key="name_0">surname:Dong;given-names:X. L.</infon><infon key="section_type">REF</infon><infon key="source">KDD '20: Conference on Knowledge Discovery and Data Mining</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>70554</offset><text>Autoknow: self-driving knowledge collection for products of thousands of types</text></passage><passage><infon key="name_0">surname:Dumitrache;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Truth in Disagreement: Crowdsourcing Labeled Data for Natural Language Processing</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>70633</offset></passage><passage><infon key="name_0">surname:Ekstrand;given-names:M. D.</infon><infon key="name_1">surname:Riedl;given-names:J. T.</infon><infon key="name_2">surname:Konstan;given-names:J. A.</infon><infon key="pub-id_doi">10.1561/9781601984432</infon><infon key="section_type">REF</infon><infon key="source">Collaborative Filtering Recommender Systems</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>70634</offset></passage><passage><infon key="fpage">201</infon><infon key="lpage">220</infon><infon key="name_0">surname:Guarino;given-names:N.</infon><infon key="name_1">surname:Welty;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Handbook on Ontologies. International Handbooks on Information Systems</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>70635</offset><text>An overview of OntoClean</text></passage><passage><infon key="name_0">surname:Herdagdelen;given-names:A.</infon><infon key="name_1">surname:Baroni;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">AAAI Fall Symposium: Commonsense Knowledge</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>70660</offset><text>The concept game: Better commonsense knowledge extraction by combining text mining and a game with a purpose</text></passage><passage><infon key="name_0">surname:Hinton;given-names:G. E.</infon><infon key="section_type">REF</infon><infon key="source">CoRR, abs/2102. 12627</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>70769</offset><text>How to represent part-whole hierarchies in a neural network</text></passage><passage><infon key="name_0">surname:Ji;given-names:S.</infon><infon key="name_1">surname:Pan;given-names:S.</infon><infon key="name_2">surname:Cambria;given-names:E.</infon><infon key="name_3">surname:Marttinen;given-names:P.</infon><infon key="name_4">surname:Yu;given-names:P. S.</infon><infon key="pub-id_pmid">33900922</infon><infon key="section_type">REF</infon><infon key="source">CoRR, abs/2002.00388</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>70829</offset><text>A survey on knowledge graphs: Representation, acquisition and applications</text></passage><passage><infon key="fpage">404</infon><infon key="lpage">412</infon><infon key="name_0">surname:Kannan;given-names:A.</infon><infon key="name_1">surname:Givoni;given-names:I. E.</infon><infon key="name_2">surname:Agrawal;given-names:R.</infon><infon key="name_3">surname:Fuxman;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">in KDD-2011</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>70904</offset><text>Matching unstructured product offers to structured product specifications</text></passage><passage><infon key="fpage">561</infon><infon key="lpage">573</infon><infon key="name_0">surname:Karagiannis;given-names:G.</infon><infon key="name_1">surname:Trummer;given-names:I.</infon><infon key="name_2">surname:Jo;given-names:S.</infon><infon key="name_3">surname:Khandelwal;given-names:S.</infon><infon key="name_4">surname:Wang;given-names:X.</infon><infon key="name_5">surname:Yu;given-names:C.</infon><infon key="pub-id_doi">10.14778/3372716.3372727</infon><infon key="section_type">REF</infon><infon key="source">Proc. VLDB Endow</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2019</infon><offset>70978</offset><text>Mining an “anti-knowledge base” from wikipedia updates with applications to fact checking and beyond</text></passage><passage><infon key="fpage">30</infon><infon key="lpage">37</infon><infon key="name_0">surname:Koren;given-names:Y.</infon><infon key="name_1">surname:Bell;given-names:R.</infon><infon key="name_2">surname:Volinsky;given-names:C.</infon><infon key="pub-id_doi">10.1109/MC.2009.263</infon><infon key="section_type">REF</infon><infon key="source">Computer</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2009</infon><offset>71083</offset><text>Matrix factorization techniques for recommender systems</text></passage><passage><infon key="fpage">2993</infon><infon key="lpage">3000</infon><infon key="name_0">surname:Lagos;given-names:N.</infon><infon key="name_1">surname:Ait-Mokhtar;given-names:S.</infon><infon key="name_2">surname:Calapodescu;given-names:I.</infon><infon key="name_3">surname:Giacomo;given-names:G. D.</infon><infon key="name_4">surname:Catalá;given-names:A.</infon><infon key="name_5">surname:Dilkina;given-names:B.</infon><infon key="name_6">surname:Milano;given-names:M.</infon><infon key="name_7">surname:Barro;given-names:S.</infon><infon key="name_8">surname:Bugarín;given-names:A.</infon><infon key="name_9">surname:Lang;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">ECAI 2020, volume 325 of Frontiers in Artificial Intelligence and Applications</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>71139</offset><text>Point-of-interest semantic tag completion in a global crowdsourced search-and-discovery database</text></passage><passage><infon key="name_0">surname:Lang;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Possibilistic Logic: Complexity and Algorithms</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>71236</offset></passage><passage><infon key="name_0">surname:Lees;given-names:A. W.</infon><infon key="name_1">surname:Welty;given-names:C.</infon><infon key="name_2">surname:Korycki;given-names:J.</infon><infon key="name_3">surname:Carthy;given-names:S. M.</infon><infon key="name_4">surname:Zhao;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">CoLing 2020</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>71237</offset><text>Embedding semantic taxonomies</text></passage><passage><infon key="name_0">surname:Lenat;given-names:D. B.</infon><infon key="name_1">surname:Guha;given-names:R. V.</infon><infon key="section_type">REF</infon><infon key="source">Building Large Knowledge-Based Systems; Representation and Inference in the Cyc Project</infon><infon key="type">ref</infon><infon key="year">1989</infon><offset>71267</offset></passage><passage><infon key="name_0">surname:Lieberman;given-names:H.</infon><infon key="name_1">surname:Smith;given-names:D.</infon><infon key="name_2">surname:Teeters;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">IUI-2007</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>71268</offset><text>Common consensus: a web-based game for collecting commonsense goals</text></passage><passage><infon key="fpage">255</infon><infon key="lpage">335</infon><infon key="name_0">surname:Martínez-Rodríguez;given-names:J. L.</infon><infon key="name_1">surname:Hogan;given-names:A.</infon><infon key="name_2">surname:López-Arévalo;given-names:I.</infon><infon key="pub-id_doi">10.3233/SW-180333</infon><infon key="section_type">REF</infon><infon key="source">Semantic Web</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2020</infon><offset>71336</offset><text>Information extraction meets the semantic web: a survey</text></passage><passage><infon key="name_0">surname:McNamee;given-names:P.</infon><infon key="name_1">surname:Dang;given-names:H. T.</infon><infon key="section_type">REF</infon><infon key="source">Text Analysis Conference (TAC-2009)</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>71392</offset><text>Overview of the tac 2009 knowledge base population track</text></passage><passage><infon key="name_0">surname:Minsky;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">A Framework for Representing Knowledge</infon><infon key="type">ref</infon><infon key="year">1974</infon><offset>71449</offset></passage><passage><infon key="fpage">409</infon><infon key="lpage">418</infon><infon key="name_0">surname:Nguyen;given-names:H.</infon><infon key="name_1">surname:Fuxman;given-names:A.</infon><infon key="name_2">surname:Paparizos;given-names:S.</infon><infon key="name_3">surname:Freire;given-names:J.</infon><infon key="name_4">surname:Agrawal;given-names:R.</infon><infon key="pub-id_doi">10.14778/1988776.1988777</infon><infon key="section_type">REF</infon><infon key="source">Proc. VLDB Endow</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2011</infon><offset>71450</offset><text>Synthesizing products for online catalogs</text></passage><passage><infon key="name_0">surname:Noy;given-names:N.</infon><infon key="name_1">surname:Rector;given-names:A.</infon><infon key="name_2">surname:Hayes;given-names:P.</infon><infon key="name_3">surname:Welty;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Defining N-ary Relations on the Semantic Web</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>71492</offset></passage><passage><infon key="fpage">2194</infon><infon key="lpage">2205</infon><infon key="name_0">surname:Qiu;given-names:D.</infon><infon key="name_1">surname:Barbosa;given-names:L.</infon><infon key="name_2">surname:Dong;given-names:L. X.</infon><infon key="name_3">surname:Shen;given-names:Y.</infon><infon key="name_4">surname:Srivastava;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">PVLDB</infon><infon key="type">ref</infon><infon key="year">2015a</infon><offset>71493</offset><text>Dexter: large-scale discovery and extraction of product specifications on the web</text></passage><passage><infon key="fpage">2194</infon><infon key="lpage">2205</infon><infon key="name_0">surname:Qiu;given-names:D.</infon><infon key="name_1">surname:Barbosa;given-names:L.</infon><infon key="name_2">surname:Dong;given-names:X. L.</infon><infon key="name_3">surname:Shen;given-names:Y.</infon><infon key="name_4">surname:Srivastava;given-names:D.</infon><infon key="pub-id_doi">10.14778/2831360.2831372</infon><infon key="section_type">REF</infon><infon key="source">Proc. VLDB Endow</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2015b</infon><offset>71575</offset><text>DEXTER: large-scale discovery and extraction of product specifications on the web</text></passage><passage><infon key="fpage">410</infon><infon key="lpage">430</infon><infon key="name_0">surname:Quillian;given-names:M. R.</infon><infon key="pub-id_doi">10.1002/bs.3830120511</infon><infon key="pub-id_pmid">6059773</infon><infon key="section_type">REF</infon><infon key="source">Behav. Sci</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">1967</infon><offset>71657</offset><text>Word concepts: a theory and simulation of some basic semantic capabilities</text></passage><passage><infon key="name_0">surname:Reiter;given-names:R.</infon><infon key="name_1">surname:Brachman;given-names:R. J.</infon><infon key="name_2">surname:Levesque;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Readings in Knowledge Representation</infon><infon key="type">ref</infon><infon key="year">1978</infon><offset>71732</offset><text>On reasoning by default</text></passage><passage><infon key="name_0">surname:Revenko;given-names:A.</infon><infon key="name_1">surname:Sabou;given-names:M.</infon><infon key="name_2">surname:Ahmeti;given-names:A.</infon><infon key="name_3">surname:Schauer;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the HCOMP 2018 Works in Progress and Demonstration Papers Track, volume 2173 of CEUR Workshop Proceedings</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>71756</offset><text>Crowd-sourced knowledge graph extension: a belief revision based approach</text></passage><passage><infon key="name_0">surname:Riedel;given-names:S.</infon><infon key="name_1">surname:Yao;given-names:L.</infon><infon key="name_2">surname:Marlin;given-names:B. M.</infon><infon key="name_3">surname:McCallum;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>71830</offset><text>Relation extraction with matrix factorization and universal schemas</text></passage><passage><infon key="fpage">25</infon><infon key="lpage">30</infon><infon key="name_0">surname:Rodosthenous;given-names:C.</infon><infon key="name_1">surname:Michael;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the enetCollect WG3 and WG5 Meeting 2018, Vol. 2390</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>71898</offset><text>A platform for commonsense knowledge acquisition using crowdsourcing</text></passage><passage><infon key="name_0">surname:Sambasivan;given-names:N.</infon><infon key="name_1">surname:Kapania;given-names:S.</infon><infon key="name_2">surname:Highfill;given-names:H.</infon><infon key="name_3">surname:Akrong;given-names:D.</infon><infon key="name_4">surname:Paritosh;given-names:P. K.</infon><infon key="name_5">surname:Aroyo;given-names:L. M.</infon><infon key="section_type">REF</infon><infon key="source">“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes ai</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>71967</offset></passage><passage><infon key="fpage">285</infon><infon key="lpage">295</infon><infon key="name_0">surname:Sarwar;given-names:B.</infon><infon key="name_1">surname:Karypis;given-names:G.</infon><infon key="name_2">surname:Konstan;given-names:J.</infon><infon key="name_3">surname:Riedl;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 10th International Conference on World Wide Web</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>71968</offset><text>Item-based collaborative filtering recommendation algorithms</text></passage><passage><infon key="fpage">351</infon><infon key="lpage">379</infon><infon key="name_0">surname:Shortliffe;given-names:E.</infon><infon key="name_1">surname:Buchanan;given-names:B.</infon><infon key="pub-id_doi">10.1016/0025-5564(75)90047-4</infon><infon key="section_type">REF</infon><infon key="source">Math. Biosci</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">1975</infon><offset>72029</offset><text>A model of inexact reasoning in medicine</text></passage><passage><infon key="name_0">surname:Surowiecki;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">The Wisdom of Crowds.</infon><infon key="type">ref</infon><infon key="year">2005</infon><offset>72070</offset></passage><passage><infon key="name_0">surname:Taylor;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Iswc 2017 Keynote: Applied Semantics: Beyond the ca||Talog</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>72071</offset></passage><passage><infon key="fpage">673</infon><infon key="lpage">684</infon><infon key="name_0">surname:Trushkowsky;given-names:B.</infon><infon key="name_1">surname:Kraska;given-names:T.</infon><infon key="name_2">surname:Franklin;given-names:M. J.</infon><infon key="name_3">surname:Sarkar;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">29th IEEE International Conference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8–12, 2013</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>72072</offset><text>Crowdsourced enumeration queries</text></passage><passage><infon key="fpage">75</infon><infon key="lpage">78</infon><infon key="name_0">surname:von Ahn;given-names:L.</infon><infon key="name_1">surname:Kedia;given-names:M.</infon><infon key="name_2">surname:Blum;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings Conference on Human Factors in Computing Systems, CHI 2006</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>72105</offset><text>Verbosity: a game for collecting common-sense facts</text></passage><passage><infon key="fpage">78</infon><infon key="lpage">85</infon><infon key="name_0">surname:Vrandečić;given-names:D.</infon><infon key="name_1">surname:Krötzsch;given-names:M.</infon><infon key="pub-id_doi">10.1145/2629489</infon><infon key="section_type">REF</infon><infon key="source">Commun, ACM</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2014</infon><offset>72157</offset><text>Wikidata: a free collaborative knowledge base</text></passage><passage><infon key="name_0">surname:Wang;given-names:Q.</infon><infon key="name_1">surname:Yang;given-names:L.</infon><infon key="name_2">surname:Kanagal;given-names:B.</infon><infon key="name_3">surname:Sanghai;given-names:S.</infon><infon key="name_4">surname:Sivakumar;given-names:D.</infon><infon key="name_5">surname:Shu;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">KDD-20</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>72203</offset><text>Learning to extract attribute value from product via question answering: a multi-task approach</text></passage><passage><infon key="name_0">surname:Welty;given-names:C.</infon><infon key="name_1">surname:Aroyo;given-names:L.</infon><infon key="name_2">surname:Korn;given-names:F.</infon><infon key="name_3">surname:McCarthy;given-names:S.</infon><infon key="name_4">surname:Zhao;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of HCOMP-2021. AAAI</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>72298</offset><text>Rapid instance-level knowledge acquisition for google maps from class-level common sense</text></passage><passage><infon key="fpage">228</infon><infon key="lpage">242</infon><infon key="name_0">surname:Welty;given-names:C.</infon><infon key="name_1">surname:Barker;given-names:K.</infon><infon key="name_2">surname:Aroyo;given-names:L.</infon><infon key="name_3">surname:Arora;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">The Semantic Web-ISWC 2012</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>72387</offset><text>Query driven hypothesis generation for answering queries over nlp graphs</text></passage><passage><infon key="fpage">672</infon><infon key="lpage">680</infon><infon key="name_0">surname:Xu;given-names:D.</infon><infon key="name_1">surname:Ruan;given-names:C.</infon><infon key="name_2">surname:Korpeoglu;given-names:E.</infon><infon key="name_3">surname:Kumar;given-names:S.</infon><infon key="name_4">surname:Achan;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of Conference on Web Search and Data Mining, WSDM '20</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>72460</offset><text>Product knowledge graph embedding for e-commerce</text></passage><passage><infon key="fpage">689</infon><infon key="lpage">719</infon><infon key="name_0">surname:Zang;given-names:L.</infon><infon key="name_1">surname:Cao;given-names:C.</infon><infon key="name_2">surname:Cao;given-names:Y.</infon><infon key="name_3">surname:Wu;given-names:Y.</infon><infon key="name_4">surname:Cao;given-names:C.</infon><infon key="pub-id_doi">10.1007/s11390-013-1369-6</infon><infon key="section_type">REF</infon><infon key="source">J. Comput. Sci. Technol</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2013</infon><offset>72509</offset><text>A survey of commonsense knowledge acquisition</text></passage><passage><infon key="fpage">1049</infon><infon key="lpage">1058</infon><infon key="name_0">surname:Zheng;given-names:G.</infon><infon key="name_1">surname:Mukherjee;given-names:S.</infon><infon key="name_2">surname:Dong;given-names:X. L.</infon><infon key="name_3">surname:Li;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of KDD '18</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>72555</offset><text>Opentag: Open attribute value extraction from product profiles</text></passage></document></collection>
