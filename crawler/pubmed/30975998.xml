<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201221</date><key>pmc.key</key><document><id>6472378</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1038/s41597-019-0035-4</infon><infon key="article-id_pmc">6472378</infon><infon key="article-id_pmid">30975998</infon><infon key="article-id_publisher-id">35</infon><infon key="elocation-id">30</infon><infon key="kwd">Computational neuroscience Image processing Databases Magnetic resonance imaging</infon><infon key="license">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.The Creative Commons Public Domain Dedication waiver http://creativecommons.org/publicdomain/zero/1.0/ applies to the metadata files associated with this article.</infon><infon key="name_0">surname:Esteban;given-names:Oscar</infon><infon key="name_1">surname:Blair;given-names:Ross W.</infon><infon key="name_10">surname:Nielson;given-names:Dylan M.</infon><infon key="name_11">surname:Varada;given-names:Jan C.</infon><infon key="name_12">surname:Marrett;given-names:Sean</infon><infon key="name_13">surname:Thomas;given-names:Adam G.</infon><infon key="name_14">surname:Poldrack;given-names:Russell A.</infon><infon key="name_15">surname:Gorgolewski;given-names:Krzysztof J.</infon><infon key="name_2">surname:Nielson;given-names:Dylan M.</infon><infon key="name_3">surname:Varada;given-names:Jan C.</infon><infon key="name_4">surname:Marrett;given-names:Sean</infon><infon key="name_5">surname:Thomas;given-names:Adam G.</infon><infon key="name_6">surname:Poldrack;given-names:Russell A.</infon><infon key="name_7">surname:Gorgolewski;given-names:Krzysztof J.</infon><infon key="name_8">surname:Esteban;given-names:Oscar</infon><infon key="name_9">surname:Blair;given-names:Ross W.</infon><infon key="section_type">TITLE</infon><infon key="title">Subject terms</infon><infon key="type">front</infon><infon key="volume">6</infon><infon key="year">2019</infon><offset>0</offset><text>Crowdsourced MRI quality metrics and expert quality annotations for training of humans and machines</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>100</offset><text>The neuroimaging community is steering towards increasingly large sample sizes, which are highly heterogeneous because they can only be acquired by multi-site consortia. The visual assessment of every imaging scan is a necessary quality control step, yet arduous and time-consuming. A sizeable body of evidence shows that images of low quality are a source of variability that may be comparable to the effect size under study. We present the MRIQC Web-API, an open crowdsourced database that collects image quality metrics extracted from MR images and corresponding manual assessments by experts. The database is rapidly growing, and currently contains over 100,000 records of image quality metrics of functional and anatomical MRIs of the human brain, and over 200 expert ratings. The resource is designed for researchers to share image quality metrics and annotations that can readily be reused in training human experts and machine learning algorithms. The ultimate goal of the database is to allow the development of fully automated quality control tools that outperform expert ratings in identifying subpar images.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1220</offset><text>Machine-accessible metadata file describing the reported data (ISA-Tab format)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1299</offset><text>Background &amp; Summary</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1320</offset><text>Ensuring the quality of neuroimaging data is a crucial initial step for any image analysis workflow because low-quality images may obscure the effects of scientific interest. Most approaches use manual quality control (QC), which entails screening every single image of a dataset individually. However, manual QC suffers from at least two problems: unreliability and time-consuming nature for large datasets. Unreliability creates great difficulty in defining objective exclusion criteria in studies and stems from intrinsically large intra-rater and inter-rater variabilities. Intra-rater variability derives from aspects such as training, subjectivity, varying annotation settings and protocols, fatigue or bookkeeping errors. The difficulty in calibrating between experts lies at the heart of inter-rater variability. In addition to the need for objective exclusion criteria, the current neuroimaging data deluge makes the manual QC of every magnetic resonance imaging (MRI) scan impractical. For these reasons, there has been great interest in automated QC, which is progressively gaining attention with the convergence of machine learning solutions. Early approaches to objectively estimate image quality have employed “image quality metrics” (IQMs) that quantify variably interpretable aspects of image quality (e.g., summary statistics of image intensities, signal-to-noise ratio, coefficient of joint variation, Euler angle, etc.). The approach has been shown sufficiently reliable in single-site samples, but it does not generalize well to new images acquired at sites unseen by the decision algorithm. Decision algorithms do not generalize to new datasets because the large between-site variability as compared to the within-site variability of features poses a challenging harmonization problem, similar to “batch-effects” in genomic analyses. Additional pitfalls limiting fully automated QC of MRI relate to the small size of databases that include quality annotations, and the unreliability of such annotations (or “labels noise”). As described previously, rating the quality of every image in large databases is an arduous, unreliable, and costly task. The convergence of limited size of samples annotated for quality and the labels noise preclude the definition of normative, standard values for the IQMs that work well for any dataset, and also, the generalization of machine learning solutions. Keshavan et al. have recently proposed a creative solution to the problem of visually assessing large datasets. They were able to annotate over 80,000 bidimensional slices extracted from 722 brain 3D images using BraindR, a smartphone application for crowdsourcing. They also proposed a novel approach to the QC problem by training a convolutional neural network on BraindR ratings, with excellent results (area under the curve, 0.99). Their QC tool performed as well as MRIQC (which uses IQMs and a random forests classifier to decide which images should be excluded) on their single-site dataset. By collecting several ratings per screened entity, they were able to effectively minimize the labels noise problem with the averaging of expert ratings. As limitations to their work, we would count the use of 2D images for annotation and the use of a single-site database. In sum, automating QC requires large datasets collected across sites, and rated by many individuals in order to ensure generalizability.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4753</offset><text>Therefore, the MRIQC Web-API (web-application program interface) provides a unique platform to address the issues raised above. The database collects two types of records: i) IQMs alongside corresponding metadata extracted by MRIQC (or any other compatible client) from T1w (T1-weighted), T2w (T2-weighted) and BOLD (blood-oxygen-level-dependent) MRI images; and ii) manual quality ratings from users of the MRIQC software. It is important to note that the original image data are not transferred to the MRIQC Web-API.</text></passage><passage><infon key="file">41597_2019_35_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5272</offset><text>A rapidly growing MRI quality control knowledge base. The database has accumulated over 50,000 records of IQMs generated for T1-weighted (T1w) images and 60,000 records for BOLD images. Records presented are unique, i.e. after exclusion of duplicated images.</text></passage><passage><infon key="file">41597_2019_35_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5531</offset><text>Experimental workflow to generate the database. A dataset is processed with MRIQC. Processing finishes with a POST request to the MRIQC Web API endpoint with a payload containing the image quality metrics (IQMs) and some anonymized metadata (e.g. imaging parameters, the unique identifier for the image data, etc.) in JSON format. Once stored, the endpoint can be queried to fetch the crowdsourced IQMs. Finally, a widget (Fig. 3) allows the user to annotate existing records in the MRIQC Web API.</text></passage><passage><infon key="file">41597_2019_35_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>6030</offset><text>MRIQC visual reports and feedback tool. The visual reports generated with MRIQC include the “Rate Image” widget. After screening of the particular dataset, the expert can assign one quality level (among “exclude”, “poor”, “acceptable”, and “excellent”) and also select from a list of MR artifacts typically found in MRI datasets. When the annotation is finished, the user can download the ratings to their local hard disk and submit them to the Web API.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6504</offset><text>Within fourteen months we have collected over 50,000 and 60,000 records of anatomical and functional IQMs, respectively (Fig. 1). These IQMs are extracted and automatically submitted (unless the user opts out) with MRIQC (Fig. 2). Second, we leverage the efficiency of MRIQC’s reports in assessing individual 3D images with a simple interface that allows experts to submit their ratings with a few clicks (Fig. 3). This assessment protocol avoids clerical errors from the operator, as ratings are automatically handled and registered. In other words, MRIQC users are building a very large database with minimal effort every day. As only the IQMs and manual ratings are crowdsourced (i.e. images are not shared), data collection is not limited to public datasets only. Nonetheless, unique image checksums are stored in order to identify matching images. Therefore, such checksums allow users to find public images that IQMs and/or ratings derive from. The presented resource is envisioned to train automatic QC tools and to develop human expert training programs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>7572</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7580</offset><text>Here we describe an open database that collects both IQM vectors extracted from functional and anatomical MRI scans, along with quality assessments done by experts based on visual inspection of images. Although it was envisioned as a lightweight web-service tailored to MRIQC, the database is able to receive new records from any other software, provided they are able to correctly query the API (application programming interface).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8013</offset><text>Data generation and collection workflow</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8053</offset><text>Execution of MRIQC and submission of IQMs: T1w, T2w, and BOLD images are processed with MRIQC, which computes a number of IQMs (described in section Technical Validation). The IQMs and corresponding metadata are formatted in JavaScript Object Notation (JSON), and MRIQC automatically submits them to a representational state transfer (REST) or RESTful endpoint of the Web-API. Users can opt-out if they do not wish to share their IQMs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8489</offset><text>JSON records are received by the endpoint, validated, and stored in the database. Each record includes the vector of IQMs, a unique checksum calculated on the original image, and additional anonymized metadata and provenance.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8715</offset><text>Visualization of the individual reports: MRIQC generates dynamic HTML (hypertext markup language) reports that speed up the visual assessment of each image of the dataset. Since its version 0.12.2, MRIQC includes a widget (see Fig. 2) that allows the researcher to assign a quality rating to the image being screened.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9034</offset><text>Crowdsourcing expert quality ratings: the RESTful endpoint receives the quality ratings, which are linked to the original image via their unique identifier.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9191</offset><text>Retrieving records: the database can be queried for records with any HTTP (HyperText Transfer Protocol) client or via the web using our interface: https://mriqc.nimh.nih.gov/. Additionally, a snapshot of the database at the time of writing has been deposited to FigShare.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9463</offset><text>The overall framework involves the following workflow (summarized in Fig. 2):</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>9542</offset><text>Data Records</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>9555</offset><text>List of data tables retrieved from MRIQC-WebAPI.</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Filename&lt;/th&gt;&lt;th&gt;Size&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;bold.csv&lt;/td&gt;&lt;td&gt;71 MB&lt;/td&gt;&lt;td&gt;IQMs and metadata of BOLD images (unique records)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;bold_curated.csv&lt;/td&gt;&lt;td&gt;162 MB&lt;/td&gt;&lt;td&gt;Same as bold.csv, after curation and checksum matching&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T1w.csv&lt;/td&gt;&lt;td&gt;79 MB&lt;/td&gt;&lt;td&gt;IQMs and metadata of T1w images (unique records)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T1w_curated.csv&lt;/td&gt;&lt;td&gt;110 MB&lt;/td&gt;&lt;td&gt;Same as T1w.csv, after curation and checksum matching&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T2w.csv&lt;/td&gt;&lt;td&gt;1.1 MB&lt;/td&gt;&lt;td&gt;IQMs and metadata of T2w images (unique records)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T2w_curated.csv&lt;/td&gt;&lt;td&gt;1.7 MB&lt;/td&gt;&lt;td&gt;Same as T2w.csv, after curation and checksum matching&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;rating.csv&lt;/td&gt;&lt;td&gt;131 kB&lt;/td&gt;&lt;td&gt;Manually assigned quality annotations&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>9604</offset><text>Filename	Size	Description	 	bold.csv	71 MB	IQMs and metadata of BOLD images (unique records)	 	bold_curated.csv	162 MB	Same as bold.csv, after curation and checksum matching	 	T1w.csv	79 MB	IQMs and metadata of T1w images (unique records)	 	T1w_curated.csv	110 MB	Same as T1w.csv, after curation and checksum matching	 	T2w.csv	1.1 MB	IQMs and metadata of T2w images (unique records)	 	T2w_curated.csv	1.7 MB	Same as T2w.csv, after curation and checksum matching	 	rating.csv	131 kB	Manually assigned quality annotations	 	</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>10142</offset><text>The following datasets are available at FigShare. The &lt; name &gt; _curated.csv file versions correspond to the original tables after matching checksums to images in publicly available databases (and further curation as shown in https://www.kaggle.com/chrisfilo/mriqc-data-cleaning).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10430</offset><text>A full data snapshot of the database at the time of submission is available at FigShare. Alternatively, data are accessible via DataLad with the dataset https://github.com/oesteban/mriqc-webapi-snapshot. Table 1 describes the structure of the dataset being released.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10698</offset><text>To obtain the latest updated records, the database can be programmatically queried online to get all the currently stored records through its RESTful API.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10853</offset><text>MRIQC reports, generated for all T1w images found in OpenfMRI are available for expert training at https://mriqc.s3.amazonaws.com/index.html#openfmri/.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11005</offset><text>Technical Validation</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>11026</offset><text>Summary table of image quality metrics for anatomical (T1w, T2w) MRI.</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs based on noise measurements&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CJV&lt;/td&gt;&lt;td&gt;The coefficient of joint variation of GM and WM was proposed as an objective function by Ganzetti &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR23&quot;&gt;23&lt;/xref&gt;&lt;/sup&gt; for the optimization of INU correction algorithms. Higher values are related to the presence of heavy head motion and large INU artifacts.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CNR&lt;/td&gt;&lt;td&gt;The contrast-to-noise ratio&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR24&quot;&gt;24&lt;/xref&gt;&lt;/sup&gt; is an extension of the SNR calculation to evaluate how separated the tissue distributions of GM and WM are. Higher values indicate better quality.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SNR&lt;/td&gt;&lt;td&gt;MRIQC includes the signal-to-noise ratio calculation proposed by Dietrich &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR25&quot;&gt;25&lt;/xref&gt;&lt;/sup&gt;, using the air background as noise reference. Additionally, for images that have undergone some noise reduction processing, or the more complex noise realizations of current parallel acquisitions, a simplified calculation using the within tissue variance is also provided.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;QI&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;The second quality index of Mortamet &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR8&quot;&gt;8&lt;/xref&gt;&lt;/sup&gt; is a calculation of the goodness-of-fit of a χ&lt;sup&gt;2&lt;/sup&gt; distribution on the air mask, once the artifactual intensities detected for computing the QI&lt;sub&gt;1&lt;/sub&gt; index have been removed. The description of the QI&lt;sub&gt;1&lt;/sub&gt; is found below.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs based on information theory&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EFC&lt;/td&gt;&lt;td&gt;The entropy-focus criterion&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR26&quot;&gt;26&lt;/xref&gt;&lt;/sup&gt; uses the Shannon entropy of voxel intensities as an indication of ghosting and blurring induced by head motion. Lower values are better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FBER&lt;/td&gt;&lt;td&gt;The foreground-background energy ratio&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/sup&gt; is calculated as the mean energy of image values within the head relative to the mean energy of image values in the air mask. Consequently, higher values are better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs targeting specific artifacts&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;INU&lt;/td&gt;&lt;td&gt;MRIQC measures the location and spread of the bias field extracted estimated by the intensity non-uniformity (INU) correction. The smaller spreads located around 1.0 are better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;QI&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;Mortamet’s first quality index&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR8&quot;&gt;8&lt;/xref&gt;&lt;/sup&gt; measures the number of artifactual intensities in the air surrounding the head above the nasio-cerebellar axis. The smaller QI&lt;sub&gt;1&lt;/sub&gt;, the better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WM2MAX&lt;/td&gt;&lt;td&gt;The white-matter to maximum intensity ratio is the median intensity within the WM mask over the 95% percentile of the full intensity distribution, that captures the existence of long tails due to hyper-intensity of the carotid vessels and fat. Values should be around the interval [0.6, 0.8]&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;Other IQMs&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FWHM&lt;/td&gt;&lt;td&gt;The full-width half-maximum&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR27&quot;&gt;27&lt;/xref&gt;&lt;/sup&gt; is an estimation of the blurriness of the image calculated with AFNI’s 3dFWHMx. Smaller is better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ICVs&lt;/td&gt;&lt;td&gt;Estimation of the intracranial volume (ICV) of each tissue calculated on the FSL fast’s segmentation. Normative values fall around 20%, 45% and 35% for cerebrospinal fluid (CSF), WM and GM, respectively.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;rPVE&lt;/td&gt;&lt;td&gt;The residual partial volume effect feature is a tissue-wise sum of partial volumes that fall in the range [5–95%] of the total volume of a pixel, computed on the partial volume maps generated by FSL fast. Smaller residual partial volume effects (rPVEs) are better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SSTATs&lt;/td&gt;&lt;td&gt;Several summary statistics (mean, standard deviation, percentiles 5% and 95%, and kurtosis) are computed within the following regions of interest: background, CSF, WM, and GM.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TPMs&lt;/td&gt;&lt;td&gt;Overlap of tissue probability maps estimated from the image and the corresponding maps from the ICBM nonlinear-asymmetric 2009c template&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR28&quot;&gt;28&lt;/xref&gt;&lt;/sup&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>11096</offset><text>IQMs based on noise measurements	 	CJV	The coefficient of joint variation of GM and WM was proposed as an objective function by Ganzetti et al. for the optimization of INU correction algorithms. Higher values are related to the presence of heavy head motion and large INU artifacts.	 	CNR	The contrast-to-noise ratio is an extension of the SNR calculation to evaluate how separated the tissue distributions of GM and WM are. Higher values indicate better quality.	 	SNR	MRIQC includes the signal-to-noise ratio calculation proposed by Dietrich et al., using the air background as noise reference. Additionally, for images that have undergone some noise reduction processing, or the more complex noise realizations of current parallel acquisitions, a simplified calculation using the within tissue variance is also provided.	 	QI2	The second quality index of Mortamet et al. is a calculation of the goodness-of-fit of a χ2 distribution on the air mask, once the artifactual intensities detected for computing the QI1 index have been removed. The description of the QI1 is found below.	 	IQMs based on information theory	 	EFC	The entropy-focus criterion uses the Shannon entropy of voxel intensities as an indication of ghosting and blurring induced by head motion. Lower values are better.	 	FBER	The foreground-background energy ratio is calculated as the mean energy of image values within the head relative to the mean energy of image values in the air mask. Consequently, higher values are better.	 	IQMs targeting specific artifacts	 	INU	MRIQC measures the location and spread of the bias field extracted estimated by the intensity non-uniformity (INU) correction. The smaller spreads located around 1.0 are better.	 	QI1	Mortamet’s first quality index measures the number of artifactual intensities in the air surrounding the head above the nasio-cerebellar axis. The smaller QI1, the better.	 	WM2MAX	The white-matter to maximum intensity ratio is the median intensity within the WM mask over the 95% percentile of the full intensity distribution, that captures the existence of long tails due to hyper-intensity of the carotid vessels and fat. Values should be around the interval [0.6, 0.8]	 	Other IQMs	 	FWHM	The full-width half-maximum is an estimation of the blurriness of the image calculated with AFNI’s 3dFWHMx. Smaller is better.	 	ICVs	Estimation of the intracranial volume (ICV) of each tissue calculated on the FSL fast’s segmentation. Normative values fall around 20%, 45% and 35% for cerebrospinal fluid (CSF), WM and GM, respectively.	 	rPVE	The residual partial volume effect feature is a tissue-wise sum of partial volumes that fall in the range [5–95%] of the total volume of a pixel, computed on the partial volume maps generated by FSL fast. Smaller residual partial volume effects (rPVEs) are better.	 	SSTATs	Several summary statistics (mean, standard deviation, percentiles 5% and 95%, and kurtosis) are computed within the following regions of interest: background, CSF, WM, and GM.	 	TPMs	Overlap of tissue probability maps estimated from the image and the corresponding maps from the ICBM nonlinear-asymmetric 2009c template.	 	</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>14269</offset><text>MRIQC produces a vector of 64 image quality metrics (IQMs) per input T1w or T2w scan. (Reproduced from our previous work).</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>14392</offset><text>Summary table of image quality metrics for functional (BOLD) MRI.</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;Spatial IQMs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;EFC, FBER, FWHM, SNR, SSTATs (see Table &lt;xref rid=&quot;Tab2&quot; ref-type=&quot;table&quot;&gt;2&lt;/xref&gt;)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs measuring temporal variations&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;tSNR&lt;/td&gt;&lt;td&gt;A simplified interpretation of the original temporal SNR definition by Krüger &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR29&quot;&gt;29&lt;/xref&gt;&lt;/sup&gt;. We report the median value of the tSNR map calculated as the average BOLD signal across time over the corresponding temporal s.d. map.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GCOR&lt;/td&gt;&lt;td&gt;Summary of time-series correlation as in&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR30&quot;&gt;30&lt;/xref&gt;&lt;/sup&gt; using AFNI’s @compute_gcor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DVARS&lt;/td&gt;&lt;td&gt;The spatial standard deviation of the data after temporal differencing. Indexes the rate of change of BOLD signal across the entire brain at each frame of data. DVARS is calculated using &lt;italic&gt;Nipype&lt;/italic&gt;, after head-motion correction&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs targeting specific artifacts&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FD&lt;/td&gt;&lt;td&gt;Framewise Displacement - Proposed by Power &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR1&quot;&gt;1&lt;/xref&gt;&lt;/sup&gt; to regress out instantaneous head-motion in fMRI studies. MRIQC reports the average FD.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GSR&lt;/td&gt;&lt;td&gt;The Ghost to Signal Ratio&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR31&quot;&gt;31&lt;/xref&gt;&lt;/sup&gt; estimates the mean signal in the areas of the image that are prone to N/2 ghosts on the phase encoding direction with respect to the mean signal within the brain mask&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/sup&gt;. Lower values are better.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DUMMY&lt;/td&gt;&lt;td&gt;The number of dummy scans - A number of volumes at the beginning of the fMRI time-series identified as nonsteady states.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;IQMs from AFNI&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;AOR&lt;/td&gt;&lt;td&gt;AFNI’s outlier ratio - Mean fraction of outliers per fMRI volume as given by AFNI’s 3dToutcount&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;AQI&lt;/td&gt;&lt;td&gt;AFNI’s quality index - Mean quality index as computed by AFNI’s 3dTqual&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>14458</offset><text>Spatial IQMs	 	EFC, FBER, FWHM, SNR, SSTATs (see Table 2)	 	IQMs measuring temporal variations	 	tSNR	A simplified interpretation of the original temporal SNR definition by Krüger et al.. We report the median value of the tSNR map calculated as the average BOLD signal across time over the corresponding temporal s.d. map.	 	GCOR	Summary of time-series correlation as in using AFNI’s @compute_gcor	 	DVARS	The spatial standard deviation of the data after temporal differencing. Indexes the rate of change of BOLD signal across the entire brain at each frame of data. DVARS is calculated using Nipype, after head-motion correction	 	IQMs targeting specific artifacts	 	FD	Framewise Displacement - Proposed by Power et al. to regress out instantaneous head-motion in fMRI studies. MRIQC reports the average FD.	 	GSR	The Ghost to Signal Ratio estimates the mean signal in the areas of the image that are prone to N/2 ghosts on the phase encoding direction with respect to the mean signal within the brain mask. Lower values are better.	 	DUMMY	The number of dummy scans - A number of volumes at the beginning of the fMRI time-series identified as nonsteady states.	 	IQMs from AFNI	 	AOR	AFNI’s outlier ratio - Mean fraction of outliers per fMRI volume as given by AFNI’s 3dToutcount	 	AQI	AFNI’s quality index - Mean quality index as computed by AFNI’s 3dTqual	 	</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>15833</offset><text>MRIQC produces a vector of 64 image quality metrics (IQMs) per input BOLD scan.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>15913</offset><text>Summary table of quality assessment values.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;Expert rating&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Exclude&lt;/td&gt;&lt;td&gt;Assigned to images that show quality defects that preclude any type of processing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Poor&lt;/td&gt;&lt;td&gt;Assigned to images that, although presenting some quality problem, may tolerate some types of processing. For instance, a T1w image that may be used as the co-registration reference, but will probably generate biased cortical thickness measurements.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Acceptable&lt;/td&gt;&lt;td&gt;Assigned to images that do not show any substantial issue that may preclude processing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Excellent&lt;/td&gt;&lt;td&gt;Assigned to images without quality issues&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;
&lt;bold&gt;Artifacts&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt; A vector of boolean values corresponding to the following list of possible artifacts found in the image:&lt;break/&gt;• Head motion artifacts&lt;break/&gt;• Eye spillover through phase-encoding axis&lt;break/&gt;• Non-eye spillover through phase-encoding axis&lt;break/&gt;• Coil failure&lt;break/&gt;• Global noise&lt;break/&gt;• Local noise&lt;break/&gt;• Electromagnetic interference/perturbation&lt;break/&gt;• Problematic field-of-view prescription/Wrap-around&lt;break/&gt;• Aliasing ghosts&lt;break/&gt;• Other ghosts&lt;break/&gt;• Intensity non-uniformity&lt;break/&gt;• Temporal field variation&lt;break/&gt;• Reconstruction and postprocessing (e.g. denoising, defacing, resamplings)&lt;break/&gt;• Uncategorized artifact&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>15957</offset><text>Expert rating	 	Exclude	Assigned to images that show quality defects that preclude any type of processing	 	Poor	Assigned to images that, although presenting some quality problem, may tolerate some types of processing. For instance, a T1w image that may be used as the co-registration reference, but will probably generate biased cortical thickness measurements.	 	Acceptable	Assigned to images that do not show any substantial issue that may preclude processing	 	Excellent	Assigned to images without quality issues	 	Artifacts	 	 A vector of boolean values corresponding to the following list of possible artifacts found in the image:• Head motion artifacts• Eye spillover through phase-encoding axis• Non-eye spillover through phase-encoding axis• Coil failure• Global noise• Local noise• Electromagnetic interference/perturbation• Problematic field-of-view prescription/Wrap-around• Aliasing ghosts• Other ghosts• Intensity non-uniformity• Temporal field variation• Reconstruction and postprocessing (e.g. denoising, defacing, resamplings)• Uncategorized artifact	 	</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>17056</offset><text>Annotations received through the feedback widget are stored in a separate database collecting one rating value and an array of artifacts present in the image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17215</offset><text>MRIQC extends the list of IQMs from the quality assessment protocol (QAP), which was constructed from a careful review of the MRI and medical imaging literature. The technical validity of measurements stored to the database is demonstrated by our previous work on the MRIQC client tool and its documentation website: https://mriqc.readthedocs.io/en/latest/measures.html. Definitions for the anatomical IQMs are given in Table 2, and for functional IQMs in Table 3. Finally, the structure of data records containing the manual QC feedback is summarized in Table 4.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17782</offset><text>Limitations</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17794</offset><text>The main limitation of the database resides in that a substantial fraction of the records (e.g., around 50% for the BOLD IQMs) miss important information about imaging parameters. The original cause is that such information was not encoded with the input dataset being fed into MRIQC. However, as BIDS is permeating the current neuroimaging workflow we can expect BIDS datasets to become more complete, thereby allowing MRIQC to submit such valuable information to the Web API. Moreover, the gradual adoption of better DICOM-to-BIDS conversion tools such as HeuDiConv, which automatically encodes all relevant fields in the BIDS structure, will surely help minimize this issue.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18472</offset><text>During the peer-review process of this manuscript, one reviewer identified a potential problem casting float numbers into integers on the content of the “bids_MagneticFieldStrength” field of all records. The bug was confirmed and consequently fixed on the MRIQC Web-API, and all records available on the database snapshot deposited at FigShare have been amended. When retrieving records directly from the Web-API, beware that those with creation date prior to Jan 16, 2019, require a revision of the tainted field. Similarly, the reviewer identified some 1,600 records with an echo-time (TE) value 30.0 with a repetition time (TR) of 2.0, which indicates that TE was misspecified in milliseconds (BIDS mandates seconds). Problematic data points can be (and are) present in the data, and there is likely no setup that could fully rule out the inclusion of misidentified results, although the automation in conversion above mentioned will surely minimize this problem.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>19443</offset><text>Usage Notes</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19455</offset><text>Sampling the distribution of IQMs and imaging parameters across datasets (including both publicly available and private), and across scanning sites.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19604</offset><text>Ease the image QC process, crowdsourcing its outcomes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19659</offset><text>Training machines and humans.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19689</offset><text>Primarily, the database was envisioned to address three use-cases:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19756</offset><text>These potential usages are revised with finer detail in the following. Note this resource is focused on quality control (QC), rather than quality assessment (QA). While QC focuses on flagging images that may endanger downstream analysis for their bad quality (i.e., identifying outliers), QA identifies issues that degrade all image’s quality (i.e., improving the overall quality of images after a problem spotted in the scanning device or acquisition protocol -via QC of actual images- is fixed).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20256</offset><text>Collecting IQMs and imaging parameters</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20295</offset><text>Based on this information, researchers can explore questions such as the relationship of particular imaging parameters (e.g. MR scan vendor, or more interestingly, the multi-band acceleration factor of newest functional MRI sequences) with respect to the signal-to-noise ratio or the power of N/2 aliasing ghosts. Jupyter notebooks demonstrating examples of this use-case are available at https://www.kaggle.com/chrisfilo/mriqc/kernels.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20732</offset><text>Crowdsourcing an optimized assessment process</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20778</offset><text>To our knowledge, the community lacks a large database of multi-site MRI annotated for quality that permits the application of machine learning techniques to automate QC. As Keshavan et al. have demonstrated, minimizing the time cost and fatigue load along with the elimination of bookkeeping tasks in the quality assessment of individual MR scans enables collection and annotation of massive datasets. The graphical user interface for this use-case is presented in Fig. 2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21253</offset><text>A database to train machines and humans</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>21293</offset><text>Training machines</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21311</offset><text>As introduced before, the major bottleneck in training models that can predict a quality score for an image or identify specific artifacts, without problems to generalize across MR scanners and sites, is the small size of existing datasets with corresponding quality annotations. Additionally, these annotations, if they exist, are done with extremely varying protocols. Thus, the ability of the presented database to crowdsource quality ratings assigned by humans after visual inspection addresses both problems. The availability of multi-site, large samples with crowdsourced quality annotations that followed a homogeneous protocol (the MRIQC reports) will allow building models that overperform the random forests classifier of MRIQC, in the task of predicting the quality rating a human would have assigned to an image, given a vector of IQMs (i.e., from IQMs to quality labels). Matching public image checksums, this resource will also enable to train end-to-end (from images to quality labels) deep-learning solutions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>22337</offset><text>Training humans</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22353</offset><text>Institutions can use the resource to train their experts and compare their assessments across themselves and against the existing quality annotations corresponding to publicly available datasets. Programs for training experts on quality assessment can be designed to leverage the knowledge shared via the proposed database.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>22677</offset><text>ISA-Tab metadata file</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">footnote</infon><offset>22699</offset><text>Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>22838</offset><text>ISA-Tab metadata</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22855</offset><text>is available for this paper at 10.1038/s41597-019-0035-4.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>22913</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>22934</offset><text>O.E. - Data curation, investigation, software, validation, visualization, writing &amp; editing. R.W.B. - Conceptualization, investigation, software, validation, visualization, writing &amp; editing. D.M.N. - Data curation, investigation, infrastructure, software, validation, writing &amp; editing. J.C.V. - Investigation, infrastructure, software, validation, writing &amp; editing. S.M. - Funding acquisition, infrastructure, resources, supervision, writing &amp; editing. A.G.T. - Funding acquisition, infrastructure, resources, supervision, writing &amp; editing. R.A.P. - Funding acquisition, resources, supervision, writing &amp; editing. K.J.G. - Conceptualization, investigation, software, validation, resources, supervision, writing &amp; editing.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>23660</offset><text>Code Availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>23678</offset><text>The MRIQC Web API is available under the Apache-2.0 license. The source code is accessible through GitHub (https://github.com/poldracklab/mriqcwebapi).</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>23830</offset><text>MRIQC is one possible client to generate IQMs and submit rating feedback. It is available under the BSD 3-clause license. The source code is publicly accessible through GitHub (https://github.com/poldracklab/mriqc).</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>24046</offset><text>Competing Interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>24066</offset><text>The authors declare no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>24110</offset><text>References</text></passage><passage><infon key="fpage">2142</infon><infon key="lpage">2154</infon><infon key="name_0">surname:Power;given-names:JD</infon><infon key="name_1">surname:Barnes;given-names:KA</infon><infon key="name_2">surname:Snyder;given-names:AZ</infon><infon key="name_3">surname:Schlaggar;given-names:BL</infon><infon key="name_4">surname:Petersen;given-names:SE</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2011.10.018</infon><infon key="pub-id_pmid">22019881</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2012</infon><offset>24121</offset><text>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</text></passage><passage><infon key="fpage">79</infon><infon key="lpage">90</infon><infon key="name_0">surname:Yendiki;given-names:A</infon><infon key="name_1">surname:Koldewyn;given-names:K</infon><infon key="name_2">surname:Kakunoori;given-names:S</infon><infon key="name_3">surname:Kanwisher;given-names:N</infon><infon key="name_4">surname:Fischl;given-names:B</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2013.11.027</infon><infon key="pub-id_pmid">24269273</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">88</infon><infon key="year">2014</infon><offset>24224</offset><text>Spurious group differences due to head motion in a diffusion MRI study</text></passage><passage><infon key="fpage">107</infon><infon key="lpage">115</infon><infon key="name_0">surname:Reuter;given-names:M</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2014.12.006</infon><infon key="pub-id_pmid">25498430</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">107</infon><infon key="year">2015</infon><offset>24295</offset><text>Head motion during MRI acquisition reduces gray matter volume and thickness estimates</text></passage><passage><infon key="fpage">2385</infon><infon key="lpage">2397</infon><infon key="name_0">surname:Alexander-Bloch;given-names:A</infon><infon key="pub-id_doi">10.1002/hbm.23180</infon><infon key="pub-id_pmid">27004471</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2016</infon><offset>24381</offset><text>Subtle in-scanner motion biases automated measurement of brain anatomy from in vivo MRI</text></passage><passage><infon key="fpage">277</infon><infon key="lpage">281</infon><infon key="name_0">surname:Gardner;given-names:EA</infon><infon key="pub-id_doi">10.1016/S1076-6332(05)80184-9</infon><infon key="pub-id_pmid">9419562</infon><infon key="section_type">REF</infon><infon key="source">Acad. Radiol.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">1995</infon><offset>24469</offset><text>Detection of degradation of magnetic resonance (MR) images: Comparison of an automated MR image-quality analysis system with trained human observers</text></passage><passage><infon key="fpage">243</infon><infon key="lpage">262</infon><infon key="name_0">surname:Woodard;given-names:JP</infon><infon key="name_1">surname:Carley-Spencer;given-names:MP</infon><infon key="pub-id_doi">10.1385/NI:4:3:243</infon><infon key="pub-id_pmid">16943630</infon><infon key="section_type">REF</infon><infon key="source">Neuroinformatics</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2006</infon><offset>24618</offset><text>No-Reference image quality metrics for structural MRI</text></passage><passage><infon key="fpage">308</infon><infon key="lpage">319</infon><infon key="name_0">surname:Gedamu;given-names:EL</infon><infon key="name_1">surname:Collins;given-names:DI</infon><infon key="name_2">surname:Arnold;given-names:DL</infon><infon key="pub-id_doi">10.1002/jmri.21434</infon><infon key="pub-id_pmid">18666143</infon><infon key="section_type">REF</infon><infon key="source">J. Magn. Reson. Imaging.</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2008</infon><offset>24672</offset><text>Automated quality control of brain MR images</text></passage><passage><infon key="fpage">365</infon><infon key="lpage">372</infon><infon key="name_0">surname:Mortamet;given-names:B</infon><infon key="pub-id_doi">10.1002/mrm.21992</infon><infon key="pub-id_pmid">19526493</infon><infon key="section_type">REF</infon><infon key="source">Magn. Reson. Med.</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2009</infon><offset>24717</offset><text>Automatic quality assessment in structural brain magnetic resonance imaging</text></passage><passage><infon key="fpage">e0184661</infon><infon key="name_0">surname:Esteban;given-names:O</infon><infon key="pub-id_doi">10.1371/journal.pone.0184661</infon><infon key="pub-id_pmid">28945803</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2017</infon><offset>24793</offset><text>MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>24877</offset><text>Shehzad, Z. et al. The Preprocessed Connectomes Project Quality Assessment Protocol - a resource for measuring the quality of MRI data. In INCF Neuroinformatics, 10.3389/conf.fnins.2015.91.00047 (2015).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>25080</offset><text>Pizarro, R. A. et al. Automated Quality Assessment of Structural Magnetic Resonance Brain Images Based on a Supervised Machine Learning Algorithm. Front. Neuroinformatics. 10, 10.3389/fninf.2016.00052 (2016).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>25289</offset><text>Backhausen, L. L. et al. Quality Control of Structural MRI Images Applied Using FreeSurfer—A Hands-On Workflow to Rate Motion Artifacts. Front. Neurosci. 10, 10.3389/fnins.2016.00558 (2016).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>25482</offset><text>Alfaro-Almagro, F. et al. Image processing and Quality Control for the first 10,000 brain imaging datasets from UK Biobank. NeuroImage, 10.1016/j.neuroimage.2017.10.034 (2017).</text></passage><passage><infon key="fpage">1218</infon><infon key="lpage">1231</infon><infon key="name_0">surname:White;given-names:T</infon><infon key="pub-id_doi">10.1002/hbm.23911</infon><infon key="pub-id_pmid">29206318</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2018</infon><offset>25659</offset><text>Automated quality assessment of structural magnetic resonance images in children: Comparison with visual inspection and surface-based reconstruction</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>25808</offset><text>Keshavan, A., Yeatman, J. &amp; Rokem, A. Combining citizen science and deep learning to amplify expertise in neuroimaging. Preprint at, 10.1101/363382 (2018).</text></passage><passage><infon key="fpage">116</infon><infon key="lpage">129</infon><infon key="name_0">surname:Klapwijk;given-names:ET</infon><infon key="name_1">surname:van de Kamp;given-names:F</infon><infon key="name_2">surname:van der Meulen;given-names:M</infon><infon key="name_3">surname:Peters;given-names:S</infon><infon key="name_4">surname:Wierenga;given-names:LM</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2019.01.014</infon><infon key="pub-id_pmid">30633965</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">189</infon><infon key="year">2019</infon><offset>25964</offset><text>Qoala-T: A supervised-learning tool for quality control of FreeSurfer segmented MRI data</text></passage><passage><infon key="fpage">104</infon><infon key="lpage">120</infon><infon key="name_0">surname:Fortin;given-names:J-P</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2017.11.024</infon><infon key="pub-id_pmid">29155184</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">167</infon><infon key="year">2018</infon><offset>26053</offset><text>Harmonization of cortical thickness measurements across scanners and sites</text></passage><passage><infon key="name_0">surname:Nielson;given-names:DM</infon><infon key="pub-id_doi">10.1101/309260</infon><infon key="section_type">REF</infon><infon key="source">Preprint at</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>26128</offset><text>Detecting and harmonizing scanner differences in the ABCD study - annual release 1.0</text></passage><passage><infon key="fpage">733</infon><infon key="lpage">739</infon><infon key="name_0">surname:Leek;given-names:JT</infon><infon key="pub-id_doi">10.1038/nrg2825</infon><infon key="pub-id_pmid">20838408</infon><infon key="section_type">REF</infon><infon key="source">Nat. Rev. Genet.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2010</infon><offset>26213</offset><text>Tackling the widespread and critical impact of batch effects in high-throughput data</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26298</offset><text>Esteban, O. et al. MRIQC WebAPI - Database snapshot. figshare, 10.6084/m9.figshare.7097879.v4 (2019).</text></passage><passage><infon key="name_0">surname:Halchenko;given-names:Y</infon><infon key="pub-id_doi">10.5281/zenodo.1000098</infon><infon key="section_type">REF</infon><infon key="source">Zenodo</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>26400</offset><text>Open Source Software: DataLad</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26430</offset><text>Halchenko, Y. O. et al. Open Source Software: Heudiconv. Zenodo, 10.5281/zenodo.1306159 (2018).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26526</offset><text>Ganzetti, M., Wenderoth, N. &amp; Mantini, D. Intensity Inhomogeneity Correction of Structural MR Images: A Data-Driven Approach to Define Input Algorithm Parameters. Front. Neuroinformatics. 10, 10.3389/fninf.2016.00010 (2016).</text></passage><passage><infon key="fpage">140</infon><infon key="lpage">147</infon><infon key="name_0">surname:Magnotta;given-names:VA</infon><infon key="name_1">surname:Friedman;given-names:L</infon><infon key="name_2">surname:Birn;given-names:F</infon><infon key="pub-id_doi">10.1007/s10278-006-0264-x</infon><infon key="pub-id_pmid">16598643</infon><infon key="section_type">REF</infon><infon key="source">J. Digit. Imaging.</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2006</infon><offset>26751</offset><text>Measurement of Signal-to-Noise and Contrast-to-Noise in the fBIRN Multicenter Imaging Study</text></passage><passage><infon key="fpage">375</infon><infon key="lpage">385</infon><infon key="name_0">surname:Dietrich;given-names:O</infon><infon key="name_1">surname:Raya;given-names:JG</infon><infon key="name_2">surname:Reeder;given-names:SB</infon><infon key="name_3">surname:Reiser;given-names:MF</infon><infon key="name_4">surname:Schoenberg;given-names:SO</infon><infon key="pub-id_doi">10.1002/jmri.20969</infon><infon key="pub-id_pmid">17622966</infon><infon key="section_type">REF</infon><infon key="source">J. Magn. Reson. Imaging.</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2007</infon><offset>26843</offset><text>Measurement of signal-to-noise ratios in MR images: Influence of multichannel coils, parallel imaging, and reconstruction filters</text></passage><passage><infon key="fpage">903</infon><infon key="lpage">910</infon><infon key="name_0">surname:Atkinson;given-names:D</infon><infon key="name_1">surname:Hill;given-names:DLG</infon><infon key="name_2">surname:Stoyle;given-names:PNR</infon><infon key="name_3">surname:Summers;given-names:PE</infon><infon key="name_4">surname:Keevil;given-names:SF</infon><infon key="pub-id_doi">10.1109/42.650886</infon><infon key="pub-id_pmid">9533590</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Med. Imaging.</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">1997</infon><offset>26973</offset><text>Automatic correction of motion artifacts in magnetic resonance images using an entropy focus criterion</text></passage><passage><infon key="fpage">958</infon><infon key="lpage">972</infon><infon key="name_0">surname:Friedman;given-names:L</infon><infon key="pub-id_doi">10.1002/hbm.20440</infon><infon key="pub-id_pmid">17636563</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2008</infon><offset>27076</offset><text>Test–retest and between-site reliability in a multicenter fMRI study</text></passage><passage><infon key="fpage">313</infon><infon key="lpage">327</infon><infon key="name_0">surname:Fonov;given-names:V</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2010.07.033</infon><infon key="pub-id_pmid">20656036</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2011</infon><offset>27147</offset><text>Unbiased average age-appropriate atlases for pediatric studies</text></passage><passage><infon key="fpage">631</infon><infon key="lpage">637</infon><infon key="name_0">surname:Krüger;given-names:G</infon><infon key="name_1">surname:Glover;given-names:GH</infon><infon key="pub-id_doi">10.1002/mrm.1240</infon><infon key="pub-id_pmid">11590638</infon><infon key="section_type">REF</infon><infon key="source">Magn. Reson. Med.</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2001</infon><offset>27210</offset><text>Physiological noise in oxygenation-sensitive magnetic resonance imaging</text></passage><passage><infon key="fpage">339</infon><infon key="lpage">352</infon><infon key="name_0">surname:Saad;given-names:ZS</infon><infon key="pub-id_doi">10.1089/brain.2013.0156</infon><infon key="pub-id_pmid">23705677</infon><infon key="section_type">REF</infon><infon key="source">Brain Connect.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2013</infon><offset>27282</offset><text>Correcting Brain-Wide Correlation Differences in Resting-State FMRI</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27350</offset><text>Giannelli, M., Diciotti, S., Tessa, C. &amp; Mascalchi, M. Characterization of Nyquist ghost in EPI-fMRI acquisition sequences implemented on two clinical 1.5 T MR scanner systems: effect of readout bandwidth and echo spacing. J. Appl. Clin. Med. Phys. 11 (2010).</text></passage></document></collection>
