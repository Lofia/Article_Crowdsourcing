<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210429</date><key>pmc.key</key><document><id>7928531</id><infon key="license">author_manuscript</infon><passage><infon key="article-id_doi">10.1007/s11263-019-01215-y</infon><infon key="article-id_manuscript">NIHMS1602760</infon><infon key="article-id_pmc">7928531</infon><infon key="article-id_pmid">33664553</infon><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="kwd">Body language Emotional expression Computer vision Crowdsourcing Video analysis Perception Statistical modeling</infon><infon key="license">
          This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
        </infon><infon key="lpage">25</infon><infon key="name_0">surname:Luo;given-names:Yu</infon><infon key="name_1">surname:Ye;given-names:Jianbo</infon><infon key="name_2">surname:Adams;given-names:Reginald B.;suffix:Jr</infon><infon key="name_3">surname:Li;given-names:Jia</infon><infon key="name_4">surname:Newman;given-names:Michelle G.</infon><infon key="name_5">surname:Wang;given-names:James Z.</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">128</infon><infon key="year">2021</infon><offset>0</offset><text>ARBEE: Towards Automated Recognition of Bodily Expression of Emotion in the Wild</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>81</offset><text>Humans are arguably innately prepared to comprehend others’ emotional expressions from subtle body movements. If robots or computers can be empowered with this capability, a number of robotic applications become possible. Automatically recognizing human bodily expression in unconstrained situations, however, is daunting given the incomplete understanding of the relationship between emotional expressions and body movements. The current research, as a multidisciplinary effort among computer and information sciences, psychology, and statistics, proposes a scalable and reliable crowdsourcing approach for collecting in-the-wild perceived emotion data for computers to learn to recognize body languages of humans. To accomplish this task, a large and growing annotated dataset with 9876 video clips of body movements and 13,239 human characters, named Body Language Dataset (BoLD), has been created. Comprehensive statistical analysis of the dataset revealed many interesting insights. A system to model the emotional expressions based on bodily movements, named Automated Recognition of Bodily Expression of Emotion (ARBEE), has also been developed and evaluated. Our analysis shows the effectiveness of Laban Movement Analysis (LMA) features in characterizing arousal, and our experiments using LMA features further demonstrate computability of bodily expression. We report and compare results of several other baseline methods which were developed for action recognition based on two different modalities, body skeleton and raw image. The dataset and findings presented in this work will likely serve as a launchpad for future discoveries in body language understanding that will enable future robots to interact and collaborate more effectively with humans.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1847</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1860</offset><text>Many future robotic applications, including personal assistant robots, social robots, and police robots demand close collaboration with and comprehensive understanding of the humans around them. Current robotic technologies for understanding human behaviors beyond their basic activities, however, are limited. Body movements and postures encode rich information about a person’s status, including their awareness, intention, and emotional state. Even at a young age, humans can “read” another’s body language, decoding movements and facial expressions as emotional keys. How can a computer program be trained to recognize human emotional expressions from body movements? This question drives our current research effort.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2590</offset><text>Previous research on computerized body movement analysis has largely focused on recognizing human activities (e.g., the person is running). Yet, a person’s emotional state is another important characteristic that is often conveyed through body movements. Recent studies in psychology have suggested that movement and postural behavior are useful features for identifying human emotions. For instance, researchers found that human participants of a study could not correctly identify facial expressions associated with winning or losing a point in a professional tennis game when facial images were presented alone, whereas they were able to correctly identify this distinction with images of just the body or images that included both the body and the face. More interestingly, when the face part of an image was paired with the body and edited to an opposite situation face (e.g., winning face paired with losing body), people still used the body to identify the outcome. A valuable insight from this psychology study is that the human body may be more diagnostic than the face in terms of emotion recognition. In our work, bodily expression is defined as human affect expressed by body movements and/or postures.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3807</offset><text>Our earlier work studied the computability of evoked emotions from visual stimuli using computer vision and machine learning. In this work, we investigate whether bodily expressions are computable. In particular, we explore whether modern computer vision techniques can match the cognitive ability of typical humans in recognizing bodily expressions in the wild, i.e., from real-world unconstrained situations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4218</offset><text>Computerized bodily expression recognition capabilities have the potential to enable a large number of innovative applications including information management and retrieval, public safety, patient care, and social media (Fig. 1). For instance, such systems can be deployed in public areas such as airports, metro or bus stations, or stadiums to help police identify potential threats. Better results might be obtained in a population with a high rate of emotional instability. A psychology clinic, for example, may install such systems to help assess and evaluate disorders, including anxiety and depression, either to pre dict danger to self and others from patients, or to track the progress of patients over time. Similarly, police may use such technology to help assess the identity of suspected criminals in naturalistic settings and/or their emotions and deceptive motives during an interrogation. Well-trained and experienced detectives and interrogators rely on a combination of body language, facial expressions, eye contact, speech patterns, and voices to differentiate a liar from a truthful person. An effective assistive technology based on emotional understanding could substantially reduce the stress of police officers as they carry out their work. Improving the bodily expression recognition of assistive robots will enrich human-computer interactions. Future assistive robots can better assist those who may suffer emotional stress or mental illness, e.g., assistive robots may detect early warning signals of manic episodes. In social media, recent popular social applications such as Snapchat and Instagram allow users to upload short clips of self-recorded and edited videos. A crucial analysis from an advertising perspective is to better identify the intention of a specific uploading act by understanding the emotional status of a person in the video. For example, a user who wants to share the memory of traveling with his family would more likely upload a video capturing the best interaction moment filled with joy and happiness. Such analysis helps companies to better personalize the services or to provide advertisement more effectively for their users, e.g., through showing travel-related products or services as opposed to business-related ones.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6498</offset><text>Automatic bodily expression recognition as a research problem is highly challenging for three primary reasons. First, it is difficult to collect a bodily expression dataset with high quality annotations. The understanding and perception of emotions from concrete observations is often subject to context, interpretation, ethnicity and culture. There is often no gold standard label for emotions, especially for bodily expressions. In facial analysis, the expression could be encoded with movements of individual muscles, a.k.a., Action Units (AU) in facial action coding system (FACS). However, psychologists have not developed an analogous notation system that directly encodes correspondence between bodily expression and body movements. This lack of such empirical guidance leaves even professionals without complete agreement about annotating bodily expressions. To date, research on bodily expression is limited to acted and constrained lab-setting video data, which are usually of small size due to lengthy human subject study regulations. Second, bodily expression is subtle and composite. According to, body movements have three categories, functional movements (e.g. walking), artistic movements (e.g. dancing), and communicative movements (e.g. gesturing while talking). In a real-world setting, bodily expression can be strongly coupled with functional movements. For example, people may represent different emotional states in the same functional movement, e.g. walking. Third, an articulated pose has many degrees of freedom. Working with real-world video data poses additional technical challenges such as the high level of heterogeneity in peoples behaviors, the highly cluttered background, and the often substantial differences in scale, camera perspective, and pose of the person in the frame.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8310</offset><text>We propose a scalable and reliable crowdsourcing pipeline for collecting in-the-wild perceived emotion data. With this pipeline, we collected a large dataset with 9,876 clips that have body movements and over 13,239 human characters. We named the dataset the BoLD (Body Language Dataset). Each short video clip in BoLD has been annotated for emotional expressions as perceived by the viewers. To our knowledge, BoLD is the first large-scale video dataset for bodily emotion in the wild.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8797</offset><text>We conducted comprehensive agreement analysis on the crowdsourced annotations. The results demonstrate the validity of the proposed data collection pipeline. We also evaluated human performance on emotion recognition on a large and highly diverse population. Interesting insights have been found in these analyses.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9112</offset><text>We investigated Laban Movement Analysis (LMA) features and action recognition-based methods using the BoLD dataset. From our experiments, hand acceleration shows strong correlation with one particular dimension of emotion—arousal, a result that is intuitive. We further show that existing action recognition-based models can yield promising results. Specifically, deep models achieve remarkable performance on emotion recognition tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9551</offset><text>In this work, we investigate the feasibility of crowdsourcing bodily expression data collection and study the computability of bodily expression using the collected data. We summarize the primary contributions as follows. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9774</offset><text>In our work, we approach the bodily expression recognition problem with the focus of addressing the first challenge mentioned earlier. Using our proposed data collection pipeline, we have collected high quality affect annotation. With the state-of-the-art computer vision techniques, we are able to address the third challenge to a certain extent. To properly address the second challenge, regarding the subtle and composite nature of bodily expression, requires breakthroughs in computational psychology. Below, we detail some of the remaining technical difficulties on the bodily expression recognition problem that the computer vision community can potentially address.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10447</offset><text>Despite significant progress recently in 2D/3D pose estimation, these tech niques are limited compared with Motion Capture (MoCap) systems, which rely on placing active or passive optical markers on the subject’s body to detect motion, because of two issues. First, these vision-based estimation methods are noisy in terms of the jitter errors. While high accuracy has been reported on pose estimation benchmarks, the criteria used in the benchmarks are not designed for our application which demands substantially higher precision of landmark locations. Consequently, the errors in the results generated through those methods propagate in our pipeline, as pose estimation is a first-step in analyzing the relationship between motion and emotion.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11196</offset><text>Second, vision-based methods (e.g.,) usually address whole-body poses, which have no missing landmarks, and only produce relative coordinates of the landmarks from the pose (e.g., with respect to the barycenter of the human skeleton) instead of the actual coordinates in the physical environment. In-the-wild videos, however, often contain upper-body or partially-occluded poses. Further, the interaction between human and the environment, such as a lift of the person’s barycenter or when the person is pacing between two positions, is often critical for bodily expression recognition. Additional modeling on the environment together with that for the human would be useful in understanding body movement.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11905</offset><text>In addition to these difficulties faced by the computer vision community broadly, the computation psychology community also needs some breakthroughs. For instance, state-of-the-art end-to-end action recognition methods developed in the computer vision community offer insufficient interpretability of bodily expression. While the LMA features that we have developed in this work has better interpretability than the action recognition based methods, to completely address the problem of body language interpretation, we believe it will be important to have comprehensive motion protocols defined or learned, as a counterpart of FACS for bodily expression.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12561</offset><text>The rest of this paper is structured as follows. Section 2 reviews related work in the literature. The data collection pipeline and statistics of the BoLD dataset are introduced in Sect. 3. We describe our modeling processes on BoLD and demonstrate findings in Sect. 4, and conclude in Sect. 5.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>12856</offset><text>Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12869</offset><text>After first reviewing basic concepts on bodily expression and related datasets, we then discuss related work on crowdsourcing subjective affect annotation and automatic bodily expression modeling.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13066</offset><text>Bodily Expression Recognition</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13096</offset><text>Existing automated bodily expression recognition studies mostly build on two theoretical models for representing affective states, the categorical and the dimensional models. The categorical model represents affective states into several emotion categories. In, Ekman et al. proposed six basic emotions, i.e., anger, happiness, sadness, surprise, disgust, and fear. However, as suggested by and, bodily expression is not limited to basic emotions. When we restricted interpretations to only basic emotions at a preliminary data collection pilot study, the participants provided feedback that they often found none of the basic emotions as suitable for the given video sample. A dimensional model of affective states is the PAD model by, which describes an emotion in three dimensions, pleasure (valence), arousal, and dominance. In the PAD model, valence characterizes the positivity versus negativity of an emotion, while arousal characterizes the level of activation and energy of an emotion, and dominance characterizes the extent of controlling others or surroundings. As summarized in, most bodily expression-related studies focus on either a small set of categorical emotions or two dimensions of valence and arousal in the PAD model. In our work, we adopt both measurements in order to acquire complementary emotion annotations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14432</offset><text>Based on how emotion is generated, emotions can be categorized into acted or elicited emotions, and spontaneous emotions. Acted emotion refers to actors’ performing a certain emotion under given contexts or scenarios. Early work was mostly built on acted emotions. analyzes videos recorded on recruited actors and established bodily emotions as an important modality of emotion recognition. In, a human subject’s emotion is elicited via interaction with computer avatar of its operator. crowdsourced emotion responses with image stimuli. Recently, natural or authentic emotions have generated more interest in the research community. In, body movements are recorded while human subjects play body movement-based video games.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15161</offset><text>Related work can be categorized based on raw data types, namely MoCap data or image/video data. For lab-setting studies such as, collecting motion capture data is usually feasible. collected a dataset with upper body movement video recorded in a studio. Other work used image/video data capturing the frontal view of the poses.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15489</offset><text>Humans perceive and understand emotions from multiple modalities, such as face, body language, touch, eye contact, and vocal cues. We review the most related vision-based facial expression analysis here. Facial expression is an important modality in emotion recognition and automated facial expression recognition is more successful compared with other modalities. The main reasons for this success are twofold. First, the discovery of FACS made facial expression less subjective. Many recent works on facial expression recognition focus on Action Unit detection, e.g.,. Second, the face has fewer degrees of freedom compared with the whole body. To address the comparatively broader freedom of bodily movement, suggest the use of a movement notation system may help identify bodily expression. Other research has considered microexpressions, e.g.,, suggesting additional nuances in facial expressions. To our knowledge, no vision-based study or dataset on complete measurement of natural bodily emotions exists.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>16502</offset><text>Crowdsourced Affect Annotation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16533</offset><text>Crowdsourcing from the Internet as a data collection process has been originally proposed to collect objective, non-affective data and received popularity in the machine learning community to acquire large-scale ground truth datasets. A school of data quality control methods has been proposed for crowdsourcing. Yet, crowdsourcing affect annotations is highly challenging due to the intertwined subjectivity of affect and uninformative participants. Very few studies report on the limitations and complexity of crowdsourcing affect annotations. As suggested by, inconsistency of crowdsourced affective data exists due to two factors. The first is the possible untrustworthiness of recruited participants due to the discrepancy between the purpose of study (collecting high quality data) and the incentive for participants (earning cash rewards). The second is the natural variability of humans perceiving others’ affective expressions, as was discussed earlier. crowdsourced personality attributes. Although they analyzed agreements among different participants, they did not conduct quality control, catering to the two stated factors in the crowdsourcing., however, used an ad hoc gold standard to control annotation quality and each sample in the training set was only annotated once. crowdsourced evoked emotions of stimuli images. Building on proposed a probabilistic model, named the GLBA, to jointly model each worker’s reliability and regularity—the two factors contributing to the inconsistent annotations—in order to improve the quality of affective data collected. Because the GLBA methodology is applicable for virtually any crowdsourced affective data, we use it for our data quality control pipeline as well.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18265</offset><text>Automatic Modeling of Bodily Expression</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18305</offset><text>Automatic modeling of bodily expression (AMBE) typically requires three steps: human detection, pose estimation and tracking, and representation learning. In such a pipeline, human(s) are detected frame-by-frame in a video and their body landmarks are extracted by a pose estimator. Subsequently, if multiple people appear in the scene, the poses of the same person are associated along all frames. With each person’s pose identified and associated across frames, an appropriate feature representation of each person is extracted.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18838</offset><text>Based on the way data is collected, we divide AMBE methods into video-based and non-video-based. For video-based methods, data are collected from a camera, in the form of color videos. In, videos are collected in a lab setting with a pure-colored background and a fixed-perspective camera. They could detect and track hands and other landmarks with simple thresholding and grouping of pixels. additionally defined motion protocols, such as whether the hand is facing up, and combined them with landmark displacement as features. used the positions of shoulders in the image frame, facial expression, and audio features as the input of a neural network. Our data, however, is not collected under such controlled settings, thus has variations in viewpoint, lighting condition, and scale.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19624</offset><text>For non-video-based methods, locations of body markers are inferred by the MoCap system. The first two steps, i.e., human detection, and pose estimation and tracking, are solved directly by the MoCap system. Geometric features, such as velocity, acceleration, and orientation of body landmarks, as well as motion protocols can then be conveniently developed and used to build predictive models. For a more comprehensive survey of automatic modeling of bodily expression, readers are referred to the three surveys.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20138</offset><text>Related to AMBE, human behaviour understanding (a.k.a. action recognition) has attracted a lot of attention. The emergence of large-scale annotated video datasets and advances in deep learning have accelerated the development in action recognition. To our knowledge, two-stream ConvNets-based models have been leading on this task. The approach uses two networks with an image input stream and an optical flow input stream to characterize appearance and motion, respectively. Each stream of ConvNet learns human-action-related features in an end-to-end fashion. Recently, some researchers have attempted to utilize human pose information., for example, modeled human skeleton sequences using a spatiotemporal graph convolutional network. leveraged pose information using a multitask-learning approach. In our work, we extract LMA features based on skeletons and use them to build predictive models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>21037</offset><text>The BoLD Dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21054</offset><text>In this section, we describe how we created the BoLD dataset and provide results of our statistical analysis of the data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21176</offset><text>Dataset Construction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21197</offset><text>The dataset construction process, detailed below, consists of three stages: movie selection and time segmentation, pose estimation and tracking, and emotion annotation. Figure 2 illustrates our dataset construction pipeline. We chose the movies included in a public dataset, the AVA dataset, which contains a list of YouTube movie IDs. To respect the copyright of the movies, we provide the movie ID in the same way as in the AVA dataset when the data is shared to the research community. Any raw movies will be kept only for feature extraction and research in the project and will not be distributed. Given raw movies crawled from Youtube, we first partitioned each into several short scenes before using other vision-based methods to locate and track each person across different frames in the scene. To facilitate tracking, the same person in each clip was marked with a unique ID number. Finally, we obtained emotion annotations of each person in these ID-marked clips by employing independent contractors (to be called participants hereafter) from the online crowdsourcing platform, the Amazon Mechanical Turk (AMT).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>22319</offset><text>Movie Selection and Time Segmentation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22357</offset><text>The Internet has vast natural human-to-human interaction videos, which serves as a rich source for our data. A large collection of video clips from daily lives is an ideal dataset for developing affective recognition capabilities because they match closely with our common real-world situations. However, a majority of those user-uploaded, in-the-wild videos suffer from poor camera perspectives and may not cover a variety of emotions. We consider it beneficial to use movies and TV shows, e.g., reality shows or uploaded videos in social media, that are unconstrained but offer highly interactive and emotional content. Movies and TV shows are typically of high quality in terms of filming techniques and the richness of plots. Such shows are thus more representative in reflecting characters’ emotional states than some other categories of videos such as DIY instructional videos and news event videos, some of which were collected recently. In this work, we have crawled 150 movies (220 hours in total) from YouTube by the video IDs curated in the AVA dataset.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23424</offset><text>Movies are typically filmed so that shots in one scene demonstrate characters’ specific activities, verbal communication, and/or emotions. To make these videos manageable for further human annotation, we partition each video into short video clips using the kernel temporal segmentation (KTS) method. KTS detects shot boundary by keeping variance of visual descriptors within a temporal segment small. Shot boundary can be either a change of scene or a change of camera perspective within the same scene. To avoid confusion, we will use the term scene to indicate both cases.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>24002</offset><text>Pose Estimation and Tracking</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24031</offset><text>We adopted an approach to detect human body landmarks and track each character at the same time (Fig. 3). Because not all short clips contain human characters, we removed those clips without humans via pose estimation). Each clip was processed by a pose estimator1 frame-by-frame to acquire human body landmarks. Different characters in one clip correspond to different samples. Each character in the clip is marked as a different sample. To make the correspondence clear, we track each character and designate them with a unique ID number. Specifically, tracking was conducted on the upper-body bounding box with the Kalman Filter and Hungarian algorithm as the key component (.2 In our implementation, the upper-body bounding box was acquired with the landmarks on face and shoulders. Empirically, to ensure reliable tracking results when presenting to the annotators, we removed short trajectories that had less than 80% of the total frames.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>24976</offset><text>Emotion Annotation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24995</offset><text>Following the above steps, we generated 122, 129 short clips from these movies. We removed facial close-up clips using results from pose estimation. Concretely, we included a clip in our annotation list if the character in it has at least three visible landmarks out of the six upper-body landmarks, i.e., wrists, elbows, and shoulders on both body sides (left and right). We further select those clips with between 100 and 300 frames for manual annotation by the participants. An identified character with landmark tracking in a single clip is called an instance. We have curated a total of 48,037 instances for annotation from a total of 26,164 video clips.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25655</offset><text>We used the AMT for crowdsourcing emotion annotations of the 48,037 instances. For each Human Intelligence Task (HIT), a human participant completes emotion annotation assignments for 20 different instances. Each of which was drawn randomly from the instance pool. Each instance is expected to be annotated by five different participants.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25994</offset><text>We asked human annotators to finish four annotation tasks per instance. Figure 4 shows screenshots of our crowdsourcing website design. As a first step, participants must check if the instance is corrupted. An instance is considered cor rupted if landmark tracking of the character is not consistent or the scene is not realistic in daily life, such as science fiction scenes. If an instance is not corrupted, participants are asked to annotate the character’s emotional expressions according to both categorical emotions and dimensional emotions [i.e., valence, arousal, dominance (VAD) in dimensional emotion state model ]. For categorical emotions, we used the list in, which contains 26 categories and is a superset of the six basic emotions. Participants are asked to annotate these categories in the way of multi-label binary classifications. For each dimensional emotion, we used integers that scales from 1 to 10. These annotation tasks are meant to reflect the truth revealed in the visual and audio data—movie characters’ emotional expressions—and do not involve the participants emotional feelings. In addition to these tasks, participants are asked to specify a time interval (i.e., the start and end frames) over the clip that best represents the selected emotion(s) or has led to their annotation. Characters’ and participants’ demographic information (gender, age, and ethnicity) is also annotated/collected for complementary analysis. Gender categories are male and female. Age categories are defined as kid (aged up to 12 years), teenager (aged 13–20), and adult (aged over 20). Ethnicity categories are American Indian or Alaska Native, Asian, African American, Hispanic or Latino, Native Hawaiian or Other Pacific Islander, White, and Other.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27769</offset><text>The participants are permitted to hear the audio of the clip, which can include a conversation in English or some other language. While the goal of this research is to study the computability of body language, we allowed the participants to use all sources of information (facial expression, body movements, sound, and limited context) in their annotation in order to obtain as high accuracy as possible in the data collected. Additionally, the participants can play the clip back-and-forth during the entire annotation process for that clip.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28312</offset><text>To sum up, we crowdsourced the annotation of categorical and dimensional emotions, time interval of interest, and character demographic information.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>28461</offset><text>Annotation Quality Control</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28488</offset><text>Quality control has always been a necessary component for crowdsourcing to identify dishonest participants, but it is much more difficult for affect data. Different people may not perceive affect in the same way, and their understanding may be influenced by their cultural background, current mood, gender, and personal experiences. An honest participant could also be uninformative in affect annotation, and consequently, their annotations can be poor in quality. In our study, the variance in acquiring affects usually comes from two kinds of participants, i.e., dishonest ones, who give useless annotations for economic motivation, and exotic ones, who give inconsistent annotations compared with others. Note that exotic participants come with the nature of emotion, and annotations from exotic participants could still be useful when aggregating final ground truth or investigating cultural or gender effects of affect. In our crowdsourcing task, we want to reduce the variance caused by dishonest participants. In the meantime, we do not expect too many exotic participants because that would lead to low consensus.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29610</offset><text>Using gold standard examples is a common practice in crowdsourcing to identify uninformative participants. This approach involves curating a set of instances with known ground truth and removing those participants who answer incorrectly. For our task, however, this approach is not as feasible as in conventional crowdsourcing tasks such as image object classification. To accommodate subjectivity of affect, gold standard has to be relaxed to a large extent. Consequently, the recall of dishonest participants is lower.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30131</offset><text>To alleviate the aforementioned dilemma, we used four complementary mechanisms for quality control, including three online approaches (i.e., analyzing while collecting the data) and an offline one (i.e., post-collection analysis). The online approaches are participant screening, annotation sanity check, and relaxed gold standard test, while the offline one is reliability analysis.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30515</offset><text>Participant screening First-time participants in our HIT must take a short empathy quotient (EQ) test. Only those who have above-average EQ are qualified. This approach aims to reduce the number of exotic participants from the beginning.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30753</offset><text>Annotation sanity check During the annotation process, the system checks consistency between categorical emotion and dimensional emotion annotations as they are entered. Specifically, we expect an “affection”, “esteem”, “happiness”, or “pleasure” instance to have an above-midpoint valence score; a “disapproval”, “aversion”, “annoyance”, “anger”, “sensitivity”, “sadness”, “disquietment”, “fear”, “pain”, or “suffering” instance to have a below-midpoint valence score; a “peace” instance to have a below-midpoint arousal score; and an “excitement” instance to have an above-midpoint arousal score. As an example, if a participant chooses “happiness” and a valence rating between 1 and 5 (out of 10) for an instance, we treat the annotation as inconsistent. In each HIT, a participant fails this annotation sanity check if there are two inconsistencies among twenty instances.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31698</offset><text>Relaxed gold standard test One control instance (relaxed gold standard) is randomly inserted in each HIT to monitor the participant’s performance. We collect control instances in our trial run within a small trusted group and choose instances with very high consensus. We manually relax the acceptable range of each control instance to avoid false alarm. For example, for an indisputable sad emotion instance, we accept an annotation if valence is not higher than 6. An annotation that goes beyond the acceptable range is treated as failing the gold standard test. We selected nine control clips and their relaxed annotations as the gold standard. We did not use more control clips because the average number of completed HITs per participant is much less than nine and the gold standard is rather relaxed and inefficient in terms of recall.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32542</offset><text>Reliability analysis To further reduce the noise introduced by dishonest participants, we conduct reliability analysis over all participants. We adopted the method by to properly handle the intrinsic subjectivity in affective data. Reliability and regularity of participants are jointly modeled. Low-reliability-score participant corresponds to dishonest participant, and low-regularity participant corresponds to exotic participant. This method was originally developed for improving the quality of dimensional annotations based on modeling the agreement multi-graph built from all participants and their annotated instances. For each dimension of VAD, this method estimates participant i’s reliability score, i.e., , , . According to, the valence and arousal dimensions are empirically meaningful for ranking participants’ reliability scores. Therefore, we ensemble the reliability score as . We mark participant i as failing in reliability analysis if ri is less than  with enough effective sample size.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33553</offset><text>Based on these mechanisms, we restrain those participants deemed ‘dishonest.’ After each HIT, participants with low performance are blocked for one hour. Low-performance participant is defined as either failing the annotation sanity check or the relaxed gold standard test. We reject the work if it shows low performance and fails in the reliability analysis. In addition to these constraints, we also permanently exclude participants with a low reliability score from participating our HITs again.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>34056</offset><text>Annotation Aggregation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34079</offset><text>Whenever a single set of annotations is needed for a clip, proper aggregation is necessary to obtain a consensus annotation from multiple participants. The Dawid-Skene method, which is typically used to combine noisy categorical observations, computes an estimated score (scaled between 0 and 1) for each instance. We used the method to aggregate annotations on each categorical emotion annotation and categorical demographic annotation. Particularly, we used the notation  to represent the estimated score of the binary categorical variable c for the instance i. We set a threshold of 0.5 for these scores when binary categorical annotation is needed. For dimensional emotion, we averaged the set of annotations for a clip with their annotators’ reliability score. Considering a particular instance, suppose it has received n annotations. The score  is annotated by participant i with reliability score ri for dimensional emotion d, where i ∈ {1, 2, … , n} and d ∈ {V, A, D} in the VAD model. The final annotation is then aggregated as  In the meantime, instance confidence according to the method by is defined as  Note that we divided the final VAD score by 10 so that the data ranges between 0 and 1. Our final dataset to be used for further analysis retained only those instances with confidence higher than 0.95.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35406</offset><text>Our website sets a default value for the start frame (0) and the end frame (total frame number of the clip) for each instance. Among the data collected, there were about a half annotations that have non-default values, which means a portion of the annotators either considered the whole clip as the basis for their annotations or did not finish the task. For each clip, we selected the time-interval entered by the participant with the highest reliability score as the final annotation for the clip.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>35906</offset><text>Dataset Statistics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35925</offset><text>We report relevant dataset statistics. We used state-of-the-art statistical techniques to validate our quality control mechanisms and thoroughly understand the consensus level of our verified data labels. Because human perceptions of a character’s emotions naturally varies across participants, we do not expect absolute consensus for collected labels. In fact, it is nontrivial to quantitatively understand and measure the quality of such affective data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>36383</offset><text>Annotation Distribution and Observations</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36424</offset><text>We have collected annotations for 13, 239 instances. The dataset continues to grow as more instances and annotations are added. Figure 5 shows some high-confidence instances in our dataset. Figures 6, 7, and 8 show the distributions of categorical emotion, dimensional emotion, and demographic information, respectively. For each categorical emotion, the distribution is highly unbalanced. For dimensional emotion, the distributions of three dimensions are Gaussian-like, while valence is right-skewed and dominance is left-skewed. Character demographics is also unbalanced: most characters in our movie-based dataset are male, white, and adult. We partition all instances into three sets: the training set (~ 70%, 9222), the validation set (~10%, 1153), and the testing set (20%, 2864). Our split protocol ensured that clips from the same raw movie video belong to the same set so that subsequent evaluations can be conducted faithfully.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>37363</offset><text>We observed interesting correlations between pairs of categorical emotions and pairs of dimensional emotions. Figure 9 shows correlations between each pair of emotion categories. Categorical emotion pairs such as pleasure and happiness (0.57), happiness and excitement (0.40), sadness and suffering (0.39), annoyance and disapproval (0.37), sensitivity and sadness (0.37), and affection and happiness (0.35) show high correlations, matching our intuition. Correlations between dimensional emotions (valence and arousal) are weak (0.007). Because these two dimensions were designed to indicate independent characteristics of emotions, weak correlations among them confirm their validity. However, correlations between valence and dominance 0.359, and between arousal and dominance (0.356) are high. This finding is evidence that dominance is not a strictly independent dimension in the VAD model.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38259</offset><text>We also observed sound correlations between dimensional and categorical emotions. Valence shows strong positive correlations with happiness (0.61) and pleasure (0.51), and strong negative correlations with disapproval (− 0.32), sadness (− 0.32), annoyance (− 0.31), and disquitement (− 0.32). Arousal shows positive correlations with excitement (0.25) and anger (0.31), and negative correlations with peace (− 0.20), and disconnection (− 0.23). Dominance shows strong correlation with confidence (0.40), and strong negative correlation with doubt/confusion (0.23), sadness (− 0.28), fear (− 0.23), sensitivity (− 0.22), disquitement (− 0.24), and suffering (− 0.25). All of these correlations match with our intuition about these emotions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>39021</offset><text>Annotation Quality and Observations</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>39057</offset><text>We computed Fleiss’ Kappa score (κ) for each categorical emotion and categorical demographic information to understand the extent and reliability of agreement among participants. Perfect agreement leads to a score of one, while no agreement leads to a score less than or equal to zero. Table 1 shows Fleiss’ Kappa among participants on each categorical emotion and categorical demographic information. κ is computed on all collected annotations for each category. For each category, we treated it as a two-category classification and constructed a subject-category table to compute Fleiss’ Kappa. By filtering out those with low reliability scores, we also computed filtered κ. Note that some instances may have less than five annotations after removing annotations from low-reliability participants. We edited the way to compute pj , defined as the proportion of all assignments which were to the j-th category. Originally, it should be  where N is the number of instances, ni j is the number of ratings annotators have assigned to the j-th category on the i-th instance, and n is the number of annotators per instance. In our filtered κ computation, n varies for different instances and we denote the number of annotators for instance i as ni.Then Eq. (3) is revised as:  Filtered κ is improved for each category, even for those objective category like gender, which also suggests the validity of our offline quality control mechanism. Note that our reliability score is computed over dimensional emotions, and thus the offline quality control approach is complementary. As shown in the table, affection, anger, sadness, fear, and pain have fair levels of agreement (0.2 &lt; κ &lt; 0.4). Happiness has moderate level of agreement (0.4 &lt; κ &lt; 0.6), which is comparable to objective tasks such as age and ethnicity. This result indicates that humans are mostly consistent in their sense of happiness. Other emotion categories fall into the level of slight agreement (0 &lt; κ &lt; 0.2). Our κ score of demographic annotation is close to previous studies reported in. Because the annotation is calculated from the same participant population, κ also represents how difficult or subjective the task is. Evidently gender is the most consistent (hence the easiest) task among all categories. The data confirms that emotion recognition is both challenging and subjective even for human beings with sufficient level of EQ. Participants in our study passed an EQ test designed to measure one’s ability to sense others’ feelings as well as response to others’ feelings, and we suspect that individuals we excluded due to a failed EQ test would likely experience greater difficulty in recognizing emotions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>41783</offset><text>For dimensional emotions, we computed both across-annotation variances and within-instance annotation variances. The variances across all annotations are 5.87, 6.66, and 6.40 for valence, arousal, and dominance, respectively. Within-instance variances (over different annotators) is computed for each instance and the means of these variances are 3.79, 5.24, and 4.96, respectively. Notice that for the dimensions, the variances are reduced by 35%, 21%, and 23%, respectively, which illustrates human performance at reducing variance given concrete examples. Interestingly, participants are better at recognizing positive and negative emotions (i.e., valence) than in other dimensions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>42469</offset><text>Human Performance</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42487</offset><text>We explored the difference between low-performance participants and low reliability-score participants. As shown in Fig. 10, low-performance participants shows lower reliability score by average. While a significantly large number of low-performance participants have rather high reliability scores, most non-low-performance participants have reliability scores larger than 0.33. These distributions suggests that participants who pass annotation sanity checks and relaxed gold standard tests are more likely to be reliable. However, participants who fail at those tests may still be reliable. Therefore, conventional quality control mechanism like the gold standard is insufficient when it comes to affect data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43200</offset><text>We further investigated how well humans can achieve on emotion recognition tasks. There are 5650 AMT participants contributing to our dataset annotation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43354</offset><text>They represent over 100 countries (including 3421 from the USA and 1119 from India), with 48.4% male and 51.6% female, and an average age of 32. In terms of ethnicity, 57.3% self-reported as White, 21.2% Asian, 7.8% African American, 7.1% Hispanic or Latino, 1.6% American Indian or Alaskan Native, 0.4% Native Hawaiian or Other Pacific Islander, and 4.5% Other. For each participant, we used annotations from other participants and aggregated final dataset annotation to evaluate the performance. We treated this participant’s annotation as prediction from an oracle model and calculate F1 score for categorical emotion, and coefficient of determination (R2) and mean squared error (MSE) for dimensional emotion to evaluate the participant’s performance. Similar to our standard annotation aggregation procedure, we ignored instances with a confidence score less than 0.95 when dealing with dimensional emotions. Figure 11 shows the cumulative distribution of participants’ F1 scores of categorical emotions, the R2 score, and the MSE score of dimensional emotion, respectively. We calculated vanilla R2 score and rank percentile-based R2 score. For the latter, we used rank percentile for both prediction and the ground truth. The areas under the curves [excluding Fig. 11(5)] can be interpreted as how difficult it is for humans to recognize the emotion. For example, humans are effective at recognizing happiness while ineffective at recognizing yearning. Similarly, humans are better at recognizing the level of valence than that of arousal or dominance. These results reflect the challenge of achieving high classification and regression performance for emotion recognition even for human beings.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>45063</offset><text>Demographic Factors</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45083</offset><text>Culture, gender, and age could be important factors of emotion understanding. As mentioned in Sect. 3.1.4, we have nine quality control videos in our crowdsourcing process that have been annotated for emotion more than 300 times. We used these quality control videos to test whether the annotations are independent of annotators’ culture, gender, and age.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45441</offset><text>For categorical annotations (including both categorical emotions and categorical character demographics), we conducted χ2 test on each video. For each control instance, we calculated the p value of the χ2 test over annotations (26 categorical emotions and 3 character demographic factors) from different groups resulting from annotators’ three demographic factors. This process results in 29 × 3 = 87 p-value scores for each control instance. For each test among 87 pairs, we further counted the total number of videos with significant p-value (p &lt; 0.01 or p &lt; 0.001). Interestingly, there is significant dependence over characters’ ethnicity and annotators’ ethnicity (9 out of 9, p &lt; 0.001). It is possible that humans are good at recognizing the ethnicity of others in the same ethnic group. Additionally, there is intermediate dependence between annotators’ ethnicity and categorical emotions (17 out of 26 × 9 = 234, p &lt; 0.001). We did not find strong dependence over other tested pairs (less than 3 out of 9, p &lt; 0.001). This lack of dependence seems to suggest that a person’s understanding of emotions depends more on their own ethnicity than on their age or gender.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46630</offset><text>For VAD annotation, we conducted one-way ANOVA tests on each instance. For each control instance, we calculated p value of one-way ANOVA test over VAD (3) annotations from different groups resulting from annotators’ demographic factors (3). This results in 3 × 3 = 9 p value scores for each control instance. We also conducted Kruskal-Wallis H-test and found similar results. We report p value of one-way ANOVA tests. Our results show that gender and age have little effect (less than 8 out of 9 × (3 + 3) = 54, p &lt; 0.001) on emotion understanding, while ethnicity has a strong effect (13 out of 9 × 3 = 27, p &lt; 0.001) on emotion understanding. Specifically, participants with different ethnicities have different understandings regarding valence for almost all control clips (7 out of 9, p &lt; 0.001). Figure 5(27–28) shows two control clips. For Fig. 5(27), valence average of person 0 among Asians is 5.56, yet 4.12 among African Americans and 4.41 among Whites. However, arousal average among Asians is 7.20, yet 8.27 among African Americans and 8.21 among Whites. For Fig. 5(28), valence average of person 1 among Asians is 6.30, yet 5.09 among African Americans and 4.97 among Whites. However, arousal average among Asians is 7.20, yet 8.27 among African Americans and 8.21 among Whites. Among all of our control instances, the average valence among Asians is consistently higher than among Whites and African Americans. This repeated finding seems to suggest that Asians tend to assume more positively when interpreting others’ emotions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>48182</offset><text>Discussion</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48193</offset><text>Our data collection efforts offer important lessons. The efforts confirmed that reliability analysis is useful for collecting subjective annotations such as emotion labels when no gold standard ground truth is available. As shown in Table 1, consensus (filtered κ value) over high-reliable participants is higher than that of all participants (κ value). This finding holds for both subjective questions (categorical emotion) and objective questions (character demographics), even though the reliability score is calculated with the different VAD annotations — an evidence that the score does not overfit. As an offline quality control component, the method we developed and used to generate reliability scores is suitable for analyzing such affective data. For example, one can also apply our proposed data collection pipeline to collect data for the task of image aesthetics modeling. In addition to their effectiveness in quality control, reliability scores are very useful for resource allocation. With a limited annotation budget, it is more reasonable to reward highly-reliable participants rather than less reliable ones.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>49329</offset><text>Bodily Expression Recognition</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>49359</offset><text>In this section, we investigate two pipelines for automated recognition of bodily expression and present quantitative results for some baseline methods. Unlike AMT participants, who were provided with all the information regardless of whether they use all in their annotation process, the first computerized pipeline relied solely on body movements, but not on facial expressions, audio, or context. The second pipeline took a sequence of cropped images of the human body as input, without explicitly modeling facial expressions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>49889</offset><text>Learning from Skeleton</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>49912</offset><text>Laban Movement Analysis</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>49936</offset><text>Laban notation, originally proposed by, is used for documenting body movement of dancing such as ballet. Laban movement analysis (LMA) uses four components to record human body movements: body, effort, shape, and space. Body category represents structural and physical characteristics of the human body movements. It describes which body parts are moving, which parts are connected, which parts are influenced by others, and general statements about body organization. Effort category describes inherent intention of a movement. Shape describes static body shapes, the way the body interacts with something, the way the body changes toward some point in space, and the way the torso changes in shape to support movements in the rest of the body. LMA or its equivalent notation systems are widely used in psychology for emotion analysis and human computer interaction for emotion generation and classification. In our experiments, we use features listed in Table 2.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>50901</offset><text>LMA is conventionally conducted for 3D motion capture data that have 3D coordinates of body landmarks. In our case, we estimated 2D pose on images using. In particular, we denote  as the coordinate of the i-th joint at the t-th frame. As the nature of the data, our 2D pose estimation usually has missing values of joint locations and varies in scale. In our implementation, we ignored an instance if the dependencies to compute the feature are missing. To address the scaling issue, we normalized each pose by the average length of all visible limbs, such as shoulder-elbow and elbow-wrist. Let ν = { (i, j)| joint i and joint j are visible} be the visible set of the instance. We computed normalized pose  by </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>51614</offset><text>The first part of features in LMA, body component, captures the pose configuration. For f1, f2, f3, f8, and f9, we computed the distance between the specified joints frame by frame. For symmetric joints like feet-hip distance, we used the mean of left-feat-hip and right-feat-hip distance in each frame. The same protocol was applied to other features that contains symmetric joints like hands velocity. For f4, the centroid was averaged over all visible joints and pelvis is the midpoint between left hip and right hip. This feature is designed to represent barycenter deviation of the body.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>52207</offset><text>The second part of features in LMA, effort component, captures body motion characteristics. Based on the normalized pose, joints velocity , acceleration , and jerk  were computed as:  Angles, angular velocity, and angular acceleration between each pair of limbs (Fig. 12) were calculated for each pose:  We computed velocity, acceleration, jerk, angular velocity, and angular acceleration of joints with τ 15. Empirically, features become less effective when τ is too small (1 ~ 2) or too large (&gt; 30).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>52714</offset><text>The third part of features in LMA, shape component, captures body shape. For f19, f20, f21, f22, and f23, the area of bounding box that contains corresponding joints is used to approximate volume.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>52911</offset><text>Finally, all features are summarized by their basic statistics (maximum, minimum, mean, and standard deviation, denoted as , , , and , respectively) over time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53071</offset><text>With all LMA features combined, each skeleton sequence can be represented by a 2, 216-D feature vector. We further build classification and regression models for bodily expression recognition tasks. Because some measurements in our feature set can be linearly correlated and features can be missing, we choose the random forest for our classification and regression task. Specifically, we impute missing feature values with a large number (1000 in our case). We then search model parameters with cross validation on the combined set of training and validation. Finally, we use the selected best parameter to retrain a model on the combined set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>53716</offset><text>Spatial Temporal Graph Convolutional Network</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53761</offset><text>Besides handcrafted LMA features, we experimented with an end-to-end feature learning method. Following, human body landmarks can be constructed as a graph with their natural connectivity. Considering the time dimension, a skeleton sequence could be represented with a spatiotemporal graph. Graph convolution in is used as building blocks in ST-GCN. ST-GCN was originally proposed for skeleton action recog nition. In our task, each skeleton sequence is first normalized between 0 and 1 with the largest bounding box of skeleton sequence. Missing joints are filled with zeros. We used the same architecture as in and trained on our task with binary cross-entropy loss and mean-squared-error loss. Our learning objective  can be written as:  where  and  are predicted probability and ground truth, respectively, for the i-th categorical emotion, and  and  are model prediction and ground truth, respectively, for the i-th dimensional emotion.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>54703</offset><text>Learning from Pixels</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>54724</offset><text>Essentially, bodily expression is expressed through body activities. Activity recognition is a popular task in computer vision. The goal is to classify human activities, like sports and housework, from videos. In this subsection, we use four classical human activity recognition methods to extract features. Current state-of-the-art results of activity recognition are achieved by two-stream network-based deep-learning methods. Prior to that, trajectory-based handcrafted features are shown to be efficient and robust.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>55244</offset><text>Trajectory Based Handcrafted Features</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>55282</offset><text>The main idea of trajectory-based feature extraction is selecting extended image features along point trajectories. Motion-based descriptors, such as histogram of flow (HOF) and motion boundary histograms (MBH), are widely used in activity recognition for their good performance. Common trajectory-based activity recognition has the following steps: (1) computing the dense trajectories based on optical flow; (2) extracting descriptors along those dense trajectories; (3) encoding dense descriptors by Fisher vector; and (4) training a classifier with the encoded histogram-based features.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>55873</offset><text>In this work, we cropped each instance from raw clips with a fixed bounding box that bounds the character over time. We used the implementation in to extract trajectory-based activity features.3 We trained 26 SVM classifiers for the binary categorical emotion classification and three SVM regressors for the dimensional emotion regression. We selected the penalty parameter based on the validation set and report results on the test set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>56311</offset><text>Deep Activity Features</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>56334</offset><text>Two-stream network-based deep-learning methods learn to extract features in an end-to-end fashion. A typical model of this type contains two convolutional neural networks (CNN). One takes static images as input and the other takes stacked optical flow as input. The final prediction is an averaged ensemble of the two networks. In our task, we used the same learning objective of  as defined in Eq. 8.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>56736</offset><text>We implemented two-stream networks in PyTorch.4 We used 101-layer ResNet as as our network architecture. Optical flow was computed via TVL1 optical flow algorithm. Both image and optical flow were cropped with the instance body centered. Since emotion understanding could be potentially related to color, angle, and position, we did not apply any data augmentation strategies. The training procedure is identical to the work of, where the learning rate is set to 0.01. We used resnet-101 model pretrained on ImageNet to initialize our network weights. The training takes around 8 minutes for one epoch with an NVIDIA Tesla K40 card. The training time is short because only one frame is sampled input for each video in the RGB stream, and 10 frames are concatenated along the channel dimension in the optical flow stream. We used the validation set to choose the model of the lowest loss. We name this model as TSResNet101.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>57659</offset><text>Besides the original two-stream network, we also evaluated its two other state-of-the-art variants of action recognition. For temporal segment networks (TSN), each video is divided into K segments. One frame is randomly sampled for each segment during the training stage. Video classification result is averaged over all sampled frames. In our task, learning rate is set to 0.001 and batch size is set to 128. For two-stream inflated 3D ConvNet (I3D), 3D convolution replaces 2D convolution in the original two-stream network. With 3D convolution, the architecture can learn spatiotemporal features in an end-to-end fashion. This architecture also leverages recent advances in image classification by duplicating weights of pretrained image classification model over the temporal dimension and using them as initialization. In our task, learning rate is set to 0.01 and batch size is set to 12. Both experiments are conducted on a server with two NVIDIA Tesla K40 cards. Other training details are the same as the original work.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>58688</offset><text>Results</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>58696</offset><text>Evaluation Metrics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>58715</offset><text>We evaluated all methods on the test set. For categorical emotion, we used average precision (AP, area under precision recall curve) and area under receiver operating characteristic curve (ROC AUC) to evaluate the classification performance. For dimensional emotion, we used R2 to evaluate regression performance. Specifically, a random baseline of AP is the proportion of the positive samples (P.P.). ROC AUC could be interpreted as the possibility of choosing the correct positive sample among one positive sample and one negative sample; a random baseline for that is 0.5. To compare performance of different models, we also report mean R2 score (mR2) over three dimensional emotion, mean average precision (mAP), and mean ROC AUC (mRA) over 26 categories of emotion. For the ease of comparison, we define emotion recognition score (ERS) as follows and use it to compare performance of different methods: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>59624</offset><text>LMA Feature Significance Test</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>59654</offset><text>For each categorical emotion and dimension of VAD, we conducted linear regression tests on each dimension of features listed in Table 2. All tests were conducted using the BoLD training set. We did not find strong correlations (R2 &lt;0.02) over LMA features and emotion dimensions other than arousal, i.e., categorical emotion and valence and dominance. Arousal, however, seems to be significantly correlated with LMA features. Figure 13 shows the kernel density estimation plots of features with top R2 on arousal. Hands-related features are good indicators for arousal. With hand acceleration,  alone, R2 can be achieved as 0.101. Other significant features for predicting arousal are hands velocity, shoulders acceleration, elbow acceleration, and hands jerk.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>60415</offset><text>Model Performance</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>60433</offset><text>Table 3 shows the results on the emotion classification and regression tasks. TSN achieves the best performance, with a mean R2 of 0.095, a mean average precision of 17.02%, a mean ROC AUC of 62.70%, and an ERS of 0.247. Figure 14 presents detailed metric comparisons over all methods of each categorical and dimensional emotion.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>60763</offset><text>For the pipeline that learns from the skeleton, both LMA and ST-GCN achieved above-chance results. Our handcrafted LMA features performs better than end-to-end ST-GCN under all evaluation metrics. For the pipeline that learns from pixels, trajectory-based activity features did not achieve above-chance results for both regression and classification task. However, two-stream network-based methods achieved significant above-chance results for both regression and classification tasks. As shown in Fig. 14 and Table 1, most top-performance categories, such as affection, happiness, pleasure, excitement, sadness, anger, and pain, receive high agreement (κ) among annotators. Similar to the results from skeleton-based methods, two-stream network-based methods show better regression performance over arousal than for valence and dominance. However, as shown in Fig. 11, workers with top 10% performance has R2 score of 0.48, −0.01, and 0.16 for valence, arousal, and dominance, respectively. Apparently, humans are best at recognizing valence and worst at recognizing arousal, and the distinction between human performance and model performance may suggest that there could be other useful features that the model has not explored.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>61999</offset><text>Ablation Study</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>62014</offset><text>To further understand the effectiveness of the two-stream-based model on our task, we conducted two sets of experiments to diagnose (1) if our task could leverage learned filters from pretrained activity-recognition model, and (2) how much a person’s face contributed to the performance in the model. Since TSN has shown the best performance among all two-stream-based models, we conducted all experiments with TSN in this subsection. For the first set of experiments, we used different pretrained models, i.e., image-classification model pretrained on ImageNet and action recognition model pretrained on Kinetics, to initialize TSN. Table 4 shows the results for each case. The results demonstrate that ini tializing with pretrained ImageNet model leads to slightly better emotion-recognition performance. For the second set of experiments, we train TSN with two other different input types, i.e., face only and faceless body. Our experiment in the last section crops the whole human body as the input. For face only, we crop the face for both spatial branch (RGB image) and temporal branch (optical flow) during both the training and testing stages. Note that for the face-only setting, orientation of faces in our dataset may be inconsistent, i.e, facing forward, facing backward, or facing to the side. For the faceless body, we still crop the whole body, but we also mask the region of face by imputing pixel value with a constant 128. Table 5 shows the results for each setting. We can see from the results that the performance of using either the face or the faceless body as input is comparable to that of using the whole body as input. This result suggests both face and the rest of the body contribute significantly to the final prediction. Although the “whole body” setting of TSN performs better than any of the single model do, it does so by leveraging both facial expression and bodily expression.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>63932</offset><text>ARBEE: Automated Recognition of Bodily Expression of Emotion</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>63993</offset><text>We constructed our emotion recognition system, ARBEE, by ensembling best models of different modalities. As suggested in the previous subsection, different modalities could provide complementary clues for emotion recognition. Concretely, we average the prediction from different models (TSN-body: TSN trained with whole body, TSN-face: TSN trained with face, and LMA: random forest model with LMA features) and evaluate the performance on the test set. Table 6 shows the results of ensembled results. According to the table, combining all modalities, i.e., body, face and skeleton, achieves the best performance. ARBEE is the average ensemble of the three models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>64657</offset><text>We further investigated how well ARBEE retrieves instances in the test set given a specific categorical emotion as query. Concretely, we calculated precision at 10, 100, and R-Precision as summarized in Table 7. R-Precision is computed as precision at R, where R is number of positive samples. Similar to the classification results, happiness and pleasure can be retrieved with a rather high level of precision.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>65069</offset><text>Conclusions and Future Work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>65097</offset><text>We proposed a scalable and reliable video-data collection pipeline and collected a large-scale bodily expression dataset, the BoLD. We have validated our data collection via statistical analysis. To our knowledge, our effort is the first quantitative investigation of human performance on emotional expression recognition with thousands of people, tens of thousands of clips, and thousands of characters. Importantly, we found significant predictive features regarding the computability of bodily emotion, i.e., hand acceleration for emotional expressions along the dimension of arousal. Moreover, for the first time, our deep model demonstrates decent generalizability for bodily expression recognition in the wild.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>65814</offset><text>Possible directions for future work are numerous. First, our model’s regression performance of arousal is clearly better than that of valence, yet our analysis shows humans are better at recognizing valence. The inadequacy in feature extraction and modeling, especially for valence, suggests the need for additional investigation. Second, our analysis has identified demographic factors in emotion perception between different ethnic groups. Our current model has largely ignored these potentially useful factors. Considering characters’ demographics in the inference of bodily expression can be a fascinating research direction. Finally, although this work has focused on bodily expression, the BoLD dataset we have collected has several other modalities useful for emotion recognition, including audio and visual context. An integrated approach to study these will likely lead to exciting real-world applications.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>66734</offset><text>https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>66796</offset><text>https://github.com/abewley/sort.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>66829</offset><text>https://github.com/vadimkantorov/fastvideofeat.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>66877</offset><text>http://pytorch.org/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>66898</offset><text>References</text></passage><passage><infon key="comment">1609.08675.</infon><infon key="name_0">surname:Abu-El-Haija;given-names:S</infon><infon key="name_1">surname:Kothari;given-names:N</infon><infon key="name_2">surname:Lee;given-names:J</infon><infon key="name_3">surname:Natsev;given-names:P</infon><infon key="name_4">surname:Toderici;given-names:G</infon><infon key="name_5">surname:Varadarajan;given-names:B</infon><infon key="name_6">surname:Vijayanarasimhan;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>66909</offset><text>Youtube-8m: A large-scale video classification benchmark</text></passage><passage><infon key="fpage">262</infon><infon key="issue">6</infon><infon key="lpage">276</infon><infon key="name_0">surname:Aristidou;given-names:A</infon><infon key="name_1">surname:Charalambous;given-names:P</infon><infon key="name_2">surname:Chrysanthou;given-names:Y</infon><infon key="section_type">REF</infon><infon key="source">Computer Graphics Forum</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2015</infon><offset>66966</offset><text>Emotion analysis and classification: understanding the performers’ emotions using the lma entities</text></passage><passage><infon key="comment">article 9.</infon><infon key="name_0">surname:Aristidou;given-names:A</infon><infon key="name_1">surname:Zeng;given-names:Q</infon><infon key="name_2">surname:Stavrakis;given-names:E</infon><infon key="name_3">surname:Yin;given-names:K</infon><infon key="name_4">surname:Cohen-Or;given-names:D</infon><infon key="name_5">surname:Chrysanthou;given-names:Y</infon><infon key="name_6">surname:Chen;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM SIG GRAPH/Eurographics Symposium on Computer Animation</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>67067</offset><text>Emotion control of unstructured dance movements</text></passage><passage><infon key="fpage">1225</infon><infon key="issue">6111</infon><infon key="lpage">1229</infon><infon key="name_0">surname:Aviezer;given-names:H</infon><infon key="name_1">surname:Trope;given-names:Y</infon><infon key="name_2">surname:Todorov;given-names:A</infon><infon key="pub-id_pmid">23197536</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">338</infon><infon key="year">2012</infon><offset>67115</offset><text>Body cues, not facial expressions, discriminate between intense positive and negative emotions</text></passage><passage><infon key="comment">.</infon><infon key="fpage">3464</infon><infon key="lpage">3468</infon><infon key="name_0">surname:Bewley;given-names:A</infon><infon key="name_1">surname:Ge;given-names:Z</infon><infon key="name_2">surname:Ott;given-names:L</infon><infon key="name_3">surname:Ramos;given-names:F</infon><infon key="name_4">surname:Upcroft;given-names:B</infon><infon key="pub-id_doi">10.1109/ICIP.2016.7533003</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>67210</offset><text>Simple online and realtime tracking</text></passage><passage><infon key="fpage">41</infon><infon key="issue">1</infon><infon key="lpage">55</infon><infon key="name_0">surname:Biel;given-names:JI</infon><infon key="name_1">surname:Gatica-Perez;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Multimedia</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2013</infon><offset>67246</offset><text>The youtube lens: Crowdsourced personality impressions and audiovisual analysis of vlogs</text></passage><passage><infon key="fpage">961</infon><infon key="lpage">970</infon><infon key="name_0">surname:Caba Heilbron;given-names:F</infon><infon key="name_1">surname:Escorcia;given-names:V</infon><infon key="name_2">surname:Ghanem;given-names:B</infon><infon key="name_3">surname:Carlos Niebles;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>67335</offset><text>Activitynet: A large-scale video benchmark for human activity understanding</text></passage><passage><infon key="fpage">7291</infon><infon key="lpage">7299</infon><infon key="name_0">surname:Cao;given-names:Z</infon><infon key="name_1">surname:Simon;given-names:T</infon><infon key="name_2">surname:Wei;given-names:SE</infon><infon key="name_3">surname:Sheikh;given-names:Y</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>67411</offset><text>Realtime multi-person 2d pose estimation using part affinity fields</text></passage><passage><infon key="fpage">115</infon><infon key="issue">1</infon><infon key="lpage">142</infon><infon key="name_0">surname:Carmichael;given-names:L</infon><infon key="name_1">surname:Roberts;given-names:S</infon><infon key="name_2">surname:Wessell;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">The Journal of Social Psychology</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">1937</infon><offset>67479</offset><text>A study of the judgment of manual expression as presented in still and motion pictures</text></passage><passage><infon key="comment">IEEE</infon><infon key="fpage">4724</infon><infon key="lpage">4733</infon><infon key="name_0">surname:Carreira;given-names:J</infon><infon key="name_1">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>67566</offset><text>Quo vadis, action recognition? A new model and the kinetics dataset</text></passage><passage><infon key="comment">.</infon><infon key="name_0">surname:Noroozi;given-names:F</infon><infon key="name_1">surname:Kaminska;given-names:D</infon><infon key="name_2">surname:Corneanu;given-names:C</infon><infon key="name_3">surname:Sapinski;given-names:T</infon><infon key="name_4">surname:Escalera;given-names:S</infon><infon key="name_5">surname:Anbarjafari;given-names:G</infon><infon key="pub-id_doi">10.1109/TAFFC.2018.2874986</infon><infon key="section_type">REF</infon><infon key="source">Journal Of IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>67634</offset><text>Survey on emotional body gesture recognition</text></passage><passage><infon key="fpage">1085</infon><infon key="issue">5</infon><infon key="name_0">surname:Dael;given-names:N</infon><infon key="name_1">surname:Mortillaro;given-names:M</infon><infon key="name_2">surname:Scherer;given-names:KR</infon><infon key="pub-id_pmid">22059517</infon><infon key="section_type">REF</infon><infon key="source">Emotion</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2012</infon><offset>67679</offset><text>Emotion expression in body action and posture</text></passage><passage><infon key="comment">Springer</infon><infon key="fpage">428</infon><infon key="lpage">441</infon><infon key="name_0">surname:Dalal;given-names:N</infon><infon key="name_1">surname:Triggs;given-names:B</infon><infon key="name_2">surname:Schmid;given-names:C</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>67725</offset><text>Human detection using oriented histograms of flow and appearance</text></passage><passage><infon key="fpage">288</infon><infon key="lpage">301</infon><infon key="name_0">surname:Datta;given-names:R</infon><infon key="name_1">surname:Joshi;given-names:D</infon><infon key="name_2">surname:Li;given-names:J</infon><infon key="name_3">surname:Wang;given-names:JZ</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>67790</offset><text>Studying aesthetics in photographic images using a computational approach</text></passage><passage><infon key="fpage">20</infon><infon key="lpage">28</infon><infon key="name_0">surname:Dawid;given-names:AP</infon><infon key="name_1">surname:Skene;given-names:AM</infon><infon key="section_type">REF</infon><infon key="source">Applied Statistics</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">1979</infon><offset>67864</offset><text>Maximum likelihood estimation of observer error-rates using the em algorithm</text></passage><passage><infon key="fpage">242</infon><infon key="issue">3</infon><infon key="lpage">249</infon><infon key="name_0">surname:De Gelder;given-names:B</infon><infon key="pub-id_pmid">16495945</infon><infon key="section_type">REF</infon><infon key="source">Nature Reviews Neuroscience</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2006</infon><offset>67941</offset><text>Towards the neurobiology of emotional body language</text></passage><passage><infon key="fpage">248</infon><infon key="lpage">255</infon><infon key="name_0">surname:Deng;given-names:J</infon><infon key="name_1">surname:Dong;given-names:W</infon><infon key="name_2">surname:Socher;given-names:R</infon><infon key="name_3">surname:Li;given-names:LJ</infon><infon key="name_4">surname:Li;given-names:K</infon><infon key="name_5">surname:Fei-Fei;given-names:L</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>67993</offset><text>Imagenet: A large-scale hierarchical image database</text></passage><passage><infon key="fpage">488</infon><infon key="lpage">500</infon><infon key="name_0">surname:Douglas-Cowie;given-names:E</infon><infon key="name_1">surname:Cowie;given-names:R</infon><infon key="name_2">surname:Sneddon;given-names:I</infon><infon key="name_3">surname:Cox;given-names:C</infon><infon key="name_4">surname:Lowry;given-names:L</infon><infon key="name_5">surname:McRorie;given-names:M</infon><infon key="name_6">surname:Martin;given-names:LJC</infon><infon key="name_7">surname:Devillers;given-names:J</infon><infon key="name_8">surname:Abrilian;given-names:A</infon><infon key="name_9">surname:Bat-liner;given-names:S</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>68045</offset><text>The humaine database: addressing the needs of the affective computing community</text></passage><passage><infon key="fpage">550</infon><infon key="issue">3</infon><infon key="lpage">553</infon><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="pub-id_pmid">1344638</infon><infon key="section_type">REF</infon><infon key="source">Psychological Review</infon><infon key="type">ref</infon><infon key="volume">99</infon><infon key="year">1992</infon><offset>68125</offset><text>Are there basic emotions?</text></passage><passage><infon key="fpage">384</infon><infon key="issue">4</infon><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">American Psychologist</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">1993</infon><offset>68151</offset><text>Facial expression and emotion</text></passage><passage><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="name_1">surname:Friesen;given-names:WV</infon><infon key="section_type">REF</infon><infon key="source">Facial Action Coding System: A technique for the measurement of facial movement</infon><infon key="type">ref</infon><infon key="year">1977</infon><offset>68181</offset></passage><passage><infon key="fpage">159</infon><infon key="issue">2</infon><infon key="lpage">168</infon><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="name_1">surname:Friesen;given-names:WV</infon><infon key="section_type">REF</infon><infon key="source">Motivation and Emotion</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">1986</infon><offset>68182</offset><text>A new pan-cultural facial expression of emotion</text></passage><passage><infon key="fpage">189</infon><infon key="issue">1</infon><infon key="lpage">204</infon><infon key="name_0">surname:Eleftheriadis;given-names:S</infon><infon key="name_1">surname:Rudovic;given-names:O</infon><infon key="name_2">surname:Pantic;given-names:M</infon><infon key="pub-id_pmid">25438312</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Image Processing</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2015</infon><offset>68230</offset><text>Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</text></passage><passage><infon key="fpage">5562</infon><infon key="lpage">5570</infon><infon key="name_0">surname:Fabian Benitez-Quiroz;given-names:C</infon><infon key="name_1">surname:Srinivasan;given-names:R</infon><infon key="name_2">surname:Martinez;given-names:AM</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>68334</offset><text>Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</text></passage><passage><infon key="fpage">6047</infon><infon key="lpage">6056</infon><infon key="name_0">surname:Gu;given-names:C</infon><infon key="name_1">surname:Sun;given-names:C</infon><infon key="name_2">surname:Ross;given-names:DA</infon><infon key="name_3">surname:Vondrick;given-names:C</infon><infon key="name_4">surname:Pantofaru;given-names:C</infon><infon key="name_5">surname:Li;given-names:Y</infon><infon key="name_6">surname:Vijayanarasimhan;given-names:S</infon><infon key="name_7">surname:Toderici;given-names:G</infon><infon key="name_8">surname:Ricco;given-names:S</infon><infon key="name_9">surname:Sukthankar;given-names:R</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>68451</offset><text>Ava: A video dataset of spatio-temporally localized atomic visual actions</text></passage><passage><infon key="fpage">3437</infon><infon key="lpage">3443</infon><infon key="name_0">surname:Gunes;given-names:H</infon><infon key="name_1">surname:Piccardi;given-names:M</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2005</infon><offset>68525</offset><text>Affect recognition from face and body: early fusion vs. late fusion</text></passage><passage><infon key="fpage">1334</infon><infon key="issue">4</infon><infon key="lpage">1345</infon><infon key="name_0">surname:Gunes;given-names:H</infon><infon key="name_1">surname:Piccardi;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Journal of Network and Computer Applications</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2007</infon><offset>68593</offset><text>Bi-modal emotion recognition from expressive face and body gestures</text></passage><passage><infon key="name_0">surname:Gwet;given-names:KL</infon><infon key="section_type">REF</infon><infon key="source">Handbook of Inter-rater Reliability: The Definitive Guide to Measuring the Extent of Agreement Among Raters</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>68661</offset></passage><passage><infon key="fpage">770</infon><infon key="lpage">778</infon><infon key="name_0">surname:He;given-names:K</infon><infon key="name_1">surname:Zhang;given-names:X</infon><infon key="name_2">surname:Ren;given-names:S</infon><infon key="name_3">surname:Sun;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>68662</offset><text>Deep residual learning for image recognition</text></passage><passage><infon key="fpage">2011</infon><infon key="lpage">2020</infon><infon key="name_0">surname:Iqbal;given-names:U</infon><infon key="name_1">surname:Milan;given-names:A</infon><infon key="name_2">surname:Gall;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>68707</offset><text>Posetrack: Joint multi-person pose estimation and tracking</text></passage><passage><infon key="fpage">2593</infon><infon key="lpage">2600</infon><infon key="name_0">surname:Kantorov;given-names:V</infon><infon key="name_1">surname:Laptev;given-names:I</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>68766</offset><text>Efficient feature extraction, encoding and classification for action recognition</text></passage><passage><infon key="fpage">341</infon><infon key="issue">4</infon><infon key="lpage">359</infon><infon key="name_0">surname:Karg;given-names:M</infon><infon key="name_1">surname:Samadani;given-names:AA</infon><infon key="name_2">surname:Gorbet;given-names:R</infon><infon key="name_3">surname:Kühnlenz;given-names:K</infon><infon key="name_4">surname:Hoey;given-names:J</infon><infon key="name_5">surname:Kulić;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2013</infon><offset>68847</offset><text>Body movements for affective expression: A survey of automatic recognition and generation</text></passage><passage><infon key="comment">1705.06950.</infon><infon key="name_0">surname:Kay;given-names:W</infon><infon key="name_1">surname:Carreira;given-names:J</infon><infon key="name_2">surname:Simonyan;given-names:K</infon><infon key="name_3">surname:Zhang;given-names:B</infon><infon key="name_4">surname:Hillier;given-names:C</infon><infon key="name_5">surname:Vijayanarasimhan;given-names:S</infon><infon key="name_6">surname:Viola;given-names:F</infon><infon key="name_7">surname:Green;given-names:T</infon><infon key="name_8">surname:Back;given-names:T</infon><infon key="name_9">surname:Natsev;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>68937</offset><text>The kinetics human action video dataset</text></passage><passage><infon key="comment">1609.02907.</infon><infon key="name_0">surname:Kipf;given-names:TN</infon><infon key="name_1">surname:Welling;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>68977</offset><text>Semi-supervised classification with graph convolutional networks</text></passage><passage><infon key="fpage">15</infon><infon key="issue">1</infon><infon key="lpage">33</infon><infon key="name_0">surname:Kleinsmith;given-names:A</infon><infon key="name_1">surname:Bianchi-Berthouze;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2013</infon><offset>69042</offset><text>Affective body expression perception and recognition: A survey</text></passage><passage><infon key="fpage">1371</infon><infon key="issue">6</infon><infon key="lpage">1389</infon><infon key="name_0">surname:Kleinsmith;given-names:A</infon><infon key="name_1">surname:De Silva;given-names:PR</infon><infon key="name_2">surname:Bianchi-Berthouze;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">Interacting with Computers</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2006</infon><offset>69105</offset><text>Cross-cultural differences in recognizing affect from body posture</text></passage><passage><infon key="fpage">1027</infon><infon key="issue">4</infon><infon key="lpage">1038</infon><infon key="name_0">surname:Kleinsmith;given-names:A</infon><infon key="name_1">surname:Bianchi-Berthouze;given-names:N</infon><infon key="name_2">surname:Steed;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2011</infon><offset>69172</offset><text>Automatic recognition of non-acted affective postures</text></passage><passage><infon key="fpage">1667</infon><infon key="lpage">1675</infon><infon key="name_0">surname:Kosti;given-names:R</infon><infon key="name_1">surname:Alvarez;given-names:JM</infon><infon key="name_2">surname:Recasens;given-names:A</infon><infon key="name_3">surname:Lapedriza;given-names:A</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>69226</offset><text>Emotion recognition in context</text></passage><passage><infon key="fpage">18</infon><infon key="issue">4</infon><infon key="lpage">19</infon><infon key="name_0">surname:Krakovsky;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Communications of the ACM</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">2018</infon><offset>69257</offset><text>Artificial (emotional) intelligence</text></passage><passage><infon key="fpage">1097</infon><infon key="lpage">1105</infon><infon key="name_0">surname:Krizhevsky;given-names:A</infon><infon key="name_1">surname:Sutskever;given-names:I</infon><infon key="name_2">surname:Hinton;given-names:GE</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>69293</offset><text>Imagenet classification with deep convolutional neural networks</text></passage><passage><infon key="name_0">surname:Laban;given-names:R</infon><infon key="name_1">surname:Ullmann;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">The Mastery of Movement</infon><infon key="type">ref</infon><infon key="year">1971</infon><offset>69357</offset></passage><passage><infon key="fpage">229</infon><infon key="lpage">238</infon><infon key="name_0">surname:Lu;given-names:X</infon><infon key="name_1">surname:Suryanarayan;given-names:P</infon><infon key="name_2">surname:Adams;given-names:RB</infon><infon key="name_3">surname:Li;given-names:J</infon><infon key="name_4">surname:Newman;given-names:MG</infon><infon key="name_5">surname:Wang;given-names:JZ</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>69358</offset><text>On shape and the computability of emotions</text></passage><passage><infon key="fpage">440</infon><infon key="lpage">447</infon><infon key="name_0">surname:Lu;given-names:X</infon><infon key="name_1">surname:Adams;given-names:RB</infon><infon key="name_2">surname:Li;given-names:J</infon><infon key="name_3">surname:Newman;given-names:MG</infon><infon key="name_4">surname:Wang;given-names:JZ</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>69401</offset><text>An investigation into three visual characteristics of complex scenes that evoke human emotion</text></passage><passage><infon key="fpage">5137</infon><infon key="lpage">5146</infon><infon key="name_0">surname:Luvizon;given-names:DC</infon><infon key="name_1">surname:Picard;given-names:D</infon><infon key="name_2">surname:Tabia;given-names:H</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>69495</offset><text>2d/3d pose estimation and action recognition using multitask deep learning</text></passage><passage><infon key="fpage">2640</infon><infon key="lpage">2649</infon><infon key="name_0">surname:Martinez;given-names:J</infon><infon key="name_1">surname:Hossain;given-names:R</infon><infon key="name_2">surname:Romero;given-names:J</infon><infon key="name_3">surname:Little;given-names:JJ</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>69570</offset><text>A simple yet effective baseline for 3d human pose estimation</text></passage><passage><infon key="fpage">16518</infon><infon key="issue">45</infon><infon key="lpage">16523</infon><infon key="name_0">surname:Meeren;given-names:HK</infon><infon key="name_1">surname:van Heijnsbergen;given-names:CC</infon><infon key="name_2">surname:de Gelder;given-names:B</infon><infon key="pub-id_pmid">16260734</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences of the United States of America</infon><infon key="type">ref</infon><infon key="volume">102</infon><infon key="year">2005</infon><offset>69631</offset><text>Rapid perceptual integration of facial expression and emotional body language</text></passage><passage><infon key="name_0">surname:Mehrabian;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies</infon><infon key="type">ref</infon><infon key="year">1980</infon><offset>69709</offset></passage><passage><infon key="fpage">261</infon><infon key="issue">4</infon><infon key="lpage">292</infon><infon key="name_0">surname:Mehrabian;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Current Psychology</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">1996</infon><offset>69710</offset><text>Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament</text></passage><passage><infon key="fpage">92</infon><infon key="issue">2</infon><infon key="lpage">105</infon><infon key="name_0">surname:Nicolaou;given-names:MA</infon><infon key="name_1">surname:Gunes;given-names:H</infon><infon key="name_2">surname:Pantic;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2011</infon><offset>69825</offset><text>Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Perronnin;given-names:F</infon><infon key="name_1">surname:Dance;given-names:C</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>69928</offset><text>Fisher kernels on visual vocabularies for image categorization</text></passage><passage><infon key="comment">Springer</infon><infon key="fpage">540</infon><infon key="lpage">555</infon><infon key="name_0">surname:Potapov;given-names:D</infon><infon key="name_1">surname:Douze;given-names:M</infon><infon key="name_2">surname:Harchaoui;given-names:Z</infon><infon key="name_3">surname:Schmid;given-names:C</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>69991</offset><text>Category-specific video summarization</text></passage><passage><infon key="fpage">369</infon><infon key="lpage">378</infon><infon key="name_0">surname:Ruggero Ronchi;given-names:M</infon><infon key="name_1">surname:Perona;given-names:P</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>70029</offset><text>Benchmarking and error diagnosis in multi-instance pose estimation</text></passage><passage><infon key="fpage">1238</infon><infon key="issue">9</infon><infon key="lpage">1246</infon><infon key="name_0">surname:Schindler;given-names:K</infon><infon key="name_1">surname:Van Gool;given-names:L</infon><infon key="name_2">surname:de Gelder;given-names:B</infon><infon key="pub-id_pmid">18585892</infon><infon key="section_type">REF</infon><infon key="source">Neural Networks</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2008</infon><offset>70096</offset><text>Recognizing emotions expressed by body pose: A biologically inspired neural model</text></passage><passage><infon key="fpage">248</infon><infon key="lpage">264</infon><infon key="name_0">surname:Shiffrar;given-names:M</infon><infon key="name_1">surname:Kaiser;given-names:MD</infon><infon key="name_2">surname:Chouchourelou;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">The Science of Social Vision</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>70178</offset><text>Seeing human movement as inherently social</text></passage><passage><infon key="fpage">568</infon><infon key="lpage">576</infon><infon key="name_0">surname:Simonyan;given-names:K</infon><infon key="name_1">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>70221</offset><text>Two-stream convolutional networks for action recognition in videos</text></passage><passage><infon key="comment">1212.0402v1.</infon><infon key="name_0">surname:Soomro;given-names:K</infon><infon key="name_1">surname:Zamir;given-names:AR</infon><infon key="name_2">surname:Shah;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>70288</offset><text>Ucf101: A dataset of 101 human actions classes from videos in the wild</text></passage><passage><infon key="fpage">64</infon><infon key="issue">2</infon><infon key="lpage">73</infon><infon key="name_0">surname:Thomee;given-names:B</infon><infon key="name_1">surname:Shamma;given-names:DA</infon><infon key="name_2">surname:Friedland;given-names:G</infon><infon key="name_3">surname:Elizalde;given-names:B</infon><infon key="name_4">surname:Ni;given-names:K</infon><infon key="name_5">surname:Poland;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Communications of the ACM</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2016</infon><offset>70359</offset><text>Yfcc100m: The new data in multimedia research</text></passage><passage><infon key="fpage">62</infon><infon key="issue">5</infon><infon key="lpage">74</infon><infon key="name_0">surname:Towns;given-names:J</infon><infon key="name_1">surname:Cockerill;given-names:T</infon><infon key="name_2">surname:Dahan;given-names:M</infon><infon key="name_3">surname:Foster;given-names:I</infon><infon key="name_4">surname:Gaither;given-names:K</infon><infon key="name_5">surname:Grimshaw;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Computing in Science &amp; Engineering</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2014</infon><offset>70405</offset><text>Xsede: Accelerating scientific discovery</text></passage><passage><infon key="fpage">929</infon><infon key="issue">5</infon><infon key="lpage">940</infon><infon key="name_0">surname:Wakabayashi;given-names:A</infon><infon key="name_1">surname:Baron-Cohen;given-names:S</infon><infon key="name_2">surname:Wheelwright;given-names:S</infon><infon key="name_3">surname:Goldenfeld;given-names:N</infon><infon key="name_4">surname:Delaney;given-names:J</infon><infon key="name_5">surname:Fine;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Personality and Individual Differences</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2006</infon><offset>70446</offset><text>Development of short forms of the empathy quotient (eq-short) and the systemizing quotient (sq-short)</text></passage><passage><infon key="fpage">879</infon><infon key="issue">6</infon><infon key="lpage">896</infon><infon key="name_0">surname:Wallbott;given-names:HG</infon><infon key="section_type">REF</infon><infon key="source">European Journal of Social Psychology</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">1998</infon><offset>70548</offset><text>Bodily expression of emotion</text></passage><passage><infon key="fpage">3551</infon><infon key="lpage">3558</infon><infon key="name_0">surname:Wang;given-names:H</infon><infon key="name_1">surname:Schmid;given-names:C</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>70577</offset><text>Action recognition with improved trajectories</text></passage><passage><infon key="fpage">3169</infon><infon key="lpage">3176</infon><infon key="name_0">surname:Wang;given-names:H</infon><infon key="name_1">surname:Kläser;given-names:A</infon><infon key="name_2">surname:Schmid;given-names:C</infon><infon key="name_3">surname:Liu;given-names:CL</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>70623</offset><text>Action recognition by dense trajectories</text></passage><passage><infon key="comment">Springer</infon><infon key="fpage">20</infon><infon key="lpage">36</infon><infon key="name_0">surname:Wang;given-names:L</infon><infon key="name_1">surname:Xiong;given-names:Y</infon><infon key="name_2">surname:Wang;given-names:Z</infon><infon key="name_3">surname:Qiao;given-names:Y</infon><infon key="name_4">surname:Lin;given-names:D</infon><infon key="name_5">surname:Tang;given-names:X</infon><infon key="name_6">surname:Van Gool;given-names:L</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>70664</offset><text>Temporal segment networks: Towards good practices for deep action recognition</text></passage><passage><infon key="fpage">254</infon><infon key="issue">2</infon><infon key="lpage">267</infon><infon key="name_0">surname:Xu;given-names:F</infon><infon key="name_1">surname:Zhang;given-names:J</infon><infon key="name_2">surname:Wang;given-names:JZ</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>70742</offset><text>Microexpression identification and categorization using a facial dynamics map</text></passage><passage><infon key="name_0">surname:Yan;given-names:S</infon><infon key="name_1">surname:Xiong;given-names:Y</infon><infon key="name_2">surname:Lin;given-names:D</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>70820</offset><text>Spatial temporal graph convolutional networks for skeleton-based action recognition</text></passage><passage><infon key="fpage">115</infon><infon key="issue">1</infon><infon key="lpage">128</infon><infon key="name_0">surname:Ye;given-names:J</infon><infon key="name_1">surname:Li;given-names:J</infon><infon key="name_2">surname:Newman;given-names:MG</infon><infon key="name_3">surname:Adams;given-names:RB</infon><infon key="name_4">surname:Wang;given-names:JZ</infon><infon key="pub-id_pmid">31576202</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Affective Computing</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2019</infon><offset>70904</offset><text>Probabilistic multigraph modeling for improving the quality of crowdsourced affective data</text></passage><passage><infon key="fpage">214</infon><infon key="lpage">223</infon><infon key="name_0">surname:Zach;given-names:C</infon><infon key="name_1">surname:Pock;given-names:T</infon><infon key="name_2">surname:Bischof;given-names:H</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Joint Pattern Recognition Symposium</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>70995</offset></passage><passage><infon key="file">nihms-1602760-f0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70996</offset><text>Examples of possible scenarios where computerized bodily expression recognition can be useful. From left to right: psychological clinic assistance, public safety and law enforcement, and social robot or social media</text></passage><passage><infon key="file">nihms-1602760-f0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>71212</offset><text>Overview of our data collection pipeline. The process involves crawling movies, segmenting them into clips, estimating the poses, and emotion annotation</text></passage><passage><infon key="file">nihms-1602760-f0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>71365</offset><text>A frame in a video clip, with different characters numbered with an ID (e.g., 0 and 1 at the bottom left corner of red bounding boxes) and the body and/or facial landmarks detected (indicated with the stick figure)</text></passage><passage><infon key="file">nihms-1602760-f0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>71580</offset><text>The web-based crowdsourcing data collection process. Screenshots of the four steps are shown. For each video clip, participants are directed to go through a sequence of screens with questions step-by-step</text></passage><passage><infon key="file">nihms-1602760-f0006.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>71785</offset><text>Examples of high-confidence instances in BoLD for the 26 categorical emotions and two instances that were used for quality control. For each subfigure, the left side is a frame from the video, along with another copy that has the character entity IDs marked in a bounding box. The right side shows the corresponding aggregated annotation, annotation confidence c, demographics of the character, and aggregated categorical and dimensional emotion. Examples of high-confidence instances in BoLD for the 26 categorical emotions and two instances (27 and 28) that were used for quality control. For each subfigure, the left side is a frame from the video, along with another copy that has the character entity IDs marked in a bounding box. The right side shows the corresponding aggregated annotation, annotation confidence c, demographics of the character, and aggregated categorical and dimensionalemotion</text></passage><passage><infon key="file">nihms-1602760-f0007.jpg</infon><infon key="id">F6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>72689</offset><text>Distributions of the 26 different categorical emotions</text></passage><passage><infon key="file">nihms-1602760-f0008.jpg</infon><infon key="id">F7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>72744</offset><text>Distributions of the three dimensional emotion ratings: valence, arousal, and dominance</text></passage><passage><infon key="file">nihms-1602760-f0009.jpg</infon><infon key="id">F8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>72832</offset><text>Demographics of characters in our dataset</text></passage><passage><infon key="file">nihms-1602760-f0010.jpg</infon><infon key="id">F9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>72874</offset><text>Correlations between pairs of categorical or dimensional emotions, calculated based on the BoLD dataset</text></passage><passage><infon key="file">nihms-1602760-f0011.jpg</infon><infon key="id">F10</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>72978</offset><text>Reliability score distribution among low-performance participants (failure) and non low-performance participants (pass)</text></passage><passage><infon key="file">nihms-1602760-f0012.jpg</infon><infon key="id">F11</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>73098</offset><text>Human regression performance on dimensional emotions. X-axis: participant population percentile. Y-axis: F1, R2 and MSE score. Tables inside each plot in the second row summarize top 30%, 20%, 10%, and 5% participant regression scores</text></passage><passage><infon key="file">nihms-1602760-f0013.jpg</infon><infon key="id">F12</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>73333</offset><text>Illustration of the human skeleton. Both red lines and black lines are considered limbs in our context</text></passage><passage><infon key="file">nihms-1602760-f0014.jpg</infon><infon key="id">F13</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>73436</offset><text>Kernel density estimation plots on selected LMA features that have high correlation with arousal</text></passage><passage><infon key="file">nihms-1602760-f0015.jpg</infon><infon key="id">F14</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>73533</offset><text>Classification performance (AP: average precision on the top left, RA: ROC AUC on the top right) and regression performance (R2 on the bottom) of different methods on each categorical and dimensional emotion</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>73741</offset><text>Agreement among participants on categorical emotions and characters’ demographic information</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Category&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;κ&lt;/italic&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Filtered &lt;italic&gt;κ&lt;/italic&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Category&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;κ&lt;/italic&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Filtered &lt;italic&gt;κ&lt;/italic&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Category&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;κ&lt;/italic&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Filtered &lt;italic&gt;κ&lt;/italic&gt;&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Peace&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.132&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.148&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Affection&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.262&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.296&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Esteem&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.077&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.094&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anticipation&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.071&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.078&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Engagement&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.110&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.126&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Confidence&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.166&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.183&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happiness&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.385&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.414&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pleasure&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.171&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.200&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Excitement&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.178&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.208&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Surprise&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.137&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.155&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sympathy&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.114&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.127&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Doubt/confusion&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.127&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.141&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disconnection&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.125&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.140&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fatigue&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.113&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.131&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Embarrassment&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.066&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.085&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yearning&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.030&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.036&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disapproval&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.140&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.153&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aversion&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.075&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.087&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Annoyance&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.176&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.197&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.287&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.307&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sensitivity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.082&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.097&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sadness&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.233&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.267&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disquietment&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.110&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.125&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fear&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.193&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.214&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pain&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.273&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.312&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Suffering&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.161&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.186&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Average&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.154&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.173&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gender&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.863&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.884&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Age&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.462&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.500&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ethnicity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.410&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.466&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>73836</offset><text>Category	κ	Filtered κ	Category	κ	Filtered κ	Category	κ	Filtered κ	 	Peace	0.132	0.148	Affection	0.262	0.296	Esteem	0.077	0.094	 	Anticipation	0.071	0.078	Engagement	0.110	0.126	Confidence	0.166	0.183	 	Happiness	0.385	0.414	Pleasure	0.171	0.200	Excitement	0.178	0.208	 	Surprise	0.137	0.155	Sympathy	0.114	0.127	Doubt/confusion	0.127	0.141	 	Disconnection	0.125	0.140	Fatigue	0.113	0.131	Embarrassment	0.066	0.085	 	Yearning	0.030	0.036	Disapproval	0.140	0.153	Aversion	0.075	0.087	 	Annoyance	0.176	0.197	Anger	0.287	0.307	Sensitivity	0.082	0.097	 	Sadness	0.233	0.267	Disquietment	0.110	0.125	Fear	0.193	0.214	 	Pain	0.273	0.312	Suffering	0.161	0.186	Average	0.154	0.173	 	Gender	0.863	0.884	Age	0.462	0.500	Ethnicity	0.410	0.466	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>74595</offset><text>Laban Movement Analysis (LMA) features</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;
                &lt;italic&gt;i&lt;/italic&gt;
              &lt;/sub&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Description&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;m&lt;/italic&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;
                &lt;italic&gt;i&lt;/italic&gt;
              &lt;/sub&gt;
            &lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Description&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;m&lt;/italic&gt;
            &lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;1&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feet-hip dist.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;2&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands-shoulder dist.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;3&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands dist.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;4&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands-head dist.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;8&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Centroid-pelvis dist.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;9&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gait size (foot dist.)&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;29&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Shoulders velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;32&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Elbow velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;13&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;12&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hip velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;35&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Knee velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;14&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feet velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;38&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Angular velocity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;inline-formula&gt;
                &lt;mml:math display=&quot;inline&quot; id=&quot;M1&quot;&gt;
                  &lt;mml:mrow&gt;
                    &lt;mml:mn&gt;4&lt;/mml:mn&gt;
                    &lt;mml:msubsup&gt;
                      &lt;mml:mi&gt;C&lt;/mml:mi&gt;
                      &lt;mml:mrow&gt;
                        &lt;mml:mn&gt;23&lt;/mml:mn&gt;
                      &lt;/mml:mrow&gt;
                      &lt;mml:mn&gt;2&lt;/mml:mn&gt;
                    &lt;/mml:msubsup&gt;
                  &lt;/mml:mrow&gt;
                &lt;/mml:math&gt;
              &lt;/inline-formula&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;30&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Shoulders accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;33&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Elbow accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;16&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;15&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hip accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;36&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Knee accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;17&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feet accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;39&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Angular accel.&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;inline-formula&gt;
                &lt;mml:math display=&quot;inline&quot; id=&quot;M2&quot;&gt;
                  &lt;mml:mrow&gt;
                    &lt;mml:mn&gt;4&lt;/mml:mn&gt;
                    &lt;mml:msubsup&gt;
                      &lt;mml:mi&gt;C&lt;/mml:mi&gt;
                      &lt;mml:mrow&gt;
                        &lt;mml:mn&gt;23&lt;/mml:mn&gt;
                      &lt;/mml:mrow&gt;
                      &lt;mml:mn&gt;2&lt;/mml:mn&gt;
                    &lt;/mml:msubsup&gt;
                  &lt;/mml:mrow&gt;
                &lt;/mml:math&gt;
              &lt;/inline-formula&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;31&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Shoulders jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;34&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Elbow jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;40&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hands jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;18&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hip jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;37&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Knee jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;41&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feet jerk&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;19&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Volume&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;20&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Volume (upper body)&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;21&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Volume (lower body)&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;22&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Volume (left side)&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;23&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Volume (right side)&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;f&lt;/italic&gt;
              &lt;sub&gt;24&lt;/sub&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Torso height&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>74634</offset><text>fi	Description	m	fi	Description	m	 	f1	Feet-hip dist.	4	f2	Hands-shoulder dist.	4	 	f3	Hands dist.	4	f4	Hands-head dist.	4	 	f8	Centroid-pelvis dist.	4	f9	Gait size (foot dist.)	4	 	f29	Shoulders velocity	4	f32	Elbow velocity	4	 	f13	Hands velocity	4	f12	Hip velocity	4	 	f35	Knee velocity	4	f14	Feet velocity	4	 	f38	Angular velocity					 	f30	Shoulders accel.	4	f33	Elbow accel.	4	 	f16	Hands accel.	4	f15	Hip accel.	4	 	f36	Knee accel.	4	f17	Feet accel.	4	 	f39	Angular accel.					 	f31	Shoulders jerk	4	f34	Elbow jerk	4	 	f40	Hands jerk	4	f18	Hip jerk	4	 	f37	Knee jerk	4	f41	Feet jerk	4	 	f19	Volume	4	f20	Volume (upper body)	4	 	f21	Volume (lower body)	4	f22	Volume (left side)	4	 	f23	Volume (right side)	4	f24	Torso height	4	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>75370</offset><text>(fi categories, m number of measurements, dist. distance, accel. acceleration)</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>75449</offset><text>Dimensional emotion regression and categorical emotion classification performance on the test set</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Regression&lt;/th&gt;
            &lt;th colspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-bottom: solid 1px&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ERS&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;m&lt;italic&gt;R&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mAP&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mRA&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td colspan=&quot;5&quot; align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot;&gt;A random method based on priors&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; Chance&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.55&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.151&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td colspan=&quot;5&quot; align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot;&gt;Learning from skeleton&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; ST-GCN&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.044&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.63&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.96&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.194&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; LMA&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.075&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;13.59&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;57.71&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.216&lt;/bold&gt;
            &lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td colspan=&quot;5&quot; align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot;&gt;Learning from pixels&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; TF&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.008&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.93&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.25&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.149&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; TS-ResNet101&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.084&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;17.04&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.29&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.240&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; I3D&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.098&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15.37&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.24&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.241&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; TSN&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.095&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.02&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;62.70&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.247&lt;/bold&gt;
            &lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; TSN-Spatial&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.048&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15.34&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.03&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.212&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; TSN-Flow&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.098&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15.78&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.28&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.241&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>75547</offset><text>Model	Regression	Classification	ERS	 		mR2	mAP	mRA		 	A random method based on priors	 	 Chance	0	10.55	50	0.151	 	Learning from skeleton	 	 ST-GCN	0.044	12.63	55.96	0.194	 	 LMA	0.075	13.59	57.71	0.216	 	Learning from pixels	 	 TF	−0.008	10.93	50.25	0.149	 	 TS-ResNet101	0.084	17.04	62.29	0.240	 	 I3D	0.098	15.37	61.24	0.241	 	 TSN	0.095	17.02	62.70	0.247	 	 TSN-Spatial	0.048	15.34	60.03	0.212	 	 TSN-Flow	0.098	15.78	61.28	0.241	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>76004</offset><text>Best performance for each evaluation metric under each modality is highlighted in bold</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>76091</offset><text>mR2 = mean of R2 over dimensional emotions, mAP(%) = average precision/area under precision recall curve (PR AUC) over categorical emotions, mRA(%) = mean of area under ROC curve (ROC AUC) over categorical emotions, and ERS = emotion recognition score. Baseline methods: ST-GCN, TF, TS-ResNet101, I3D, and TSN </text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>76402</offset><text>Ablation study on the effect of pretrained models</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;Pretrained model&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Regression&lt;/th&gt;
            &lt;th colspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-bottom: solid 1px&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;ERS&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;m&lt;italic&gt;R&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mAP&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mRA&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ImageNet&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.095&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.02&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.70&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.247&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Kinetics&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.093&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.77&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.53&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.245&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>76452</offset><text>Pretrained model	Regression	Classification	ERS	 	mR2	mAP	mRA	 	ImageNet	0.095	17.02	62.70	0.247	 	Kinetics	0.093	16.77	62.53	0.245	 	</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>76586</offset><text>Ablation study on the effect of face</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;Input type&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Regression&lt;/th&gt;
            &lt;th colspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-bottom: solid 1px&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;ERS&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;m&lt;italic&gt;R&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mAP&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mRA&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Whole body&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.095&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.02&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.70&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.247&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Face only&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.092&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.21&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.18&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.242&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Faceless body&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.088&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.61&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.241&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>76623</offset><text>Input type	Regression	Classification	ERS	 	mR2	mAP	mRA	 	Whole body	0.095	17.02	62.70	0.247	 	Face only	0.092	16.21	62.18	0.242	 	Faceless body	0.088	16.61	62.30	0.241	 	</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>76794</offset><text>Ensembled results</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Regression&lt;/th&gt;
            &lt;th colspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-bottom: solid 1px&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;top&quot; colspan=&quot;1&quot;&gt;ERS&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;m&lt;italic&gt;R&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mAP&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mRA&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TSN-body&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.095&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.02&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.70&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.247&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TSN-body + LMA&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.101&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.70&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.75&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.249&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TSN-body + TSN-face&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.101&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.31&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.46&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.252&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TSN-body + TSN-face + LMA&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.103&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.14&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.52&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.253&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>76812</offset><text>Model	Regression	Classification	ERS	 	mR2	mAP	mRA	 	TSN-body	0.095	17.02	62.70	0.247	 	TSN-body + LMA	0.101	16.70	62.75	0.249	 	TSN-body + TSN-face	0.101	17.31	63.46	0.252	 	TSN-body + TSN-face + LMA	0.103	17.14	63.52	0.253	 	</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>77039</offset><text>Retrieval results of our deep model. P@K(%) = precision at K, R-P(%)=R-Precision</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;colgroup span=&quot;1&quot;&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
          &lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;
        &lt;/colgroup&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Category&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;P@10&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;P@100&lt;/th&gt;
            &lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;R-P&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Peace&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Affection&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Esteem&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anticipation&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Engagement&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Confidence&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happiness&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;36&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pleasure&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Excitement&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Surprise&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sympathy&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Doubt/confusion&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disconnection&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fatigue&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Embarrassment&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yearning&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disapproval&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Aversion&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Annoyance&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sensitivity&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sadness&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;34&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disquietment&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fear&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pain&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Suffering&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Average&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;
            &lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>77120</offset><text>Category	P@10	P@100	R-P	 	Peace	40	33	28	 	Affection	50	32	26	 	Esteem	30	14	12	 	Anticipation	30	24	20	 	Engagement	50	46	42	 	Confidence	40	33	31	 	Happiness	30	36	31	 	Pleasure	40	25	23	 	Excitement	50	41	31	 	Surprise	20	6	8	 	Sympathy	10	14	12	 	Doubt/confusion	20	33	25	 	Disconnection	20	20	18	 	Fatigue	40	20	17	 	Embarrassment	0	5	5	 	Yearning	0	2	4	 	Disapproval	30	28	22	 	Aversion	10	10	11	 	Annoyance	30	28	23	 	Anger	40	24	20	 	Sensitivity	30	19	19	 	Sadness	50	34	25	 	Disquietment	10	26	25	 	Fear	10	8	8	 	Pain	20	9	12	 	Suffering	10	17	18	 	Average	27	23	20	 	</text></passage></document></collection>
