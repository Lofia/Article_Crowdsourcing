<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220611</date><key>pmc.key</key><document><id>9046317</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3758/s13428-021-01672-9</infon><infon key="article-id_pmc">9046317</infon><infon key="article-id_pmid">34341961</infon><infon key="article-id_publisher-id">1672</infon><infon key="fpage">663</infon><infon key="issue">2</infon><infon key="kwd">Visual production Mental representations Computer vision Online experiments</infon><infon key="license">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.</infon><infon key="lpage">675</infon><infon key="name_0">surname:Bainbridge;given-names:Wilma A.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">54</infon><infon key="year">2022</infon><offset>0</offset><text>A tutorial on capturing mental representations through drawing and crowd-sourced scoring</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>89</offset><text>When we draw, we are depicting a rich mental representation reflecting a memory, percept, schema, imagination, or feeling. In spite of the abundance of data created by drawings, drawings are rarely used as an output measure in the field of psychology, due to concerns about their large variance and their difficulty of quantification. However, recent work leveraging pen-tracking, computer vision, and online crowd-sourcing has revealed new ways to capture and objectively quantify drawings, to answer a wide range of questions across fields of psychology. Here, I present a tutorial on modern methods for drawing experiments, ranging from how to quantify pen-and-paper type studies, up to how to administer a fully closed-loop online experiment. I go through the concrete steps of designing a drawing experiment, recording drawings, and objectively quantifying them through online crowd-sourcing and computer vision methods. Included with this tutorial are code examples at different levels of complexity and tutorials designed to teach basic lessons about web architecture and be useful regardless of skill level. I also discuss key methodological points of consideration, and provide a series of potential jumping points for drawing studies across fields in psychology. I hope this tutorial will arm more researchers with the skills to capture these naturalistic snapshots of a mental image.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1484</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1497</offset><text>A key goal of psychological research is to understand the mind and brain through observations of behavior. These behavioral observations are often limited to low-resolution outputs—such as reaction time and choice—largely because of their ease of quantification and clear links to several cognitive processes. However, there are fundamental questions that cannot be answered by single-value outputs, such as the content of one’s mental representations for an item. Here, I will demonstrate how drawing as a high-dimensional behavioral output can be utilized to reveal new insights about human cognition. Although drawings may seem subjective and highly variable, recent techniques from Big Data and citizen science have made objective quantification of drawings at a large scale accessible to psychologists across the field.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2328</offset><text>Historically, drawings have had relatively limited usage as a behavioral output in psychology. Drawings of simple objects or abstract shapes have been used clinically, to diagnose memory disorders as part of the Wechsler Memory Scale test battery (Wechsler,), to quantify spatial neglect (Agrell &amp; Dehlin,), and to investigate parietal cortex lesions (Makuuchi et al.,). Outside of clinical diagnosis, a high proportion of drawing studies focus on examining the drawings of children as insight into underlying thoughts or feelings, often towards a psychoanalytic or therapeutic aim (e.g., Kosslyn et al.,; Otgaar et al.,; Thomas &amp; Jolley,). However, the potential reach of drawing research goes far beyond diagnostic or therapeutic applications; drawing also holds promise as a basic research method that can reveal general principles about cognition across ages. Psychology research in the 1980s and 1990s occasionally utilized drawing as a measure, for example, to understand the schemas of familiar objects (Rubin &amp; Kontis,), or to quantify memory for scene boundaries (Inraub &amp; Bodamer,). However, as computers became more popular tools for administering experiments and recording behavior (e.g., reaction times), drawing as a behavioral measure became less common. Many researchers grappled with the subjective nature of drawings, and utilized small and simple stimulus sets along with basic drawing measures to limit variability in drawing behavior.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3784</offset><text>Such variability can instead be leveraged to gain rich insight into a wide range of cognitive processes reflective of how people view, prioritize, remember, and interpret both external information and internal representations. One can easily transform drawings into a large set of measures by combining high numbers of participants per image, principles from computer vision, and objective, crowd-sourced quantification. These measures can capture varied information including object detail, spatial accuracy, or errors. In a recent study, drawings of real-world photographs revealed an unprecedented richness and accuracy to visual memory, with participants drawing from memory an average of 150 objects across 12 scenes, in pixel-precise locations, with few false memories (Bainbridge et al.,). This level of performance reached much beyond the prediction of nine items at maximum that would be made by verbal recall memory studies (Murdock,). In fact, drawing as a mnemonic strategy has been shown to outperform verbally based, imagery-based, and semantic elaborative strategies (Wammes et al.,; Wammes et al.,), even for memory of highly abstract concepts (Roberts &amp; Wammes,). Furthermore, learning to draw may boost one’s ability to efficiently perceive and encode visual information (Perdreau &amp; Cavanagh,; Perdreau &amp; Cavanagh,; Vogt &amp; Magnussen,). Computer vision-based metrics such as saliency models and meaning maps (Henderson &amp; Hayes,) are also able to significantly predict the objects that are drawn from memory of a scene (Bainbridge et al.,), revealing a new potential for utilizing computational models to make precise, content-based predictions of memory.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5458</offset><text>We have continued to explore a wide range of important cognitive questions using drawings to reveal the detail within memory. We used scene drawings to reveal evidence against the classic textbook phenomenon of boundary extension—a propensity to extrapolate beyond the borders of a photograph, said to reflect our mind’s propensity to automatically “fill in” information. By investigating drawings made from a diverse range of scenes mirroring the statistics of the real world, we found the opposite effect of boundary contraction was just as likely (Bainbridge &amp; Baker,), and this work has ignited a new debate in the fields of scene perception and memory (Bainbridge &amp; Baker,; Intraub,; Park et al.,). In another study, drawings from participants with aphantasia—a newly identified condition characterized by a lack of visual imagery (Jacobs et al.,; Keogh &amp; Pearson,; Zeman et al.,)—revealed a specific deficit in object memory over spatial memory, suggesting separate systems to support visual imagery (Bainbridge, Pounder, et al.,). This study has motivated follow-up neuroimaging research on differences between perception and imagery (Bainbridge et al., ). We have also revealed how semantic consistency (Bainbridge, Kwok, &amp; Baker,) and categorical competition (Hall et al.,) influence distortions in memory drawings. While these examples largely deal with topics related to perception and memory, these drawing methods can easily be applied to other topics, such as attention, emotion, morality, conceptual thinking, decision-making, and social psychology. Drawings could also be used to characterize mental representations in unique populations, such as children, older adults, and those with conditions that may influence percepts, concepts, or memories, such as autism spectrum disorder, schizophrenia, or Alzheimer’s disease.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7311</offset><text>This tutorial will describe how researchers can implement their own drawing-based experiments, and how they can score these drawings through online crowd-sourcing methods. I provide examples of how these methods can be combined with other methods such as computer vision or machine learning to add a predictive component to this research. This tutorial also includes a publicly available code base and tutorials from which researchers can build their experiments, even with limited programming experience (https://osf.io/tgavx/). In a time when online research is becoming increasingly important and popular, these tutorials have been designed to teach basic principles of web architecture and online research, so that experimenters will become more comfortable thinking about online data collection more broadly, in this framework of designing a drawing experiment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>8178</offset><text>The bipartite structure of a drawing study</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8221</offset><text>Researchers have traditionally shirked away from drawing as a behavioral measure because of its high level of variability: drawings can seem highly subjective, drawings are influenced by strategy and technique, and drawings are highly influenced by the drawing abilities of the participants. However, with enough participants and/or well-selected control conditions, these levels of variability can be easily washed out. In fact, so far all of our drawing studies have recruited from non-artists, with many participants expressing a lack of confidence in their drawing abilities.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8801</offset><text>The main crux of a successful drawing experiment is a bipartite structure: 1) a drawing task with careful control conditions, and 2) a battery of online scoring experiments to quantify the drawings. In Bainbridge et al., alongside having participants draw images from memory, a separate set of participants drew from the image (copying it), and a set of participants drew from a label of the image category (e.g., living room). Drawings copied from an image can serve as an upper-bound—few people will draw every pixel in an image, so what is drawn during perception serves as the maximum level of information one can expect to be drawn from memory. This upper bound also serves as a control for drawing ability; equal variability in drawing skill should be captured during both perception and memory. So when assessing memory drawings, the key question is how they compare to perceptual drawings, not how they compare to the original image. Drawings made from the name of an image category serve as a lower-bound, to demonstrate what information could be accurately drawn by solely remembering a semantic label for the image. For our memory task, we observed that participants recalled high levels of detail beyond this category representation; they did not draw just any living room, they drew the specific image of a living room that they studied (Bainbridge et al.,). This sort of task could also be used to understand people’s developments of schemas or concepts. Thus, carefully selected control tasks can manage the level of variability present in the condition of interest. When designing your own drawing experiments, it is essential to consider: what are your lower-bound and upper-bound measures (or, what are your controls)? Will you compare drawings to the original image, to drawings by another person, or within-participant drawings of another form? For example, in our study of aphantasia (Bainbridge, Pounder, et al.,), we compared aphantasic memory drawings to same-participant perceptual drawings (within-subjects), but we also compared aphantasic memory drawings to control participant memory drawings (between-subjects). Having the same participant draw an image from memory and from perception can give a powerful measure of how their performance shifts from one process to the other. We also frequently compare drawings for a given image across participants, testing whether one condition or one group draws more detail than the other for the same image.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11284</offset><text>After obtaining these drawings, the next key point is that anything that can be observed in the drawings can be objectively quantified online. While classic drawing tasks require interpretation of the drawing by the experimenter or a clinician (e.g., Wechsler,), in this era of online citizen science, scoring of these drawings can be crowd-sourced. This removes any subjectivity with interpreting the drawings, and the ease of crowd-sourcing allows for a large number of creative measures to be quantified. For example, one could measure the presence or location of a specific object, the viewing angle of the drawing, its mood or aesthetic qualities, etc. These online scoring experiments will be discussed in more detail later, with a list of properties that have been quantified from drawings thus far.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12091</offset><text>After solving these potential issues of subjectivity or drawing ability, we are left with an incredible gem of information—a visual mental representation. These drawings reveal how people see the world, with rich visual, semantic, and spatial information. While an individual drawing may be difficult to interpret, combining drawings for a given image, task, or participant creates a compelling picture book of underlying cognitive processes. These drawings can reveal not only memories, but concepts, interpretations, schemas, imaginings, and dreams. In this section, I describe how to collect drawings both in-lab and online, and discuss important considerations for designing these experiments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>12791</offset><text>Drawing as measured in the laboratory</text></passage><passage><infon key="file">13428_2021_1672_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12829</offset><text>A broad flow-chart of the general methods of drawing experiments. First, participants view (or imagine) images in an experiment. The task and participant sample can vary flexibly—the only requirement is that the task require participants to draw. Then, participants create drawings, with one of three different interface options: a pen and paper, a pen and tablet, or mouse on a website. Some of the key pros (green pluses) and cons (red minuses) of each approach are listed here. Finally, these drawings are uploaded to an online crowd-sourcing platform where large numbers of online scorers judge these drawings for a range of fine-grained details</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13481</offset><text>Drawing is an incredibly versatile methodology for in-person research, and adaptable to different experimental needs (Fig. 1). In its simplest form, it only requires paper and a pen, and is easily understood by most people. Because of its simplicity, a drawing task can be administered to a patient group, a classroom of students, and in different social groups. Conversely, one can also design a more complex and well-controlled study for in-lab participation. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13944</offset><text>In some of our recent drawing studies (Bainbridge, Kwok, &amp; Baker,; Hall et al.,), in-lab participants studied images on a computer at a fixed visual angle while an eye-tracker (an EyeLink 1000 Plus) recorded their fixation patterns. They then drew each image from memory on a piece of paper with a rectangular border matching the size and dimensions of the original image. Importantly, they drew using a pen-tracking tablet (Wacom Intuos Pro Paper) so that we could capture pen movement patterns, and assess their link to eye movement patterns. Through pen tracking, we can measure many types of information, such as: time spent on specific details, order of drawing information, action trajectories reflecting unconscious processes in decision-making (Song &amp; Nakayama,), and errors. For example, we observed erasing behavior (reflected in pen movements but not the final drawing) and found that those with aphantasia show less editing than those with typical imagery (Bainbridge et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14934</offset><text>One could envision modifying such a drawing task so it could be run simultaneously with neuroimaging. A temporally resolved method such as electroencephalography (EEG) or magnetoencephalography (MEG) could be utilized to link changes in brain activity to the time course of drawing (e.g., can we decode what is being drawn at a given moment?). Thus far, EEG research has shown alpha band activity patterns during drawing that could suggest improved learning (Belkofer et al.,; van der Meer &amp; van der Weel,). While slower in time scale, functional magnetic resonance imaging (fMRI) could be used to examine questions about inter-subject correlations during recall, or representations of information at a coarser time scale, adopting methods used to analyze verbal recall during fMRI (Chen et al.,). With MRI-compatible touchscreen interfaces (e.g., MRItab: Vinci-Booher et al.,), drawing can be natural and seamless inside an MRI scanner. Indeed, some labs have already identified brain networks engaged during drawing tasks (Gowen &amp; Miall,; Schaer et al.,), and compared representations during object drawing and object recognition (Fan et al.,). Additionally, experiments testing letter drawing during fMRI have revealed visual-motor networks of letter recognition (James &amp; Gauthier,; Vinci-Booher et al.,). Thus, drawing inside a scanner is certainly feasible, and could present a new way in which to decode mental representations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>16369</offset><text>Drawing as measured digitally</text></passage><passage><infon key="file">13428_2021_1672_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16399</offset><text>Example drawings from experimental data of the same living room photograph. Shown are example drawings all taken from separate participants, where they drew the same living room while viewing the image (“Perceptual Drawings”) or while recalling the image (“Memory Drawings”). Drawing quality was high regardless of whether participants were drawing with pen on paper (from Bainbridge et al.,), or with a mouse on a website (from Bainbridge, Pounder, et al.,). It is also readily apparent when participants are not taking the task seriously or missed the stimulus, as seen from poor attempts at drawing the living room from two online participants (“Low-Effort”, from Greenberg &amp; Bainbridge,)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17104</offset><text>Online research is becoming increasingly popular in the psychological sciences, and offers a method to efficiently capture large amounts of data from diverse groups. For example, even though the prevalence of aphantasia is around 1–3%, we were able to recruit 61 online participants through Reddit and Facebook forums (Bainbridge, Pounder, et al.,). The growing popularity of pen-based tablet devices also means that many online participants may have a comfortable way to draw digitally. However, even using an average computer mouse, participants can produce high-quality drawings in a web interface (Fig. 2). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17718</offset><text>In terms of implementation, we conduct our online drawing experiments using in-house code written in HTML and JavaScript (along with jQuery), available on the Open Science Framework page with this tutorial (https://osf.io/tgavx/). Coding without the need of proprietary software makes the code incredibly flexible and adaptable (and can be edited in a standard text editor). For drawing, we adapt an open-source jQuery plugin called wPaint (Websanova,). This plugin works like a standard drawing program, where the participant can use a pen tool in different colors to draw lines, and an eraser to remove those lines. There are also undo, redo, and clear buttons to let the user fix any mistakes. While we thus far have only allowed the pen tool in our studies (, wPaint by default includes a wide range of tools, such as lines, text, ellipses, rectangles, and a fill tool. These tools could be useful to the experimenter, but in some cases the experimenter may want to remove extraneous tools to reduce the participant degrees of freedom. Once the drawing is complete, the image can then be saved as text using base64 encoding. Most common programming languages can flexibly convert between an image format (like JPEG) and base64 (see code). We also use jQuery or JavaScript (a combination of its mouseup(), mousedown(), and mousemove() functions) to track the location and timing of the mouse while drawing. This results in output much like the in-lab pen tablet experiments, where you know what stroke is being drawn by the participant at any time. You can also track the movements they make before undoing or clearing data, which can capture drawing errors, or navigations away from the task.</text></passage><passage><infon key="file">13428_2021_1672_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19416</offset><text>The four online drawing experiment examples available with this tutorial. The code is designed to provide examples at different levels of complexity, starting from a basic drawing interface (Example 1), and building up to a full-fledged experiment with timed trials, mouse tracking, and saving to a private server (Example 4). These examples could also serve as an iterative approach to learning web design, starting with basic HTML and JavaScript, building up to more complex JavaScript functions, and finishing with server-side scripting in PHP</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19963</offset><text>Attached to this tutorial, we include code examples for four drawing experiments with increasing complexity, designed so that the code can be flexibly adapted for the reader’s uses, and also designed to teach some basic principles about web programming and architecture (Fig. 3). Example 1 includes the code for the simplest drawing interface, with HTML forming a basic page structure, and JavaScript used to load in the drawing interface and save the drawing. Building from Example 1, Example 2 adds JavaScript code that tracks and saves the mouse movements of the user. Example 3 provides an example of how multiple drawing interfaces can be integrated into a timed experiment with multiple trials. Finally, Example 4 shows how to integrate this code with a PHP script to save data to a private server. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20771</offset><text>When running an online experiment, there are some specific decisions one must make that are less important for in-person experiments. First, with a plethora of programming and online platform options available, it can be difficult to choose the right online host for any experiment. I discuss recommendations and considerations applicable to online experiments in Supplemental Information S1 (https://osf.io/q2vwz/). Second, online research is often plagued with an increased concern over data quality, because of the emergence of bots and task farms (Chmielewski &amp; Kucker,). Conveniently, it is usually relatively straightforward to identify low effort in drawings, and bots cannot yet perform these tasks. In Supplemental Information S2 (https://osf.io/q2vwz/), I provide targeted advice on avoiding cheating in online drawing tasks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>21607</offset><text>Considerations when designing a task</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21644</offset><text>Task design decisions</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>21666</offset><text>Important considerations when designing a drawing experiment</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Experimental design&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Output measures&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;italic&gt;Specific to in-lab studies:&lt;/italic&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;italic&gt;Specific to in-lab studies:&lt;/italic&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;• What visual angle, resolution, and size will my images be? (Ensure drawing area matches this.)• Will I (the experimenter) watch the drawing process (and how might this influence participants)?&lt;italic&gt;Specific to online studies:&lt;/italic&gt;• Will I restrict the hardware that can be used with the experiment (e.g., phone, tablet, monitor)?• Will I include catch trials or questions to ensure high data quality?&lt;italic&gt;General considerations:&lt;/italic&gt;• How long will images be displayed for, and what fixation behavior will be allowed?• When will participants know they will be drawing? (At the beginning of the study? Right before the drawing portion?)• What instructions/cues will I use for each drawing trial? (Or will I make it a purely free recall task?)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;• Will I record eye-tracking during the study phase?&lt;italic&gt;Specific to online studies:&lt;/italic&gt;• Will I restrict the response devices that can be used (e.g., finger, stylus, mouse, joystick)?&lt;italic&gt;General considerations:&lt;/italic&gt;• Will I record pen-/mouse-tracking during the recall phase? (And what measures do I care about? Time/stroke order? Speed? Pressure? Trajectory?)• Will I allow for color and/or text?• How long will participants be allowed to draw for?• Will I track erasures in some way?• What demographic information do I want to record? (e.g., artistic ability)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21727</offset><text>Experimental design	Output measures	 	Specific to in-lab studies:	Specific to in-lab studies:	 	• What visual angle, resolution, and size will my images be? (Ensure drawing area matches this.)• Will I (the experimenter) watch the drawing process (and how might this influence participants)?Specific to online studies:• Will I restrict the hardware that can be used with the experiment (e.g., phone, tablet, monitor)?• Will I include catch trials or questions to ensure high data quality?General considerations:• How long will images be displayed for, and what fixation behavior will be allowed?• When will participants know they will be drawing? (At the beginning of the study? Right before the drawing portion?)• What instructions/cues will I use for each drawing trial? (Or will I make it a purely free recall task?)	• Will I record eye-tracking during the study phase?Specific to online studies:• Will I restrict the response devices that can be used (e.g., finger, stylus, mouse, joystick)?General considerations:• Will I record pen-/mouse-tracking during the recall phase? (And what measures do I care about? Time/stroke order? Speed? Pressure? Trajectory?)• Will I allow for color and/or text?• How long will participants be allowed to draw for?• Will I track erasures in some way?• What demographic information do I want to record? (e.g., artistic ability)	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23122</offset><text>While the implementation of a drawing task can be flexible, there are various decisions an experimenter must consider (Table 1). In our prior studies, we have never imposed a time limit on drawing time, however a time limit could be useful (e.g., to only capture top-priority items in one’s representations for an image). We often do not tell participants that they will be tested with drawing until they get to the drawing portion of the experiment (Bainbridge et al.,). We take this approach to limit drawing-targeted encoding strategies by participants, such as focusing on remembering information they know they will be better at drawing. We also test participants in a free recall manner, to avoid a risk of participants inserting false information based on cued information (e.g., if you tell them to “draw the living room” they studied, they may insert canonical living room objects—like a couch—even if they don’t specifically remember one). However, future experiments could intentionally manipulate task instructions or cues to see the role of strategies on memory performance. One final consideration is how long a delay the experimenter wants between presentation of the original image and the drawing (if any). Drawings take much longer than other behavioral outputs, with participants generally taking 2 min per drawing when given no time constraints (Bainbridge et al.,). Thus, memory for information might decay during the drawing period itself, and detail could dissipate with later drawings. We actually did not observe evidence for this in our original study; participants drew on average 12 images from memory and there was no evidence that later drawings contained less detail than earlier drawings (Bainbridge et al.,). However, there was a loss of detail when drawing after an intervening 12-minute distractor task in comparison to immediate recall. Thus, timing may be an important consideration, and one could use pen movements to quantify what information is recalled first or last. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>25144</offset><text>Additional information beyond the drawing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25186</offset><text>Another consideration is whether participants should be able to include additional information beyond the line drawing. We have provided participants with colored pencils, and found that individuals with aphantasia use less color than those with typical imagery (Bainbridge, Pounder, et al.,). We also allow participants to write text labels when they are unable to recall details, or are unconfident about their drawing for an object. These text labels can help an experimenter score or interpret an image (Bainbridge, Kwok, &amp; Baker,), and have also revealed that individuals with aphantasia rely on semantic representations to scaffold their memory for an image (Bainbridge, Pounder, et al.,). We also sometimes conclude experiments with a task where participants can indicate what image and objects they were intending to draw, to make scoring more straightforward (Hall et al.,). One could envision other types of information that could be collected in combination with a drawing. For example, experimenters could record participants verbally describing their drawing as they create it. Other sensor measurements such as pen speed or pressure could reflect unconscious cognitive processes, such as confidence or task difficulty (Song &amp; Nakayama,). Outside of the drawing task, I recommend collecting basic demographic questions about artistic experience (i.e., years of artistic training, ratings of one’s own drawing ability, occupation) in order to quantify individual variability in performance. We used these measures to demonstrate that individuals with aphantasia showed memory-specific deficits in their drawings, not explained by differences in general artistic ability (Bainbridge, Pounder, et al.,).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>26902</offset><text>Limitations in drawing tasks</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26931</offset><text>While drawings are information-rich, there are also limitations to drawing as a behavioral measure. First, drawings can be laborious to create, so the experimenter is limited in the number of drawings they can request before the participant is fatigued or out of time (the most we have requested is about 30: Bainbridge et al.,). With online experiments, the amount of dedicated focus the experimenter can expect is probably even shorter. Some individuals may also feel resistant to drawing—embarrassed about their abilities, or unsure where to begin. Finally, drawing ability can still be a barrier to accurately representing one’s mental representations; for example, one may have a clear image of a face but still be unable to draw it. Nonetheless, drawings can often capture more vivid visual details than other methods such as verbal report.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>27782</offset><text>Objective quantification of drawings</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27819</offset><text>Examples of information that can be quantified from drawings</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Image-level metrics&lt;/th&gt;&lt;th&gt;Detail-level metrics&lt;/th&gt;&lt;th&gt;Computational metrics&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Image/scene identity&lt;/td&gt;&lt;td&gt;Object identities&lt;/td&gt;&lt;td&gt;Saliency&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Drawing quality&lt;/td&gt;&lt;td&gt;Feature size/location/orientation&lt;/td&gt;&lt;td&gt;Color/luminance&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Aesthetic quality&lt;/td&gt;&lt;td&gt;Similarity to a schema/exemplar&lt;/td&gt;&lt;td&gt;Symmetry&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Emotional valence&lt;/td&gt;&lt;td&gt;Texture and material of objects&lt;/td&gt;&lt;td&gt;Spatial frequency, edges, GIST&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Viewing perspective&lt;/td&gt;&lt;td&gt;Caricaturization of features&lt;/td&gt;&lt;td&gt;Mid-level measures: SIFT, HOG&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Realism/imaginativeness&lt;/td&gt;&lt;td&gt;Usage of color&lt;/td&gt;&lt;td&gt;Deep learning: classification &amp;amp; heatmaps&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Similarity to a schema&lt;/td&gt;&lt;td&gt;Usage of text labeling&lt;/td&gt;&lt;td&gt;Meaning maps&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Accuracy&lt;/td&gt;&lt;td&gt;Drawing order &amp;amp; time&lt;/td&gt;&lt;td&gt;Motion maps&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Erasures and errors&lt;/td&gt;&lt;td&gt;Object-object relationships&lt;/td&gt;&lt;td&gt;Semantic segmentation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Interpretability&lt;/td&gt;&lt;td&gt;Insertions of false details&lt;/td&gt;&lt;td&gt;Generative Adversarial Network inputs&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27880</offset><text>Image-level metrics	Detail-level metrics	Computational metrics	 	Image/scene identity	Object identities	Saliency	 	Drawing quality	Feature size/location/orientation	Color/luminance	 	Aesthetic quality	Similarity to a schema/exemplar	Symmetry	 	Emotional valence	Texture and material of objects	Spatial frequency, edges, GIST	 	Viewing perspective	Caricaturization of features	Mid-level measures: SIFT, HOG	 	Realism/imaginativeness	Usage of color	Deep learning: classification &amp; heatmaps	 	Similarity to a schema	Usage of text labeling	Meaning maps	 	Accuracy	Drawing order &amp; time	Motion maps	 	Erasures and errors	Object-object relationships	Semantic segmentation	 	Interpretability	Insertions of false details	Generative Adversarial Network inputs	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28633</offset><text>After collecting drawing data, the next important step is to quantify these drawings. Unlike classic drawing studies where the experimenter must use their discretion to score each drawing, a key innovation with this method is that the experimenter must outsource quantification of these drawings. In this way, we can leverage the rich ability of people to extract information from ambiguous input (a drawing), but utilize this ability at a large scale, unbiased by the experimental questions, to create objective scores for each drawing. Table 2 lists a survey of the types of measures that can be captured from drawings. Using human ratings, one can quantify a range of information from image-based metrics, to detail-level metrics analyzing specific portions of an image. Computationally derived measures can also be directly compared with these human metrics. This tutorial includes a base of code to create, test, and analyze online scoring experiments for drawings, as well as compare their results to computationally derived measures. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>29675</offset><text>Image-level metrics</text></passage><passage><infon key="file">13428_2021_1672_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29695</offset><text>Example online scoring experiments for the drawings. One can use online scoring to collect image-level metrics, such as how well a drawing matches an image (“Drawing Judgment”) or what differences may exist between the drawing and image (“False Objects”). Online scoring can also give fine-grained detail-level metrics, with online participants providing a response for every object, such as whether each object exists in a drawing (“Object Selection”) or where it is located and at what size (“Object Location”). These four specific experiments have been validated across studies (Bainbridge et al.,; Bainbridge, Kwok, &amp; Baker,; Bainbridge, Pounder, et al.,; Hall et al.,), and are provided in the code base of this tutorial</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30437</offset><text>The most straightforward of metrics to crowd-source are those where online scorers must make a singular judgment of a drawing (Fig. 4). Viewing a drawing, a scorer could be asked to make a range of responses along a Likert scale, for example scoring the drawing quality, aesthetic quality, emotional valence, realism, interpretability, or other similar measures. They can also be asked to compare the drawing to another image or set of images, for example judging: How similar is this drawing to this image?; Which viewing perspective does this drawing take (e.g., front, ¾, side, birds-eye)?; How similar is this drawing to the average schema across these photographs? Online scorers can also be asked to make short responses related to the drawing, such as: What is this a drawing of?; Name the objects in this drawing; How would you describe this drawing to someone? Finally, online scorers can also make judgments based on a video of the mouse movements of a drawing, labeling features such as the existence of erasures/editing, the order of pen strokes, or the speed of drawing. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>31524</offset><text>Detail-level metrics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31545</offset><text>Perhaps most interesting and innovative of the described approach is to rely heavily on detailed scoring of images. A first principle is that many of our experiments are automatically derived. For example, one common experiment is having online scorers judge which objects from a photograph exist in a drawing of that photograph (Fig. 4). To create this experiment, we first identify the outlines of all of the objects in the photograph. This can be conducted by someone in-lab or through crowd-sourcing using LabelMe (http://labelme.csail.mit.edu/Release3.0/, Russell et al.,), an online interface where you define the polygons that form the outlines of each object in an image. LabelMe saves these annotations as an XML file defining the coordinates of each object polygon’s points, which serves as a file that can be flexibly read into many programming languages. Similar outlines could also be derived using a computer vision or deep learning algorithm such as VGG-16 for object classification (Simonyan &amp; Zisserman,). With these outlines, we know what objects are in an image, at what size, and what location. We can create (and have provided) code that can automatically create one image for every object, in which its outline is highlighted in red in the context of the scene. We then automatically create thousands of online scoring trials where a scorer sees a drawing and an object highlighted in red and has to respond whether that object is in the drawing or not. These ratings are collected across a number of scorers (we typically use five per object, given the large number of objects we collect judgments on), and then the majority response wins, to determine what objects exist in a drawing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33256</offset><text>A second principle is that we run multiple nested experiments, where the trials of one experiment are derived from the trials of the previous experiment. For example, another possible experiment is to derive the locations of every object in a drawing (Fig. 4). To do this, we automatically create online scoring trials only for the objects that are judged to exist in a drawing in the task described previously. For those specific trials, we show online scorers the photograph with an object highlighted in red, and the matching drawing with a superimposed red ellipse. The job of the participant is to move and resize the ellipse to highlight that corresponding object in the drawing. We collect five ellipses per object, and take the median center and radii as the final ellipse for the object. Across all of our studies, these measures have indicated a high spatial accuracy, where objects tend to be drawn at the same sizes and locations as the original objects in the photographs (Bainbridge et al.,; Bainbridge et al.,), suggesting this ellipse measure is successful. Future versions of this experiment could use a LabelMe-like interface to instead measure a complex contour for a drawn object.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34457</offset><text>After these object identification and object location experiments are complete, you now have a direct correspondence of objects in the drawing to objects in the photograph. This allows the experimenter to ask a wide range of questions. You can examine how usage of detail or color varies within and across individual objects. You can examine what task manipulations modulate the drawing-photograph correspondences (for example: does a longer memory delay reduce the spatial correspondence between the two?). You can investigate when an object was drawn and for how long, just as you can measure when and how long an object was fixated on in a photograph. You can ask online scorers to judge individually drawn objects in ways they can judge the entire image: scoring an object for aesthetic or emotional value, or describing it or attributing it a label. You can apply similar methods not just to singular objects, but also to clusters of objects, object parts, or features (e.g., the features in a face). Finally, you can look at false memories – what objects are drawn that do not exist in an original image, and where are they?</text></passage><passage><infon key="file">13428_2021_1672_Fig5_HTML.jpg</infon><infon key="id">Fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35590</offset><text>Different quantifications of the same image. Here we illustrate how different human- and computationally derived metrics can be compared in the pixel or object space between images and drawings. From the Object Selection online experiment, one can create a heatmap indicating the proportion of participants who drew each object (“Object Memory”). One can also use the Object Location online experiment to derive the locations of drawn objects (“Memory Locations”, showing the median ellipse for the top four drawn objects). These object values and locations can then be compared to the values and locations of participants’ fixation behavior on the image (“Eye Fixations”), or computer-derived image metrics like Graph-Based Visual Saliency (“GBVS”) or DeepGaze II neural network predictions (“DeepGaze II”). The original photograph and example data come from Bainbridge, Kwok, &amp; Baker, </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36500</offset><text>This image-object-to-drawn-object correspondence also means that drawings and images can be transformed into representations defined by the object space. It may be difficult to compare drawings and photographs at the pixel level because drawings tend to be sparser across their pixels (although see the next section). Instead, drawings and photographs can be compared at the object level, where a given image or drawing is represented by a vector of objects. For example, you could compare a vector of fixation durations on objects in a photograph and a vector of drawing durations on objects in a drawing. As another example, you could compare ratings of aesthetic values of objects in a photograph to ratings of drawing quality of objects in a drawing. The same methods could also be used for other types of discrete elements, for example to compare the locations of facial features in a photograph to the locations of facial features in a drawing. Conceptualizing images and drawings in an object space also allows one to perform computations across drawings – for example, looking at the average aesthetic rating across individuals for a given drawn object, or collecting a proportion of people who drew a given object. This allows for the creation of object-based heatmaps that can visualize these values using the stable locations of the objects in the original image, even when showing data from the drawings (Fig. 5). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>37929</offset><text>Computationally derived metrics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>37961</offset><text>Many measures that can be computationally derived from a photograph can be computationally derived from a drawing as well. These algorithms can vary in complexity by pixel-level computations, more complex image-based metrics, and deep-learning-derived metrics.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38222</offset><text>First, many measures can be derived through directly examining the pixels of the drawing. By averaging the pixel values at each color channel, one can get an idea of how much information is present for each basic color (R, G, B), for example, how blue the drawing is. By instead measuring the variance, one captures a measure of the contrast in the image. If drawings are made in an online interface, the experimenter can limit the specific colors that can be used in the interface, and the number of pixels matching each color can be quantified to provide a fine-grained measurement of the distribution of colors. The density or amount of “ink” in the drawing can be quantified as the total number of non-white pixels. From the pixels of an image, one can also measure symmetry and spatial frequency. To quantify symmetry, one can divide the image into two halves and subtract one half with the mirror-flipped version of the other. The more non-zero pixels that remain, the more asymmetric the image is. Finally, spatial frequency captures the edge information or spectral energy present in the image, and can be calculated by looking at the Fourier transform of the drawing. The Natural Image Statistical Toolbox is an open-source MATLAB toolbox that can quantify these measures of color, contrast, symmetry, and spatial frequency (Bainbridge &amp; Oliva,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>39582</offset><text>Many tools developed in the realm of computer vision already exist for quantifying higher-level information about images, such as HOG, SIFT, and GIST. HOG (histogram of oriented gradients) is an image feature often used for object classification that analyzes edge orientations in local image regions (Dalal &amp; Triggs,). SIFT (scale-invariant feature transform) is a feature often used for image alignment that compares local regions of an image to a reference set (Lowe,). Finally, GIST captures the gist and spatial envelope presented by the spectral energy within an image (Oliva &amp; Torralba,). These features can be used to parse the content of an image, for example, they can be used to automatically classify the genre of a painting (Agarwal et al.,). However, they have not been used in the field of psychology as metrics to quantify drawings, although there are toolboxes available for quantifying images (Khosla,). Visual saliency is also a common metric that can be quantified using tools such as Graph-Based Visual Saliency (GBVS, Harel et al.,). GBVS creates a heatmap of the salient portions of an image (in other words, the parts of an image that are most visually dissimilar from other parts), and can predict eye movement patterns.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>40829</offset><text>More recently, deep learning neural networks (DNNs) are becoming increasingly common ways to quantify an image. They serve as compelling models of the early human visual system (Cichy et al.,; Yamins et al.,), and have made strides in predicting object perception through networks like AlexNet (Krizhevsky et al.,) and VGG-16 (Simonyan &amp; Zisserman,), scene perception through networks like Places-CNN (Zhou et al.,), fixations through networks like DeepGaze II (Kümmerer et al.,), and memory through networks like MemNet (Khosla et al.,) and ResMem (Needell &amp; Bainbridge,). Most of these pre-trained networks are publicly available, and can be used to make inferences about novel images. For example, one could measure: To what degree can the objects or scene of a drawing be classified? How does memorability of a drawing compare to memorability of the original image? Do predicted fixations on an image relate to what objects people will draw? Some DNNs also create heatmaps across the image, such as MemNet, which creates a heatmap of what image information most contributes to its predicted memorability score. There has also been burgeoning work in relating DNN-based representations of drawings to those of the corresponding image (Fan et al.,) and using DNNs to score drawing recognizability (Long et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42145</offset><text>There are two main ways in which these computational methods can be used to quantify drawings. Most directly, they can be used to quantify both the drawing and its original photograph, and then those two can be directly compared. For example, a singular value of color contrast could be derived for each photograph and each drawing and then correlated. For another example, a GBVS heatmap could be created for a photograph and a GBVS heatmap could be created for a drawing, and these saliency measures could be correlated pixel-by-pixel. However, drawings will intrinsically differ on many visual features in comparison to photographs, and often not for particularly interesting reasons. For example, drawings likely will include less color across all the pixels than a photograph, since it is effortful and inefficient to color each pixel. There also may not be as many fine details within a drawing and lines may not be as straight, thus influencing spatial frequency and symmetry measures. Instead, it may make more sense to transform these measures to the object space. For example, one could look at the color contrast of each object, normalized by the mean color contrast across objects, and compare this measure between the photograph and a drawing. Through this normalization, you would remove the differential influences of color usage in drawing versus an image. For heatmap measures like GBVS, you can compute the average value across the pixels contained within an object (e.g., the average saliency of the chair), and compare it to another measure (e.g., the proportion of people who drew that chair). Putting these computational metrics into the object space also makes it easier to combine data across drawings of the same image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>43890</offset><text>Applications of this method to other questions and fields</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>43948</offset><text>While this drawing method has thus far been applied to questions related to memory and visual imagery, this method has large-reaching implications across questions in psychology. Here I will provide some launching points for ideas in alternate subdomains of psychology and neuroscience, although I envision the number of possible questions is infinite.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44301</offset><text>This method can serve as a tool for understanding schemas or concepts. Drawing has been used to investigate the emergence of schemas in children (Freeman &amp; Janikoun,; Long et al.,), the influence of culture on schemas (Axia et al.,), and to question whether children even can use drawings to represent schemas (Kosslyn et al.,). Drawings have also revealed how experts differentially organize information through chunking (e.g., investigating electronic technicians’ memories for circuit diagrams: Egan &amp; Schwartz,). Online crowd-sourced scoring and digitized drawings can allow for even more fine-grained views into category schemas and the role of expertise across the lifespan.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44984</offset><text>Drawings can be used to capture current cognitive states and their fluctuations within an individual. Just as emotion and stress can color our memories or perception, they likely color our drawings as well. In fact, drawing is occasionally used as a method for children to indicate emotional states (Thomas &amp; Jolley,) and moments of stress (Rollins,). Regardless of age, the content and nature of one’s drawings may shift with changes in emotional state or stress, and these drawings could be useful as diagnostic or communicative tools. For example, what errors may be introduced in a drawing (e.g., false additional items, missing items) during times of stress? How do anxiety or depression influence the features of a drawing? And, can emotional state be predicted from drawings, even when made from images without an inherent emotion?</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>45825</offset><text>The act of drawing itself may also have a direct influence on cognitive abilities, informing questions on the plasticity of the brain. Artists, who are highly trained at drawing, show different fixation patterns from non-artists when viewing an image, and recall more details (Vogt &amp; Magnussen,). Even for non-artists, drawing when learning information shows larger benefits over verbally rehearsing information in adults (Perdreau &amp; Cavanagh,; Wammes et al.,) and children (Gross &amp; Hayne,). However, drawing can also inflate the existence of falsely recalled information (Bruck et al.,; Otgaar et al.,). Future studies could examine how artistic training influences the accuracy of drawings, and translates to performance in other non-drawing tasks. Drawing also acts as an interaction of multiple processes: perception, attention, and motor control (Cohen &amp; Bennett,; Makuuchi et al.,). The highly quantified drawings proposed here can investigate how subtle manipulations in perception, integration, attention, and motor ability influence the content of drawings.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>46892</offset><text>Drawings also can reveal insight into higher-level questions about social interactions, decision making, and morality. Children draw more accurately when their drawings are used as a form of social communication (Light &amp; McEwen,). Children also draw family members differently based on their attachment style (Goldner &amp; Scharf,). Thus, there are clear social influences on drawing even from an early age. Along a similar vein, what differences might emerge when drawing people from different social categories? In a study by Uddenberg and Scholl, when white participants reconstructed a face from memory utilizing an interface that generates virtual faces along a continuum, they tended to reproduce a face as more white that it was. Thus, reconstruction tasks like drawing could unveil subtle biases related to our representations and perceptions of social categories like race and gender. Drawings could also be used to indicate perceptions of different decision-making options or moral choices. For example, we often internalize an intrinsic value for objects, such as tastier foods having high value (Bakkour et al.,), and such value differences may appear in drawings of these objects.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>48083</offset><text>Finally, while drawings of specific shapes are sometimes included in test batteries with patients (the Ray-Osterrich figure to test memory: Shin et al.,; the clock-drawing test for spatial neglect: Agrell &amp; Dehlin,), drawings of complex, naturalistic images promise a deeper look into variations in cognitive experience. I have already described work showing separate systems for object and spatial imagery in individuals with aphantasia (Bainbridge et al.,). Aphasic patients (individuals with diminished language ability) also show marked deficits in drawing objects from memory, correlated with diminished language abilities (Gainotti et al.,). Performance on a clock drawing test can also differentiate healthy elderly adults from those with Alzheimer’s disease (Cahn et al.,). Thus, in many ways, deficits may be reflected in one’s drawings, as drawing requires an interwoven combination of many cognitive abilities.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>49009</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49021</offset><text>While drawing has often been regarded as a noisy task, delegated mostly to test batteries in the clinical domain and historic studies before the advent of computers, there is much untapped potential in drawing as a rich, informative behavioral measure. Leveraging modern methods of large-scale online experiments, crowd-sourcing, and high quantification through computer vision and pen-tracking, new drawing experiments have already revealed exciting insights about memory, perception, and attention, and promise to answer plentiful questions across psychology. This tutorial serves as a basis to equip any researcher—regardless of programming knowledge—with the tools to conduct high-quality drawing experiments, and be aware of the big picture questions one must keep in mind when running these studies. With this, we will be better able to draw out the mental representations in people.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>49915</offset><text>Publisher’s note</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>49934</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>50053</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50064</offset><text>Agarwal, S., Karnick, H., Pant, N., &amp; Patel, U. (2015). Genre and style based painting classification. 2015 IEEE Winter Conference on Applications of Computer Vision, 588–594.</text></passage><passage><infon key="fpage">399</infon><infon key="lpage">404</infon><infon key="name_0">surname:Agrell;given-names:B</infon><infon key="name_1">surname:Dehlin;given-names:O</infon><infon key="pub-id_doi">10.1093/ageing/27.3.399</infon><infon key="section_type">REF</infon><infon key="source">Age and Ageing</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">1998</infon><offset>50242</offset><text>The clock-drawing test</text></passage><passage><infon key="fpage">423</infon><infon key="lpage">437</infon><infon key="name_0">surname:Axia;given-names:G</infon><infon key="name_1">surname:Bremner;given-names:JG</infon><infon key="name_2">surname:Deluca;given-names:P</infon><infon key="name_3">surname:Andreasen;given-names:G</infon><infon key="pub-id_doi">10.1111/j.2044-835X.1998.tb00762.x</infon><infon key="section_type">REF</infon><infon key="source">British Journal of Developmental Psychology</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">1998</infon><offset>50265</offset><text>Children drawing Europe: The effects of nationality, age and teaching</text></passage><passage><infon key="fpage">537</infon><infon key="lpage">543</infon><infon key="name_0">surname:Bainbridge;given-names:WA</infon><infon key="name_1">surname:Baker;given-names:CI</infon><infon key="pub-id_doi">10.1016/j.cub.2019.12.004</infon><infon key="pub-id_pmid">31983637</infon><infon key="section_type">REF</infon><infon key="source">Current Biology</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2020</infon><offset>50335</offset><text>Boundaries extend and contract in scene memory depending on image properties</text></passage><passage><infon key="fpage">R1465</infon><infon key="lpage">R1466</infon><infon key="name_0">surname:Bainbridge;given-names:WA</infon><infon key="name_1">surname:Baker;given-names:CI</infon><infon key="pub-id_doi">10.1016/j.cub.2020.10.032</infon><infon key="pub-id_pmid">33352123</infon><infon key="section_type">REF</infon><infon key="source">Current Biology</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2020</infon><offset>50412</offset><text>Reply to Intraub</text></passage><passage><infon key="fpage">846</infon><infon key="lpage">851</infon><infon key="name_0">surname:Bainbridge;given-names:WA</infon><infon key="name_1">surname:Oliva;given-names:A</infon><infon key="pub-id_doi">10.1016/j.dib.2015.10.030</infon><infon key="pub-id_pmid">26693521</infon><infon key="section_type">REF</infon><infon key="source">Data in Brief</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2015</infon><offset>50429</offset><text>A toolbox and sample object perception data for equalization of natural images</text></passage><passage><infon key="fpage">5</infon><infon key="name_0">surname:Bainbridge;given-names:WA</infon><infon key="name_1">surname:Hall;given-names:EH</infon><infon key="name_2">surname:Baker;given-names:CI</infon><infon key="pub-id_doi">10.1038/s41467-018-07830-6</infon><infon key="pub-id_pmid">30602785</infon><infon key="section_type">REF</infon><infon key="source">Nature Communications</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2019</infon><offset>50508</offset><text>Drawings of real-world scenes during free recall reveal detailed object and spatial information in memory</text></passage><passage><infon key="fpage">159</infon><infon key="lpage">172</infon><infon key="name_0">surname:Bainbridge;given-names:WA</infon><infon key="name_1">surname:Pounder;given-names:Z</infon><infon key="name_2">surname:Eardley;given-names:AF</infon><infon key="name_3">surname:Baker;given-names:CI</infon><infon key="pub-id_doi">10.1016/j.cortex.2020.11.014</infon><infon key="pub-id_pmid">33383478</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">135</infon><infon key="year">2021</infon><offset>50614</offset><text>Quantifying Aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50730</offset><text>Bainbridge, W.A., Kwok, W.Y., &amp; Baker, C.I. (2021b). Disrupted object-scene semantics boost scene recall but diminish object recall in drawings from memory. Memory &amp; Cognition, https://doi.org/10.3758/s13421-021-01180-3.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50951</offset><text>Bainbridge, W.A., Hall, E.H., &amp; Baker, C.I. (2021c). Distinct representational structure and localization for visual encoding and recall during visual imagery. Cerebral Cortex,31, 1898-1913.</text></passage><passage><infon key="fpage">e46080</infon><infon key="name_0">surname:Bakkour;given-names:A</infon><infon key="name_1">surname:Palombo;given-names:DJ</infon><infon key="name_2">surname:Zylberberg;given-names:A</infon><infon key="name_3">surname:Kang;given-names:YHR</infon><infon key="name_4">surname:Reid;given-names:A</infon><infon key="name_5">surname:Verfaellie;given-names:M</infon><infon key="name_6">surname:Shadlen;given-names:MN</infon><infon key="name_7">surname:Shohamy;given-names:D</infon><infon key="pub-id_doi">10.7554/eLife.46080</infon><infon key="pub-id_pmid">31268419</infon><infon key="section_type">REF</infon><infon key="source">eLife</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2019</infon><offset>51142</offset><text>The hippocampus supports deliberation during value-based decisions</text></passage><passage><infon key="fpage">61</infon><infon key="lpage">68</infon><infon key="name_0">surname:Belkofer;given-names:CM</infon><infon key="name_1">surname:Van Hecke;given-names:AV</infon><infon key="name_2">surname:Konopka;given-names:LM</infon><infon key="pub-id_doi">10.1080/07421656.2014.903821</infon><infon key="section_type">REF</infon><infon key="source">Art Therapy</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2014</infon><offset>51209</offset><text>Effects of drawing on alpha activity: A quantitative EEG study with implications for art therapy</text></passage><passage><infon key="fpage">169</infon><infon key="lpage">196</infon><infon key="name_0">surname:Bruck;given-names:M</infon><infon key="name_1">surname:Melnyk;given-names:L</infon><infon key="name_2">surname:Ceci;given-names:SJ</infon><infon key="pub-id_doi">10.1006/jecp.1999.2560</infon><infon key="pub-id_pmid">11023656</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Child Psychology</infon><infon key="type">ref</infon><infon key="volume">77</infon><infon key="year">2000</infon><offset>51306</offset><text>Draw it again Sam: The effect of drawing on children’s suggestibility and source monitoring ability</text></passage><passage><infon key="fpage">529</infon><infon key="lpage">539</infon><infon key="name_0">surname:Cahn;given-names:DA</infon><infon key="name_1">surname:Salmon;given-names:DP</infon><infon key="name_2">surname:Monsch;given-names:AU</infon><infon key="name_3">surname:Butters;given-names:N</infon><infon key="name_4">surname:Wiederholt;given-names:WC</infon><infon key="name_5">surname:Corey-Bloom;given-names:J</infon><infon key="name_6">surname:Barrett-Connor;given-names:E</infon><infon key="pub-id_doi">10.1093/arclin/11.6.529</infon><infon key="pub-id_pmid">14588458</infon><infon key="section_type">REF</infon><infon key="source">Archives of Clinical Neuropsychology</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1996</infon><offset>51408</offset><text>Screening for dementia of the Alzheimer type in the community: The utility of the clock drawing test</text></passage><passage><infon key="fpage">115</infon><infon key="lpage">125</infon><infon key="name_0">surname:Chen;given-names:J</infon><infon key="name_1">surname:Leong;given-names:YC</infon><infon key="name_2">surname:Honey;given-names:CJ</infon><infon key="name_3">surname:Yong;given-names:CH</infon><infon key="name_4">surname:Norman;given-names:KA</infon><infon key="name_5">surname:Hasson;given-names:U</infon><infon key="pub-id_doi">10.1038/nn.4450</infon><infon key="pub-id_pmid">27918531</infon><infon key="section_type">REF</infon><infon key="source">Nature Neuroscience</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2017</infon><offset>51509</offset><text>Shared memories reveal shared structure in neural activity across individuals</text></passage><passage><infon key="fpage">464</infon><infon key="lpage">473</infon><infon key="name_0">surname:Chmielewski;given-names:M</infon><infon key="name_1">surname:Kucker;given-names:SC</infon><infon key="pub-id_doi">10.1177/1948550619875149</infon><infon key="section_type">REF</infon><infon key="source">Social Psychological and Personality Science</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2019</infon><offset>51587</offset><text>An MTurk Crisis? Shifts in data quality and the impact on study results</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">13</infon><infon key="name_0">surname:Cichy;given-names:RM</infon><infon key="name_1">surname:Khosla;given-names:A</infon><infon key="name_2">surname:Pantazis;given-names:D</infon><infon key="name_3">surname:Torralba;given-names:A</infon><infon key="name_4">surname:Oliva;given-names:A</infon><infon key="pub-id_doi">10.1038/srep27755</infon><infon key="pub-id_pmid">28442746</infon><infon key="section_type">REF</infon><infon key="source">Scientific Reports</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>51659</offset><text>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</text></passage><passage><infon key="fpage">609</infon><infon key="lpage">621</infon><infon key="name_0">surname:Cohen;given-names:DJ</infon><infon key="name_1">surname:Bennett;given-names:S</infon><infon key="pub-id_pmid">9180037</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: Human Perception and Performance</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">1997</infon><offset>51802</offset><text>Why can’t most people draw what they see?</text></passage><passage><infon key="fpage">886</infon><infon key="lpage">893</infon><infon key="name_0">surname:Dalal;given-names:N</infon><infon key="name_1">surname:Triggs;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2005</infon><offset>51846</offset><text>Histograms of oriented gradients for human detection</text></passage><passage><infon key="fpage">149</infon><infon key="lpage">158</infon><infon key="name_0">surname:Egan;given-names:DE</infon><infon key="name_1">surname:Schwartz;given-names:BJ</infon><infon key="pub-id_doi">10.3758/BF03197595</infon><infon key="pub-id_pmid">88658</infon><infon key="section_type">REF</infon><infon key="source">Memory &amp; Cognition</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">1979</infon><offset>51899</offset><text>Chunking in recall of symbolic drawings</text></passage><passage><infon key="fpage">2670</infon><infon key="lpage">2698</infon><infon key="name_0">surname:Fan;given-names:JE</infon><infon key="name_1">surname:Yamins;given-names:DLK</infon><infon key="name_2">surname:Turk-Browne;given-names:NB</infon><infon key="pub-id_doi">10.1111/cogs.12676</infon><infon key="pub-id_pmid">30125986</infon><infon key="section_type">REF</infon><infon key="source">Cognitive Science</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2018</infon><offset>51939</offset><text>Common object representations for visual production and recognition</text></passage><passage><infon key="fpage">1710</infon><infon key="lpage">1721</infon><infon key="name_0">surname:Fan;given-names:JE</infon><infon key="name_1">surname:Wammes;given-names:JD</infon><infon key="name_2">surname:Gunn;given-names:JB</infon><infon key="name_3">surname:Yamins;given-names:DLK</infon><infon key="name_4">surname:Norman;given-names:KA</infon><infon key="name_5">surname:Turk-Browne;given-names:NB</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.1843-19.2019</infon><infon key="pub-id_pmid">31871278</infon><infon key="section_type">REF</infon><infon key="source">The Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2020</infon><offset>52007</offset><text>Relating visual production and recognition of objects in human visual cortex</text></passage><passage><infon key="fpage">1116</infon><infon key="lpage">1121</infon><infon key="name_0">surname:Freeman;given-names:NH</infon><infon key="name_1">surname:Janikoun;given-names:R</infon><infon key="pub-id_doi">10.2307/1127668</infon><infon key="pub-id_pmid">5056604</infon><infon key="section_type">REF</infon><infon key="source">Child Development</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">1972</infon><offset>52084</offset><text>Intellectual realism in children’s drawings of a familiar object with distinctive features</text></passage><passage><infon key="fpage">613</infon><infon key="lpage">622</infon><infon key="name_0">surname:Gainotti;given-names:G</infon><infon key="name_1">surname:Silveri;given-names:MC</infon><infon key="name_2">surname:Villa;given-names:G</infon><infon key="name_3">surname:Caltagirone;given-names:C</infon><infon key="pub-id_doi">10.1093/brain/106.3.613</infon><infon key="pub-id_pmid">6640272</infon><infon key="section_type">REF</infon><infon key="source">Brain</infon><infon key="type">ref</infon><infon key="volume">106</infon><infon key="year">1983</infon><offset>52177</offset><text>Drawing objects from memory in aphasia</text></passage><passage><infon key="fpage">11</infon><infon key="lpage">18</infon><infon key="name_0">surname:Goldner;given-names:L</infon><infon key="name_1">surname:Scharf;given-names:M</infon><infon key="pub-id_doi">10.1080/07421656.2011.557350</infon><infon key="section_type">REF</infon><infon key="source">Art Therapy</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2011</infon><offset>52216</offset><text>Children’s family drawings: A study of attachment, personality, and adjustment</text></passage><passage><infon key="fpage">396</infon><infon key="lpage">410</infon><infon key="name_0">surname:Gowen;given-names:E</infon><infon key="name_1">surname:Miall;given-names:RC</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2007.03.005</infon><infon key="pub-id_pmid">17448689</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2007</infon><offset>52297</offset><text>Differentiation between external and internal cuing: An fMRI study comparing tracing with drawing</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52395</offset><text>Greenberg, R., &amp; Bainbridge, W.A. (2021). Drawings reveal accurate visual information in memory after just 100ms of exposure. Vision Sciences Society Meeting, St. Pete Beach, FL.</text></passage><passage><infon key="fpage">163</infon><infon key="lpage">179</infon><infon key="name_0">surname:Gross;given-names:J</infon><infon key="name_1">surname:Hayne;given-names:H</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: Applied</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">1998</infon><offset>52574</offset><text>Drawing facilitates children’s verbal reports of emotionally laden events</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52650</offset><text>Hall, E.H., Bainbridge, W.A., &amp; Baker, C.I. (2021). Highly similar and competing visual scenes lead to diminished object but not spatial detail in memory drawings. PsyArXiv. 10.31234/osf.io/2az8x.</text></passage><passage><infon key="fpage">545</infon><infon key="lpage">552</infon><infon key="name_0">surname:Harel;given-names:J</infon><infon key="name_1">surname:Koch;given-names:C</infon><infon key="name_2">surname:Perona;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Advanced Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2007</infon><offset>52847</offset><text>Graph-based visual saliency</text></passage><passage><infon key="fpage">743</infon><infon key="lpage">747</infon><infon key="name_0">surname:Henderson;given-names:JM</infon><infon key="name_1">surname:Hayes;given-names:TR</infon><infon key="pub-id_doi">10.1038/s41562-017-0208-0</infon><infon key="pub-id_pmid">31024101</infon><infon key="section_type">REF</infon><infon key="source">Nature Human Behaviour</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2017</infon><offset>52875</offset><text>Meaning-based guidance of attention in scenes as revealed by meaning maps</text></passage><passage><infon key="fpage">1387</infon><infon key="lpage">1397</infon><infon key="name_0">surname:Inraub;given-names:H</infon><infon key="name_1">surname:Bodamer;given-names:JL</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: Learning, Memory, and Cognition</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">1993</infon><offset>52949</offset><text>Boundary extension: Fundamental aspect of pictorial representation or encoding artifact?</text></passage><passage><infon key="fpage">R1463</infon><infon key="lpage">R1464</infon><infon key="name_0">surname:Intraub;given-names:H</infon><infon key="pub-id_doi">10.1016/j.cub.2020.10.031</infon><infon key="pub-id_pmid">33352122</infon><infon key="section_type">REF</infon><infon key="source">Current Biology</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2020</infon><offset>53038</offset><text>Searching for boundary extension</text></passage><passage><infon key="fpage">61</infon><infon key="lpage">73</infon><infon key="name_0">surname:Jacobs;given-names:C</infon><infon key="name_1">surname:Schwarzkopf;given-names:DS</infon><infon key="name_2">surname:Silvanto;given-names:J</infon><infon key="pub-id_doi">10.1016/j.cortex.2017.10.014</infon><infon key="pub-id_pmid">29150139</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2018</infon><offset>53071</offset><text>Visual working memory performance in aphantasia</text></passage><passage><infon key="fpage">2937</infon><infon key="lpage">2949</infon><infon key="name_0">surname:James;given-names:KH</infon><infon key="name_1">surname:Gauthier;given-names:I</infon><infon key="pub-id_doi">10.1016/j.neuropsychologia.2006.06.026</infon><infon key="pub-id_pmid">16920164</infon><infon key="section_type">REF</infon><infon key="source">Neuropsychologia</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2006</infon><offset>53119</offset><text>Letter processing automatically recruits a sensory-motor brain network</text></passage><passage><infon key="fpage">53</infon><infon key="lpage">60</infon><infon key="name_0">surname:Keogh;given-names:R</infon><infon key="name_1">surname:Pearson;given-names:J</infon><infon key="pub-id_doi">10.1016/j.cortex.2017.10.012</infon><infon key="pub-id_pmid">29175093</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2018</infon><offset>53190</offset><text>The blind mind: No sensory visual imagery in aphantasia</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53246</offset><text>Khosla, A. (2017). Computer vision feature extraction toolbox (https://github.com/adikhosla/feature-extraction), GitHub. Retrieved February 10, 2021.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53396</offset><text>Khosla, A., Raju, A.S., Torralba, A., &amp; Oliva, A. (2015). Understanding and predicting image memorability at a large scale. Proceedings of the IEEE International Conference on Computer Vision, 2390–2398.</text></passage><passage><infon key="fpage">191</infon><infon key="lpage">211</infon><infon key="name_0">surname:Kosslyn;given-names:SM</infon><infon key="name_1">surname:Heldmeyer;given-names:KH</infon><infon key="name_2">surname:Locklear;given-names:EP</infon><infon key="pub-id_doi">10.1016/0022-0965(77)90099-6</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Child Psychology</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">1977</infon><offset>53602</offset><text>Children’s drawings as data about internal representations</text></passage><passage><infon key="fpage">1097</infon><infon key="lpage">1105</infon><infon key="name_0">surname:Krizhevsky;given-names:A</infon><infon key="name_1">surname:Sutskever;given-names:I</infon><infon key="name_2">surname:Hinton;given-names:GE</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2012</infon><offset>53663</offset><text>ImageNet classification with deep convolutional neural networks</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53727</offset><text>Kümmerer, M., Wallis, T.S.A., &amp; Bethge, M. (2016). DeepGaze II: Reading fixations from deep features trained on object recognition. arXiv, 1610.01563.</text></passage><passage><infon key="fpage">53</infon><infon key="lpage">59</infon><infon key="name_0">surname:Light;given-names:P</infon><infon key="name_1">surname:McEwen;given-names:F</infon><infon key="pub-id_doi">10.1111/j.2044-835X.1987.tb01041.x</infon><infon key="section_type">REF</infon><infon key="source">British Journal of Developmental Psychology</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">1987</infon><offset>53879</offset><text>Drawings as messages: The effect of a communication game upon production of view-specific drawings</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53978</offset><text>Long, B., Fan, J., Chai, Z., &amp; Frank, C.M. (2021). Parallel developmental changes in children’s drawing and recognition of visual concepts. PsyArXiv. 10.31234/osf.io/5yv7x.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54153</offset><text>Lowe, D.G. (1999). Object recognition from local scale-invariant features. Proceedings of the IEEE International Conference on Computer Vision, 2, 1150–1157.</text></passage><passage><infon key="fpage">338</infon><infon key="lpage">347</infon><infon key="name_0">surname:Makuuchi;given-names:M</infon><infon key="name_1">surname:Kaminaga;given-names:T</infon><infon key="name_2">surname:Sugishita;given-names:M</infon><infon key="pub-id_doi">10.1016/S0926-6410(02)00302-6</infon><infon key="pub-id_pmid">12706214</infon><infon key="section_type">REF</infon><infon key="source">Cognitive Brain Research</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2003</infon><offset>54313</offset><text>Both parietal lobes are involved in drawing: a functional MRI study and implications for constructional apraxia</text></passage><passage><infon key="fpage">482</infon><infon key="lpage">488</infon><infon key="name_0">surname:Murdock;given-names:BB</infon><infon key="pub-id_doi">10.1037/h0045106</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">1962</infon><offset>54425</offset><text>The serial position effect of free recall</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54467</offset><text>Needell, C.D., &amp; Bainbridge, W.A. (2021). Embracing new techniques in deep learning for estimating image memorability. ArXiv, 2105.10598.</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">175</infon><infon key="name_0">surname:Oliva;given-names:A</infon><infon key="name_1">surname:Torralba;given-names:A</infon><infon key="pub-id_doi">10.1023/A:1011139631724</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2001</infon><offset>54605</offset><text>Modeling the shape of the scene: A holistic representation of the spatial envelope</text></passage><passage><infon key="fpage">279</infon><infon key="lpage">287</infon><infon key="name_0">surname:Otgaar;given-names:H</infon><infon key="name_1">surname:van Ansem;given-names:R</infon><infon key="name_2">surname:Pauw;given-names:C</infon><infon key="name_3">surname:Horselenberg;given-names:R</infon><infon key="pub-id_doi">10.1007/s11896-016-9190-0</infon><infon key="section_type">REF</infon><infon key="source">Journal of Police and Criminal Psychology</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2016</infon><offset>54688</offset><text>Improving children’s interviewing methods? The effects of drawing and practice on children’s memories for an event</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54807</offset><text>Park, J., Josephs, E., &amp; Konkle, T. (2021). Systematic transition from boundary extension to contraction along an object-to-scene continuum. PsyArXiv. 10.31234/osf.io/84exs</text></passage><passage><infon key="fpage">101</infon><infon key="lpage">119</infon><infon key="name_0">surname:Perdreau;given-names:F</infon><infon key="name_1">surname:Cavanagh;given-names:P</infon><infon key="pub-id_doi">10.1068/i0635</infon><infon key="pub-id_pmid">25469216</infon><infon key="section_type">REF</infon><infon key="source">I-Perception</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2014</infon><offset>54980</offset><text>Drawing skill is related to the efficiency of encoding object structure</text></passage><passage><infon key="fpage">5</infon><infon key="name_0">surname:Perdreau;given-names:F</infon><infon key="name_1">surname:Cavanagh;given-names:P</infon><infon key="pub-id_doi">10.1167/15.5.5</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2016</infon><offset>55052</offset><text>Drawing experts have better visual memory while drawing</text></passage><passage><infon key="fpage">259</infon><infon key="lpage">267</infon><infon key="name_0">surname:Roberts;given-names:BRT</infon><infon key="name_1">surname:Wammes;given-names:JD</infon><infon key="pub-id_doi">10.3758/s13423-020-01804-w</infon><infon key="pub-id_pmid">32939630</infon><infon key="section_type">REF</infon><infon key="source">Psychonomic Bulletin &amp; review</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2020</infon><offset>55108</offset><text>Drawing and memory: Using visual production to alleviate concreteness effects</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55186</offset><text>Rollins. (2005). Tell me about it: Drawing as a communication tool for children with cancer. Journal of Pediatric Oncology Nursing, 22, 203–221.</text></passage><passage><infon key="fpage">335</infon><infon key="lpage">341</infon><infon key="name_0">surname:Rubin;given-names:DC</infon><infon key="name_1">surname:Kontis;given-names:TC</infon><infon key="pub-id_doi">10.3758/BF03202446</infon><infon key="pub-id_pmid">6633250</infon><infon key="section_type">REF</infon><infon key="source">Memory &amp; Cognition</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1983</infon><offset>55333</offset><text>A schema for common cents</text></passage><passage><infon key="fpage">157</infon><infon key="lpage">173</infon><infon key="name_0">surname:Russell;given-names:B</infon><infon key="name_1">surname:Torralba;given-names:A</infon><infon key="name_2">surname:Murphy;given-names:K</infon><infon key="name_3">surname:Freeman;given-names:WT</infon><infon key="pub-id_doi">10.1007/s11263-007-0090-8</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision</infon><infon key="type">ref</infon><infon key="volume">77</infon><infon key="year">2007</infon><offset>55359</offset><text>LabelMe: A database and web-based tool for image annotation</text></passage><passage><infon key="fpage">209</infon><infon key="lpage">216</infon><infon key="name_0">surname:Schaer;given-names:K</infon><infon key="name_1">surname:Jahn;given-names:G</infon><infon key="name_2">surname:Lotze;given-names:M</infon><infon key="pub-id_doi">10.1016/j.bbr.2012.05.009</infon><infon key="pub-id_pmid">22609273</infon><infon key="section_type">REF</infon><infon key="source">Behavioural Brain Research</infon><infon key="type">ref</infon><infon key="volume">233</infon><infon key="year">2012</infon><offset>55419</offset><text>fMRI-activation during drawing a naturalistic or sketchy portrait</text></passage><passage><infon key="fpage">892</infon><infon key="lpage">899</infon><infon key="name_0">surname:Shin;given-names:M-S</infon><infon key="name_1">surname:Park;given-names:S-Y</infon><infon key="name_2">surname:Park;given-names:S-R</infon><infon key="name_3">surname:Seol;given-names:S-H</infon><infon key="name_4">surname:Kwon;given-names:JS</infon><infon key="pub-id_doi">10.1038/nprot.2006.115</infon><infon key="pub-id_pmid">17406322</infon><infon key="section_type">REF</infon><infon key="source">Nature Protocols</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2006</infon><offset>55485</offset><text>Clinical and empirical applications of the Rey-Osterrieth Complex Figure Test</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55563</offset><text>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv, 1409.1556.</text></passage><passage><infon key="fpage">360</infon><infon key="lpage">366</infon><infon key="name_0">surname:Song;given-names:JH</infon><infon key="name_1">surname:Nakayama;given-names:K</infon><infon key="pub-id_doi">10.1016/j.tics.2009.04.009</infon><infon key="pub-id_pmid">19647475</infon><infon key="section_type">REF</infon><infon key="source">Trends in Cognitive Sciences</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2009</infon><offset>55687</offset><text>Hidden cognitive states revealed in choice reaching tasks</text></passage><passage><infon key="fpage">127</infon><infon key="lpage">139</infon><infon key="name_0">surname:Thomas;given-names:GV</infon><infon key="name_1">surname:Jolley;given-names:RP</infon><infon key="pub-id_doi">10.1111/j.2044-8260.1998.tb01289.x</infon><infon key="pub-id_pmid">9631202</infon><infon key="section_type">REF</infon><infon key="source">The British Journal of Clinical Psychology</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">1998</infon><offset>55745</offset><text>Drawing conclusions: A re-examination of empirical and conceptual bases for psychological evaluation of children from their drawings</text></passage><passage><infon key="fpage">1466</infon><infon key="lpage">1487</infon><infon key="name_0">surname:Uddenberg;given-names:S</infon><infon key="name_1">surname:Scholl;given-names:BJ</infon><infon key="pub-id_doi">10.1037/xge0000446</infon><infon key="pub-id_pmid">30010372</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: General</infon><infon key="type">ref</infon><infon key="volume">147</infon><infon key="year">2018</infon><offset>55878</offset><text>TeleFace: Serial reproduction of faces reveals a whiteward bias in race memory</text></passage><passage><infon key="fpage">706</infon><infon key="name_0">surname:van der Meer;given-names:ALH</infon><infon key="name_1">surname:van der Weel;given-names:FR</infon><infon key="pub-id_doi">10.3389/fpsyg.2017.00706</infon><infon key="pub-id_pmid">28536546</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in Psychology</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>55957</offset><text>Only three fingers write, but the whole brain works: A high-density EEG study showing advantages of drawing over typing for learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56090</offset><text>Vinci-Booher, S., Sturgeon, J., James, T., &amp; James, K. (2018). The MRItab: A MR-compatible touchscreen with video-display. Journal of Neuroscience Methods, 306, 10–18.</text></passage><passage><infon key="fpage">138</infon><infon key="lpage">154</infon><infon key="name_0">surname:Vinci-Booher;given-names:S</infon><infon key="name_1">surname:Cheng;given-names:H</infon><infon key="name_2">surname:James;given-names:KH</infon><infon key="pub-id_doi">10.1162/jocn_a_01340</infon><infon key="pub-id_pmid">30240307</infon><infon key="section_type">REF</infon><infon key="source">Journal of Cognitive Neuroscience</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2019</infon><offset>56260</offset><text>An analysis of the brain systems involved with producing letters by hand</text></passage><passage><infon key="fpage">91</infon><infon key="lpage">100</infon><infon key="name_0">surname:Vogt;given-names:S</infon><infon key="name_1">surname:Magnussen;given-names:S</infon><infon key="pub-id_doi">10.1068/p5262</infon><infon key="pub-id_pmid">17357707</infon><infon key="section_type">REF</infon><infon key="source">Perception</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2007</infon><offset>56333</offset><text>Expertise in pictorial perception: eye-movement patterns and visual memory in artists and laymen</text></passage><passage><infon key="fpage">1752</infon><infon key="lpage">1776</infon><infon key="name_0">surname:Wammes;given-names:JD</infon><infon key="name_1">surname:Meade;given-names:ME</infon><infon key="name_2">surname:Fernandes;given-names:MA</infon><infon key="pub-id_doi">10.1080/17470218.2015.1094494</infon><infon key="pub-id_pmid">26444654</infon><infon key="section_type">REF</infon><infon key="source">The Quarterly Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">69</infon><infon key="year">2016</infon><offset>56430</offset><text>The drawing effect: Evidence for reliable and robust memory benefits in free recall</text></passage><passage><infon key="fpage">103955</infon><infon key="name_0">surname:Wammes;given-names:JD</infon><infon key="name_1">surname:Jonker;given-names:TR</infon><infon key="name_2">surname:Fernandes;given-names:MA</infon><infon key="pub-id_doi">10.1016/j.cognition.2019.04.024</infon><infon key="pub-id_pmid">31254746</infon><infon key="section_type">REF</infon><infon key="source">Cognition</infon><infon key="type">ref</infon><infon key="volume">191</infon><infon key="year">2019</infon><offset>56514</offset><text>Drawing improves memory: The importance of multimodal encoding context</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56585</offset><text>Websanova. (2011). wPaint. https://github.com/websanova/wPaint. Accessed 3 March 2021</text></passage><passage><infon key="name_0">surname:Wechsler;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">WMS-IV: Wechsler memory scale</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>56671</offset></passage><passage><infon key="fpage">8619</infon><infon key="lpage">8624</infon><infon key="name_0">surname:Yamins;given-names:DLK</infon><infon key="name_1">surname:Hong;given-names:H</infon><infon key="name_2">surname:Cadieu;given-names:CF</infon><infon key="name_3">surname:Solomon;given-names:EA</infon><infon key="name_4">surname:Seibert;given-names:D</infon><infon key="name_5">surname:DiCarlo;given-names:J</infon><infon key="pub-id_doi">10.1073/pnas.1403112111</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">111</infon><infon key="year">2014</infon><offset>56672</offset><text>Performance-optimized hierarchical models predict neural responses in higher visual cortex</text></passage><passage><infon key="fpage">378</infon><infon key="lpage">380</infon><infon key="name_0">surname:Zeman;given-names:AZJ</infon><infon key="name_1">surname:Dewar;given-names:MT</infon><infon key="name_2">surname:Della Sala;given-names:S</infon><infon key="pub-id_doi">10.1016/j.cortex.2015.05.019</infon><infon key="pub-id_pmid">26115582</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">73</infon><infon key="year">2015</infon><offset>56763</offset><text>Lives without imagery – congenital aphantasia</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56811</offset><text>Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., &amp; Oliva, A. (2014). Learning deep features for scene recognition using places database. Advances in Neural Information Processing Systems, 27.</text></passage></document></collection>
