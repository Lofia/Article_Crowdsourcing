<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210711</date><key>pmc.key</key><document><id>8222356</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1038/s41597-021-00937-4</infon><infon key="article-id_pmc">8222356</infon><infon key="article-id_pmid">34162883</infon><infon key="article-id_publisher-id">937</infon><infon key="elocation-id">156</infon><infon key="kwd">Respiratory signs and symptoms Diagnostic markers</infon><infon key="license">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.The Creative Commons Public Domain Dedication waiver http://creativecommons.org/publicdomain/zero/1.0/ applies to the metadata files associated with this article.</infon><infon key="name_0">surname:Orlandic;given-names:Lara</infon><infon key="name_1">surname:Teijeiro;given-names:Tomas</infon><infon key="name_2">surname:Atienza;given-names:David</infon><infon key="name_3">surname:Orlandic;given-names:Lara</infon><infon key="name_4">surname:Teijeiro;given-names:Tomas</infon><infon key="name_5">surname:Orlandic;given-names:Lara</infon><infon key="section_type">TITLE</infon><infon key="title">Subject terms</infon><infon key="type">front</infon><infon key="volume">8</infon><infon key="year">2021</infon><offset>0</offset><text>The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>100</offset><text>Cough audio signal classification has been successfully used to diagnose a variety of respiratory conditions, and there has been significant interest in leveraging Machine Learning (ML) to provide widespread COVID-19 screening. The COUGHVID dataset provides over 25,000 crowdsourced cough recordings representing a wide range of participant ages, genders, geographic locations, and COVID-19 statuses. First, we contribute our open-sourced cough detection algorithm to the research community to assist in data robustness assessment. Second, four experienced physicians labeled more than 2,800 recordings to diagnose medical abnormalities present in the coughs, thereby contributing one of the largest expert-labeled cough datasets in existence that can be used for a plethora of cough audio classification tasks. Finally, we ensured that coughs labeled as symptomatic and COVID-19 originate from countries with high infection rates. As a result, the COUGHVID dataset contributes a wealth of cough recordings for training ML models to address the world’s most urgent health crises.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1182</offset><text>Machine-accessible metadata file describing the reported data: 10.6084/m9.figshare.14377019</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1274</offset><text>Background &amp; Summary</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1295</offset><text>The novel coronavirus disease (COVID-19), declared a pandemic by the World Health Organization on March 11, 2020, has claimed over 2.5 million lives worldwide as of March 2021. Epidemiology experts concur that mass testing is essential for isolating infected individuals, contact tracing, and slowing the progression of the virus. While advances in testing have made these tools more widespread in recent months, there remains a need for inexpensive, rapid, and scalable COVID-19 screening technology.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1797</offset><text>One of the most common symptoms of COVID-19 is a dry cough, which is present in approximately 67.7% of cases. Cough sound classification is an emerging field of research that has successfully leveraged signal processing and artificial intelligence (AI) tools to rapidly and unobtrusively diagnose respiratory conditions like pertussis, pneumonia and asthma using nothing more than a smartphone and its built-in microphone. Several research groups have begun developing algorithms for diagnosing COVID-19 from cough sounds. One such initiative, AI4COVID, provides a proof-of-concept algorithm but laments the lack of an extensive, labeled dataset that is needed to effectively train AI algorithms.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2494</offset><text>There are several existing COVID-19 cough sound datasets used to train Machine Learning (ML) models. Brown et al. have amassed a crowdsourced database of more than 10,000 cough samples from 7,000 unique users, 235 of which claim to have been diagnosed with COVID-19. However, the authors have not automated the data filtering procedure and consequently needed to endure the time-consuming process of manually verifying each recording. Furthermore, this dataset is not yet publicly available and therefore cannot be used by other teams wishing to train their ML models. The Coswara project, on the other hand, has publicly provided manual annotations of the crowdsourced COVID-19 coughs they have received, but as of March 2021, their dataset contains only 1,500 samples. An alternative approach to crowdsourcing is the NoCoCoDa, a database of cough sounds selected from media interviews of COVID-19 patients. However, this database only includes coughs from 10 unique subjects, which is not enough for AI algorithms to successfully generalize to the global population.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3563</offset><text>In this work, we present the COUGHVID crowdsourcing dataset, which is an extensive, publicly-available dataset of cough recordings. With more than 25,000 recordings – 1,155 claiming to have COVID-19 – originating from around the world, it is the largest known public COVID-19-related cough sound dataset in existence. In addition to publicly providing most of our cough corpus, we have trained and open-sourced a cough detection ML model to filter non-cough recordings from the database. This automated cough detection tool assists developers in creating robust applications that automatically remove non-cough sounds from their databases. Furthermore, our open-sourced cough detection model, preprocessing methods, cough segmentation algorithm, and SNR estimation code enable the research community to seamlessly integrate their own datasets with COUGHVID while keeping the data processing pipeline consistent.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4479</offset><text>We have undergone an additional layer of validation whereby four expert physicians annotated a fraction of the dataset to determine which crowdsourced samples realistically originate from COVID-19 patients. In addition to COVID-19 diagnoses, our expert labels and metadata provide a wealth of insights beyond those of existing public cough datasets. These datasets either do not provide labels or contain a small number of samples. For example, the Google Audio Set contains 871 cough sounds, but it does not specify the diagnoses or pathologies of the coughs. Conversely, the IIIT-CSSD labels coughs as wet vs dry and short-term vs long-term ailments, but it only includes 30 unique subjects. The COUGHVID dataset publicly contributes over 2,800 expert-labeled coughs, all of which provide a diagnosis, severity level, and whether or not audible health anomalies are present, such as dyspnea, wheezing, and nasal congestion. Using these expert labels along with participant metadata, our dataset can be used to train models that detect a variety of participants’ information based on their cough sounds. Overall, our dataset contains samples from a wide array of participant ages, genders, COVID-19 statuses, pre-existing respiratory conditions, and geographic locations, which potentially enable ML models to successfully perform generalization.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5829</offset><text>Finally, we ensure that samples labeled as COVID-19 originate from countries where the virus was prevalent at the time of recording, and we perform an evaluation of the quality of the recordings containing cough sounds. The first step to building robust AI algorithms for the detection of COVID-19 from cough sounds is having an extensive dataset, and the COUGHVID dataset effectively meets this pressing global need.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>6247</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>6255</offset><text>Data collection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6271</offset><text>All of the recordings were collected between April 1st, 2020 and December 1st, 2020 through a Web application deployed on a private server located at the facilities of the École Polytechnique Fédérale de Lausanne (EPFL), Switzerland. The application was designed with a simple workflow and following the principle “one recording, one click”, according to which if someone simply wants to send a cough recording, they should have to click on no more than one item.</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>6742</offset><text>Metadata variables, as they appear in the JSON files.</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Mandatory&lt;/th&gt;&lt;th&gt;Range of possible values&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;datetime&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;UTC date and time in ISO 8601 format&lt;/td&gt;&lt;td&gt;Timestamp of the received recording.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cough_detected&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;Floating point in the interval [0, 1]&lt;/td&gt;&lt;td&gt;Probability that the recording contains cough sounds, according to the automatic detection algorithm described in the Methods section.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SNR&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;Floating point in the interval [0, ∞)&lt;/td&gt;&lt;td&gt;An estimation of the signal-to-noise ratio of the cough recording.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;latitude&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;Floating point value&lt;/td&gt;&lt;td&gt;Self-reported latitude geolocation coordinate with reduced precision.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;longitude&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;Floating point value&lt;/td&gt;&lt;td&gt;Self-reported longitude geolocation coordinate with reduced precision.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;age&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;Integer value&lt;/td&gt;&lt;td&gt;Self-reported age value.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;gender&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;{female, male, other}&lt;/td&gt;&lt;td&gt;Self-reported gender.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;respiratory_condition&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;The patient has other respiratory conditions (self-reported).&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;fever_muscle_pain&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;The patient has fever or muscle pain (self-reported).&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;status&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;{COVID, symptomatic, healthy}&lt;/td&gt;&lt;td&gt;The patient self-reports that has been diagnosed with COVID-19 (COVID), that has symptoms but no diagnosis (symptomatic), or that is healthy (healthy).&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;expert_labels_{1,2,3}&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;JSON dictionary with the manual labels from expert 1, 2 or 3&lt;/td&gt;&lt;td&gt;The expert annotation variables are described in Table &lt;xref rid=&quot;Tab4&quot; ref-type=&quot;table&quot;&gt;4&lt;/xref&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>6796</offset><text>Name	Mandatory	Range of possible values	Description	 	datetime	Yes	UTC date and time in ISO 8601 format	Timestamp of the received recording.	 	cough_detected	Yes	Floating point in the interval [0, 1]	Probability that the recording contains cough sounds, according to the automatic detection algorithm described in the Methods section.	 	SNR	Yes	Floating point in the interval [0, ∞)	An estimation of the signal-to-noise ratio of the cough recording.	 	latitude	No	Floating point value	Self-reported latitude geolocation coordinate with reduced precision.	 	longitude	No	Floating point value	Self-reported longitude geolocation coordinate with reduced precision.	 	age	No	Integer value	Self-reported age value.	 	gender	No	{female, male, other}	Self-reported gender.	 	respiratory_condition	No	{True, False}	The patient has other respiratory conditions (self-reported).	 	fever_muscle_pain	No	{True, False}	The patient has fever or muscle pain (self-reported).	 	status	No	{COVID, symptomatic, healthy}	The patient self-reports that has been diagnosed with COVID-19 (COVID), that has symptoms but no diagnosis (symptomatic), or that is healthy (healthy).	 	expert_labels_{1,2,3}	No	JSON dictionary with the manual labels from expert 1, 2 or 3	The expert annotation variables are described in Table 4.	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8102</offset><text>The main Web interface has just one “Record” button that starts recording audio from the microphone for up to 10 seconds. Once the audio recording is completed, a small questionnaire is shown to get some metadata about the age, gender, and current condition of the user, but even if the questionnaire is not filled, the audio is sent to the server. The variables captured in the questionnaire are described in Table 1. Also, the user is asked for permission to provide their geolocation information, which is not mandatory. Finally, since coughing is a potentially dangerous activity in the scope of a global pandemic, we provide easy-to-follow safe coughing instructions, such as coughing into the elbow and holding the phone at arm’s length, that can be accessed from the main screen.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8898</offset><text>Database cleaning</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8916</offset><text>A common pitfall of crowdsourced data is that it frequently contains samples unrelated to the desired content of the database. In order to allow users of the COUGHVID dataset to quickly exclude non-cough sounds from their analyses, we developed a classifier to determine the degree of certainty to which a given recording contains a cough sound. These output probabilities of the classifier were subsequently included in the metadata of each record under the cough_detected entry. The recordings determined by the ML model to be non-cough sounds are included in the database and can be utilized by developers to enhance the robustness of their applications, such as alerting users when they do not upload a valid cough sound. Developers may use these samples to train their own classifiers to detect specific sounds, such as breathing or wheezing, and create many other possible applications.</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>9809</offset><text>Cough Classifier Features.</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Feature Class&lt;/th&gt;&lt;th&gt;Domain&lt;/th&gt;&lt;th&gt;Count&lt;/th&gt;&lt;th&gt;Computation Parameters&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MFCC&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Mel Frequency&lt;/td&gt;&lt;td&gt;26&lt;/td&gt;&lt;td&gt;Mean and St. Dev of 13 MFCCs over time&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EEPD&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR32&quot;&gt;32&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Time&lt;/td&gt;&lt;td&gt;19&lt;/td&gt;&lt;td&gt;BPF intervals in 50-1000 Hz; See Chatrazzin &lt;italic&gt;et al&lt;/italic&gt;.&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR32&quot;&gt;32&lt;/xref&gt;&lt;/sup&gt; for details&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Power Spectral Density&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR14&quot;&gt;14&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;Frequency bands (Hz): 0-200, 300-425, 500-650, 950-1150, 1400-1800, 2300–2400, 2850–2950, 3800–3900&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;RMS Power&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Time&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Zero Crossing Rate&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Time&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Crest Factor&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Time&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Recording Length&lt;/td&gt;&lt;td&gt;Time&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Dominant Frequency&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Centroid&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Rolloff &lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Spread&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Skewness&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Kurtosis&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Bandwidth&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Flatness&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral St. Dev&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Slope&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spectral Decrease&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR6&quot;&gt;6&lt;/xref&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR33&quot;&gt;33&lt;/xref&gt;&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;Frequency&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>9836</offset><text>Feature Class	Domain	Count	Computation Parameters	 	MFCC	Mel Frequency	26	Mean and St. Dev of 13 MFCCs over time	 	EEPD	Time	19	BPF intervals in 50-1000 Hz; See Chatrazzin et al. for details	 	Power Spectral Density	Frequency	8	Frequency bands (Hz): 0-200, 300-425, 500-650, 950-1150, 1400-1800, 2300–2400, 2850–2950, 3800–3900	 	RMS Power	Time	1	None	 	Zero Crossing Rate	Time	1	None	 	Crest Factor	Time	1	None	 	Recording Length	Time	1	None	 	Dominant Frequency	Frequency	1	None	 	Spectral Centroid	Frequency	1	None	 	Spectral Rolloff 	Frequency	1	None	 	Spectral Spread	Frequency	1	None	 	Spectral Skewness	Frequency	1	None	 	Spectral Kurtosis	Frequency	1	None	 	Spectral Bandwidth	Frequency	1	None	 	Spectral Flatness	Frequency	1	None	 	Spectral St. Dev	Frequency	1	None	 	Spectral Slope	Frequency	1	None	 	Spectral Decrease	Frequency	1	None	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10695</offset><text>To train the classifier, we employed similar procedures to those of state-of-the-art cough classification models. We first randomly selected a set of 215 recordings from our database. Then, we manually classified each recording as a cough sound if at least one cough was present, otherwise listing it as a non-cough sound. In the case that a recording contained both cough and non-cough audio events, the recording was discarded and a new one was randomly selected from the database. This process resulted in a nearly balanced sample of 121 cough sounds and 94 non-cough sounds including speaking, laughing, silence, and miscellaneous background noises. These recordings were preprocessed by lowpass filtering (fcutoff = 6 kHz) and downsampling to 12 kHz. Next, 68 audio features commonly used for cough classification were extracted from each recording. The details of these state-of-the-art features are listed in Table 2. The power spectral density (PSD) feature was computed using 8 hand-selected frequency bands similar to those selected by Alvarez et al.. These bands were chosen by analyzing the PSDs of cough vs non-cough signals and selecting the frequency ranges with the highest variation between the two classes.</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>11927</offset><text>Cough Classifier Performance (cough_detected threshold = 0.8).</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;CV Mean ± St. Dev&lt;/th&gt;&lt;th&gt;Final Model&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Precision&lt;/td&gt;&lt;td&gt;95.4 ± 7.1&lt;/td&gt;&lt;td&gt;95.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sensitivity&lt;/td&gt;&lt;td&gt;78.2 ± 10&lt;/td&gt;&lt;td&gt;80.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Specificity&lt;/td&gt;&lt;td&gt;95.3 ± 8.4&lt;/td&gt;&lt;td&gt;95.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Balanced Accuracy&lt;/td&gt;&lt;td&gt;86.7 ± 3.9&lt;/td&gt;&lt;td&gt;88.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;AUC&lt;/td&gt;&lt;td&gt;96.4 ± 3.3&lt;/td&gt;&lt;td&gt;96.5&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>11994</offset><text>Metric	CV Mean ± St. Dev	Final Model	 	Precision	95.4 ± 7.1	95.5	 	Sensitivity	78.2 ± 10	80.8	 	Specificity	95.3 ± 8.4	95.5	 	Balanced Accuracy	86.7 ± 3.9	88.1	 	AUC	96.4 ± 3.3	96.5	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12209</offset><text>We developed an eXtreme Gradient Boosting (XGB) classifier to perform the cough classification based on audio features. In order to assess the performance of the model across various sets of testing and training participants, we employed a nested k-fold cross-validation (CV) procedure. First, the dataset was partitioned into 10 segments, each with an equal number of recordings, hereinafter known as the “outer CV folds”. In each outer fold, one segment was used for testing and the rest were used for training. Then, within each outer CV fold, the XGB hyperparameters were tuned using Tree-structured Parzen Estimators (TPE). TPE works by further partitioning the training data into 10 segments, known as the “inner CV folds”, and then determining which set of hyperparameters exhibits the highest mean precision across the 10 folds. Following each TPE procedure, we tested the tuned XGB model using the testing data of its respective outer CV fold and computed several accuracy metrics. We then averaged each accuracy metric across the 10 outer CV folds and recorded its mean and standard deviation, as displayed in Table 3. We can see that this cough classifier exhibits a mean AUC of 96.4% and a standard deviation of only 3.3%, indicating that the model performs well and shows little variance among different testing groups. To develop a final, usable model to share with the research community, we reassembled the entire dataset and randomly selected 22% of the recordings for assessing the generalization capabilities of our model to unseen test participants. The remainig 78% of the data was used to train the final XGB classifier and determine one final set of hyperparameters using the aforementioned TPE cross-validation procedure, this time using a 5-fold random permutation-based ShuffleSplit CV.</text></passage><passage><infon key="file">41597_2021_937_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>14031</offset><text>Averaged receiver operating characteristic curve across the 10 cross-validation folds of the cough classifier.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14142</offset><text>The ROC curve of the cough classifier, which shows the mean true positive rate versus the mean false positive rate of the testing results across the 10 outer CV folds, is displayed in Fig. 1. Users of the COUGHVID database can consult this figure to set a cough detection threshold that suits their specifications. We recommend using a cough_detected value of 0.8 because, as shown in Fig. 1 and in Table 3, this threshold exhibits an average precision of 95.4%. Therefore, only 4.6% of recordings with a cough_detected probability greater than 0.8 can be expected to contain non-cough events, which is not a large enough portion of the dataset to significantly bias cough classification algorithms. Similarly, the final model exhibits a precision of 95.5% and sensitivity of 80.8%, indicating that the model successfully removes most non-cough recordings while maintaining over 80% of recordings that truly contain coughs. Finally, the samples confirmed by experts to contain coughs have a cough_detected value of 1.0, whereas those that experts noted did not contain coughs (less than 3% of the annotated data) have a cough_detected value of 0.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15292</offset><text>Expert annotation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15310</offset><text>Quality: Good; Ok; Poor; No cough present.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15353</offset><text>Type of cough: Wet (productive); Dry; Can’t tell.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15405</offset><text>Audible dyspnea: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15432</offset><text>Audible wheezing: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15460</offset><text>Audible stridor: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15487</offset><text>Audible choking: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15514</offset><text>Audible nasal congestion: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15550</offset><text>Nothing specific: Checkbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15578</offset><text>Impression: I think this patient has…: An upper respiratory tract infection; A lower respiratory tract infection; Obstructive lung disease (Asthma, COPD, …); COVID-19; Nothing (healthy cough).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15775</offset><text>Impression: the cough is probably…: Pseudocough/Healthy cough (from a healthy person); Mild (from a sick person); Severe (from a sick person); Can’t tell.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15934</offset><text>To enhance the quality of the dataset with clinically validated information, we were assisted by four expert physicians. Each of them revised 1000 recordings, selecting one of the predefined options to each of the following 10 items:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16168</offset><text>As a labeling support tool, we used an online spreadsheet using Google Sheets©. Thus, the experts could play the recordings directly inside the browser and select their answers in a convenient way. Once a recording had been labeled, the background color of the full row turned to green for easier navigation. The time required for each expert for labeling the 1000 recordings was around 10 hours, without significant differences among the four experts.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16624</offset><text>For binary variables (Columns E-J) the box should be ticked for any reasonable suspicion of the sounds being heard.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16740</offset><text>In Column K (“Impression: I think this patient has…”), you should check what you consider the most likely diagnosis, knowing that the records were collected between 01/04/2020 and 01/12/2020.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16938</offset><text>Each row is automatically marked as “completed” after providing an answer to column K (question number 9). However, please try to give an answer to every column.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17104</offset><text>In addition to the personal spreadsheets, the experts were provided with the following general instructions:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17213</offset><text>Good: Cough present with minimal background noise.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17264</offset><text>Ok: Cough present with background noise.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17305</offset><text>Poor: Cough present with significant background noise.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17360</offset><text>25% of the recordings with COVID value.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17400</offset><text>35% of the recordings with symptomatic value.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17446</offset><text>25% of the recordings with healthy value.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17488</offset><text>15% of the recordings with no reported status.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17535</offset><text>The selection of the recordings to be labeled was done through stratified random sampling and after pre-filtering using the automatic cough classifier described above, requiring a minimum probability of 0.8 of containing cough sounds. The distribution of recordings labeled by each expert was based on the self-reported status variable, as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17884</offset><text>Also, the following criteria for assessing quality was indicated to the experts:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17965</offset><text>This stratification ensured that between the four experts, 100% of the recordings labeled as COVID-19 by users that have a cough_detected value above 0.8 were labeled by at least one expert. Finally, we ensured that 15% of the recordings were labeled by all three reviewers, so that we could assess the level of agreement among them.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18299</offset><text>Ethical compliance</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18318</offset><text>All of the data collection and annotation was done in compliance with relevant ethical regulations. Informed consent was obtained by all participants who uploaded their cough sounds and metadata to our servers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>18529</offset><text>Data Records</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18542</offset><text>All of the publicly-available data records are stored on a Zenodo repository. Considering the recordings with a cough_detected value greater than 0.8, as explained in the Database Cleaning section, the database consists of nearly 35 hours of audio samples. This corresponds to approximately 37,000 segmented coughs, as determined using the segmentation procedure described in the Recording Quality Estimation subsection. Each cough recording consists of two files with the same name but different extensions. One of the files contains the audio data, as it was directly received at the COUGHVID servers, and it can be in the WEBM or OGG formats, respectively with the .webm and .ogg extensions. In all cases, the audio codec is Opus, with a sampling frequency of 48 kHz and operating in variable bitrate (VBR) mode. For more than 40% of the recordings, the effective bitrate is 48 kbit/s, which is in the highest bandwidth range for mono recordings. For the rest of the recordings, the reason for having a lower effective bitrate is the presence of long periods of silence, which is in fact desirable given the application. The second file contains the metadata encoded as plain text in the JSON format, and has the .json extension. The file name is a random string generated according to the UUID V4 protocol.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>19857</offset><text>Variables provided by the expert annotators.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Range of possible values&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;quality&lt;/td&gt;&lt;td&gt;{good, ok, poor, no_cough}&lt;/td&gt;&lt;td&gt;Quality of the recorded cough sound.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cough_type&lt;/td&gt;&lt;td&gt;{wet, dry, unknown}&lt;/td&gt;&lt;td&gt;Type of cough.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;dyspnea&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Audible dyspnea.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;wheezing&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Audible wheezing.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;stridor&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Audible stridor.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;choking&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Audible choking.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;congestion&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Audible nasal congestion.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;nothing&lt;/td&gt;&lt;td&gt;{True, False}&lt;/td&gt;&lt;td&gt;Nothing specific is audible.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;diagnosis&lt;/td&gt;&lt;td&gt;{upper_infection, lower_infection, obstructive_disease, COVID-19, healthy_cough}&lt;/td&gt;&lt;td&gt;Impression of the expert about the condition of the patient. It can be an upper or lower respiratory tract infection, an obstructive disease (Asthma, COPD, etc), COVID-19, or a healthy cough.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;severity&lt;/td&gt;&lt;td&gt;{pseudocough, mild, severe, unknown}&lt;/td&gt;&lt;td&gt;Impression of the expert about the severity of the cough. It can be a pseudocough from a healthy patient, a mild or severe cough from a sick patient, or unknown if the expert can’t tell.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>19902</offset><text>Name	Range of possible values	Description	 	quality	{good, ok, poor, no_cough}	Quality of the recorded cough sound.	 	cough_type	{wet, dry, unknown}	Type of cough.	 	dyspnea	{True, False}	Audible dyspnea.	 	wheezing	{True, False}	Audible wheezing.	 	stridor	{True, False}	Audible stridor.	 	choking	{True, False}	Audible choking.	 	congestion	{True, False}	Audible nasal congestion.	 	nothing	{True, False}	Nothing specific is audible.	 	diagnosis	{upper_infection, lower_infection, obstructive_disease, COVID-19, healthy_cough}	Impression of the expert about the condition of the patient. It can be an upper or lower respiratory tract infection, an obstructive disease (Asthma, COPD, etc), COVID-19, or a healthy cough.	 	severity	{pseudocough, mild, severe, unknown}	Impression of the expert about the severity of the cough. It can be a pseudocough from a healthy patient, a mild or severe cough from a sick patient, or unknown if the expert can’t tell.	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20863</offset><text>In the metadata we may distinguish three types of variables, related to: (1) context information (timestamp and the probability that the recording actually contains cough sounds), (2) self-reported information provided by the user, and (3) the labels provided by expert medical annotators about the clinical assessment of the cough recording. The only processing performed on the metadata was the reduction in the precision of the geolocation coordinates to just one decimal digit to ensure privacy protection. A full description of all the metadata variables is provided in Tables 1 and 4.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21455</offset><text>As an illustrative example, let us consider the recording with UUID 4e47612c-6c09-4580-a9b6-2eb6bf2ab40c depicted in Box 1. We can see the audio properties using a tool such as ffprobe (included in the ffmpeg software package), while the metadata can be directly displayed as a text file.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21745</offset><text>For convenience, a file compiling all of the available metadata is also provided. This file is named metadata_compiled.csv, and is a CSV file with 51 columns and one row per record. The first column corresponds to the UUID of each recording, and may be used as an index, while the rest of the columns correspond to the variables described in Tables 1 and 4, plus the SNR column described below in the “Recording Quality Estimation” section. The expert annotation variables have been expanded, and are named quality_1, cough_type_1 … diagnosis_4, severity_4.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22310</offset><text>4e47612c-6c09-4580-a9b6-2eb6bf2ab40c.webm</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22352</offset><text>4e47612c-6c09-4580-a9b6-2eb6bf2ab40c.json</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>22394</offset><text>Technical Validation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22415</offset><text>Demographic representativeness</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22446</offset><text>An important requirement for large datasets is that they must represent a wide range of participant demographics. Demographic statistics were collected across all recordings in our public dataset that provided metadata and that were classified as coughs with a probability above 0.8 by the XGB cough detection model. There were slightly more male participants than female (66.0% and 33.5%, respectively), and the majority of participants did not have pre-existing respiratory conditions (83.6%). The percentages of healthy, COVID-19 symptomatic, and COVID-19 positive participants were 78%, 16.1%, and 5.8%, respectively. The average age of the recordings was 36.5 years with a standard deviation of 13.9 years. This shows that a wide variety of ages, genders, and health statuses are captured within our dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23260</offset><text>Geographic representativeness</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23290</offset><text>In order to assess the plausibility that samples labeled as COVID-19 truly originated from people who tested positive for the disease, we analyzed the geographic locations of the samples in our public dataset for which this information was provided. We then evaluated the COVID-19 statistics of the countries at the time each recording was sent to determine if these countries had high infection rates at the time of recording. Of the 14,787 samples that the XGB model classified as cough sounds, 8887 of them provided GPS information, 1014 of which reported COVID-19 symptoms and 410 claim to have been diagnosed with COVID-19. An analysis of this geodata revealed that our dataset contains recordings originating from 125 unique countries, reflecting the diversity of the dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24073</offset><text>Studies have shown that coughing persists in a significant proportion of COVID-19 cases 14-21 days after receiving a positive PCR test. Therefore, we combined the World Health Organization’s statistics on daily new COVID-19 cases with the United Nations 2019 population database to determine the rate of new infections in the country from which each recording originated in the 14 days prior to it being uploaded to our web server. This analysis revealed that 94.9% of our recordings labeled as COVID-19 came from countries with more than 20 newly-confirmed COVID-19 cases per 1 million people. Similarly, 93.1% of recordings labeled as symptomatic originated from these countries.</text></passage><passage><infon key="file">41597_2021_937_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24757</offset><text>Cumulative COVID-19 cases in April and May 2020 per 1 million population, along with the GPS coordinates of the received recordings.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24890</offset><text>Figure 2 shows a map of the world with countries color coded according to the cumulative COVID-19 positive tests in April and May 2020 per 1 million population. We also show the COVID-19 and symptomatic recordings in our public corpus providing GPS information that were collected within this time period. This figure shows that most recordings were sent from countries with moderate-to-high COVID-19 infection rates at the time.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>25321</offset><text>Inter-rater reliability</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25345</offset><text>Inter-Expert Label Consistency.</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Item&lt;/th&gt;&lt;th&gt;&lt;italic&gt;K&lt;/italic&gt;&lt;sub&gt;&lt;italic&gt;Fleiss&lt;/italic&gt;&lt;/sub&gt;&lt;/th&gt;&lt;th&gt;Agreement&lt;sup&gt;&lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR29&quot;&gt;29&lt;/xref&gt;&lt;/sup&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;quality&lt;/td&gt;&lt;td&gt;−0.06&lt;/td&gt;&lt;td&gt;Poor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cough_type&lt;/td&gt;&lt;td&gt;0.26&lt;/td&gt;&lt;td&gt;Fair&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;dyspnea&lt;/td&gt;&lt;td&gt;−0.02&lt;/td&gt;&lt;td&gt;Poor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;wheezing&lt;/td&gt;&lt;td&gt;0.06&lt;/td&gt;&lt;td&gt;Slight&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;stridor&lt;/td&gt;&lt;td&gt;−0.01&lt;/td&gt;&lt;td&gt;Poor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;choking&lt;/td&gt;&lt;td&gt;−0.01&lt;/td&gt;&lt;td&gt;Poor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;congestion&lt;/td&gt;&lt;td&gt;0.41&lt;/td&gt;&lt;td&gt;Moderate&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;nothing&lt;/td&gt;&lt;td&gt;0.13&lt;/td&gt;&lt;td&gt;Slight&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;diagnosis&lt;/td&gt;&lt;td&gt;0.07&lt;/td&gt;&lt;td&gt;Slight&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;severity&lt;/td&gt;&lt;td&gt;0.15&lt;/td&gt;&lt;td&gt;Slight&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>25377</offset><text>Item	KFleiss	Agreement	 	quality	−0.06	Poor	 	cough_type	0.26	Fair	 	dyspnea	−0.02	Poor	 	wheezing	0.06	Slight	 	stridor	−0.01	Poor	 	choking	−0.01	Poor	 	congestion	0.41	Moderate	 	nothing	0.13	Slight	 	diagnosis	0.07	Slight	 	severity	0.15	Slight	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25637</offset><text>In order to determine the extent to which the four expert physicians agreed on their cough sound labeling, Fleiss’ Kappa scores, KFleiss, were computed for each question among the 130 common recordings in the public dataset. KFleiss is a standard method for gauging the consistency of categorical labels across multiple reviewers. This measure ranges from −1 – indicating no agreement among any reviewers – to 1, corresponding to perfect agreement among all reviewers. The results are displayed in Table 5, which depicts the KFleiss measures along with the strength of agreement according to the benchmark scale proposed by Landis and Koch. This analysis revealed moderate agreement on audible nasal congestion, fair agreement on the type of cough, as well as slight agreement on the cough severity, nothing specific, audible wheezing, and diagnosis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26497</offset><text>There was a slight agreement among experts on the cough diagnosis (KFleiss = 0.07), which is reflective of the fact that COVID-19 symptomology includes symptoms of both upper respiratory tract infections (e.g., rhinorrhea, sore throat, etc.) and lower respiratory tract infections (e.g., pneumonia, ground-glass opacities, etc.). When the expert labels that are not “COVID-19” or “healthy cough” are changed to “other”, the agreement slightly increases (KFleiss = 0.08). To determine the commonalities in labels between individual experts, we computed KFleiss on the cough diagnoses of pairs of experts. There was a fair agreement between Experts 3 and 4 (KFleiss = 0.22), a slight agreement between Experts 1 and 4 (KFleiss = 0.04), and a slight agreement between Experts 1 and 2 (KFleiss = 0.07). Other pairs of experts exhibited poor agreement. While a lack of a clear diagnostic consensus between all experts is to be expected for a novel pathology, it is advisable that users take this into consideration when using the experts’ labels as ground-truth.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>27589</offset><text>Trends in expert COVID-19 cough labeling</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27630</offset><text>All of the coughs in the public database that were labeled as COVID-19 among the four experts were subsequently pooled together and analyzed for trends in the attributes of the cough recordings. The vast majority of coughs do not exhibit audible dyspnea (93.72%), wheezing (92.43%), stridor (98.71%), choking (99.20%), or nasal congestion (99.03%). Additionally, 86.96% of COVID-19-labeled coughs are annotated as dry, which is consistent with literature stating that a dry cough is a common COVID-19 symptom. Finally, 83.58% of these coughs are labeled as mild. These commonalities among COVID-19 labeled coughs reflect the consistency of the database.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>28284</offset><text>Recording quality estimation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28313</offset><text>Every user who uploaded their cough sound to the COUGHVID dataset presumably used a different device, potentially introducing a variation in recording quality due to the different recording hardware and software of each device. Furthermore, the recordings were captured at various locations around the world with non-constant degrees of background noise. In order to assist users of the COUGHVID dataset in estimating the quality of each signal, we provide open-sourced code to estimate the Signal-to-Noise Ratio (SNR) of each cough recording.</text></passage><passage><infon key="file">41597_2021_937_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28857</offset><text>Histogram of estimated SNRs of every recording in the database with a cough_detected value greater than 0.8.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28966</offset><text>The SNR estimation method begins with a simple cough segmentation algorithm based on a digital hysteresis comparator on the signal power. The signal is first normalized to the [−1, 1] range, lowpass filtered (fcutoff = 6 kHz), and downsampled to 12 kHz. Then, the hysteresis comparator identifies regions of the signal with rapid spikes in power, which is customary for cough sounds. Since the expiratory phase of the cough lasts 230-550 ms, we discard any cough sounds lasting less than 200 ms. Furthermore, we consider the 200 ms before and after a cough sound as part of the cough, as this period may comprise the low-amplitude compressive and expiratory phases of the cough sound. Following the cough segmentation, the SNR is computed as following Eq. 1,where xcough are the signal samples determined through segmentation to be part of a cough, and xnoise are all other signal samples, presumed to be background noise. Intuitively, this formula compares the signal power of the cough portion of the signal to that of the background noise. This SNR information is included under the SNR column of the recording metadata CSV file. A histogram of all SNR values for recordings with a cough_detected probability greater than 0.8 is shown in Fig. 3, showing a wider variety of signal qualities present in the dataset. Users of the database may employ these computed SNR values to filter the dataset by maintaining only recordings of a desired quality.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>30435</offset><text>Private Set and Testing Protocol</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30468</offset><text>In order to ensure the reproducibility of the experimental results that use the COUGHVID crowdsourcing dataset, a private test set of 625 cough recordings has been kept out from publishing. Recordings in this private set have been randomly selected from those having at least labels from one expert. Tens of non-cough samples have been included in the test set (labeled with a cough_detected probability of 0) to assist developers in assessing the robustness of their models.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30944</offset><text>The evaluation of models on the private test set will be open to the entire scientific community, but to ensure a fair use, the performance measurements will be obtained by an independent evaluator. Any researcher that demonstrates promising results on the public dataset using cross-validation may apply for an independent evaluation on the private test set according to the protocol described in the data repository. This protocol shall be regularly updated in line with the available technology to ensure that it is as convenient as possible for all parties.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31506</offset><text>Since one of the aims of this project is to go beyond the study of COVID-19, every variable except datetime and cough_detected may be considered the target of a prediction model. This opens the possibility to study several different problems within the same dataset, from just cough identification and sound quality assessment to the detection of different conditions, or even the estimation of age or gender. Conversely, a prediction model may require as input not just the sound recording, but also other metadata variables to provide the necessary context, such as the age or gender of the participant. The restrictions on the sets of variables that can be used as input or as a result of the algorithms will be kept to a minimum.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">footnote</infon><offset>32240</offset><text>Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">footnote</infon><offset>32378</offset><text>These authors contributed equally: Lara Orlandic, Tomas Teijeiro.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>32444</offset><text>Author contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>32465</offset><text>L.O. and T.T. devised the idea and research. L.O. developed the cough signal processing code, created the cough detection model, and performed data analysis on the user metadata and expert labels. T.T. deployed and managed the data collection website, coordinated expert labeling, and compiled all of the metadata. All authors contributed to the writing and editing of the manuscript.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>32850</offset><text>Code availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>32868</offset><text>The aforementioned XGB classifier used to remove non-cough recordings, feature extraction source code, cough preprocessing methods, cough segmentation function, and SNR estimation algorithm are available on our public repository.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>33098</offset><text>Competing interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>33118</offset><text>The authors declare no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>33162</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33173</offset><text>World Health Organization. Coronavirus (COVID-19) Cases and Deaths. Humanitarian Data Exchangehttps://data.humdata.org/dataset/coronavirus-covid-19-cases-and-deaths (2021).</text></passage><passage><infon key="fpage">915</infon><infon key="lpage">916</infon><infon key="name_0">surname:Rosenthal;given-names:PJ</infon><infon key="pub-id_doi">10.4269/AJTMH.20-0216</infon><infon key="section_type">REF</infon><infon key="source">American Journal of Tropical Medicine and Hygiene</infon><infon key="type">ref</infon><infon key="volume">102</infon><infon key="year">2020</infon><offset>33346</offset><text>The importance of diagnostic testing during a viral pandemic: Early lessons from novel coronavirus disease (CoVID-19)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33464</offset><text>Marcel, S. et al. COVID-19 epidemic in Switzerland: On the importance of testing, contact tracing and isolation. Swiss Medical Weekly150, 10.4414/smw.2020.20225 (2020).</text></passage><passage><infon key="fpage">1021</infon><infon key="lpage">1024</infon><infon key="name_0">surname:MacKay;given-names:MJ</infon><infon key="pub-id_doi">10.1038/s41587-020-0655-4</infon><infon key="pub-id_pmid">32820257</infon><infon key="section_type">REF</infon><infon key="source">Nature Biotechnology</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2020</infon><offset>33633</offset><text>The COVID-19 XPRIZE and the need for scalable, fast, and widespread testing</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33709</offset><text>World Health Organization. Report of the WHO-China Joint Mission on Coronavirus Disease 2019 (COVID-19). https://www.who.int/publications/i/item/report-of-the-who-china-joint-mission-on-coronavirus-disease-2019-(covid-19) (2020).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33939</offset><text>Pramono, R. X. A., Imtiaz, S. A. &amp; Rodriguez-Villegas, E. A cough-based algorithm for automatic diagnosis of pertussis. PLoS ONE11, 10.1371/journal.pone.0162128 (2016).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>34108</offset><text>Amrulloh, Y., Abeyratne, U., Swarnkar, V. &amp; Triasih, R. Cough Sound Analysis for Pneumonia and Asthma Classification in Pediatric Population. In Proceedings - International Conference on Intelligent Systems, Modelling and Simulation, ISMS, vol. 2015-October, 127–131, 10.1109/ISMS.2015.41 (IEEE Computer Society, 2015).</text></passage><passage><infon key="fpage">100378</infon><infon key="name_0">surname:Imran;given-names:A</infon><infon key="pub-id_doi">10.1016/j.imu.2020.100378</infon><infon key="pub-id_pmid">32839734</infon><infon key="section_type">REF</infon><infon key="source">Informatics in Medicine Unlocked</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>34430</offset><text>AI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>34519</offset><text>Brown, C. et al. Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, vol. 11, 3474–3484, 10.1145/3394486.3412865 (Association for Computing Machinery, New York, NY, USA, 2020).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>34828</offset><text>Sharma, N. et al. Data repository of Project Coswara. Githubhttps://github.com/iiscleap/Coswara-Data (2021).</text></passage><passage><infon key="fpage">154087</infon><infon key="lpage">154094</infon><infon key="name_0">surname:Cohen-McFarlane;given-names:M</infon><infon key="name_1">surname:Goubran;given-names:R</infon><infon key="name_2">surname:Knoefel;given-names:F</infon><infon key="pub-id_doi">10.1109/access.2020.3018028</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>34937</offset><text>Novel Coronavirus (2019) Cough Database: NoCoCoDa</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>34987</offset><text>Gemmeke, J. F. et al. Audio Set: An ontology and human-labeled dataset for audio events. In ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 776–780, 10.1109/ICASSP.2017.7952261 (Institute of Electrical and Electronics Engineers Inc., 2017).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>35278</offset><text>Singh, V. P., Rohith, J. M., Mittal, Y. &amp; Mittal, V. K. IIIT-S CSSD: A Cough Speech Sounds Database. In 2016 22nd National Conference on Communication, NCC 2016, 10.1109/NCC.2016.7561190 (Institute of Electrical and Electronics Engineers Inc., 2016).</text></passage><passage><infon key="fpage">2319</infon><infon key="lpage">2330</infon><infon key="name_0">surname:Monge-Álvarez;given-names:J</infon><infon key="name_1">surname:Hoyos-Barceló;given-names:C</infon><infon key="name_2">surname:San-José-Revuelta;given-names:LM</infon><infon key="name_3">surname:Casaseca-de-la-Higuera;given-names:P</infon><infon key="pub-id_doi">10.1109/TBME.2018.2888998</infon><infon key="pub-id_pmid">30575527</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Biomedical Engineering</infon><infon key="type">ref</infon><infon key="volume">66</infon><infon key="year">2019</infon><offset>35529</offset><text>A machine hearing system for robust cough detection based on a high-level representation of band-specific audio features</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>35650</offset><text>XGBoost developers. Xgboost documentation. https://xgboost.readthedocs.io/en/latest (2020).</text></passage><passage><infon key="fpage">2079</infon><infon key="lpage">2107</infon><infon key="name_0">surname:Cawley;given-names:GC</infon><infon key="name_1">surname:Talbot;given-names:NLC</infon><infon key="section_type">REF</infon><infon key="source">Journal of Machine Learning Research</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2010</infon><offset>35742</offset><text>On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>35833</offset><text>Bergstra, J. S., Bardenet, R., Bengio, Y. &amp; Kégl, B. Algorithms for hyper-parameter optimization. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F. &amp; Weinberger, K. Q. (eds.) Advances in Neural Information Processing Systems24, 2546–2554 (Curran Associates, Inc., 2011).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36121</offset><text>ShuffleSplit scikit-learn 0.24.1 documentation. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html (2021).</text></passage><passage><infon key="name_0">surname:Orlandic;given-names:L</infon><infon key="name_1">surname:Teijeiro;given-names:T</infon><infon key="name_2">surname:Atienza;given-names:D</infon><infon key="pub-id_doi">10.5281/zenodo.4048311</infon><infon key="section_type">REF</infon><infon key="source">Zenodo</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>36269</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36270</offset><text>Bankoski, J. Intro to WebM. In Proceedings of the 21st international workshop on Network and operating systems support for digital audio and video - NOSSDAV ’11, 1, 10.1145/1989240.1989242 (ACM Press, New York, New York, USA, 2011).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36505</offset><text>Pfeiffer, S. The Ogg Encapsulation Format Version 0. RFC 3533, 10.17487/RFC3533 (2003).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36593</offset><text>Valin, J.-M., Vos, K. &amp; Terriberry, T. Definition of the Opus Audio Codec. RFC 6716, 10.17487/RFC6716 (2012).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36703</offset><text>Bray, T. The JavaScript Object Notation (JSON) Data Interchange Format. RFC 7159, 10.17487/RFC7159 (2014).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36810</offset><text>Leach, P. J., Salz, R. &amp; Mealling, M. H. A Universally Unique IDentifier (UUID) URN Namespace. RFC 4122, 10.17487/RFC4122 (2005).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36940</offset><text>FFmpeg developers. Ffmpeg documentation. http://ffmpeg.org/documentation.html (2020).</text></passage><passage><infon key="fpage">993</infon><infon key="lpage">998</infon><infon key="name_0">surname:Tenforde;given-names:MW</infon><infon key="pub-id_doi">10.15585/mmwr.mm6930e1</infon><infon key="pub-id_pmid">32730238</infon><infon key="section_type">REF</infon><infon key="source">MMWR. Morbidity and Mortality Weekly Report</infon><infon key="type">ref</infon><infon key="volume">69</infon><infon key="year">2020</infon><offset>37026</offset><text>Symptom Duration and Risk Factors for Delayed Return to Usual Health Among Outpatients with COVID-19 in a Multistate Health Care Systems Network United States, March June 2020</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>37202</offset><text>United Nations Population Division. World Population Prospects. https://population.un.org/wpp/Download/Standard/CSV (2020).</text></passage><passage><infon key="fpage">378</infon><infon key="lpage">382</infon><infon key="name_0">surname:Fleiss;given-names:JL</infon><infon key="pub-id_doi">10.1037/h0031619</infon><infon key="section_type">REF</infon><infon key="source">Psychological Bulletin</infon><infon key="type">ref</infon><infon key="volume">76</infon><infon key="year">1971</infon><offset>37326</offset><text>Measuring nominal scale agreement among many raters</text></passage><passage><infon key="fpage">159</infon><infon key="name_0">surname:Landis;given-names:JR</infon><infon key="name_1">surname:Koch;given-names:GG</infon><infon key="pub-id_doi">10.2307/2529310</infon><infon key="pub-id_pmid">843571</infon><infon key="section_type">REF</infon><infon key="source">Biometrics</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">1977</infon><offset>37378</offset><text>The Measurement of Observer Agreement for Categorical Data</text></passage><passage><infon key="fpage">102433</infon><infon key="name_0">surname:Rothan;given-names:HA</infon><infon key="name_1">surname:Byrareddy;given-names:SN</infon><infon key="pub-id_doi">10.1016/j.jaut.2020.102433</infon><infon key="pub-id_pmid">32113704</infon><infon key="section_type">REF</infon><infon key="source">Journal of Autoimmunity</infon><infon key="type">ref</infon><infon key="volume">109</infon><infon key="year">2020</infon><offset>37437</offset><text>The epidemiology and pathogenesis of coronavirus disease (COVID-19) outbreak</text></passage><passage><infon key="fpage">2</infon><infon key="lpage">8</infon><infon key="name_0">surname:Chang;given-names:AB</infon><infon key="pub-id_doi">10.1016/j.prrv.2005.11.009</infon><infon key="pub-id_pmid">16473809</infon><infon key="section_type">REF</infon><infon key="source">Paediatric Respiratory Reviews</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2006</infon><offset>37514</offset><text>The physiology of cough</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>37538</offset><text>Chatrzarrin, H., Arcelus, A., Goubran, R. &amp; Knoefel, F. Feature extraction for the differentiation of dry and wet cough sounds. In MeMeA 2011 - 2011 IEEE International Symposium on Medical Measurements and Applications, Proceedings, 10.1109/MeMeA.2011.5966670 (2011).</text></passage><passage><infon key="fpage">107020</infon><infon key="name_0">surname:Sharma;given-names:G</infon><infon key="name_1">surname:Umapathy;given-names:K</infon><infon key="name_2">surname:Krishnan;given-names:S</infon><infon key="pub-id_doi">10.1016/j.apacoust.2019.107020</infon><infon key="section_type">REF</infon><infon key="source">Applied Acoustics</infon><infon key="type">ref</infon><infon key="volume">158</infon><infon key="year">2020</infon><offset>37806</offset><text>Trends in audio signal feature extraction methods</text></passage></document></collection>
