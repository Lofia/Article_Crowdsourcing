<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210605</date><key>pmc.key</key><document><id>8157248</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.7717/peerj-cs.528</infon><infon key="article-id_pmc">8157248</infon><infon key="article-id_pmid">34084930</infon><infon key="article-id_publisher-id">cs-528</infon><infon key="elocation-id">e528</infon><infon key="kwd">Crowdsourcing Convolutional neural networks Artificial intelligence Beach monitoring GIS Ecological application Crowd-mapping Geotagged images</infon><infon key="license">This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</infon><infon key="name_0">surname:Arellano-Verdejo;given-names:Javier</infon><infon key="name_1">surname:Lazcano-Hernández;given-names:Hugo E.</infon><infon key="name_2">surname:Procter;given-names:James</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">7</infon><infon key="year">2021</infon><offset>0</offset><text>Collective view: mapping Sargassum distribution along beaches</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>62</offset><text>The atypical arrival of pelagic Sargassum to the Mexican Caribbean beaches has caused considerable economic and ecological damage. Furthermore, it has raised new challenges for monitoring the coastlines. Historically, satellite remote-sensing has been used for Sargassum monitoring in the ocean; nonetheless, limitations in the temporal and spatial resolution of available satellite platforms do not allow for near real-time monitoring of this macro-algae on beaches. This study proposes an innovative approach for monitoring Sargassum on beaches using Crowdsourcing for imagery collection, deep learning for automatic classification, and geographic information systems for visualizing the results. We have coined this collaborative process “Collective View”. It offers a geotagged dataset of images illustrating the presence or absence of Sargassum on beaches located along the northern and eastern regions in the Yucatan Peninsula, in Mexico. This new dataset is the largest of its kind in surrounding areas. As part of the design process for Collective View, three convolutional neural networks (LeNet-5, AlexNet and VGG16) were modified and retrained to classify images, according to the presence or absence of Sargassum. Findings from this study revealed that AlexNet demonstrated the best performance, achieving a maximum recall of 94%. These results are good considering that the training was carried out using a relatively small set of unbalanced images. Finally, this study provides a first approach to mapping the Sargassum distribution along the beaches using the classified geotagged images and offers novel insight into how we can accurately map the arrival of algal blooms along the coastline.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1774</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1787</offset><text>Studies have demonstrated the negative impact of large concentrations of Sargassum along the coast and beaches on existing ecosystems. Some examples include: enhanced beach erosion, mortality of near-shore seagrass and fauna due to Sargassum leachates and abundant suspended organic matter that obstructs the passage of light to deeper areas of the coast, and the alteration of the trophic structure of the sea urchin Diadema antillarum along coastal marine systems. Additionally, it is known that onshore and nearshore masses of Sargassum interfere with the seaward journeys of the juvenile turtles, affecting their nesting. Finally, the high arsenic content present in Sargassum is of concern for environmental contamination of the sea and aquifer. In the study of, 86% of the total samples contained arsenic concentration amounts exceeding the maximum limits for seaweed used as animal fodder (40 ppm DW). Thus, the authors recommended instrumenting obligatory practices of metal content analyses in Sargassum or avoiding the use of it for nutritional purposes. As demonstrated in these studies, there is sufficient scientific evidence that confirms the negative impact of large volumes of Sargassum on the coastal ecosystems of the Caribbean Sea. However, what seems to be missing from this body of research is how technological solutions (e.g., the use of an adequate spatial and temporal scale for regional and local monitoring) can contribute to the management and disposal of Sargassum along the beaches.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3300</offset><text>Since 2011, the massive arrival of pelagic Sargassum has altered the balance of coastal ecosystems in the Caribbean Sea. Although Sargassum is of great ecological advantage in the open ocean, the high concentrations present in the coastal zone have generated ecological and economic damage. The Mexican Caribbean coastline began receiving massive amounts of Sargassum during late 2014, reaching its highest peak in September 2015. During 2016 and 2017, the influx of Sargassum slowly declined and then increased again in 2018. In 2019, the arrival of Sargassum continued, but with lower amounts than in 2018. A similar trend can be seen in the year 2020, according to the Outlook of 2020 Sargassum blooms in the Caribbean Sea. The amount of Sargassum approaching the coastline seems to be lower in 2020, when compared to 2019. Despite the gradual decrease in the yearly amount of Sargassum arriving to the shores, one should be careful not to underestimate the continued effects. The total amount of Sargassum is still very high and continues to critically damage the ecosystem.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4379</offset><text>Traditionally, the monitoring of Sargassum has been carried out using satellite remote sensing techniques. The Terra, Aqua, and Landsat platforms have been the most widely used due to their sensors onboard, their technical features, and for providing open data. However, because of the temporal and spatial resolution limitations, it is not possible to monitor Sargassum efficiently with the open access satellite platforms previously described. Important methodologies for detecting Sargassum have been developed by inputting the data obtained from the aforementioned platforms. Currently, the most widely accepted remote sensing methodologies worldwide for detecting pelagic Sargassum are the Floating Algae Index (FAI) and Alternative Floating Algae Index (AFAI). However, the frequent presence of clouds in the region is an issue that causes false positives. Additionally, these methodologies do not offer precision in nearshore waters that are relevant to the local communities where the ecological and economic challenges occur. Recent research has aimed to monitor Sargassum along the coastline using computing science paired with remote sensing data. One example is ERISNet, which uses a new artificial neural network architecture to classify geospatial dataset values related to the presence or absence of Sargassum across various spectral bands. A second example is the use of a convolutional neural network (CNN) for automatic classification of Moderate Resolution Imaging Spectroradiometer (MODIS) satellite products, enabling high-generalization classifications of more than 250,000 images with a 99.99% accuracy. Finally, conducted an innovative study where Sargassum features were automatically extracted from Sentinel-2 MSI Images. One of the shortcomings of these studies can be found in the resolutions of the sensors. The data obtained from the MODIS sensor, which is onboard Terra and Aqua platforms, has a spatial resolution between 250 m and 1.2 km, depending on the spectral band, with a revisit time of one day. On the other hand, the MSI sensor onboard Sentinel-2 offers a spatial resolution of 10, 20, or 60 m depending on the spectral band. The revisit frequency of each single Sentinel-2 satellite is ten days and the combined constellation revisit is five days. Therefore, limitations in the spatial (MODIS) and temporal (MSI) resolutions of the sensors are still present.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6781</offset><text>Another approach that emerged recently for monitoring Sargassum involves the use of social networks. However, the lack of metadata in the images makes this approach ineffective. Other initiatives, inspired by the use of “Citizen Science” and “crowdsourcing”, have collected information on the arrival of the Sargassum to the coast. These initiatives have offered promising results as well as new challenges. The term crowdsourcing was coined by, to describe “the act of taking a job traditionally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call”. Several studies have demonstrated that Crowdsourcing is a useful methodology for collecting and managing data in different content areas, including: image classification, character recognition, genome annotation, document translation, protein folding, RNA structure design, algorithm development, and Sargassum monitoring. For the successful implementation of crowdsourcing, it is advisable to use a platform to standardize and automatize the study processes. In this sense, there are two possible ways: to use an existing platform or, if necessary, to create a platform according to the goals and context of the study. There are important Citizen Science platforms that can be used to collect information (e.g., iNaturalist and Epicollect). iNaturalist (), managed by the California Academy of Sciences and the National Geographic Society, is a Citizen Science project and online social network of naturalists, scientists, and biologists. This application maps and exchanges biodiversity observations around the world. The main advantages of this platform are that the data is openly accessible on the Web and that much of it is licensed for re-use or free of intellectual property restrictions. Epicollect (), managed by the Big Data Institute of the University of Oxford, is a very robust platform in which, in addition to images, different data can be captured in text format through its user interface. One of the disadvantages of this platform is the design. Intended for an expert user (i.e., scientists, technicians, and students), it becomes complex for someone who is not familiarized with the topic. Applications such as iNaturalist and Epicollect are designed to measure species presence/absence rather than abundance estimation or surveillance. However, the present study requires periodic monitoring over time rather than sporadic sightings, which means photographs are also required, even when there is no Sargassum on the beach. Therefore, we chose to build a platform according to the goals and context of the study region.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9483</offset><text>The creation of new neural network architectures in recent years has enabled their use in massive commercial applications. Some examples include: Adversarial Generative Networks (GANs), Autoencoders, Residual Networks (ResNets) and Unets. However, Transfer Learning, defined as taking previously trained neuronal models and adapting them to different problems for which they were created, has solved problems like the one presented in this work, both in a timely and successful manner. In addition, increased storage capacity and the emergence of new massive processing technologies, including GPU and TPU clusters, have dramatically increased the processing power we have nowadays. This has led to an increase in research employing the development of new algorithms of Artificial Intelligence (AI) and Machine Learning (ML), resulting also in the design and effective operation of various applications, as mentioned previously.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10412</offset><text>Maps are essential resources for visitors to an unfamiliar place because they visually highlight landmarks and other places of interest. However, hand-designed maps are static representations that cannot always be adapted to the study of the phenomenon of interest, especially when the changes in the phenomenon are highly dynamic. In the last decade, digital maps, such as those provided by Google Maps, Waze Maps, Uber, and Rappi, have become increasingly popular. Many phenomena are usually in a perpetual state of change and renewal. Therefore, one of the advantages of these digital maps over hand-designed maps is that they are based on continuously updated models and generally reflect the most current information. Modern earth-observation research aims to study the variation in landscapes over multiple spatial and temporal scales. As mentioned previously, there is not always availability of satellite data on land coverage, so the combined use of crowdsourcing, mobile telecommunications, and the internet has become a viable solution. In this sense, previous research has generated the crowdmapping of urban objects with geo-location precision of approximately 3 m and land use maps in big cities via Twitter. In addition, based on photographs uploaded to social networking sites, there are studies that generate maps for evaluating the flow of services of the cultural ecosystem. These few samples show that map generation, through crowdsourcing, is a reliable methodology that has been implemented successfully in several studies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11958</offset><text>In terms of Sargassum monitoring, research points to important ocean scale mapping efforts. Some examples include: the Sargassum Early Advisory System (SEAS), developed by Texas A&amp;M University at Galveston, and the Satellite-based Sargassum Watch System (SaWS), at the University of South Florida (USF). The latter relies on near-real-time satellite and modeling results to monitor pelagic Sargassum, which serve to create monthly bulletins and show the distribution maps in the central-west Atlantic Ocean and Caribbean regions. Other tools have been developed to integrate SaWS products for visualization using Google Earth, which facilitates the application of SaWS products through a widely known visualization tool. Finally, “Citizen Science” platforms, such as Epicollect and iNaturalist, allow groups of researchers to build sets with geo-referenced images of Sargassum. In this sense, the Marine Macroalgae Research lab at Florida International University (MMRL-FIU) is studying the occurrences of washed-up Sargassum landings on South Florida and Caribbean coastal areas, through the crowdsourcing project called “Sargassum Watch”. This project can be accessed through the iNaturalist and Epicollect platforms. At the end of 2020, the Sargassum Watch project had collected 980 observations using iNaturalist () (carried out by 577 people) and over 2,155 photographs using the Epicollect () platform. On the other hand, in Mexico, the National Commission for the Knowledge and Use of Biodiversity (CONABIO, for its acronym in Spanish) manages the project “Monitoring pelagic Sargassum in the Mexican Atlantic”, which can be accessed through the Naturalista platform (). At the end of 2020, CONABIO’s project had collected 154 observations, carried out by 50 people. Photographs included both close-up and panoramic shots of Sargassum. Despite these important efforts to monitor Sargassum along the beaches, the diversity of images (i.e., visual attributes like angle, background elements, lighting, lens distortion) for the region of the study is limited. Thus, the number of attributes shown in the images collected with these applications is limited in our study. However, this is not unusual given the purpose for which these applications were originally created. They were initially used to integrate a database that contributes to the knowledge of the biodiversity of the species, and in particular with Sargassum, to know the distribution of this macroalga throughout the Caribbean. Furthermore, there is not a constant flow of photographs uploaded to the platforms by citizens. Another limitation is that, in order to produce useful tools for decision-making processes on how to manage and dispose Sargassum, it is necessary to have an automatic classification algorithm of the imagery, which can generate automatic Sargassum distribution maps. To the best of our knowledge, there is no system, at the beach scale, that automatically generates maps regarding the presence or absence of Sargassum. A tool of this type will allow organizing geotagged photographs at scales of less than one meter, for the construction of a collaborative network that complements remote sensing observations. The monitoring of Sargassum on beaches is a great challenge.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15237</offset><text>This study proposes a collaborative scheme based on crowdsourcing for the capture of images, artificial intelligence algorithms for the automatic classification of photographs, and geographic information systems for displaying the results. This collaborative process has been called “Collective View”, which is a living process that, due to the social collaboration, currently continues generating geotagged images along the northern and eastern coasts of the Yucatan Peninsula. Because crowdsourcing relies on the contribution of society, we used several strategies to involve individuals in the monitoring of Sargassum on the beach. Some of these strategies included: lectures given at numerous institutions in the region, direct invitations sent to academics and students interested in Sargassum issues, and the creation of a short video explaining how to use the app “Collective View” (). Therefore, we consider this study contributes to improving Sargassum monitoring along the beach and supports its management and disposal. The actions carried out in each stage of the “Collective View” design process are described in detail below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>16389</offset><text>Materials and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16411</offset><text>Following a crowdsourcing paradigm, the software that compose “Collective View” were designed, developed, and implemented for the acquisition, storage, and processing of geotagged photographs. The software program combines three main components: a mobile application for smartphones with Android operating system (). Currently, the “Collective View” application is available for Mexico, Belize, Colombia, Guadalupe Island, and Florida (USA). This app was designed to acquire images, the coordinates of the mobile device, and other data from the phone sensors (e.g., accelerometer values, gyroscope) that help to know the phone’s attitude when a picture is taken. The overall objective of the App is to have an easy-to-use tool that stores information in the cloud simply and transparently for the user.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17224</offset><text>The data collected by the App is sent and stored in the cloud using Google’s firebase service. Although there are other tools, the purpose of using firebase is to serve as temporary storage of the information collected by users and to manage high bandwidth, as well as multiple simultaneous requests quickly and uninterruptedly. At the same time, we have a backend service that continually analyzes the data stored in the cloud and synchronizes them with our central servers, responsible for classifying and analyzing the information provided by users. All of this is done using the Python programming language and the PyTorch library running on a Lenovo workstation with an Intel Xeon EP processor, 64 GB of RAM, NVidia Quadro K5000 GPU running the Linux operating system Ubuntu 18.04 64 bits. Details and versions of all libraries are provided in a file in the Supplementary Material.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18113</offset><text>Once the data is analyzed and processed, a geographic information systems compatible shapefile is created containing the points where the images were taken. The information related to those points is visualized in the form of maps. A dashboard was designed using Arcgis Online to visualize the information, providing the user with different layers of information. Figure 1 shows each of the stages followed in the process of designing the Collective View ecosystem.</text></passage><passage><infon key="file">peerj-cs-07-528-g001.jpg</infon><infon key="id">fig-1</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>18579</offset><text>Stages in the design process of the Collective View ecosystem.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18642</offset><text>Data collection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18658</offset><text>This study was carried out in the northern (from 21.247893, −89.834655 to 21.357756, −89.115214) and eastern regions (from 21.233803, −86.801127 to 18.266237, −87.835622) in the Yucatan Peninsula, in Mexico. This area is composed of beaches where massive arrivals of Sargassum have been recorded in recent years (2015, 2018–2020).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18999</offset><text>The images gathered to build the dataset of the present study were collected through the crowdsourcing mobile app of “Collective View”. Due to the origin of the photographs, their quality is beyond our control, and may depend on users’ skills to take pictures, the environmental conditions and the device’s features (i.e., the sensor size, resolution in megapixels, etc). However, the variety of photographs (i.e., angle, background elements, lighting, lens distortion), contributes to the neural network training and improves its performance, photographs out of focus or with poor lighting also contribute to the training of the neural network. That is why it is desirable to have photographs with different characteristics. On the other hand, the information in a photograph has its own limits; a greater amount per area, per day, will allow us to achieve the best results. However, this analysis is beyond the scope of this study. These images enabled researchers to compile geotagged photographs regarding the presence or absence of Sargassum along the beaches in the Yucatan Peninsula. The geotagged photographs that individual members of society upload to the platform can be viewed through a web dashboard. At the time of conducting the experiment, the Collective View dataset had 4,525 photographs of eleven cities in the states of Yucatan and Quintana Roo. This reveals that Collective View offers the biggest dataset worldwide of the presence or absence of Sargassum along the beaches in this region. Sixty-two users participated in the construction of this data set. The images collected through the Collective View app include metadata regarding the latitude and longitude of the place where each photograph was taken, the date, the hour with minutes and seconds, as well as additional data from the gyroscopes, accelerometers, and other sensors of the smartphone. At the time of building the dataset for the training stage, the number of images on the platform was lower, therefore the sum of images of the four instances is less than the total number of images. Crowdsourcing dataset features are shown in Table 1. Currently, geotagged photographs can be consulted on the internet (). Figure 2 shows an image of the online dashboard.</text></passage><passage><infon key="file">peerj-cs-07-528-g002.jpg</infon><infon key="id">fig-2</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>21255</offset><text>Dashboard in Collective View.</text></passage><passage><infon key="file">table-1.xml</infon><infon key="id">table-1</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>21285</offset><text>Crowdsourcing dataset feature.</text></passage><passage><infon key="file">table-1.xml</infon><infon key="id">table-1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; content-type=&quot;text&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Crowdsourcing dataset features&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of images&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4,525&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of classes&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of instances with &lt;italic&gt;Sargassum&lt;/italic&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;647&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of instances without &lt;italic&gt;Sargassum&lt;/italic&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,012&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of instances with other algae&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,645&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of instances with other elements&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Metadata of the photograph&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;lat, long, date, hour, minute, second&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of cities along the beach&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of states in the Yucatán Peninsula&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of participants in the Crowdsourcing activity&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21316</offset><text>Crowdsourcing dataset features	 	Number of images	4,525	 	Number of classes	4	 	Number of instances with Sargassum	647	 	Number of instances without Sargassum	1,012	 	Number of instances with other algae	1,645	 	Number of instances with other elements	25	 	Metadata of the photograph	lat, long, date, hour, minute, second	 	Number of cities along the beach	11	 	Number of states in the Yucatán Peninsula	2	 	Number of participants in the Crowdsourcing activity	62	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21784</offset><text>The next step followed in the data collection involved the use of crowdsourcing information to generate automatic maps illustrating the accumulations of this macroalgae. These maps could also be used by citizens and tourists to know the conditions of the beaches, either for decision-making processes or simply for planning a trip. To generate these products from the collected images, two requirements needed to be met: first, the platform needed to receive a constant flow of information; and second, images needed to be classified automatically. To ensure the first requirement was met, we widely disseminated the crowdsourcing campaign and carried out various activities to raise awareness in society. It is important to mention that this requirement can pose different challenges across cultures, depending on their habits and customs, and is outside of the scope of this study. The following section describes the second requirement, the methodology used for the automatic classification of images with the presence or absence of Sargassum.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22831</offset><text>Dataset creation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22848</offset><text>The following sections describe the processes followed to build the image data set, which was later used to train the different neural network architectures for classifying images illustrating the presence or absence of Sargassum. The initial dataset used for the training, testing, and validation of the neural networks was classified manually by experts in the subject of study.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>23229</offset><text>Training and validation dataset</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23261</offset><text>To analyze the accumulation of Sargassum on the beaches in Quintana Roo and generate maps to show its distribution, we had to train and select the hyper-parameters of the neural network. This was an essential step of the process that allowed us to classify images according to the presence and absence of Sargassum. However, it is also one of the parts of the process that presented multiple challenges.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23665</offset><text>The lack of geotagged Sargassum images dataset was one of the main obstacles to overcome. In order to address this issue, we integrated a set of 2,400 images stemming from several sources of information. A total of 1,720 geotagged images were collected through Collective View, as described above Other images were collected using the Google image search engine. We conducted a search of historical images for the coasts of Quintana Roo in recent years, originating a total of 600 images, which were downloaded from the internet. Finally, 80 pictures were taken with a traditional digital camera and were added to the dataset. Although these images were not geotagged, they were used only during the training, testing and validation processes of the neural network.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24431</offset><text>Within the 2,400 images collected, 1,200 exhibited the presence of Sargassum and 1,200 images did not contain the presence of Sargassum. The dataset of images used to train the neural network, was classified by hand by experts in the field in order to ensure that the images are correctly classified and thus to train the neural network. For the validation of the neural network, we randomly selected 20% of the images (480), from which 50% contained images with Sargassum presence, and the remaining 50% of images without Sargassum presence. The remaining 80% of the original dataset of images (1,920) was randomly divided again into two subsets. The first subset contained 80% of the images (1,536), which were used for the training process. The remaining 20% of images (384) were used to test the network during the same training and hyper-parameter selection process. As one can observe, the dataset used to train the neural network is too small when compared to the classical data sets. This represented a challenge for training the neural network without falling prey to overfitting. In order to allow for reproducing the results obtained in this study, the images were stored maintaining the structure mentioned above.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25657</offset><text>Figure 3 shows an example with six Sargassum images that form part of the dataset we have built. As demonstrated in the images, there is extensive variation in the features, angles, lighting, and other characteristics of the images. This causes the classification process to be even more challenging. Augmented Data and Transfer Learning methodologies were used to address this challenge, which in turn, decreased overfitting and improved network generalization.</text></passage><passage><infon key="file">peerj-cs-07-528-g003.jpg</infon><infon key="id">fig-3</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>26120</offset><text>Sample photos of the dataset, (located in the “train/sargassum” folder). The dataset are available for download in the link: .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>26251</offset><text>Image data augmentation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26275</offset><text>Convolutional neural networks (CNN) and Deep Learning (DL) have been proven effective for image recognition as well as data processing. These techniques are based on supervised learning algorithms, so their effectiveness is strongly related to the quality and size of the datasets used for the training and validation of the models.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26608</offset><text>Nowadays, there are different open access datasets available. Most of them are mainly used for research purposes. However, many others are part of competitions that are carried out by big companies (e.g., Google and Netflix). Data sets such as MNIST, CIFAR10, ImageNet and COCO have been widely used as benchmarks to train, test, and compare new models with previous findings. One of the most relevant features of these datasets is their size. For example, MNIST used 60,000 images for training purposes and another 10,000 for the validation. Other models, such as COCO, likely have millions of data entries available.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27227</offset><text>Applying DL techniques to real problems presents multiple challenges. Since the data set is small, the diversity in the images is low, which reduces the amount of information available to the neural network during training. This will most likely cause the network to attempt to memorize the data rather than learn to generalize it (overfitting). There are multiple techniques that can be used to reduce overfitting. Some of them include dropout, batch normalization, and augmented data,,. Since the size of the dataset for training the neural network is small compared to traditional datasets, we have used two strategies to deal with this challenge; on the one hand we have used augmented data to increase the number of images in the dataset and on the other hand we have employed transfer learning to use previously trained networks with large datasets, which increases the generalization capability of the neural network.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28152</offset><text>Augmented data offers the possibility of dynamically transforming the input data. Some of the most common transformations performed include: flipping the images, zooming in/out of certain areas, random cuts, angle rotation, varying the amount of illumination and image contrast, among others. The models presented in this paper were trained and validated using augmented data. The main adjustments to the data involved a random horizontal flip, a random rotation of 10 degrees, and a variation in the parameters of brightness, contrast, and saturation. Due to the use of augmented data, the accuracy of the network is improved while overfitting is controlled for.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28816</offset><text>The two images in Fig. 4 graphically demonstrate the effect of using augmented data during the training phase (epochs) in our dataset. When using augmented data (Fig. 4A), the loss (error) between the results of the training data classification and the validation data is similar.</text></passage><passage><infon key="file">peerj-cs-07-528-g004.jpg</infon><infon key="id">fig-4</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>29097</offset><text>(A) Effects on error value when using augmented data in a CNN and (B) without augmented data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29191</offset><text>When no augmented data is used (Fig. 4B), the classification error of the training data decreases while the validation error increases, which means that the network has started to memorize the noise from the images. This causes their generalization to decrease, resulting in overfitting. In summary, we can conclude that the use of augmented data in our dataset promotes the generalization of the neural network by maintaining a balance between error and overfitting.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>29659</offset><text>Dataset classification: convolutional neural networks</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29713</offset><text>The following sections describe the different Neural Network architectures used to classify the dataset. We tested a total of three classic models. The first one consisted of using an adapted LeNet-5 CNN. The other two involved Transfer Learning, where two pre-trained models were adapted, namely the AlexNet and the VGG16 neural networks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>30053</offset><text>LeNet-5</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30061</offset><text>LeNet-5 (Fig. 5) was developed In 1989 by and was one of the earliest CNN. By modern standards, LeNet-5 is a very simple network, consisting of: two sets of convolutional and average pooling layers, a flattening convolutional layer, two fully-connected layers, and a softmax classifier. Initially, LeNet-5 was used on a large scale to automatically classify hand-written digits on bank cheques in the United States.</text></passage><passage><infon key="file">peerj-cs-07-528-g005.jpg</infon><infon key="id">fig-5</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>30477</offset><text>LeNet-5 Architecture.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30499</offset><text>Comparable to virtually all CNN-based architectures, LeNet-5 encompasses two phases, a feature extraction segment and a classification segment. Figure 5 illustrates both phases. First, given an input image of 32 × 32 pixels, it uses a convolutional layer with a filter size of 5 × 5 and stride of 1. This generates a total of 6 attribute filters of 28 × 28 pixels each (28 × 28 × 6). To obtain the relevant information of each one of the generated filters, the output is processed by an Average Pooling operation (avg-pool). Using a filter size of 2 × 2 and stride of 2, the avg-pool generates a decrease in the size of the input from 28 × 28 to 14 × 14 pixels. In the next step, a convolutional operation and avg-pool are again applied to produce a 5 × 5 × 16 output.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31278</offset><text>The second phase corresponds to the classification of the extracted attributes. Since the input of the next phase is a fully connected layer, and the output of the attribute extraction phase is a set of 5 × 5 × 16 filters, a reshaped flattened operation is used for the input elements.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31566</offset><text>During the attribute classification phase, the last layer of LeNet-5 was modified to use two neurons (one for images with Sargassum presence and one for those without Sargassum presence). Thereby, the attribute classification phase was assembled with a fully connected network composed of three layers of 120, 80 and 2 neurons respectively.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31907</offset><text>Finally, it should be noted that all layers of the model, except the last one, used the hyperbolic tangent activation function. The last layer used the softmax function. For the training process, an Adam optimizer was employed with a learning rate of 0.001. The Cross-Entropy loss function was used.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>32207</offset><text>AlexNet</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32215</offset><text>Deep CNN models may take days or even weeks to train using large datasets. One way to reduce the length of this process is to re-use the model weights from pre-trained developed models for standard computer vision benchmark datasets (i.e., the ImageNet image recognition tasks). An assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, like in the Sargassum image classification presented in this study, this may not remain true. In deep learning, transfer learning is a technique where a neural network model is trained first using a sample dataset, similar to the one that represents the problem for which it is being created. Then, one or more layers from the trained model are used (learning transfer) in the training of the new model which responds to the real problem.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33144</offset><text>AlexNet (Fig. 6) has 60 million parameters and 650,000 neurons. It contains eight learned layers. Among these eight layers, we find five convolutional layers, some of which are followed by max-pooling layers; and three fully connected layers, using the ReLu activation function. We used the PyTorch library and the Python programming language to lock all AlexNet attribute extraction layers. We also modified the last classification layer, which enabled us to adapt the output of AlexNet to the characteristics of our dataset. Through the application of augmented data and transfer learning, we were able to classify our dataset. For the training process, an Adam optimizer was employed with a learning rate of 0.0001. The Cross-Entropy loss function was used, and a Batch Size of 100 images for each epoch was employed. A total of 80% of the dataset was randomly selected for the training phase, while the remaining 20% was used during the validation phase. As demonstrated later in this section, we used Google Colab, as well as an execution environment using GPUs as processing units, to perform the AlexNet training. This resulted in nearly 90% accuracy, maintaining an adequate balance between the error and the overfitting.</text></passage><passage><infon key="file">peerj-cs-07-528-g006.jpg</infon><infon key="id">fig-6</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>34374</offset><text>AlexNet Architecture.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>34396</offset><text>VGG16</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34402</offset><text>The VGG16 is a CNN model proposed by K. Simonyan and A. Zisserman, from the Visual Geometry Group (VGG) at University of Oxford in 2014. The VGG16 was created with the purpose of enhancing classification accuracy by increasing the depth of the CNNs. The 16 in the VGG16 name (or 19 in the case of the VGG19) refers to the number of layers. In this case, the VGG16 model has 16 layers with trainable parameters. This network is quite large and comprises approximately 138 million of parameters in all. The VGG16 is considered one of the best computer vision models to date. What is unique about the VGG16 is that, instead of containing a large number of hyper-parameters, it is composed of convolution layers with very small filters.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35135</offset><text>The input layer of the VGG16 is an RGB image of 224 × 244 pixels. The image is processed by a stack of convolutional layers with the fixed filter size of 3 × 3 and stride of 1. There are five max pooling filters embedded between the convolutional layers in order to down-sample the input representation. The stack of convolutional layers is followed by three fully connected layers, consisting of 4,096, 4,096 and 1,000 channels, respectively. The last layer is a soft-max layer (Fig. 7).</text></passage><passage><infon key="file">peerj-cs-07-528-g007.jpg</infon><infon key="id">fig-7</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>35626</offset><text>VGG16 Architecture.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35646</offset><text>Similar to the process carried out with AlexNet, we modified the last classification layer of the VGG16 to adapt it to the number of classifications in our dataset. We employed transfer learning, a model previously trained for ImageNet. All feature extraction layer weights were locked, so during the training phase, only the classification layer weights were retrained. Augmented data for the training process was used as well as the Adam optimizer, with a learning rate of 0.001. The Cross-Entropy loss function was used.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>36170</offset><text>Results and discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36193</offset><text>To classify the image dataset, we used three types of convolutional neural networks: LeNet-5, AlexNet, and VGG16. Each of them were adjusted for the classification process using our dataset. LeNet-5 was modified to work with RGB (three-band) images. The last layer of 1,000 categories in the AlexNet and the VGG16 architectures was replaced by a fully-connected layer for two classes (Sargassum and non-Sargassum images). Finally, for both the AlexNet and the VGG, we used augmented data and pre-trained models (transfer learning) to maximize the generalization capacity of the network for the given dataset.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36802</offset><text>The training and testing of these architectures phases were carried out using different batch sizes and learning rates to identify which of the proposed architectures offered the best classification results (hyper-parameter search). Figure 8 shows the accuracy of each of the proposed architectures for a batch size of 20 and a learning rate of 0.001. As demonstrated, the lowest performance resulted when using the LeNet-5 architecture (Fig. 8A). On the contrary, the best performance indicators could be noted using the VGG and the AlexNet architectures (Figs. 8B and 8C). Nevertheless, as illustrated in Fig. 9, the VGG16 architecture reveals premature overfitting. Overfitting can be observed in the VGG16 architecture almost from the beginning of the training, around epoch 20 (Fig. 9B). In the case of the AlexNet architecture, overfitting started around epoch 40 (Fig. 9C). These findings suggest that, under these parameters, AlexNet has a better capacity of generalization than its competitors.</text></passage><passage><infon key="file">peerj-cs-07-528-g008.jpg</infon><infon key="id">fig-8</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>37806</offset><text>Accuracy results for batch size = 20 and learning rate = 0.001 for (A) LeNet-5, (B) VGG16, and (C) AlexNet.</text></passage><passage><infon key="file">peerj-cs-07-528-g009.jpg</infon><infon key="id">fig-9</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>37914</offset><text>Loss results for batch size = 20 and learning rate = 0.001 for (A) LeNet-5, (B) VGG16, and (C) AlexNet.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38018</offset><text>Figures 10 and 11 show the results obtained when adjusting the batch size from 20 to 100. As illustrated in Fig. 10, the behavior of the models is similar to the results presented in Fig. 8. However, one main difference can be observed in Fig. 10. In the case of VGG16 (Fig. 10B), the increase in batch size had a negative impact because the overfitting problem occurred practically at the beginning of the training process. In the case of the AlexNet architecture (Fig. 10C), the impact is considered positive since it allows the overfitting to occur later. This has a effect on the model’s capability of generalization. When comparing Figs. 8 and 10, it is clearly proven that AlexNet gained 20 additional training epochs. Therefore, it is possible to conclude that the increase in batch size enhanced AlexNet’s generalization ability by preventing overfitting.</text></passage><passage><infon key="file">peerj-cs-07-528-g010.jpg</infon><infon key="id">fig-10</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>38886</offset><text>Accuracy results for batch size = 100 and learning rate = 0.001 for (A) LeNet-5, (B) VGG16, and (C) AlexNet.</text></passage><passage><infon key="file">peerj-cs-07-528-g011.jpg</infon><infon key="id">fig-11</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>38995</offset><text>Loss results for batch size = 100 and learning rate = 0.001 for (A) LeNet-5, (B) VGG16, and (C) AlexNet.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39100</offset><text>Table 2 shows the basic statistical results obtained during the training process of the three models. As demonstrated, the architectures with the highest accuracy rates are the AlexNet and the VGG16. However, as illustrated in Figs. 9 and 10, the VGG16 is more likely to present overfitting.</text></passage><passage><infon key="file">table-2.xml</infon><infon key="id">table-2</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>39392</offset><text>Accuracy rate.</text></passage><passage><infon key="file">table-2.xml</infon><infon key="id">table-2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; content-type=&quot;text&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Max(%)&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Min(%)&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean(%)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AlexNet&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.54&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.50&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.09&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGG16&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.54&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.88&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.37&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LeNet-5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.67&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.13&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;73.64&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>39407</offset><text>Model	Max(%)	Min(%)	Mean(%)	 	AlexNet	93.54	82.50	91.09	 	VGG16	93.54	81.88	90.37	 	LeNet-5	76.67	63.13	73.64	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39520</offset><text>After analyzing the behavior of the different neural networks studied so far, and modifying some of their hyper-parameters, findings from this study indicate that the AlexNet architecture demonstrated the best performance. In consequence, AlexNet exhibited the best capacity for generalization while maintaining the lowest overfitting. As shown in Fig. 10, the AlexNet neural network does not begin to present overfitting behaviors until the 40’s epoch. With all this information, we proceeded to retrain the AlexNet neural network, using the following parameters: learning speed of 0.001, batch size of 100, and 40 training periods.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40156</offset><text>Figure 12 shows AlexNets’ evolution during the training period. As demonstrated, there is no indication of overfitting. Evidently, the capacity of generalization of the final model is superior to those shown previously. The accuracy of the model (Fig. 12A), using the evaluation dataset consistently improved the results with the training dataset. It is also clear that because the AlexNet loss (Fig. 12B) for the evaluation dataset remained below the test dataset, the accuracy achieved by the network during training is of 94%.</text></passage><passage><infon key="file">peerj-cs-07-528-g012.jpg</infon><infon key="id">fig-12</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>40688</offset><text>AlexNet for batch = 100 and learning rate = 0.001. (A) Accuracy and (B) Loss.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40766</offset><text>Figure 13 shows the confusion matrices for the studied architectures. The highest number of false positives and false negatives were obtained by the LeNet-5 neural network. In the case of the VGG16, a bias was observed when the network confused images with and without Sargassum in a high proportion. The AlexNet presented a less biased and more balanced behavior, resulting in a neural network with a higher capacity of generalization.</text></passage><passage><infon key="file">peerj-cs-07-528-g013.jpg</infon><infon key="id">fig-13</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>41203</offset><text>Confusion Matrix for 480 test images. (A) LeNet-5, (B) VGG16, and (C) AlexNet.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41282</offset><text>Finally, Table 3 demonstrates the precision, recall, F1-score, and support results for the AlexNet. It is clearly evidenced that the evaluation dataset is balanced. The results show a maximum recall of 94% and an f1 score of 92%. This is considered a significant result considering that the AlexNet neural network training was performed with a relatively small set of balanced images.</text></passage><passage><infon key="file">table-3.xml</infon><infon key="id">table-3</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>41667</offset><text>AlexNet classification results.</text></passage><passage><infon key="file">table-3.xml</infon><infon key="id">table-3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; content-type=&quot;text&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1-score (%)&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Support&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;Sargassum&lt;/italic&gt; absence&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;240 images&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;Sargassum&lt;/italic&gt; presence&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;240 images&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;480 images&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>41699</offset><text>	Precision (%)	Recall (%)	F1-score (%)	Support	 	Sargassum absence	90	94	92	240 images	 	Sargassum presence	93	90	91	240 images	 	Accuracy			92	480 images	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41857</offset><text>In the case of the Sargassum monitoring along the beaches, it is important that service providers, tourists, visitors, and authorities in charge of beach sanitation know where it accumulates for decision-making processes. There is evidence in social networks (), of some citizen initiatives proposing maps that indicate the presence of Sargassum along the coast. However, because these maps are hand-designed, they present some disadvantages. Some of these include a quick expiration and the lack of measures to verify the accuracy or reliability of the sources used for map generation. Therefore, a next step followed was to classify the geotagged images from the Collective View platform to automatically generate maps showing the presence or absence of Sargassum along the beaches. As far as we know, the present study proposes the first automatic system to generate a Sargassum presence or absence map that helps society manage high concentrations of Sargassum along the beaches.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42841</offset><text>The first approximations of maps that have been generated from previously classified images are shown in Figs. 14 and 15. Photographs classified with and without the presence of Sargassum, and grouped by region and cities, along the coast in the Yucatan Peninsula are shown in Fig. 14. The images were taken between August 15 and December 18, 2019. The red dots represent the presence of Sargassum in the images, and the blue dots represent the absence of Sargassum.</text></passage><passage><infon key="file">peerj-cs-07-528-g014.jpg</infon><infon key="id">fig-14</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>43308</offset><text>Sargassum distribution map.</text></passage><passage><infon key="file">peerj-cs-07-528-g014.jpg</infon><infon key="id">fig-14</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>43336</offset><text>Sargassum distribution map built with the geotagged photographs previously classified, collected through crowdsourcing.</text></passage><passage><infon key="file">peerj-cs-07-528-g015.jpg</infon><infon key="id">fig-15</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>43456</offset><text>Sargassum distribution map along the beaches of three cities.</text></passage><passage><infon key="file">peerj-cs-07-528-g015.jpg</infon><infon key="id">fig-15</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>43518</offset><text>Sargassum distribution map along the beaches of three cities, built with the geotagged photographs previously classified, collected through crowdsourcing. (A) Mahahual, Q. Roo, (B) Playa delCarmen, Q. Roo and (C) Puerto Progreso, Yucatan.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43757</offset><text>Figure 15 allows for a closer examination of the beaches in three cities in the region, namely Mahahual (Fig. 15A), Playa del Carmen (Fig. 15B), and Puerto Progreso (Fig. 15C). In this case, the position of each point represents the place where the photograph was taken. Again, the red dots represent the presence of Sargassum in the images, and the blue dots represent the absence of Sargassum. The photographs in the maps in Fig. 15 were taken on different days. The analysis of each situation required adding information such as the time and date the photo was taken, if the area was periodically cleaned, among other factors. As an example, in Mahahual, most of the photographs were taken between September 14 and 15. This is a vacation weekend at the end of the Sargassum arrival season, and as demonstrated, the Sargassum density is highest in the north and south of the hotels and restaurants area, which suggests that service providers cleaned their beaches to improve their service to tourists. In the case of Playa del Carmen, the photographs in front of the City Center were taken between November 28 and 29, when the arrival season for Sargassum had already ended. The photographs northeast of the City Center were taken between August 21 and 22, during the arrival season. Finally, in the case of Puerto Progreso, the map shows points that correspond to photographs taken between August 15 and December 18, 2019. Thus, to see the detail by month, week, or day, additional maps are required to the study date period. The economic activities are more diverse in Puerto Progreso than in Mahahual or Playa del Carmen, so the presence of Sargassum in Progreso is not as critical. Therefore, the beaches are not cleaned as frequently as in the other sites.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45521</offset><text>The maps demonstrating the presence or absence of Sargassum are a first approximation of what is possible to achieve with data obtained from crowdsourcing. The information exhibited in Figs. 14 and 15 reflect this initial approximation. It is known that Sargassum monitoring is dynamic in time, and that these maps represent the distribution of Sargassum between August 15 and December 18 in 2019. Therefore, further research should explore generating automatized maps by uploading daily pictures to the platform. However, achieving this goal requires a constant flow of information. Another area for further research could be to develop methods to assess the coverage of the Sargassum within the images, and label them based on the amount of Sargassum in the area (e.g., excessive, a lot, or little). As a validation of the methodology, we consider that it has been a success; however, we also know that the basis of this proposal relies on constant citizen participation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>46495</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>46507</offset><text>Sargassum monitoring through traditional satellite remote sensing is not always effective due to various factors. Some limitations that traditional methods cannot account for include: environmental conditions in the region (e.g., high humidity and the presence of clouds); the conditions of the coastline which have been mentioned above; as well as the spatial and temporal limitations of the satellite platforms. Therefore, we consider this study proposes a significant and viable addition to satellite monitoring.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>47023</offset><text>Supported by a crowdsourcing application called Collective View, which was developed by the authors and with the contribution of members of society, this study offers the largest set of geotagged images in the world for the northern region in the Yucatan Peninsula and the Mexican Caribbean. At the time of this study, there were 4035 photographs in the platform. Additionally, using augmented data and knowledge transfer, the AlexNet neural network was trained and reached a maximum recall of 94% and an f1 score of 92%. This can be considered a good result considering that the training was performed with a relatively small set of balanced images.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>47674</offset><text>Through the use of an automatic classification of geotagged images, the present study proposes an automatic system to generate Sargassum presence and/or absence maps that support society to manage high concentrations of Sargassum along the beaches in the Yucatan Peninsula. To the best of our knowledge, this is the first proposal of its kind for this region. For the continuous updating of the maps, it is necessary to capture daily images from different zones and classify them automatically. In this way, it will be possible to take advantage of this technology for the benefit of service providers, authorities involved in the sanitation of beaches, as well as tourists, and visitors. The constant participation of society, as a whole, is required for the full use of current technologies.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>48468</offset><text>Regarding Sargassum observation, important efforts have been implemented to monitor pelagic Sargassum and other macroalgal abundance and distribution. However, these efforts use a wide variety of methods that are often not comparable. We believe that it is necessary to build a common platform to facilitate communication and collaboration regarding this issue. In this sense, combining crowdsourcing, current communication technologies, and Artificial Intelligence techniques like neural networks and computer vision, are a viable option to build maps on a global and regional scale in near real-time. Generating maps automatically requires an instant classification of the collected information. Thus, automatic classification becomes relevant.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49215</offset><text>The operation of the crowdsourcing platform remains a technological challenge. Nevertheless, in our experience, the main challenge of crowdsourcing was to maintain the permanent involvement of society members throughout the study. A more difficult challenge posed is for society to adopt crowdsourcing as a useful and routine habit in this constantly changing digital civilization.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49597</offset><text>One of the goals of our future work involves completing and exploiting the Sargassum dataset. Although we achieved our initial goal of creating the largest dataset in the world of geotagged images for the coasts of Quintana Roo, crowdsourcing also allows for images from different countries around the Caribbean Sea to be entered. In fact, the dataset currently has some images of Guadeloupe and Florida. Thus, an ongoing challenge is that the geotagged dataset continues to grow with images from different places and dates.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>50122</offset><text>A second goal of our future work is related to the coverage of Sargassum within images. As shown throughout this work, images were classified in two categories: images with and without Sargassum. This is an arduous process that requires the use of various techniques to be performed successfully. As part of our future work, we will try to determine the coverage of the Sargassum within the images, developing a method that allows an effective semantic segmentation of the images to measure the amount of Sargassum present on the beaches. This will be supported by machine learning techniques to create models that allow us to accurately estimate the coverage of this macro-algae along the beaches in Quintana Roo.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>50837</offset><text>Supplemental Information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>50862</offset><text>Additional Information and Declarations</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>50902</offset><text>Competing Interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>50922</offset><text>The authors declare that they have no competing interests.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>50981</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>51002</offset><text>Javier Arellano-Verdejo conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>51248</offset><text>Hugo E. Lazcano-Hernández conceived and designed the experiments, performed the experiments, analyzed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>51465</offset><text>Data Availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>51483</offset><text>The following information was supplied regarding data availability:</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>51551</offset><text>Data for the Neural Networks training is available at figshare: Arellano-Verdejo, Javier; Lazcano-Hernandez, Hugo E. (2020): Sargassum Dataset. figshare. Dataset. .</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>51716</offset><text>References</text></passage><passage><infon key="elocation-id">e00938</infon><infon key="issue">11</infon><infon key="name_0">surname:Abiodun;given-names:OI</infon><infon key="name_1">surname:Jantan;given-names:A</infon><infon key="name_2">surname:Omolara;given-names:AE</infon><infon key="name_3">surname:Dada;given-names:KV</infon><infon key="name_4">surname:Mohamed;given-names:NA</infon><infon key="name_5">surname:Arshad;given-names:H</infon><infon key="pub-id_doi">10.1016/j.heliyon.2018.e00938</infon><infon key="pub-id_pmid">30519653</infon><infon key="section_type">REF</infon><infon key="source">Heliyon</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2018</infon><offset>51727</offset><text>State-of-the-art in artificial neural network applications: a survey</text></passage><passage><infon key="fpage">25</infon><infon key="lpage">33</infon><infon key="name_0">surname:Álvarez-Carranza;given-names:G</infon><infon key="name_1">surname:Lazcano-Hernández;given-names:HE</infon><infon key="section_type">REF</infon><infon key="source">International Congress of Telematics and Computing</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>51796</offset><text>Methodology to create geospatial modis dataset</text></passage><passage><infon key="fpage">61</infon><infon key="lpage">70</infon><infon key="name_0">surname:Arellano-Verdejo;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">International Congress of Telematics and Computing</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>51843</offset><text>Moderate resolution imaging spectroradiometer products classification using deep learning</text></passage><passage><infon key="fpage">49</infon><infon key="lpage">62</infon><infon key="name_0">surname:Arellano-Verdejo;given-names:J</infon><infon key="name_1">surname:Lazcano-Hernandez;given-names:HE</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>51933</offset><text>Crowdsourcing for Sargassum monitoring along the beaches in Quintana Roo</text></passage><passage><infon key="elocation-id">e6842</infon><infon key="issue">4</infon><infon key="name_0">surname:Arellano-Verdejo;given-names:J</infon><infon key="name_1">surname:Lazcano-Hernandez;given-names:HE</infon><infon key="name_2">surname:Cabanillas-Terán;given-names:N</infon><infon key="pub-id_doi">10.7717/peerj.6842</infon><infon key="pub-id_pmid">31106059</infon><infon key="section_type">REF</infon><infon key="source">PeerJ</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2019</infon><offset>52006</offset><text>ERISNet: deep neural network for Sargassum detection along the coastline of the Mexican Caribbean</text></passage><passage><infon key="elocation-id">e7589</infon><infon key="issue">4</infon><infon key="name_0">surname:Cabanillas-Terán;given-names:N</infon><infon key="name_1">surname:Hernández-Arana;given-names:HA</infon><infon key="name_2">surname:Ruiz-Zárate;given-names:M-Á</infon><infon key="name_3">surname:Vega-Zepeda;given-names:A</infon><infon key="name_4">surname:Sanchez-Gonzalez;given-names:A</infon><infon key="pub-id_doi">10.7717/peerj.7589</infon><infon key="pub-id_pmid">31531271</infon><infon key="section_type">REF</infon><infon key="source">PeerJ</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2019</infon><offset>52104</offset><text>Sargassum blooms in the caribbean alter the trophic structure of the sea urchin diadema antillarum</text></passage><passage><infon key="fpage">317</infon><infon key="name_0">surname:Duffy;given-names:JE</infon><infon key="name_1">surname:Benedetti-Cecchi;given-names:L</infon><infon key="name_10">surname:Cullen-Unsworth;given-names:LC</infon><infon key="name_11">surname:Diaz-Pulido;given-names:G</infon><infon key="name_12">surname:Duarte;given-names:CM</infon><infon key="name_13">surname:Edgar;given-names:GJ</infon><infon key="name_14">surname:Fortes;given-names:M</infon><infon key="name_15">surname:Goni;given-names:G</infon><infon key="name_16">surname:Hu;given-names:C</infon><infon key="name_17">surname:Huang;given-names:X</infon><infon key="name_18">surname:Hurd;given-names:CL</infon><infon key="name_19">surname:Johnson;given-names:C</infon><infon key="name_2">surname:Trinanes;given-names:J</infon><infon key="name_20">surname:Konar;given-names:B</infon><infon key="name_21">surname:Krause-Jensen;given-names:D</infon><infon key="name_22">surname:Krumhansl;given-names:K</infon><infon key="name_23">surname:Macreadie;given-names:P</infon><infon key="name_24">surname:Marsh;given-names:H</infon><infon key="name_25">surname:McKenzie;given-names:LJ</infon><infon key="name_26">surname:Mieszkowska;given-names:N</infon><infon key="name_27">surname:Miloslavich;given-names:P</infon><infon key="name_28">surname:Montes;given-names:E</infon><infon key="name_29">surname:Nakaoka;given-names:M</infon><infon key="name_3">surname:Muller-Karger;given-names:FE</infon><infon key="name_30">surname:Norderhaug;given-names:KM</infon><infon key="name_31">surname:Norlund;given-names:LM</infon><infon key="name_32">surname:Orth;given-names:RJ</infon><infon key="name_33">surname:Prathep;given-names:A</infon><infon key="name_34">surname:Putman;given-names:NF</infon><infon key="name_35">surname:Samper-Villarreal;given-names:J</infon><infon key="name_36">surname:Serrao;given-names:EA</infon><infon key="name_37">surname:Short;given-names:F</infon><infon key="name_38">surname:Pinto;given-names:IS</infon><infon key="name_39">surname:Steinberg;given-names:P</infon><infon key="name_4">surname:Ambo-Rappe;given-names:R</infon><infon key="name_40">surname:Stuart-Smith;given-names:R</infon><infon key="name_41">surname:Unsworth;given-names:RKF</infon><infon key="name_42">surname:Van Keulen;given-names:M</infon><infon key="name_43">surname:Van Tussenbroek;given-names:BI</infon><infon key="name_44">surname:Wang;given-names:M</infon><infon key="name_45">surname:Waycott;given-names:M</infon><infon key="name_46">surname:Weatherdon;given-names:LV</infon><infon key="name_47">surname:Wernberg;given-names:T</infon><infon key="name_48">surname:Yaakub;given-names:SM</infon><infon key="name_5">surname:Bostr’om;given-names:C</infon><infon key="name_6">surname:Buschmann;given-names:AH</infon><infon key="name_7">surname:Byrnes;given-names:J</infon><infon key="name_8">surname:Coles;given-names:RG</infon><infon key="name_9">surname:Creed;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in Marine Science</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2019</infon><offset>52203</offset><text>Toward a coordinated global observing system for seagrasses and marine macroalgae</text></passage><passage><infon key="name_0">surname:Frias-Martinez;given-names:V</infon><infon key="name_1">surname:Frias-Martinez;given-names:E</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>52285</offset><text>Crowdsourcing land use maps via twitter</text></passage><passage><infon key="fpage">1925</infon><infon key="issue">16</infon><infon key="lpage">1933</infon><infon key="name_0">surname:Good;given-names:BM</infon><infon key="name_1">surname:Su;given-names:AI</infon><infon key="pub-id_doi">10.1093/bioinformatics/btt333</infon><infon key="pub-id_pmid">23782614</infon><infon key="section_type">REF</infon><infon key="source">Bioinformatics</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2013</infon><offset>52325</offset><text>Crowdsourcing for bioinformatics</text></passage><passage><infon key="fpage">764</infon><infon key="issue">8</infon><infon key="lpage">773</infon><infon key="name_0">surname:Gower;given-names:J</infon><infon key="name_1">surname:Young;given-names:E</infon><infon key="name_2">surname:King;given-names:S</infon><infon key="pub-id_doi">10.1080/2150704X.2013.796433</infon><infon key="section_type">REF</infon><infon key="source">Remote Sensing Letters</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2013</infon><offset>52358</offset><text>Satellite images suggest a new Sargassum source region in 2011</text></passage><passage><infon key="fpage">153</infon><infon key="issue">4</infon><infon key="lpage">162</infon><infon key="name_0">surname:Graaf;given-names:Sv d</infon><infon key="pub-id_doi">10.17645/mac.v6i4.1710</infon><infon key="section_type">REF</infon><infon key="source">Media and Communication</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2018</infon><offset>52421</offset><text>In waze we trust: algorithmic governance of the public sphere</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="lpage">11</infon><infon key="name_0">surname:Grabler;given-names:F</infon><infon key="name_1">surname:Agrawala;given-names:M</infon><infon key="name_2">surname:Sumner;given-names:RW</infon><infon key="name_3">surname:Pauly;given-names:M</infon><infon key="pub-id_doi">10.1145/1360612.1360699</infon><infon key="section_type">REF</infon><infon key="source">ACM Transactions on Graphics (TOG)</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2008</infon><offset>52483</offset><text>Automatic generation of tourist maps</text></passage><passage><infon key="fpage">1</infon><infon key="issue">6</infon><infon key="lpage">4</infon><infon key="name_0">surname:Howe;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Wired magazine</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2006</infon><offset>52520</offset><text>The rise of crowdsourcing</text></passage><passage><infon key="fpage">2118</infon><infon key="issue">10</infon><infon key="lpage">2129</infon><infon key="name_0">surname:Hu;given-names:C</infon><infon key="pub-id_doi">10.1016/j.rse.2009.05.012</infon><infon key="section_type">REF</infon><infon key="source">Remote Sensing of Environment</infon><infon key="type">ref</infon><infon key="volume">113</infon><infon key="year">2009</infon><offset>52546</offset><text>A novel ocean color index to detect floating algae in the global oceans</text></passage><passage><infon key="fpage">10</infon><infon key="lpage">15</infon><infon key="name_0">surname:Hu;given-names:C</infon><infon key="name_1">surname:Murch;given-names:B</infon><infon key="name_10">surname:Siuda;given-names:A</infon><infon key="name_2">surname:Barnes;given-names:BB</infon><infon key="name_3">surname:Wang;given-names:M</infon><infon key="name_4">surname:Maréchal;given-names:J-P</infon><infon key="name_5">surname:Franks;given-names:J</infon><infon key="name_6">surname:Johnson;given-names:D</infon><infon key="name_7">surname:Lapointe;given-names:B</infon><infon key="name_8">surname:Goodwin;given-names:D</infon><infon key="name_9">surname:Schell;given-names:J</infon><infon key="pub-id_doi">10.1029/2016EO058355</infon><infon key="section_type">REF</infon><infon key="source">Eos</infon><infon key="type">ref</infon><infon key="volume">97</infon><infon key="year">2016</infon><offset>52618</offset><text>Sargassum watch warns of incoming seaweed</text></passage><passage><infon key="fpage">448</infon><infon key="lpage">456</infon><infon key="name_0">surname:Ioffe;given-names:S</infon><infon key="name_1">surname:Szegedy;given-names:C</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>52660</offset><text>Batch normalization: accelerating deep network training by reducing internal covariate shift</text></passage><passage><infon key="fpage">158</infon><infon key="issue">5</infon><infon key="name_0">surname:Karasov;given-names:O</infon><infon key="name_1">surname:Heremans;given-names:S</infon><infon key="name_2">surname:Külvik;given-names:M</infon><infon key="name_3">surname:Domnich;given-names:A</infon><infon key="name_4">surname:Chervanyov;given-names:I</infon><infon key="pub-id_doi">10.3390/land9050158</infon><infon key="section_type">REF</infon><infon key="source">Land</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2020</infon><offset>52753</offset><text>On how crowdsourced data and landscape organisation metrics can facilitate the mapping of cultural ecosystem services: an estonian case study</text></passage><passage><infon key="fpage">2278</infon><infon key="issue">11</infon><infon key="lpage">2324</infon><infon key="name_0">surname:LeCun;given-names:Y</infon><infon key="name_1">surname:Bottou;given-names:L</infon><infon key="name_2">surname:Bengio;given-names:Y</infon><infon key="name_3">surname:Haffner;given-names:P</infon><infon key="pub-id_doi">10.1109/5.726791</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE</infon><infon key="type">ref</infon><infon key="volume">86</infon><infon key="year">1998</infon><offset>52895</offset><text>Gradient-based learning applied to document recognition</text></passage><passage><infon key="fpage">54</infon><infon key="lpage">63</infon><infon key="name_0">surname:Maréchal;given-names:J-P</infon><infon key="name_1">surname:Hellio;given-names:C</infon><infon key="name_2">surname:Hu;given-names:C</infon><infon key="pub-id_doi">10.1016/j.rsase.2017.01.001</infon><infon key="section_type">REF</infon><infon key="source">Remote Sensing Applications: Society and Environment</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>52951</offset><text>A simple, fast, and reliable method to predict Sargassum washing ashore in the Lesser Antilles</text></passage><passage><infon key="fpage">394</infon><infon key="issue">7</infon><infon key="lpage">395</infon><infon key="name_0">surname:Maurer;given-names:AS</infon><infon key="name_1">surname:De Neef;given-names:E</infon><infon key="name_2">surname:Stapleton;given-names:S</infon><infon key="pub-id_doi">10.1890/1540-9295-13.7.394</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in Ecology and the Environment</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2015</infon><offset>53046</offset><text>Sargassum accumulation may spell trouble for nesting sea turtles</text></passage><passage><infon key="fpage">1345</infon><infon key="issue">10</infon><infon key="lpage">1359</infon><infon key="name_0">surname:Pan;given-names:SJ</infon><infon key="name_1">surname:Yang;given-names:Q</infon><infon key="pub-id_doi">10.1109/TKDE.2009.191</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Knowledge and Data Engineering</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2009</infon><offset>53111</offset><text>A survey on transfer learning</text></passage><passage><infon key="fpage">8026</infon><infon key="lpage">8037</infon><infon key="name_0">surname:Paszke;given-names:A</infon><infon key="name_1">surname:Gross;given-names:S</infon><infon key="name_10">surname:Desmaison;given-names:A</infon><infon key="name_11">surname:Köpf;given-names:A</infon><infon key="name_12">surname:Yang;given-names:E</infon><infon key="name_13">surname:DeVito;given-names:Z</infon><infon key="name_14">surname:Raison;given-names:M</infon><infon key="name_15">surname:Tejani;given-names:A</infon><infon key="name_16">surname:Chilamkurthy;given-names:S</infon><infon key="name_17">surname:Steiner;given-names:B</infon><infon key="name_18">surname:Fang;given-names:L</infon><infon key="name_19">surname:Bai;given-names:J</infon><infon key="name_2">surname:Massa;given-names:F</infon><infon key="name_20">surname:Chintala;given-names:S</infon><infon key="name_3">surname:Lerer;given-names:A</infon><infon key="name_4">surname:Bradbury;given-names:J</infon><infon key="name_5">surname:Chanan;given-names:G</infon><infon key="name_6">surname:Killeen;given-names:T</infon><infon key="name_7">surname:Lin;given-names:Z</infon><infon key="name_8">surname:Gimelshein;given-names:N</infon><infon key="name_9">surname:Antiga;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2019</infon><offset>53141</offset><text>Pytorch: an imperative style, high-performance deep learning library</text></passage><passage><infon key="fpage">1521</infon><infon key="lpage">1531</infon><infon key="name_0">surname:Qiu;given-names:S</infon><infon key="name_1">surname:Psyllidis;given-names:A</infon><infon key="name_2">surname:Bozzon;given-names:A</infon><infon key="name_3">surname:Houben;given-names:G-J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>53210</offset><text>Crowd-mapping urban objects from street-level imagery</text></passage><passage><infon key="fpage">201</infon><infon key="issue">8</infon><infon key="lpage">205</infon><infon key="name_0">surname:Rodríguez-Martínez;given-names:R</infon><infon key="name_1">surname:Medina-Valmaseda;given-names:A</infon><infon key="name_2">surname:Blanchon;given-names:P</infon><infon key="name_3">surname:Monroy-Velázquez;given-names:L</infon><infon key="name_4">surname:Almazán-Becerril;given-names:A</infon><infon key="name_5">surname:Delgado-Pech;given-names:B</infon><infon key="name_6">surname:Vásquez-Yeomans;given-names:L</infon><infon key="name_7">surname:Francisco;given-names:V</infon><infon key="name_8">surname:Garca-Rivas;given-names:M</infon><infon key="pub-id_doi">10.1016/j.marpolbul.2019.06.015</infon><infon key="pub-id_pmid">31426147</infon><infon key="section_type">REF</infon><infon key="source">Marine Pollution Bulletin</infon><infon key="type">ref</infon><infon key="volume">146</infon><infon key="year">2019</infon><offset>53264</offset><text>Faunal mortality associated with massive beaching and decomposition of pelagic sargassum</text></passage><passage><infon key="elocation-id">e8667</infon><infon key="name_0">surname:Rodríguez-Martínez;given-names:RE</infon><infon key="name_1">surname:Roy;given-names:PD</infon><infon key="name_2">surname:Torrescano-Valle;given-names:N</infon><infon key="name_3">surname:Cabanillas-Terán;given-names:N</infon><infon key="name_4">surname:Carrillo-Domnguez;given-names:S</infon><infon key="name_5">surname:Collado-Vides;given-names:L</infon><infon key="name_6">surname:Garca-Sánchez;given-names:M</infon><infon key="name_7">surname:Van Tussenbroek;given-names:BI</infon><infon key="pub-id_doi">10.7717/peerj.8667</infon><infon key="pub-id_pmid">32149030</infon><infon key="section_type">REF</infon><infon key="source">PeerJ</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>53353</offset><text>Element concentrations in pelagic Sargassum along the mexican caribbean coast in 2018–2019</text></passage><passage><infon key="comment">2016</infon><infon key="fpage">352</infon><infon key="lpage">365</infon><infon key="name_0">surname:Rodríguez-Martínez;given-names:RE</infon><infon key="name_1">surname:Van Tussenbroek;given-names:B</infon><infon key="name_2">surname:Jordán-Dahlgren;given-names:E</infon><infon key="name_3">surname:García-Mendoza;given-names:E</infon><infon key="name_4">surname:Quijano-Scheggia;given-names:SI</infon><infon key="name_5">surname:Olivos-Ortiz;given-names:A</infon><infon key="name_6">surname:Núñez-Vázquez;given-names:EJ</infon><infon key="section_type">REF</infon><infon key="source">Florecimientos Algales Nocivos en México</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>53446</offset><text>Afluencia masiva de sargazo pelágico a la costa del caribe mexicano (2014–2015)</text></passage><passage><infon key="fpage">60</infon><infon key="issue">1</infon><infon key="name_0">surname:Shorten;given-names:C</infon><infon key="name_1">surname:Khoshgoftaar;given-names:TM</infon><infon key="pub-id_doi">10.1186/s40537-019-0197-0</infon><infon key="section_type">REF</infon><infon key="source">Journal of Big Data</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2019</infon><offset>53529</offset><text>A survey on image data augmentation for deep learning</text></passage><passage><infon key="name_0">surname:Simonyan;given-names:K</infon><infon key="name_1">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>53583</offset><text>Very deep convolutional networks for large-scale image recognition</text></passage><passage><infon key="fpage">1929</infon><infon key="issue">1</infon><infon key="lpage">1958</infon><infon key="name_0">surname:Srivastava;given-names:N</infon><infon key="name_1">surname:Hinton;given-names:G</infon><infon key="name_2">surname:Krizhevsky;given-names:A</infon><infon key="name_3">surname:Sutskever;given-names:I</infon><infon key="name_4">surname:Salakhutdinov;given-names:R</infon><infon key="section_type">REF</infon><infon key="source">The Journal of Machine Learning Research</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2014</infon><offset>53650</offset><text>Dropout: a simple way to prevent neural networks from overfitting</text></passage><passage><infon key="fpage">743</infon><infon key="name_0">surname:Uribe-Martnez;given-names:A</infon><infon key="name_1">surname:Guzmán-Ramrez;given-names:A</infon><infon key="name_2">surname:Arregun-Sánchez;given-names:F</infon><infon key="name_3">surname:Cuevas;given-names:E</infon><infon key="section_type">REF</infon><infon key="source">Gobernanza y Manejo de las Costas y Mares ante la Incertidumbre</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>53716</offset><text>El sargazo en el caribe Mexicano, revisión de una historia impensable</text></passage><passage><infon key="fpage">272</infon><infon key="issue">1–2</infon><infon key="lpage">281</infon><infon key="name_0">surname:Van Tussenbroek;given-names:BI</infon><infon key="name_1">surname:Arana;given-names:HAH</infon><infon key="name_2">surname:Rodríguez-Martínez;given-names:RE</infon><infon key="name_3">surname:Espinoza-Avalos;given-names:J</infon><infon key="name_4">surname:Canizales-Flores;given-names:HM</infon><infon key="name_5">surname:González-Godoy;given-names:CE</infon><infon key="name_6">surname:Barba-Santos;given-names:MG</infon><infon key="name_7">surname:Vega-Zepeda;given-names:A</infon><infon key="name_8">surname:Collado-Vides;given-names:L</infon><infon key="pub-id_doi">10.1016/j.marpolbul.2017.06.057</infon><infon key="pub-id_pmid">28651862</infon><infon key="section_type">REF</infon><infon key="source">Marine Pollution Bulletin</infon><infon key="type">ref</infon><infon key="volume">122</infon><infon key="year">2017</infon><offset>53787</offset><text>Severe impacts of brown tides caused by Sargassum spp. on near-shore caribbean seagrass communities</text></passage><passage><infon key="fpage">350</infon><infon key="issue">18</infon><infon key="lpage">367</infon><infon key="name_0">surname:Wang;given-names:M</infon><infon key="name_1">surname:Hu;given-names:C</infon><infon key="pub-id_doi">10.1016/j.rse.2016.04.019</infon><infon key="section_type">REF</infon><infon key="source">Remote Sensing of Environment</infon><infon key="type">ref</infon><infon key="volume">183</infon><infon key="year">2016</infon><offset>53887</offset><text>Mapping and quantifying Sargassum distribution and coverage in the central west atlantic using MODIS observations</text></passage><passage><infon key="fpage">2579</infon><infon key="issue">3</infon><infon key="lpage">2597</infon><infon key="name_0">surname:Wang;given-names:M</infon><infon key="name_1">surname:Hu;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Geoscience and Remote Sensing</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2020</infon><offset>54001</offset><text>Automatic extraction of Sargassum features from sentinel-2 msi images</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="name_0">surname:Webster;given-names:RK</infon><infon key="name_1">surname:Linton;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Shore &amp; Beach</infon><infon key="type">ref</infon><infon key="volume">81</infon><infon key="year">2013</infon><offset>54071</offset><text>Development and implementation of Sargassum early advisory system (seas)</text></passage></document></collection>
