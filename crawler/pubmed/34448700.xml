<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210930</date><key>pmc.key</key><document><id>8433867</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.2196/24408</infon><infon key="article-id_pmc">8433867</infon><infon key="article-id_pmid">34448700</infon><infon key="article-id_publisher-id">v23i8e24408</infon><infon key="elocation-id">e24408</infon><infon key="issue">8</infon><infon key="kwd">machine learning image classification convolutional neural network object detection crowdsourcing tobacco point of sale public health surveillance</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.</infon><infon key="name_0">surname:Kukafka;given-names:Rita</infon><infon key="name_1">surname:Heckmann;given-names:Bryan</infon><infon key="name_10">surname:Schneider;given-names:Jordan</infon><infon key="name_11">surname:Rose;given-names:Shyanika W</infon><infon key="name_12">surname:Patel;given-names:Minal</infon><infon key="name_13">surname:Schillo;given-names:Barbara A</infon><infon key="name_2">surname:Banik;given-names:Palash</infon><infon key="name_3">surname:English;given-names:Ned</infon><infon key="name_4">surname:Anesetti-Rothermel;given-names:Andrew</infon><infon key="name_5">surname:Zhao;given-names:Chang</infon><infon key="name_6">surname:Latterner;given-names:Andrew</infon><infon key="name_7">surname:Benson;given-names:Adam F</infon><infon key="name_8">surname:Herman;given-names:Peter</infon><infon key="name_9">surname:Emery;given-names:Sherry</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">23</infon><infon key="year">2021</infon><offset>0</offset><text>Image Processing for Public Health Surveillance of Tobacco Point-of-Sale Advertising: Machine Learning–Based Methodology</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>123</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>134</offset><text>With a rapidly evolving tobacco retail environment, it is increasingly necessary to understand the point-of-sale (POS) advertising environment as part of tobacco surveillance and control. Advances in machine learning and image processing suggest the ability for more efficient and nuanced data capture than previously available.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>463</offset><text>Objective</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>473</offset><text>The study aims to use machine learning algorithms to discover the presence of tobacco advertising in photographs of tobacco POS advertising and their location in the photograph.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>651</offset><text>Methods</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>659</offset><text>We first collected images of the interiors of tobacco retailers in West Virginia and the District of Columbia during 2016 and 2018. The clearest photographs were selected and used to create a training and test data set. We then used a pretrained image classification network model, Inception V3, to discover the presence of tobacco logos and a unified object detection system, You Only Look Once V3, to identify logo locations.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1087</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1095</offset><text>Our model was successful in identifying the presence of advertising within images, with a classification accuracy of over 75% for 8 of the 42 brands. Discovering the location of logos within a given photograph was more challenging because of the relatively small training data set, resulting in a mean average precision score of 0.72 and an intersection over union score of 0.62.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1475</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1487</offset><text>Our research provides preliminary evidence for a novel methodological approach that tobacco researchers and other public health practitioners can apply in the collection and processing of data for tobacco or other POS surveillance efforts. The resulting surveillance information can inform policy adoption, implementation, and enforcement. Limitations notwithstanding, our analysis shows the promise of using machine learning as part of a suite of tools to understand the tobacco retail environment, make policy recommendations, and design public health interventions at the municipal or other jurisdictional scale.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2103</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>2116</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2127</offset><text>Tobacco point-of-sale (POS) advertising, consisting of signs, displays, and other promotional materials, is considered a very deliberate and effective marketing strategy. The 1998 Master Settlement Agreement restricted tobacco advertising in general, raising the importance of POS advertising as one of the only remaining channels tobacco companies could use to directly reach consumers. In 2018, POS advertising represented the largest category of advertising expenditure for cigarette manufacturers in the United States. Importantly, research has consistently demonstrated the direct influence of tobacco POS advertising on tobacco use. Exposure to tobacco POS advertising has been positively associated with the urge to smoke and negatively associated with cessation among adult smokers. Exposure to tobacco POS advertising is positively associated with susceptibility, initiation, and current tobacco use among youth and never smokers. In communities where there is more tobacco POS advertising, tobacco use is higher; conversely, smoking rates are lower in communities that have adopted policies restricting tobacco POS advertising. Furthermore, the effects of tobacco POS advertising contribute to disparities in smoking and tobacco-related diseases, as potentially vulnerable populations are often explicitly targeted. Key examples include greater tobacco retailer density in communities of color and pervasive POS advertising of menthol cigarettes in lower-income African American communities.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3629</offset><text>The regulation of tobacco POS advertising varies substantially across communities and states within the United States. Surveillance of tobacco POS advertising is important for assessing compliance with local, state, and federal regulations; informing evidence-based policy making; understanding industry behavior; and identifying factors contributing to ongoing disparities in tobacco use and tobacco-related disease burden. At present, store audits represent the most common and rigorous approach to measuring tobacco POS advertisements. In general, researchers recommend a store audit to involve the careful observation of advertisements and retail spaces, speaking with store clerks, and manually photographing and annotating the advertisements present in brick-and-mortar tobacco retailers. Such audits require substantial resources, training, and time; thus, they may be difficult to conduct in underresourced communities. For example, a 2014 study surveying 48 states found that the majority of surveillance work at the local level was conducted by volunteer staff. Furthermore, at the time of the study, only slightly more than half (54%) of the surveyed states reported conducting surveillance activities in the past several years. Among those conducting surveillance, only 19% reported that these activities were routine, reinforcing the challenges of consistent data collection via traditional surveillance methods and the importance of leveraging new technology and strategies. Technological advancements would thus be beneficial for improving the depth and breadth of in-store surveillance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5232</offset><text>Over the past decade, 2 technological innovations have advanced sufficiently to offer the possibility of a more efficient, less resource-intensive approach for measuring POS advertising at scale. Specifically, the combination of crowdsourcing and machine learning offers a promising strategy to automate the store auditing process and allow researchers to gain insights into actual advertisements. Machine learning is a general term to describe a collection of related computer tasks, including image classification and object detection—key elements in identifying tobacco brands in photographs of store environments. One advantage of machine learning is that it eliminates the burden of manual review and coding and carries the potential for major cost and time savings for research projects. Although both traditional surveillance activities and crowdsourcing efforts have been used to collect data from tobacco retailers, machine learning is yet to be applied to improve digital photograph extraction.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6239</offset><text>Objectives</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6250</offset><text>The primary aim of this study is to assess the feasibility of using machine learning approaches to identify and quantify information accurately in tobacco POS advertising. The proposed approach has 2 components. First, we make use of a large collection of interior photographs of tobacco retailers in West Virginia and Washington, DC. Second, we apply image classification and object detection to identify the brand, number, and size of tobacco advertisements within digital photographs. With respect to image classification, we aim to identify individual tobacco brands in the archive of photographs. We aim to accurately determine the location of brand-specific advertising images in individual photographs by using object detection. By accurately classifying branded images and automatically detecting the location of branded imagery in a retail environment, our approach can provide a practical alternative to in-person POS store audits. In addition, being able to capture enhanced contextual information about the POS environment may provide insights into efforts to combat ongoing efforts from the tobacco industry to use their advertisements to target vulnerable populations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>7433</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>7441</offset><text>Data Collection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7457</offset><text>Camera glasses were used by field staff to collect interior photographs from 410 tobacco retailers across 37 counties in West Virginia from September 2013 to August 2014, the majority of which were in 6 counties with full coverage of all stores. A total of 86,683 digital photographs of tobacco product counters and advertisements were collected. These photographs were then used in the first machine learning task, which attempted to classify specific tobacco brands within the photos. We also collected an additional 13,264 photographs from 82 tobacco retailers in Washington, DC, during the fall of 2018 for the second machine learning task, which focused on identifying the location of advertisements within each photograph.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8186</offset><text>Data Cleaning</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8200</offset><text>We manually sorted the photographs into training and validation sets for purposes of training a neural network to classify the retailer photographs by brand. In so doing, we removed the images that contained no advertising or had unclear information, leaving 0.8% (694/86,683) photographs deemed most useful for brand detection analysis between West Virginia and Washington, DC. Although 0.8% (694/86,683) seems low, our cameras took photographs every 1 second, meaning most did not contain any tobacco POS advertising. Once the 694 clearest photographs were selected, they were manually sorted by which brands were contained within them. Finally, we created a training set of 70% (486/694) of the photographs and a testing/validation set of 30% (208/694) of the photographs for classification.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8995</offset><text>To detect the location of advertisements within each photograph, only photographs with Marlboro advertisements were ultimately used, but from a larger pool of photographs, as not all brands needed to be clear. We decided only to attempt to detect the location of Marlboro advertisements because they were by far the most common brand, giving us the largest amount of data on which to train our model. A sample of the 843 clearest Marlboro photos across Washington, DC, and West Virginia were sorted into training (589/843, 69.9% photographs) and testing sets (254/843, 30.1% photographs).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9584</offset><text>Classifying the Presence of Brands in Photographs</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9634</offset><text>Once the images in the training and test sets were manually classified, we used the Python TensorFlow and Keras libraries through the Jupyter Notebooks platform to recreate the manual brand classification with machine learning. Fundamentally, our model analyzed each image, resulting in scored and predicted probabilities that a given image contained specific tobacco brands.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10010</offset><text>Several steps were taken to speed up learning and minimize the computation time owing to the computationally expensive nature of image classification models. First, all processes were executed on a Linux server hosted by Amazon Web Services. Second, we used a pretrained image classification network, Inception V3, which has already been trained to classify millions of labeled images in the ImageNet repository. Our brand classification model extends Inception V3 to categorize tobacco brands by training a new classification layer on top of its existing architecture, allowing us to successfully classify brands with considerably fewer images than would otherwise be necessary.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10690</offset><text>After importing the pretrained Inception V3 classification network, we built a deep learning neural network to classify whether the images contained specific tobacco brands. A deep learning network can recognize features in the images that are associated with a targeted brand, including the colors, shapes, or patterns in a given logo. Specifically, we used TensorFlow to configure our computer graphics processing units before training our neural network using Keras. To prevent overfitting and make the most of the 486 photographs in our training set, we configured several random transformations so that our model would never see the same exact picture twice. We randomly applied zooms, shearing, and horizontal transformations to our training set and processed 50 images per batch at a resolution of 299×299 pixels. Once the neural network was trained, we generated the predicted probabilities that each image contained a brand of interest. In our analysis, an image was classified as containing the logo of the brand if the probability was ≥0.5, but other cutoffs could have been chosen.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11787</offset><text>Discovering Object Location Within Images</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11829</offset><text>Beyond identifying the presence of specific brands, our second goal is to discover their locations in a given photograph to inform their positioning and density in an individual store. Doing so required a different technology than the Inception V3 classification network described earlier, which was designed to discover logos anywhere in an image. We trained YOLO (You Only Look Once) V3, a state-of-the-art, real-time unified object detection system to detect tobacco advertisements within images, as illustrated in Figure 1. Compared with region-proposal-based convolutional neural networks (eg, R-CNN [region-based convolutional neural networks], fast R-CNN, and faster R-CNN), YOLO V3 uses a single convolutional neural network optimized end to end to full images to simultaneously predict multiple bounding boxes and their class probabilities. By being informed of the global context of the image, YOLO V3 has shown the ability to predict fewer false positives in background image areas where objects are not present.</text></passage><passage><infon key="file">jmir_v23i8e24408_fig1.jpg</infon><infon key="id">figure1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12853</offset><text>Example image from a tobacco point of sale with YOLO (You Only Look Once) bounding boxes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12943</offset><text>As part of our process, we first used the open-source Visual Object Tagging Tool to draw bounding boxes around each Marlboro advertisement within images and generate related annotations for training. We then trained YOLO V3 using a pretrained, publicly available Darknet-53 model on ImageNet to perform customized object detection. Anchor boxes, predefined boxes to improve speed and efficiency in detection of typical objects of interest, were recomputed using K-means clustering with intersection over union (IOU) as the distance measure. We stopped training at the 10,000th batch, where the training loss levels off. To reduce overfitting and generalization error, we tested the network using weights from alternative stopping points generated earlier via the testing data set.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13724</offset><text>We then evaluated the model accuracy for location detection using testing images based on two metrics: the mean average precision (mAP) and IOU. The mAP is a composite accuracy indicator that ranges from 0 to 1 and accounts for both precision and recall, which is computed as the area under the precision-recall curve. An mAP score of 1 indicates that 100% of the model’s predictions are correct and that 100% of the truth objects are detected by the model. The IOU measures the extent to which the prediction overlaps with the ground truth, which is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground-truth bounding box.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>14403</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>14411</offset><text>Overview</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>14420</offset><text>We present our results following our 2 primary research questions, as outlined earlier. First, we describe the success of identifying individual tobacco brands in our set of photographs. Second, we detail how we determined the location of such images in individual stores. We then discuss our findings and their implications in the Discussion and Conclusions sections.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>14789</offset><text>Brand Detection</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>14805</offset><text>Our Inception V3 model fundamentally generated the predicted probabilities of the presence of each brand for each image in the validation set. Although we ultimately decided to focus on the brand Marlboro, we attempted to detect a series of brand logos. Success varied by brand, but our model achieved a classification accuracy of more than 75% for 8 of the 42 brands it was trained to detect (Textbox 1 and Table 1). For all but 7 brands—Camel, Marlboro, Pyramid, Pall Mall, Grizzly, Swisher, and Newport—the number of labeled example images constituted less than 11.7% (57/486) of the training data set. Table 2 shows the predicted probabilities of discovering specific logos in Figures 2 and 3 to illustrate the variability in the size and design of advertisements and the ability of the model to interpret branding in complex images. As shown, Newport was predicted with the highest probability in Figure 2, with Marlboro having the highest probability in Figure 3, with other brands still having high probabilities in each. Each image varies in terms of the heterogeneity and scale of the logos in question, implying the importance of both color and design.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2_caption</infon><offset>15972</offset><text>Tobacco brands considered for classification.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16018</offset><text> Cigarette </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16030</offset><text>Marlboro</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16039</offset><text>Newport</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16047</offset><text>Camel</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16053</offset><text>Pall Mall</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16063</offset><text>Pyramid</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16071</offset><text>Maverick</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16080</offset><text>Santa Fe</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16089</offset><text>Winston</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16097</offset><text>Kool</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16102</offset><text>American Spirit</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16118</offset><text> Cigar </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16126</offset><text>Swisher</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16134</offset><text>Black and Mild</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16149</offset><text>White Owl</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16159</offset><text>Dutch Masters</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16173</offset><text>Winchester</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16184</offset><text>Garcia y Vega</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16198</offset><text>Phillies</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16207</offset><text>Cheyenne</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16216</offset><text>Backwoods</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16226</offset><text> Smokeless tobacco </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16246</offset><text>Levi Garrett Plug</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16264</offset><text>Day’s Work</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16277</offset><text>Red Man Plug</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16290</offset><text>Grizzly</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16298</offset><text>Garrett</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16306</offset><text>Skoal</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16312</offset><text>Red Man</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16320</offset><text>Copenhagen</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16331</offset><text>Red Seal</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16340</offset><text>Timberwolf</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16351</offset><text>Kayak</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16357</offset><text>Beechnut</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16366</offset><text>Kodiak</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16373</offset><text>Longhorn</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16382</offset><text> Snus </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16389</offset><text>Skoal</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16395</offset><text>General</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16403</offset><text> e-Cigarettes </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16418</offset><text>Blu</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16422</offset><text>FIN</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16426</offset><text>Logic</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16432</offset><text>MARKTEN</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16440</offset><text>NJOY</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16445</offset><text>V2</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16448</offset><text>VUSE</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16453</offset><text>Classification accuracy for validation data by tobacco brands.</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Brand&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classification accuracy (%)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Copenhagen&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.4&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Winston&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.1&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pyramid&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.7&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Blu&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.7&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;American Spirit&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.3&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Marlboro&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.8&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Camel&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.9&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pall Mall&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16516</offset><text>Brand	Classification accuracy (%)	 	Copenhagen	90.4	 	Winston	85.1	 	Pyramid	82.7	 	Blu	81.7	 	American Spirit	80.3	 	Marlboro	78.8	 	Camel	75.9	 	Pall Mall	75.9	 	</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16681</offset><text>Predicted probability of logos by Inception V3.</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;30&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;470&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;0&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;Photograph and brand&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Probability&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;
&lt;xref rid=&quot;figure2&quot; ref-type=&quot;fig&quot;&gt;Figure 2&lt;/xref&gt;
&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Marlboro&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;0.635&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Newport&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;0.982&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Camel&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;0.661&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;
&lt;xref rid=&quot;figure3&quot; ref-type=&quot;fig&quot;&gt;Figure 3&lt;/xref&gt;
&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Marlboro&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;0.993&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Newport&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;0.868&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16729</offset><text>Photograph and brand	Probability	 	Figure 2	 		Marlboro	0.635	 		Newport	0.982	 		Camel	0.661	 	Figure 3	 		Marlboro	0.993	 		Newport	0.868	 	</text></passage><passage><infon key="file">jmir_v23i8e24408_fig2.jpg</infon><infon key="id">figure2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16872</offset><text>Panoramic image of a tobacco point of sale—view 1.</text></passage><passage><infon key="file">jmir_v23i8e24408_fig3.jpg</infon><infon key="id">figure3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16925</offset><text>Panoramic image of a tobacco point of sale—view 2.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>16978</offset><text>Location of Advertisement Detection</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>17014</offset><text>As described in the Methods section, for the purpose of location detection, we evaluated YOLO V3 model accuracy using the mAP and IOU on testing images. The network with weights that yielded the highest testing mAP (0.72; Figure 4) and IOU score (0.62; Figure 4) was chosen as the best model for detection. Figure 5 shows the detection results for an example image, where five Marlboro advertisements were detected. The object detector also generated a confidence score for each box, along with estimates of the upper left coordinates and the absolute width and height of the bounding boxes (Table 3).</text></passage><passage><infon key="file">jmir_v23i8e24408_fig4.jpg</infon><infon key="id">figure4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>17616</offset><text>Training loss and testing accuracy of the YOLO (You Only Look Once) V3 objection detector. IOU: intersection over union; mAP: mean average precision.</text></passage><passage><infon key="file">jmir_v23i8e24408_fig5.jpg</infon><infon key="id">figure5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>17766</offset><text>Prediction of Marlboro signs by YOLO (You Only Look Once) V3.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>17828</offset><text>Bounding boxes of Marlboro signs detected by YOLO (You Only Look Once) V3.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;310&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;200&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;130&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;80&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bounding box in &lt;xref rid=&quot;figure5&quot; ref-type=&quot;fig&quot;&gt;Figure 5&lt;/xref&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Confidence score (%)&lt;/td&gt;&lt;td colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;Bounding box measures (pixels)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Left x&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Top y&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Width&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Height&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;737&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;376&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;724&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1355&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;293&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1149&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;505&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;708&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;163&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1189&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;604&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;648&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;154&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2084&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;745&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;358&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>17903</offset><text>Bounding box in Figure 5	Confidence score (%)	Bounding box measures (pixels)	 			Left x	Top y	Width	Height	 	1	100	0	16	737	376	 	2	100	724	30	1355	293	 	3	98	1149	505	708	163	 	4	100	1189	604	648	154	 	5	100	2084	0	745	358	 	</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>18130</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>18141</offset><text>Principal Findings</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>18160</offset><text>Our study provides evidence of the feasibility of a novel methodological approach that tobacco researchers and other public health practitioners can apply in the collection and processing of data for tobacco and other POS surveillance efforts. Trained on a small set of labeled tobacco POS photographs, our classifier and object detector were able to identify tobacco brands and their location and dimension successfully within images. Although the initial labeling of the training data set was time-consuming, with an average of 3 minutes per photograph for staff to label advertisements, the costs of processing the photographic data decreased once the neural networks were trained. We were able to achieve a brand classification accuracy of over 75% for 8 of the 42 brands that our classifier (Inception V3) was trained to detect. Furthermore, we were able to accurately predict the location of branded advertising within the store environment (YOLO V3). Our location predictions achieved an mAP score of 0.72 and an IOU score of 0.62.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>19199</offset><text>Our Inception V3 brand detection model had 2 major characteristics. First, the model was optimized to predict true positives, but with the unintended consequence of predicting more false positives. The model, therefore, tended to decide that a brand was in a photograph when none was present, in the process of finding true positives. Unfortunately, there was no alternative; tuning the model to favor predicting true negatives would have been too conservative (ie, unable to make any prediction on most photographs). Second, the model was heavily dependent on making predictions based on color. For example, in Figure 2, the size of the Marlboro advertisement is much larger than that in Figure 3. However, the predicted probability of a Marlboro advertisement in Figure 3 is higher owing to the presence of a familiar red logo, as shown in Table 1. The presence of green on the shelves of the menthol product displays may have caused the very high predicted probability of Newport in both photos, despite the Newport advertisements themselves being relatively small. Such variation in advertisement design and color, especially for Marlboro, may also explain why the accuracy of predicting the presence of Marlboro advertisements was among the lowest, despite having the most representation across photographs. However, our results indicate the potential for accurately identifying the presence of specific tobacco brands in digital photographs using Inception V3 or similar models. Although the accuracy rates shown above do not approach some in the medical field with very large training data sets, they do show how our approach is considerably more effective than random initialization constraints, even with a relatively small training set. For context, a data set containing at least 2000 images of each brand with varying sizes, rotation angles, lighting schemes, and backgrounds would be needed for improved object detection accuracy.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>21143</offset><text>This promising technology offers potential opportunities for tobacco POS surveillance to move forward. First, conducting timely, long-term surveillance of the POS environment can be resource intensive. Some existing state and local audit systems require a substantial amount of training and resources; however, they do provide critical information for enforcement activities as well as an understanding of the impact of marketing on current tobacco user behaviors and tobacco use initiation. By applying machine learning techniques for efficient image classification and object detection, the methods described in this paper can assist in ensuring that retailers comply with specific retail provisions, such as state or local flavored tobacco bans, in addition to potentially improving the generalizability of research results by increasing the reliability and standardization of advertising information and classification.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>22067</offset><text>The use of improved surveillance technologies can also assist in the assessment of local policy impacts in an evolving POS retail environment. With the adoption of local and state restrictions on the sale of flavored tobacco products, jurisdictions have an ever-growing need to assess the impact of these policies on tobacco retail environments. For example, more routine and detailed retail POS advertising data could be used to assess differences before and after policies are implemented to examine key questions regarding changes in product availability, advertising, and promotion or discounting efforts.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>22677</offset><text>Finally, this concept of applying machine learning to examine tobacco POS could also be extended to other public health applications. For example, image classification and object detection can be applied to identify other products of interest such as sugary drinks, candy, and processed foods from retail store images. Such an approach would allow public health researchers and policy makers to gauge the prevalence and types of advertisements for each product and understand how a specific retail food environment may interact with population demographics. Given the current attention on obesity, related health outcomes, and efforts to tax sugary drinks, we might anticipate interest in a machine learning–based approach, as illustrated.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>23419</offset><text>Limitations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>23431</offset><text>Despite the success of applying deep neural networks to tobacco advertisement object detection, our analysis revealed several challenges, especially with respect to object location detection. Although our brand classification algorithms were generally successful (Inception V3) with some false-positive classifications, object location detection (YOLO V3) was more challenging, requiring a large amount of training data to identify patterns. Owing to the relatively small sample size, we decided to limit training our object detection algorithm to Marlboro advertisements, the dominant tobacco brand in our data set. Even when considering one brand within a given POS, we observe substantial variability in advertising content and appearance, including differences in color, shading, perspective, size, and rotation (Figure 5). Such variability made it difficult for object detection algorithms to learn and recognize brand patterns. Therefore, object location detection can be considered a more complex task prone to false negatives (ie, missing an object that is actually present). Our object detection model is conservative in its present form, with the side effect of missing true objects present in a photo, in contrast to our brand recognition model that generated false positives. Either bias would be improved by training the image classifier or object detector with additional images but would require further resources.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>24861</offset><text>One key factor that limited our training data set is that relatively few tobacco POS advertising training images are available to the public, as distinct from generic computer vision applications where large and clean benchmark data sets exist. Collecting primary data on tobacco advertising and creating sufficient labeled training samples are a challenge in performing deep neural network–based object detection owing to time and cost implications. As shown in the literature, primary data collection for POS surveillance poses many challenges related to technology, workforce training, and overall logistics. Data availability presents a particular concern for detecting tobacco brands that are less common in the marketplace, such as vape products, which are of increasing concern to researchers and health officials. Owing to the rapid evolution of branding, marketing, and delivery of tobacco products, we expect such analytical and data collection challenges to persist. Such challenges are further complicated by the lack of a centralized data store to track advertising materials. No comprehensive, publicly available, federal repository for tobacco advertising materials currently exists, although efforts by academic and nonprofit institutions to conduct marketing surveillance persist, including the Rutgers Center for Tobacco Studies and Campaign for Tobacco-Free Kids.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>26246</offset><text>Beyond technology, there are also specific practical considerations when implementing such assessments. First, there are privacy and safety concerns associated with the use of camera glasses to collect POS photographs in public, especially when one or more identifying people are incidentally captured in photographs. Many store owners may not be comfortable with individuals using camera phones or handheld cameras to capture POS information. To accommodate this, camera glasses are often used to capture hands-free images in a more inconspicuous way. However, wearing a hidden camera in public may raise legal issues, depending on the location. For example, although capturing public scenes requires no consent in the United States, the opposite exists in Spain. Although consent is not required in the United States for public photography, it may not always be socially acceptable. Clear guidelines about the “dos and don’ts” of wearing camera glasses for data collection should be created to broaden the applications of such technology to POS surveillance.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>27313</offset><text>Finally, data quality must be considered when collecting data for machine learning and related analysis. As we discovered, cameras in glasses may not always capture high-quality or usable images. Photographs may be blurry, overexposed, or missing the desired images. As such, input data quality may be subject to bias and error, which could affect the development of subsequent machine learning algorithms. In addition, because many glasses do not have a remote trigger or a viewing screen, the individual using the glasses may be unaware of the image quality until the photos are uploaded. Such effects could result in a significant reduction in the availability of the training images used for image classification and object detection. However, having multiple overlapping images of any given POS location helps to ensure that all available POS advertising is captured with one or more clear photographs.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>28221</offset><text>Future work on image classification in the tobacco POS retail environment would benefit from exploring additional methods to reduce false positives and false negatives for respective machine learning algorithms. For example, we could take advantage of image repositories that contain tobacco branding to help train models and add additional nuances. Although such repositories would not provide sufficient data to help describe the US retail environment comprehensively, they could assist by providing different geographical contexts or supplemental material in other advertising channels (eg, magazines, newspapers, films, the web, and social media) may still be helpful. The introduction of crowdsourced data collection for use with machine learning techniques, as well as incorporating social media data, may also assist in gaining new sources of data and reducing the logistical burdens of surveillance programs. Integrating these efforts into a shared repository of tobacco-related images among tobacco control programs would also assist in improving brand identification and location detection and training of new algorithms. Such collaboration among local, state, federal, and academic institutions could be a powerful tool in understanding fast-changing retail trends and regional heterogeneity. In addition, our study considered location to be relative to each photograph, rather than in a defined geographic location within the POS. Although tobacco advertising tends to be consistent in the retail environment, such as in the commonly used Power Wall, value could be added by considering the purely geographic factor in future analyses.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>29869</offset><text>Conclusions</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>29881</offset><text>To summarize, our study demonstrated the utility of machine learning for POS assessment and highlighted some existing technological limitations. Although the current machine learning algorithms are advanced, they still have room for improvement. Large-scale POS photographic data sets that are comprehensive enough to capture a wider range of tobacco brands are needed to train multiclass algorithms capable of detecting advertisements from less common tobacco brands. Future studies should also attempt to detect the prices associated with within-store advertisements by incorporating text recognition algorithms to gain more contextual information on tobacco marketing. Attempting to classify an absolute geographic location within stores instead of relative image location within a photograph may also be of interest to researchers and surveillance efforts.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30742</offset><text>Despite these limitations, our analysis shows the promise of using machine learning as part of a suite of tools to understand the tobacco retail environment and inform public health interventions at multiple scales. The accurate and automatic classification of product brands and detection of their location within a retail environment could assist in developing a practical alternative to in-person POS audits, especially in resource-limited environments. For example, the necessary classifiers—documentation—could be made available in the public domain to facilitate their use by public health departments. In addition, coded photographs could be shared as part of a centralized resource to reduce the level of effort required to conduct or continue similar evaluations. With the increasing sales restrictions at the POS, surveillance products with enhanced contextual information about the retail environment can provide states, counties, and municipalities the opportunity to better understand the impact of existing and proposed policies, including ongoing efforts by the tobacco industry to target potentially vulnerable populations.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>31886</offset><text>Conflicts of Interest: None declared.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>31924</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31938</offset><text>IOU</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31942</offset><text>intersection over union</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31966</offset><text>mAP</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31970</offset><text>mean average precision</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31993</offset><text>POS</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>31997</offset><text>point-of-sale</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>32011</offset><text>R-CNN</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>32017</offset><text>region-based convolutional neural networks</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>32060</offset><text>YOLO</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>32065</offset><text>You Only Look Once</text></passage><passage><infon key="fpage">25</infon><infon key="issue">1</infon><infon key="lpage">35</infon><infon key="name_0">surname:Paynter;given-names:J</infon><infon key="name_1">surname:Edwards;given-names:R</infon><infon key="pub-id_doi">10.1093/ntr/ntn002</infon><infon key="pub-id_medline">19246438</infon><infon key="pub-id_pii">ntn002</infon><infon key="pub-id_pmid">19246438</infon><infon key="section_type">REF</infon><infon key="source">Nicotine Tob Res</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2009</infon><offset>32084</offset><text>The impact of tobacco promotion at the point of sale: a systematic review</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Federal Trade Commission Cigarette Report for 2018</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>32158</offset></passage><passage><infon key="fpage">322</infon><infon key="issue">2</infon><infon key="lpage">8</infon><infon key="name_0">surname:Wakefield;given-names:M</infon><infon key="name_1">surname:Germain;given-names:D</infon><infon key="name_2">surname:Henriksen;given-names:L</infon><infon key="pub-id_doi">10.1111/j.1360-0443.2007.02062.x</infon><infon key="pub-id_medline">18042190</infon><infon key="pub-id_pii">ADD2062</infon><infon key="pub-id_pmid">18042190</infon><infon key="section_type">REF</infon><infon key="source">Addiction</infon><infon key="type">ref</infon><infon key="volume">103</infon><infon key="year">2008</infon><offset>32159</offset><text>The effect of retail cigarette pack displays on impulse purchase</text></passage><passage><infon key="fpage">e83</infon><infon key="issue">e2</infon><infon key="lpage">9</infon><infon key="name_0">surname:Robertson;given-names:L</infon><infon key="name_1">surname:Cameron;given-names:C</infon><infon key="name_2">surname:McGee;given-names:R</infon><infon key="name_3">surname:Marsh;given-names:L</infon><infon key="name_4">surname:Hoek;given-names:J</infon><infon key="pub-id_doi">10.1136/tobaccocontrol-2015-052586</infon><infon key="pub-id_medline">26728139</infon><infon key="pub-id_pii">tobaccocontrol-2015-052586</infon><infon key="pub-id_pmid">26728139</infon><infon key="section_type">REF</infon><infon key="source">Tob Control</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2016</infon><offset>32224</offset><text>Point-of-sale tobacco promotion and youth smoking: a meta-analysis</text></passage><passage><infon key="fpage">210</infon><infon key="issue">2</infon><infon key="lpage">4</infon><infon key="name_0">surname:Henriksen;given-names:L</infon><infon key="name_1">surname:Feighery;given-names:EC</infon><infon key="name_2">surname:Schleicher;given-names:NC</infon><infon key="name_3">surname:Cowling;given-names:DW</infon><infon key="name_4">surname:Kline;given-names:RS</infon><infon key="name_5">surname:Fortmann;given-names:SP</infon><infon key="pub-id_doi">10.1016/j.ypmed.2008.04.008</infon><infon key="pub-id_medline">18544462</infon><infon key="pub-id_pii">S0091-7435(08)00208-9</infon><infon key="pub-id_pmid">18544462</infon><infon key="section_type">REF</infon><infon key="source">Prev Med</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2008</infon><offset>32291</offset><text>Is adolescent smoking related to the density and proximity of tobacco outlets and retail cigarette advertising near schools?</text></passage><passage><infon key="fpage">232</infon><infon key="issue">2</infon><infon key="lpage">8</infon><infon key="name_0">surname:Henriksen;given-names:L</infon><infon key="name_1">surname:Schleicher;given-names:NC</infon><infon key="name_2">surname:Feighery;given-names:EC</infon><infon key="name_3">surname:Fortmann;given-names:SP</infon><infon key="pub-id_doi">10.1542/peds.2009-3021</infon><infon key="pub-id_medline">20643725</infon><infon key="pub-id_pii">peds.2009-3021</infon><infon key="pub-id_pmid">20643725</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">126</infon><infon key="year">2010</infon><offset>32416</offset><text>A longitudinal study of exposure to retail cigarette advertising and smoking initiation</text></passage><passage><infon key="fpage">2</infon><infon key="issue">1</infon><infon key="lpage">17</infon><infon key="name_0">surname:Robertson;given-names:L</infon><infon key="name_1">surname:McGee;given-names:R</infon><infon key="name_2">surname:Marsh;given-names:L</infon><infon key="name_3">surname:Hoek;given-names:J</infon><infon key="pub-id_doi">10.1093/ntr/ntu168</infon><infon key="pub-id_medline">25173775</infon><infon key="pub-id_pii">ntu168</infon><infon key="pub-id_pmid">25173775</infon><infon key="section_type">REF</infon><infon key="source">Nicotine Tob Res</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2015</infon><offset>32504</offset><text>A systematic review on the impact of point-of-sale tobacco promotion on smoking</text></passage><passage><infon key="fpage">136</infon><infon key="issue">2</infon><infon key="lpage">51</infon><infon key="name_0">surname:Widome;given-names:R</infon><infon key="name_1">surname:Brock;given-names:B</infon><infon key="name_2">surname:Noble;given-names:P</infon><infon key="name_3">surname:Forster;given-names:JL</infon><infon key="pub-id_doi">10.1080/13557858.2012.701273</infon><infon key="pub-id_medline">22789035</infon><infon key="pub-id_pmid">22789035</infon><infon key="section_type">REF</infon><infon key="source">Ethn Health</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2013</infon><offset>32584</offset><text>The relationship of neighborhood demographic characteristics to point-of-sale tobacco advertising and marketing</text></passage><passage><infon key="fpage">468</infon><infon key="issue">5</infon><infon key="lpage">74</infon><infon key="name_0">surname:Loomis;given-names:BR</infon><infon key="name_1">surname:Kim;given-names:AE</infon><infon key="name_2">surname:Busey;given-names:AH</infon><infon key="name_3">surname:Farrelly;given-names:MC</infon><infon key="name_4">surname:Willett;given-names:JG</infon><infon key="name_5">surname:Juster;given-names:HR</infon><infon key="pub-id_doi">10.1016/j.ypmed.2012.08.014</infon><infon key="pub-id_medline">22960255</infon><infon key="pub-id_pii">S0091-7435(12)00387-8</infon><infon key="pub-id_pmid">22960255</infon><infon key="section_type">REF</infon><infon key="source">Prev Med</infon><infon key="type">ref</infon><infon key="volume">55</infon><infon key="year">2012</infon><offset>32696</offset><text>The density of tobacco retailers and its association with attitudes toward smoking, exposure to point-of-sale tobacco advertising, cigarette purchasing, and smoking among New York youth</text></passage><passage><infon key="fpage">e8</infon><infon key="issue">9</infon><infon key="lpage">18</infon><infon key="name_0">surname:Lee;given-names:JG</infon><infon key="name_1">surname:Henriksen;given-names:L</infon><infon key="name_2">surname:Rose;given-names:SW</infon><infon key="name_3">surname:Moreland-Russell;given-names:S</infon><infon key="name_4">surname:Ribisl;given-names:KM</infon><infon key="pub-id_doi">10.2105/AJPH.2015.302777</infon><infon key="pub-id_medline">26180986</infon><infon key="section_type">REF</infon><infon key="source">Am J Public Health</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2015</infon><offset>32882</offset><text>A systematic review of neighborhood disparities in point-of-sale tobacco marketing</text></passage><passage><infon key="fpage">381</infon><infon key="lpage">8</infon><infon key="name_0">surname:Ribisl;given-names:KM</infon><infon key="name_1">surname:D'Angelo;given-names:H</infon><infon key="name_2">surname:Feld;given-names:AL</infon><infon key="name_3">surname:Schleicher;given-names:NC</infon><infon key="name_4">surname:Golden;given-names:SD</infon><infon key="name_5">surname:Luke;given-names:DA</infon><infon key="name_6">surname:Henriksen;given-names:L</infon><infon key="pub-id_doi">10.1016/j.ypmed.2017.04.010</infon><infon key="pub-id_medline">28392252</infon><infon key="pub-id_pii">S0091-7435(17)30130-5</infon><infon key="pub-id_pmid">28392252</infon><infon key="section_type">REF</infon><infon key="source">Prev Med</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2017</infon><offset>32965</offset><text>Disparities in tobacco marketing and product availability at the point of sale: Results of a national study</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Point-of-Sale Strategies: A Tobacco Control Guide</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>33073</offset></passage><passage><infon key="fpage">98</infon><infon key="issue">2</infon><infon key="lpage">106</infon><infon key="name_0">surname:Lee;given-names:JGL</infon><infon key="name_1">surname:Henriksen;given-names:L</infon><infon key="name_2">surname:Myers;given-names:AE</infon><infon key="name_3">surname:Dauphinee;given-names:AL</infon><infon key="name_4">surname:Ribisl;given-names:KM</infon><infon key="pub-id_doi">10.1136/tobaccocontrol-2012-050807</infon><infon key="pub-id_medline">23322313</infon><infon key="pub-id_pii">tobaccocontrol-2012-050807</infon><infon key="pub-id_pmid">23322313</infon><infon key="section_type">REF</infon><infon key="source">Tob Control</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2014</infon><offset>33074</offset><text>A systematic review of store audit methods for assessing tobacco marketing and products at the point of sale</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Point-of-Sale Report to the Nation: Policy Activity 2012-2014</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>33183</offset></passage><passage><infon key="fpage">533</infon><infon key="issue">4</infon><infon key="lpage">8</infon><infon key="name_0">surname:Mortensen;given-names:K</infon><infon key="name_1">surname:Hughes;given-names:TL</infon><infon key="pub-id_doi">10.1007/s11606-017-4246-0</infon><infon key="pub-id_medline">29302882</infon><infon key="pub-id_pii">10.1007/s11606-017-4246-0</infon><infon key="pub-id_pmid">29302882</infon><infon key="section_type">REF</infon><infon key="source">J Gen Intern Med</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2018</infon><offset>33184</offset><text>Comparing amazon's mechanical Turk platform to conventional data collection methods in the health and medical research literature</text></passage><passage><infon key="fpage">e20</infon><infon key="issue">2</infon><infon key="name_0">surname:Cantrell;given-names:J</infon><infon key="name_1">surname:Ganz;given-names:O</infon><infon key="name_2">surname:Ilakkuvan;given-names:V</infon><infon key="name_3">surname:Tacelosky;given-names:M</infon><infon key="name_4">surname:Kreslake;given-names:J</infon><infon key="name_5">surname:Moon-Howard;given-names:J</infon><infon key="name_6">surname:Aidala;given-names:A</infon><infon key="name_7">surname:Vallone;given-names:D</infon><infon key="name_8">surname:Anesetti-Rothermel;given-names:A</infon><infon key="name_9">surname:Kirchner;given-names:TR</infon><infon key="pub-id_doi">10.2196/publichealth.4191</infon><infon key="pub-id_medline">27227138</infon><infon key="pub-id_pii">v1i2e20</infon><infon key="pub-id_pmid">27227138</infon><infon key="section_type">REF</infon><infon key="source">JMIR Public Health Surveill</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2015</infon><offset>33314</offset><text>Implementation of a multimodal mobile system for point-of-sale surveillance: lessons learned from case studies in Washington, DC, and New York city</text></passage><passage><infon key="fpage">e22</infon><infon key="issue">2</infon><infon key="name_0">surname:Ilakkuvan;given-names:V</infon><infon key="name_1">surname:Tacelosky;given-names:M</infon><infon key="name_2">surname:Ivey;given-names:KC</infon><infon key="name_3">surname:Pearson;given-names:JL</infon><infon key="name_4">surname:Cantrell;given-names:J</infon><infon key="name_5">surname:Vallone;given-names:DM</infon><infon key="name_6">surname:Abrams;given-names:DB</infon><infon key="name_7">surname:Kirchner;given-names:TR</infon><infon key="pub-id_doi">10.2196/resprot.3277</infon><infon key="pub-id_medline">24717168</infon><infon key="pub-id_pii">v3i2e22</infon><infon key="pub-id_pmid">24717168</infon><infon key="section_type">REF</infon><infon key="source">JMIR Res Protoc</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2014</infon><offset>33462</offset><text>Cameras for public health surveillance: a methods protocol for crowdsourced annotation of point-of-sale photographs</text></passage><passage><infon key="comment">
https://github.com/fchollet/keras/
</infon><infon key="name_0">surname:Chollet;given-names:F</infon><infon key="section_type">REF</infon><infon key="source">Github Repository</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>33578</offset><text>Keras</text></passage><passage><infon key="name_0">surname:Kluyver;given-names:T</infon><infon key="name_1">surname:Ragan-Kelley;given-names:B</infon><infon key="name_10">surname:Ivanov;given-names:P</infon><infon key="name_11">surname:Avila;given-names:Damian</infon><infon key="name_12">surname:Abdalla;given-names:Safia</infon><infon key="name_13">surname:Willing;given-names:Carol</infon><infon key="name_2">surname:Pérez;given-names:F</infon><infon key="name_3">surname:Granger;given-names:B</infon><infon key="name_4">surname:Bussonnier;given-names:M</infon><infon key="name_5">surname:Frederic;given-names:J</infon><infon key="name_6">surname:Kelley;given-names:K</infon><infon key="name_7">surname:Hamrick;given-names:J</infon><infon key="name_8">surname:Grout;given-names:J</infon><infon key="name_9">surname:Corlay;given-names:S</infon><infon key="pub-id_doi">10.3233/978-1-61499-649-1-87</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 20th International Conference on Electronic Publishing</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>33584</offset><text>Jupyter Notebooks—a Publishing Format for Reproducible Computational Workflows</text></passage><passage><infon key="name_0">surname:Szegedy;given-names:C</infon><infon key="name_1">surname:Vanhoucke;given-names:V</infon><infon key="name_2">surname:Ioffe;given-names:S</infon><infon key="name_3">surname:Shlens;given-names:J</infon><infon key="name_4">surname:Wojna;given-names:Z</infon><infon key="pub-id_doi">10.1109/cvpr.2016.308</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE conference on computer visionpattern recognition (CVPR)</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>33665</offset><text>Rethinking the Inception Architecture for Computer Vision</text></passage><passage><infon key="fpage">647</infon><infon key="lpage">55</infon><infon key="name_0">surname:Donahue;given-names:J</infon><infon key="name_1">surname:Jia;given-names:Y</infon><infon key="name_2">surname:Vinyals;given-names:O</infon><infon key="name_3">surname:Hoffman;given-names:J</infon><infon key="name_4">surname:Zhang;given-names:N</infon><infon key="name_5">surname:Tzeng;given-names:E</infon><infon key="name_6">surname:Darrell;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st International Conference on Machine Learning</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>33723</offset><text>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</text></passage><passage><infon key="comment">
https://image-net.org/about.php
</infon><infon key="section_type">REF</infon><infon key="source">Image Net</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>33801</offset><text>Summary and Statistics</text></passage><passage><infon key="comment">
https://arxiv.org/abs/1506.02640
</infon><infon key="name_0">surname:Redmon;given-names:J</infon><infon key="name_1">surname:Divvala;given-names:S</infon><infon key="name_2">surname:Girshick;given-names:R</infon><infon key="name_3">surname:Farhadi;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>33824</offset><text>You Only Look Once: Unified, Real-time Object Detection</text></passage><passage><infon key="comment">
https://arxiv.org/abs/1804.02767
</infon><infon key="name_0">surname:Redmon;given-names:J</infon><infon key="name_1">surname:Farhadi;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>33880</offset><text>Yolov3: An Incremental Improvement</text></passage><passage><infon key="comment">
https://github.com/microsoft/VoTT
</infon><infon key="section_type">REF</infon><infon key="source">GitHub Repository</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>33915</offset><text>Microsoft/VoTT</text></passage><passage><infon key="comment">
https://pjreddie.com/darknet/
</infon><infon key="name_0">surname:Redmon;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">PJ Reddie</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>33930</offset><text>Darknet: Open Source Neural Networks in C</text></passage><passage><infon key="fpage">1122</infon><infon key="issue">5</infon><infon key="lpage">31.e9</infon><infon key="name_0">surname:Kermany;given-names:DS</infon><infon key="name_1">surname:Goldbaum;given-names:M</infon><infon key="name_10">surname:Dong;given-names:J</infon><infon key="name_11">surname:Prasadha;given-names:MK</infon><infon key="name_12">surname:Pei;given-names:J</infon><infon key="name_13">surname:Ting;given-names:MYL</infon><infon key="name_14">surname:Zhu;given-names:J</infon><infon key="name_15">surname:Li;given-names:C</infon><infon key="name_16">surname:Hewett;given-names:S</infon><infon key="name_17">surname:Dong;given-names:J</infon><infon key="name_18">surname:Ziyar;given-names:I</infon><infon key="name_19">surname:Shi;given-names:A</infon><infon key="name_2">surname:Cai;given-names:W</infon><infon key="name_20">surname:Zhang;given-names:R</infon><infon key="name_21">surname:Zheng;given-names:L</infon><infon key="name_22">surname:Hou;given-names:R</infon><infon key="name_23">surname:Shi;given-names:W</infon><infon key="name_24">surname:Fu;given-names:X</infon><infon key="name_25">surname:Duan;given-names:Y</infon><infon key="name_26">surname:Huu;given-names:VA</infon><infon key="name_27">surname:Wen;given-names:C</infon><infon key="name_28">surname:Zhang;given-names:ED</infon><infon key="name_29">surname:Zhang;given-names:CL</infon><infon key="name_3">surname:Valentim;given-names:CC</infon><infon key="name_30">surname:Li;given-names:O</infon><infon key="name_31">surname:Wang;given-names:X</infon><infon key="name_32">surname:Singer;given-names:MA</infon><infon key="name_33">surname:Sun;given-names:X</infon><infon key="name_34">surname:Xu;given-names:J</infon><infon key="name_35">surname:Tafreshi;given-names:A</infon><infon key="name_36">surname:Lewis;given-names:MA</infon><infon key="name_37">surname:Xia;given-names:H</infon><infon key="name_38">surname:Zhang;given-names:K</infon><infon key="name_4">surname:Liang;given-names:H</infon><infon key="name_5">surname:Baxter;given-names:SL</infon><infon key="name_6">surname:McKeown;given-names:A</infon><infon key="name_7">surname:Yang;given-names:G</infon><infon key="name_8">surname:Wu;given-names:X</infon><infon key="name_9">surname:Yan;given-names:F</infon><infon key="pub-id_doi">10.1016/j.cell.2018.02.010</infon><infon key="pub-id_medline">29474911</infon><infon key="pub-id_pii">S0092-8674(18)30154-5</infon><infon key="pub-id_pmid">29474911</infon><infon key="section_type">REF</infon><infon key="source">Cell</infon><infon key="type">ref</infon><infon key="volume">172</infon><infon key="year">2018</infon><offset>33972</offset><text>Identifying medical diagnoses and treatable diseases by image-based deep learning</text></passage><passage><infon key="comment">
https://github.com/AlexeyAB/darknet
</infon><infon key="name_0">surname:Bochkovskiy;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Github Repository</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>34054</offset><text>Darknet</text></passage><passage><infon key="fpage">1356</infon><infon key="issue">11</infon><infon key="lpage">64</infon><infon key="name_0">surname:Malik;given-names:VS</infon><infon key="name_1">surname:Popkin;given-names:BM</infon><infon key="name_2">surname:Bray;given-names:GA</infon><infon key="name_3">surname:Després;given-names:J</infon><infon key="name_4">surname:Hu;given-names:FB</infon><infon key="pub-id_doi">10.1161/circulationaha.109.876185</infon><infon key="pub-id_pmid">20308626</infon><infon key="section_type">REF</infon><infon key="source">Circulation</infon><infon key="type">ref</infon><infon key="volume">121</infon><infon key="year">2010</infon><offset>34062</offset><text>Sugar-sweetened beverages, obesity, type 2 diabetes mellitus, and cardiovascular disease risk</text></passage><passage><infon key="comment">
http://sph.rutgers.edu/centers/cts/index.html
</infon><infon key="section_type">REF</infon><infon key="source">Rutgers University School of Public Health</infon><infon key="type">ref</infon><offset>34156</offset><text>Center for Tobacco Studies</text></passage><passage><infon key="comment">
https://www.tobaccofreekids.org/
</infon><infon key="section_type">REF</infon><infon key="source">Campaign for Tobacco-Free Kids</infon><infon key="type">ref</infon><offset>34183</offset></passage><passage><infon key="comment">
https://commons.wikimedia.org/wiki/Commons:Country_specific_consent_requirements
</infon><infon key="section_type">REF</infon><infon key="source">Wikimedia Commons</infon><infon key="type">ref</infon><offset>34184</offset><text>Country Specific Consent Requirements</text></passage><passage><infon key="comment">
http://tobacco.stanford.edu/tobacco_main/index.php
</infon><infon key="section_type">REF</infon><infon key="source">Stanford University Research into The Impact of Tobacco Advertising</infon><infon key="type">ref</infon><offset>34222</offset></passage><passage><infon key="comment">
https://smokefreemovies.ucsf.edu/
</infon><infon key="section_type">REF</infon><infon key="source">Smoke Free Movies</infon><infon key="type">ref</infon><offset>34223</offset></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Martino;given-names:SC</infon><infon key="name_1">surname:Setodji;given-names:CM</infon><infon key="name_2">surname:Dunbar;given-names:MS</infon><infon key="name_3">surname:Shadel;given-names:WG</infon><infon key="pub-id_doi">10.1016/j.addbeh.2018.07.024</infon><infon key="pub-id_medline">30098502</infon><infon key="pub-id_pii">S0306-4603(18)30552-5</infon><infon key="pub-id_pmid">30098502</infon><infon key="section_type">REF</infon><infon key="source">Addict Behav</infon><infon key="type">ref</infon><infon key="volume">88</infon><infon key="year">2019</infon><offset>34224</offset><text>Increased attention to the tobacco power wall predicts increased smoking risk among adolescents</text></passage></document></collection>
