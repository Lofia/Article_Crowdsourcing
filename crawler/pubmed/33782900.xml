<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220218</date><key>pmc.key</key><document><id>8516782</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3758/s13428-020-01535-9</infon><infon key="article-id_pmc">8516782</infon><infon key="article-id_pmid">33782900</infon><infon key="article-id_publisher-id">1535</infon><infon key="fpage">2158</infon><infon key="issue">5</infon><infon key="kwd">Virtual lab Online research Crowdsourcing</infon><infon key="license">Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.</infon><infon key="lpage">2171</infon><infon key="name_0">surname:Almaatouq;given-names:Abdullah</infon><infon key="name_1">surname:Becker;given-names:Joshua</infon><infon key="name_2">surname:Houghton;given-names:James P.</infon><infon key="name_3">surname:Paton;given-names:Nicolas</infon><infon key="name_4">surname:Watts;given-names:Duncan J.</infon><infon key="name_5">surname:Whiting;given-names:Mark E.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">53</infon><infon key="year">2021</infon><offset>0</offset><text>Empirica: a virtual lab for high-throughput macro-level experiments</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>68</offset><text>Virtual labs allow researchers to design high-throughput and macro-level experiments that are not feasible in traditional in-person physical lab settings. Despite the increasing popularity of online research, researchers still face many technical and logistical barriers when designing and deploying virtual lab experiments. While several platforms exist to facilitate the development of virtual lab experiments, they typically present researchers with a stark trade-off between usability and functionality. We introduce Empirica: a modular virtual lab that offers a solution to the usability–functionality trade-off by employing a “flexible defaults” design strategy. This strategy enables us to maintain complete “build anything” flexibility while offering a development platform that is accessible to novice programmers. Empirica’s architecture is designed to allow for parameterizable experimental designs, reusable protocols, and rapid development. These features will increase the accessibility of virtual lab experiments, remove barriers to innovation in experiment design, and enable rapid progress in the understanding of human behavior.</text></passage><passage><infon key="file">13428_2020_1535_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>1227</offset><text>Schematic of the design space of lab experiments. Whereas many real-world social processes and phenomenon involve large numbers of people interacting in complex ways over long time intervals (days to years), physical lab experiments are generally constrained to studying individuals or small groups interacting in relatively simple ways over short time intervals (e.g., less than 1 h). The potential of virtual lab experiments is that, in relaxing some of the constraints associated with in-person experiments, they can expand the accessible design space for social, behavioral, and economic experiments</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1831</offset><text>Laboratory experiments are the gold standard for the study of human behavior because they allow careful examination of the complex processes driving information processing, decision-making, and collaboration. Shortly after the World Wide Web had been invented, researchers began to employ “virtual lab” experiments, in which the traditional model of an experiment conducted in a physical lab is translated into an online environment (Musch &amp; Reips,; Horton, Rand, &amp; Zeckhauser,; Mason &amp; Suri,; Reips,; Paolacci, Chandler, &amp; Ipeirotis,). Virtual labs are appealing on the grounds that, in principle, they relax some important constraints on traditional lab experiments that arise from the necessity of physically co-locating human participants in the same room as the experimenter. Most obviously, virtual environments can accommodate much larger groups of participants than can fit in a single physical lab. However, as illustrated in Fig. 1, virtual lab experiments can also run for much longer intervals of time (e.g., days to months rather than hours) than is usually feasible in a physical lab and can also exhibit more complex (e.g., complex network topologies, multifactor treatments) and more digitally realistic designs. Finally, virtual experiments can be run faster and more cheaply than physical lab experiments, allowing researchers to explore more of the design space for experiments, with corresponding improvements in the replicability and robustness of findings. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3316</offset><text>Unfortunately, the potential of virtual lab experiments has thus far been limited by the often-substantial up-front investment in programming and administrative effort required to launch them, effort that is often not transferable from one experiment to the next. An important step towards lowering the barrier to entry for researchers has therefore been the development of general-purpose virtual lab platforms (e.g., Qualtrics, jsPsych, nodeGame, oTree, lab.js). These platforms perform many of the functions of a virtual lab (e.g., data management, assignment to conditions, message handling) without the logic specific to a given experiment. In doing so, however, these platforms also present researchers with a trade-off between usability and flexibility. While some platforms provide graphical user interfaces (GUI) that are accessible to researchers with little or no programming experience, they achieve their usability by limiting the experiment designer to predetermined research paradigms or templates. In contrast, other platforms provide unlimited “build anything” functionality but require advanced programming skills to implement. As a result of this trade-off, many scientifically interesting virtual laboratory experiments that are theoretically possible remain prohibitively difficult to implement in practice.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4649</offset><text>A platform that maintains both usability and functionality will support methodological advancement in at least two high-priority areas. First, a highly usable platform is necessary for designing and administering high-throughput experiments in which researchers can run, in effect, thousands of experimental conditions that systematically cover the parameter space of a given experimental design. A legacy of the traditional lab model is that researchers typically identify one or a few theoretical factors of interest, and focus their experiment on the influence of those factors on some outcome behavior. Selectivity in conditions to be considered is sensible when only small numbers of participants are available. However, when many more participants are available, there is an opportunity to run many more conditions, and it is no longer necessary to focus on those that researchers believe a priori to be the most informative. In principle, researchers can define a set of dimensions along which the experiment can vary, and then a process can be used to generate and sample the set of conditions to be used in the experiment (Balietti, Klein, &amp; Riedl,; McClelland,). For example, this approach was taken in the Choice Prediction Competitions, where human decision-making was studied by automatically generating over 100 pairs of gambles following a predefined algorithm (Erev, Ert, Plonsky, Cohen, &amp; Cohen,; Plonsky et al.,). Recent work took advantage of the larger sample sizes that can be obtained through virtual labs to scale up this approach, collecting human decisions for over 10,000 pairs of gambles (Bourgin, Peterson, Reichman, Russell, &amp; Griffiths,). The resulting data set can be used to evaluate models of decision-making and is at a scale where machine learning methods can be used to augment the insights of human researchers (Agrawal, Peterson, &amp; Griffiths,). Also, there is still a lot of room to develop other kinds of experimental designs that are optimized for the high-throughput environment created by virtual labs. For example, one can navigate the increasingly large spaces of possible conditions and stimuli by making use of adaptive designs that intelligently determine the next conditions to run (Balietti, Klein, &amp; Riedl,; Suchow &amp; Griffiths,; Balandat et al.,). In order to make such experiments feasible, researchers need a platform that enables “experiment-as-code,” in which experiment design, experiment administration, and experiment implementation are separated and treated as code (where each can be formally recorded and replicated). This process allows for parameterizable designs, algorithmic administration, reusable protocols, reduced cost, and rapid development.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7365</offset><text>A second high priority in social science is the implementation of macro-level experiments in which the unit of analysis is a collective entity such as a group (Becker, Brackbill, &amp; Centola,; Whiting et al.,), market (Salganik, Dodds, &amp; Watts,), or an organization (Valentine et al.,) comprising dozens or even hundreds of interacting individuals. As we move up the unit of analysis from individuals to groups, new questions emerge that are not answerable even with a definitive understanding of individual behavior (Schelling,). At its most ambitious, macro-level experimentation offers a new opportunity to run experiments at the scale of societies. Previously, researchers who wanted to run experiments involving the interaction of hundreds of thousands of people only had the opportunity to do so in the context of field experiments. While this approach to experimentation is valuable for providing a naturalistic setting, it has major weaknesses in that such experiments are hard to replicate and typically provide only a single sample. Macro-level lab experiments typically require the design of complex tasks and user interfaces, the ability to facilitate synchronous real-time interaction between participants, and the coordination, recruitment, and engagement of a large number of participants for the duration of the experiment. Implementing large-scale macro experiments remains challenging in the absence of a virtual laboratory designed with multi-participant recruitment, assignment, and interaction as a core principle. Furthermore, running experiments that are both high-throughput and macro-scale requires a platform that simultaneously offers high usability while also maintaining a “build anything” functionality.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9101</offset><text>To promote these methodological goals, Empirica offers a reusable, modular platform that facilitates rapid development through a “flexible default” design. This design provides a platform that is accessible to individuals with basic JavaScript skills but allows advanced users to easily override defaults for increased functionality. Empirica employs design features intended to aid and promote high-throughput and macro-scale experimentation methodologies. For example, the platform explicitly separates experiment design and administration from implementation, promoting the development of reliable, replicable, and extendable research by enabling “experimentation-as-code.” This modular structure encourages strategies such as multifactor (Almaatouq, Noriega-Campero, et al.,), adaptive (Letham, Karrer, Ottoni, &amp; Bakshy,; Balietti et al.,; Paolacci et al.,; Balandat et al.,), and multiphase experimentation designs (Mao, Dworkin, Suri, &amp; Watts,; Almaatouq, Yin, &amp; Watts,), which dramatically expand the range of experimental conditions that can be studied. Additionally, the platform provides built-in data synchronization, concurrency control, and reactivity to natively support multi-participant experiments and support the investigation of macro-scale research questions. Empirica requires greater technical skill than GUI platforms, a design choice that responds to the emerging quorum of computational social scientists with moderate programming skills. Thus Empirica is designed to be “usable” for the majority of researchers while maintaining uncompromised functionality, i.e., the ability to build anything that can be displayed in a web browser.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10773</offset><text>After reviewing prior solutions, this paper provides a technical and design overview of Empirica. We then discuss several case studies in which Empirica was successfully employed to address ongoing research problems, and discuss the methodological advantages of Empirica. We conclude with a discussion of limitations and intended directions for future development. Throughout this paper, we will refer to “games” (experimental trials) as the manner in which “players” (human participants or artificial bots) interact and provide their data to researchers. This usage is inspired by the definition of human computation as “games with a purpose” (von Ahn &amp; Dabbish,), although many of the tasks would not be recognized as games as such.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>11520</offset><text>Related work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11533</offset><text>Virtual lab participants</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11558</offset><text>It has long been recognized that the internet presents researchers with new opportunities to recruit remote participants for behavioral, social, and economic experiments (Grootswagers,). For instance, remote participation allows researchers to solve some of the issues that limit laboratory research, such as (1) recruiting more diverse samples of participants than are available on college campuses or in local communities (Reips,; Berinsky, Huber, &amp; Lenz,); (2) increasing statistical power by enabling access to larger samples (Awad et al.,; Reips,); and (3) facilitating longitudinal and other multiphase studies by eliminating the need for participants to repeatedly travel to the laboratory (Almaatouq, Yin, &amp; Watts,; Reips,). The flexibility around time and space that is afforded by remote participation has enabled researchers to design experiments that would be difficult or even impossible to run in a physical lab.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12485</offset><text>Arguably the most common current strategy for recruiting online participants involves crowdsourcing services (Horton et al.,; Mason &amp; Suri,). The main impact of these services has been to dramatically reduce the cost per participant in lab studies, resulting in an extraordinary number of publications in the past decade. Unfortunately, a limitation of the most popular platforms such as Amazon Mechanical Turk or TurkPrime (Litman, Robinson, &amp; Abberbock,) is that they were designed for simple labeling tasks that can typically be completed independently and with little effort by individual “workers” who vary widely in quality and persistence on the service (Goodman, Cryder, &amp; Cheema,). Moreover, Amazon’s terms of use prevent researchers from knowing whether their participants have participated in similar experiments in the past, raising concerns that many Amazon “turkers” are becoming “professional” experiment participants (Chandler, Mueller, &amp; Paolacci,). In response to concerns such as these, services such as Prolific1 (Palan &amp; Schitter,) have adapted the crowd work model to accommodate the special needs of behavioral research. For example, Prolific offers researchers more control over participant sampling and quality as well as recruiting participants who are intrinsically motivated to contribute to scientific studies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13839</offset><text>In addition to crowdsourcing services, online experiments have attracted even larger and more diverse populations of participants who participate voluntarily out of intrinsic interest to assist in scientific research. For example, one experiment collected almost 40 million moral decisions from over a million unique participants in over 200 countries (Awad et al.,). Unfortunately, while the appeal of “massive samples for free” is obvious, all such experiments necessarily rely on some combination of gamification, personalized feedback, and other strategies to make participation intrinsically rewarding (Hartshorne, Leeuw, Goodman, Jennings, &amp; O’Donnell,). As a consequence, the model has proven hard to generalize to arbitrary research questions of interest.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>14609</offset><text>Existing virtual lab solutions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14640</offset><text>While early online experiments often required extensive up-front customized software development, a number of virtual lab software packages and frameworks have now been developed that reduce the overhead associated with building and running experiments. As a result, it is now easier to implement designs in which dozens of individuals interact synchronously in groups (Arechar, Gächter, &amp; Molleman,; Almaatouq, Yin, &amp; Watts,; Whiting, Blaising, et al.,) or via networks (Becker et al.,), potentially comprising a mixture of human and algorithmic agents (Ishowo-Oloko et al.; Traeger, Sebo, Jung, Scassellati, &amp; Christakis,; Shirado &amp; Christakis,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15290</offset><text>Virtual lab solutions can be roughly grouped by their emphasis on usability or functionality. Here we describe free or open-source tools that allow synchronous, real-time interaction between participants, leaving aside tools such as jsPsych (de Leeuw,), lab.js (Henninger, Shevchenko, Mertens, Kieslich, &amp; Hilbig,), and Pushkin (Hartshorne et al.,) that do not explicitly support multi-participant interactions as well as commercial platforms such as Testable, Inquisit, Labvanced (Finger, Goeke, Diekamp, Standvoß, &amp; König,, and Gorilla (Anwyl-Irvine, Massonnié, Flitton, Kirkham, &amp; Evershed,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15889</offset><text>Platforms such as WEXTOR (Reips &amp; Neuhaus,), Breadboard (McKnight &amp; Christakis,), and LIONESS (Giamattei, Molleman, Seyed Yahosseini, &amp; Gächter,) provide excellent options for individuals with little-to-no coding experience. These platforms allow researchers to design their experiments either directly with a graphical user interface (GUI) or via a simple, proprietary scripting language. However, while these structures enable researchers to quickly develop experiments within predetermined paradigms, they constrain the range of possible interface designs. These platforms do not allow the researcher to design “anything that can run in a web browser.”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16550</offset><text>On the other hand, many excellent tools including oTree (Chen, Schonger, &amp; Wickens,), nodeGame (Balietti,), Dallinger,2 and TurkServer (Mao et al.,) offer high flexibility in experiment design. However, this flexibility comes at the expense of decreased usability, as these tools require significant time and skill to employ. They are flexible precisely because they are very general, which means additional labor is required to achieve any complete design.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>17008</offset><text>Empirica</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17017</offset><text>The Empirica platform3 is a free, open-source, general-purpose virtual lab platform for developing and conducting synchronous and interactive human-participant experiments. The platform implements an application programming interface (API) that allows an experiment designer to devote their effort to implementing participant-facing views and experiment-specific logic. In the background, Empirica handles the necessary but generic tasks of coordinating browser–server interactions, batching participants, launching games, and storing and retrieving data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17575</offset><text>Experiments are deployed from a GUI web interface that allows the researcher to watch the experiment progress in real time. With no installation required on the participant’s part, experiments can run on any web browser including desktop computers, laptops, smartphones, and tablets (See Appendix).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17877</offset><text>Empirica is designed using a “flexible default” strategy: the platform provides a default structure and settings that enable novice JavaScript users to design an experiment by modifying pre-populated templates; at the same time, unlimited customization is possible for advanced users. The goal of this design is to develop a platform that is accessible to researchers with modest programming experience—the target user is the typical computational social science researcher—while maintaining a “build anything” level of flexibility.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18422</offset><text>Empirica has an active and growing community of contributors, including professional developers, method-focused researchers, question-driven social scientists, and outcome-oriented professionals. Although Empirica is under steady development, it has already been used to build (at least) 31 experiments by more than 18 different research teams across 12 different institutions, generating at least 12 manuscripts between 2019 and 2020 (Feng, Carstensdottir, El-Nasr, &amp; Marsella,; Pescetelli, Rutherford, Kao, &amp; Rahwan,; Becker, Porter, &amp; Centola,; Becker, Guilbeault, &amp; Smith,; Almaatouq, Noriega-Campero, et al.,; Houhton; Becker, Almaatouq, &amp; Horvat,; Almaatouq, Yin, &amp; Watts,; Noriega et al.; Feng; Guilbeault, Woolley, &amp; Becker,; Jahani et al.).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>19172</offset><text>System design</text></passage><passage><infon key="file">13428_2020_1535_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19186</offset><text>Empirica provides a scaffolding for researchers to design and administer experiments via three components: (1) Server-side callbacks use JavaScript to define the running of a game through the client-side and server-side API; (2) the client-side interface uses JavaScript to define the player experience; and (3) the GUI admin interface enables configuration and monitoring of experiments (see Appendix). These components are all run and connected by the Empirica core engine</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19662</offset><text>Empirica’s architecture was designed from the start to enable real-time multi-participant interactions, although single-player experiments are easy to create as well. The API is purposefully concise, using a combination of data synchronization primitives and callbacks (i.e., event hooks) triggered in different parts of the experiment. The core functionality is abstracted by the platform: data synchronization, concurrency control, reactivity, network communication, experiment sequencing, persistent storage, timer management, and other low-level functions are provided automatically by Empirica. As a result, researchers can focus on designing the logic of their participants’ experience (see Fig. 2 for an overview). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20390</offset><text>To initiate development, Empirica provides an experiment scaffold generator that initializes an empty (but fully functioning) experiment and a simple project organization that encourages modular thinking. To design an experiment, researchers separately configure the client (front end), which defines everything that participants experience in their web browser, thus defining the experimental treatment or stimulus, and the server (back end), which consists of callbacks defining the logic of an experimental trial. The front end consists of a sequence of five modules: consent, intro (e.g., instructions, quiz), lobby, game, and outro (e.g., survey). The lobby4 serves the purpose of starting a new experimental trial when specific criteria are met (e.g., a certain number of participants are simultaneously connected) and it is automatically generated and managed by Empirica according to parameters set in the GUI. The researcher need only modify the intro, outro, and game design via JavaScript. The back end consists of callbacks defining game initialization, start and end behavior for rounds and stages, and event handlers for changes in data states.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21549</offset><text>Empirica structures the game (experimental trial) as players (humans or artificial participants) interacting in an environment defined by one or more rounds (to allow for “repeated” play); each round consists of one or more stages (discrete time steps), and each stage allows players to interact continuously in real time. Empirica provides a timer function that can automatically advance the game from stage to stage, or researchers can define logic that advances games based on participant behavior or other conditions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22075</offset><text>As Empirica requires some level of programming experience for experiment development, the platform accommodates the possibility that different individuals may be responsible for designing, programming, and administering experiments. To support this division of labor, Empirica provides a high-level interface for the selection of experimental conditions and the administration of live trials. From this interface, experiment administrators can assign players to trials, manage participants, and monitor the status of games. Experiment designers can configure games to have different factors and treatments, and export or import machine-readable YAML5 files that fully specify entire experiment protocols (i.e., the data generation process) and support replication via experiment-as-code. Experiment configuration files can also be generated programmatically by researchers wishing to employ procedural generation and adaptive experimentation methods to effectively and efficiently explore the parameter space.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23085</offset><text>The ultimate test of an experiment’s design is that it is able to evaluate its target theory. In addition to creating artificial players to use as part of an experiment, Empirica’s “bot” API also allows users to perform full integration tests of their experiment. By simulating the complete experiment under all treatments with simulated participants, the experiment designer can ensure that their as-implemented design matches their expectation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23540</offset><text>Implementation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23555</offset><text>Empirica is built using common web development tools. It is based on the Meteor6 application development framework and employs JavaScript on both the front end (browser) and the back end (server). Meteor implements tooling for data reactivity around the MongoDB database, WebSockets, and RPC (remote procedure calls). Meteor also has strong authentication, which secures the integrated admin interface (see Appendix). Experiment designers will not need to be familiar with Meteor to use the Empirica platform. Only those who wish to contribute to the development of Empirica and contribute to the codebase will need to use Meteor.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24187</offset><text>The front end is built with the UI framework React,7 which supports the system’s reactive data model. Automatic data reactivity implemented by Empirica alleviates the need for the experiment designer to be concerned with data synchronization between players. React has a vibrant and growing ecosystem, with many resources from libraries to online courses to a large talent pool of experienced developers, and is used widely in production in a variety of combinations with different frameworks (Fedosejev,; Wieruch,). For Empirica, React is also desirable because it encourages a modular, reusable design philosophy. Empirica extends these front-end libraries by providing experimenter-oriented UI components such as breadcrumbs showing experiment progression, player profile displays, and user input components (e.g., Sliders, text-based Chat, Random Dot Kinematogram). These defaults reduce the burden on experiment designers while maintaining complete customizability. It is important to note that it is up to the experiment developer to follow the best practices of UI development that are appropriate for their experiment. For instance, behavioral researchers interested in timing-dependent procedures should be cautious when developing their UIs and should test the accuracy and precision of the experimental interface (Garaizar &amp; Reips,). Similarly, browser compatibility will depend on which React packages are being used in the particular experiment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25648</offset><text>Empirica’s back end is implemented in node.js8—a framework for developing high-performance, concurrent programs (Tilkov &amp; Vinoski,). Callbacks are the foundation of the server-side API. Callbacks are hooks where the experiment developer can add custom behavior. These callbacks are triggered by events of an experiment run (e.g., onRoundStart, onRoundEnd, onGameEnd, etc.). The developer is given access to the data related to each event involving players and games and can thus define logic in JavaScript that will inspect and modify this data as experiments are running.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26225</offset><text>This design allows Empirica to reduce the technical burden on experiment designers by providing a data interface that is tailored to the needs of behavioral lab experiments. The developer has no need to interact with the database directly. Rather, Empirica provides simple accessors (get, set, append, log) that facilitate data monitoring and updating. These accessor methods are available on both the front end and the back end. All data are scoped to an experiment-relevant construct such as game, player, round, or stage. Data can also be scoped to the intersection of two constructs, e.g., a player and a game object: player.round and player.stage, which contain the data for a player at a given round or stage. The accessor methods are reactive, meaning that data is automatically saved and propagated to all players. Empirica’s front end and back end are connected over WebSocket (a computer communications protocol), where a heartbeat (or ping) continuously monitors the connection and allows the server to determine if the client is still responsive. On the player side, on disconnection, the client will passively attempt to reconnect with a session identifier stored in the browser’s local storage. From the experiment developers’ point of view, they can configure the experiment to: (1) continue without the missing player; (2) cancel the entire experimental trial; (3) pause the experimental trial (currently being implemented for future release); or (4) implement a custom behavior (e.g., a combination of 1–3).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27758</offset><text>Another ease-of-use feature is that an Empirica experiment is initialized with a one-line command in the terminal (Windows, macOS, Linux) to populate an empty project scaffold. A simple file structure separates front-end (client) code from back-end (server) code to simplify the development process. Because Empirica is built using the widely adopted Meteor framework, a completed experiment can also be deployed with a single command to either an in-house server or to a software-as-a-service platform such as Meteor Galaxy. Additionally, Empirica provides its own simple open-source tool to facilitate deploying Empirica experiments to the cloud for production.9 This facilitates iterative development cycles in which researchers can rapidly revise and redeploy experiment designs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28542</offset><text>Empirica is designed to operate with online labor markets such as Prolific or other participant recruitment sources (e.g., volunteers, in-person participants, classrooms).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>28714</offset><text>Case studies</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28727</offset><text>Throughout its development, Empirica has been used in the design of cutting-edge experimental research. Below, we illustrate Empirica’s power and flexibility in four examples, each of which highlights a different functionality. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>28958</offset><text>Exploring the parameter space: dynamic social networks and collective intelligence</text></passage><passage><infon key="file">13428_2020_1535_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29041</offset><text>This screenshot of the “Guess the Correlation Game” shows the view that participants use to update their social network in the dynamic network condition with full feedback (i.e., as opposed to no feedback or only self-feedback). In all of the experimental condition, the maximum number of outgoing connections was set to 3 and the group size is set to 12. The interface uses reactive and performant front-end components</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29465</offset><text>The “Guess the Correlation” (Almaatouq, Noriega-Campero, et al.,)10 game was developed to study how individual decisions shape social network structure ultimately determining group accuracy (Fig. 3). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29671</offset><text>In this game, participants were tasked with estimating statistical correlations from a visual plot of two variables (such as height and weight). For each image, participants first guessed individually and could then update their guesses while seeing other participants guesses and updates in real time. Between rounds, participants could see feedback on each other’s accuracy and could add/drop people from the social network that determined whose answers were shown.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30141</offset><text>In this game, participants were tasked with estimating statistical correlations from a visual plot of two variables (such as height and weight). For each image, participants first guessed individually and could then update their guesses while seeing other participants guesses and updates in real time. Between rounds, participants could see feedback on each other’s accuracy and could add/drop people from the social network that determined whose answers were shown.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30611</offset><text>The final publication reported seven experimental conditions with three varied levels of social interaction and four levels of performance feedback, and found that a variety of subtle changes could dramatically influence macro-scale group outcomes. The results show that even subtle changes in the environment can lead to dramatically different macro-scale group outcomes despite any micro-scale changes in individual experience.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>31041</offset><text>Real-time interaction at scale: A large-scale game of high-speed “Clue”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31117</offset><text>The “Detective Game” (Houghton,)11 examined the effect of belief interaction on social contagion. In the game, teams of 20 players worked together to solve a mystery by exchanging clues. To coordinate recruitment and ensure proper randomization, the experimenter planned to recruit up to 320 participants to participate in each block of games.</text></passage><passage><infon key="file">13428_2020_1535_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>31465</offset><text>This screenshot of the “Detective Game” shows the view that participants use to categorize mystery clues as either Promising Leads (which are shared with their social network neighbors) or Dead Ends (which are not). The interface uses reactive and performant front-end components</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31749</offset><text>However, this number of simultaneous participants is two orders of magnitude larger than in typical behavioral experiments, and the participants needed to interact in real time. The interface showed players when peers updated their beliefs and when they added clues around to their “detective’s notebook,” as shown in Fig. 4. The experimenter needed a platform with short load times, high-performance display libraries, and imperceptible latency at scale. At the same time, their code needed to be readable enough for academic transparency. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32298</offset><text>The experimenter used Empirica’s “flexible default” design and modular API to quickly evaluate a number of open-source display libraries, selecting from the multiplicity of modern web tools those which best supported the experiment. They then used Empirica’s “bot” API to simulate player’s actions in the game, testing that the back-end could provide the low-latency coordination between client and server crucial to the game’s performance. The experiment confirmed theoretical predictions that belief interaction could lead to social polarization.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>32863</offset><text>Two-phase experiment design: Distributed human computation problems</text></passage><passage><infon key="file">13428_2020_1535_Fig5_HTML.jpg</infon><infon key="id">Fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32931</offset><text>This screenshot shows the “Room Assignment” task. The real-time interaction, the ability to assign students to rooms in parallel, and text-based chat employs default features and interaction components provided by Empirica</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33158</offset><text>The “Room Assignment” game (Almaatouq, Yin, &amp; Watts,)12 explored how factors such as task complexity and group composition allow a collaborating team to outperform its individual members. The task consisted of a “constraint satisfaction and optimization” problem in which N “students” were to be assigned to M “dorm rooms”, subject to constraints and preferences (Fig. 5). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33549</offset><text>Unlike many group experiments, this study required the same group of participants to perform the task twice. In the first round, participants needed to perform the task individually so that their individual skill level, social perceptiveness, and cognitive style could be measured. Then, in the second round, participants would be assigned to collaborate in teams using Empirica’s included chatroom plugin chat, a standard Empirica plugin13. This simple design enabled researchers to measure task performance for independent and interacting groups while controlling communication, group composition, and task complexity.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34172</offset><text>The experimenters used Empirica’s careful participant data management and flexible randomization architecture to reliably match the same subject pool across the two phases of this experiment and to coordinate the large block-randomized design. While this may have been possible with other platforms, Empirica’s admin interface made these considerations as simple as making selections from a drop-down list.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>34583</offset><text>Rapid-turnaround replication: Echo chambers and belief accuracy</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34647</offset><text>The “Estimation Challenge” experiment (Becker, Porter, &amp; Centola,)14 tested how politically biased echo chambers shape belief accuracy and polarization. Participants answered factual questions (such as “How has the number of unauthorized immigrants living in the US changed in the past 10 years?”) before and after observing answers given by other participants. The experimenters found that collective intelligence can increase accuracy and decreased polarization despite popular arguments to the contrary.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35162</offset><text>This experiment was implemented using a custom platform in partnership with a third-party developer and generated an ad hoc social network to determine how information flowed among participants. After submitting these results for publication, the reviewers expressed concern that the experiment design did not fully capture the effects of a politicized environment. The experimenters were given 60 days to revise and resubmit their paper.</text></passage><passage><infon key="file">13428_2020_1535_Fig6_HTML.jpg</infon><infon key="id">Fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35601</offset><text>This screenshot shows the second stage of the first round of the revised “Politics Challenge” estimation task. The illustrated breadcrumb feature employs customized default UI elements provided from Empirica, and the timer was employed without modification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35862</offset><text>Revising the original interface in the time available or rehiring the original developer would have required skills or monetary resources not available to the project. Using Empirica, they were able to replicate the initial experiment with a modified user interface to address the questions posed by reviewers, as seen in Fig. 6. The new interface was designed, constructed, and tested in approximately 2 weeks. This experiment required negligible alteration from the prepopulated Empirica scaffolding beyond customizing the visual design and introductory steps, demonstrating the capability of flexible defaults. </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>36478</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>36489</offset><text>Ethical considerations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36512</offset><text>As with any human-subjects research, virtual lab experiments are subject to ethical considerations. These include (but are not limited to) pay rates for participants (Whiting, Hugh, &amp; Bernstein,), data privacy protection (Birnbaum,), and the potential psychological impact of stimulus design. While most of these decisions will be made by the researchers implementing an experiment using Empirica, we have adopted a proactive strategy that employs default settings designed to encourage ethical experiment design. As one example, the initial scaffolding generated by Empirica includes a template for providing informed consent, considered a bare minimum for ethical research practice. The scaffolding also includes a sample exit survey which models inclusive language; e.g., the field for gender is included as a free-text option. To encourage privacy protection, Empirica by default omits external identifiers when exporting data to prevent leaking of personal information such as e-mail addresses or Amazon Turk account identifiers.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>37547</offset><text>Limitations and future developments</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37583</offset><text>As with other leading computational tools, Empirica is not a static entity, but a continually developing project. This paper reflects the first version of the Empirica platform, which lays the groundwork for an ecosystem of tools to be built over time. Due to its design, modules that are part of the current platform can be switched out and improved independently without rearchitecting the system. Indeed it is precisely because Empirica (or for that matter, any experiment platform) cannot be expected to offer optimal functionality indefinitely that this modular design was chosen.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38169</offset><text>The usability–functionality trade-off faced by existing experiment platforms is endemic to tightly integrated “end-to-end” solutions developed for a particular class of problems. By moving toward an ecosystem approach, Empirica has a chance to resolve this trade-off. As such, future development of Empirica will include the development of a set of open standards that defines what this encapsulation (service/component) is, how to communicate with it, and how to find and use it.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38656</offset><text>An ecosystem will allow the reuse of software assets, in turn lowering development costs, decreasing development time, reducing risk, and leveraging existing platform investments and strengths.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38850</offset><text>The individual components of the ecosystem will be loosely coupled to reduce vendor/provider lock-in and create a flexible infrastructure. As a result, the individual components of the ecosystem will be modular in the sense that each can be modified or replaced without needing to modify or replace any other component because the interface to the component remains the same. The resulting functional components will be available for end users (i.e., researchers) to amalgamate (or mashup) into situational, creative, and novel experiments in ways that the original developers may not originally envision.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39456</offset><text>The functional scope of these components will allow for the possibility to directly define experiment requirements as a collection of these functional components, rather than translating experiment requirements into lower-level software development requirements. As a result, the ecosystem will abstract away many of the logistical concerns of running experiments, analogous to how cloud computing has abstracted away from the management of technical resources for many companies.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39937</offset><text>The use of the “ecosystem” as a design principle presents several opportunities for operational efficiency.  </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40051</offset><text>By distancing ourselves from a monolithic approach, and adopting a truly modular architecture with careful design of the low-level abstractions of experiments, we hope Empirica will decouple flexibility from ease-of-use and open the door to an economy of software built around conducting new kinds of virtual labs experiments.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>40378</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40390</offset><text>Empirica provides a complete virtual lab for designing and running online lab experiments taking the form of anything that can be viewed in a web browser. The primary philosophy guiding the development of Empirica is the use of “flexible defaults,” which is core to our goal of providing a “do anything” platform that remains accessible to a typical computational social scientist. In its present form, Empirica enables rapid development of virtual lab experiments, and the researcher need only provide a recruitment mechanism to send participants to the page at the appropriate time. Future versions of Empirica will abstract the core functionality into an ecosystem that allows the development and integration of multiple tools including automated recruitment. This future version will also maintain as a “tool” the current Empirica API, continuing to enable the rapid development of experiments.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">title_1</infon><offset>41301</offset><text>Appendix: Empirica Admin Interface</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>41336</offset><text>View of the admin interface provided by Empirica. Panel (A) shows the experiment “monitoring” view. Panel (B) shows the experiment “configuration” view. </text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>41498</offset><text>www.prolific.co</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>41514</offset><text>docs.dallinger.io</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>41532</offset><text>empirica.ly</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>41544</offset><text>Because participants usually do not arrive at precisely the same time, and also because different participants require more or less time to read the instructions and pass the quiz, Empirica implements a virtual “lobby” feature. While waiting in the lobby, participants receive information about how much time they have been waiting and how many other players are still needed for the experiment to start.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>41953</offset><text>YAML Ain’t Markup Language (YAML) is a data serialization language designed to be human-friendly and work well with modern programming languages (Ben-Kiki, Evans, &amp; Ingerson,).</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42132</offset><text>www.meteor.com</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42147</offset><text>reactjs.org</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42159</offset><text>nodejs.org</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42170</offset><text>github.com/empiricaly/meteor-deploy</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42206</offset><text>The source code for the “Guess the Correlation” experiment can be found at https://github.com/amaatouq/guess-the-correlation</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42335</offset><text>The source code for the “Detective Game” experiment can be found at https://github.com/JamesPHoughton/detective_game_demo</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42461</offset><text>The source code for the “Room Assignment” experiment can be found at https://github.com/amaatouq/room-assignment</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42578</offset><text>The Chat component is available at https://github.com/empiricaly/chat</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42648</offset><text>The source code for the “Estimation Challenge” experiment can be found at https://github.com/joshua-a-becker/politics-challenge</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42780</offset><text>github.com/empiricaly</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42802</offset><text>Publisher’s note</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42821</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42940</offset><text>Open Practices Statements</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">footnote</infon><offset>42966</offset><text>Empirica is entirely open-source and in active development. The codebase is currently hosted on Github.15 Documentation and tutorial videos are available at https://docs.empirica.ly/. We encourage readers who are interested in the software to contribute ideas or code that can make it more useful to the community.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>43281</offset><text>References</text></passage><passage><infon key="fpage">8825</infon><infon key="issue">16</infon><infon key="lpage">8835</infon><infon key="name_0">surname:Agrawal;given-names:M</infon><infon key="name_1">surname:Peterson;given-names:JC</infon><infon key="name_2">surname:Griffiths;given-names:TL</infon><infon key="pub-id_doi">10.1073/pnas.1915841117</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">117</infon><infon key="year">2020</infon><offset>43292</offset><text>Scaling up psychology via scientific regret minimization</text></passage><passage><infon key="fpage">11379</infon><infon key="issue">21</infon><infon key="lpage">11386</infon><infon key="name_0">surname:Almaatouq;given-names:A</infon><infon key="name_1">surname:Noriega-Campero;given-names:A</infon><infon key="name_2">surname:Alotaibi;given-names:A</infon><infon key="name_3">surname:Krafft;given-names:PM</infon><infon key="name_4">surname:Moussaid;given-names:M</infon><infon key="name_5">surname:Pentland;given-names:A</infon><infon key="pub-id_doi">10.1073/pnas.1917687117</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">117</infon><infon key="year">2020</infon><offset>43349</offset><text>Adaptive social networks promote the wisdom of crowds</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>43403</offset><text>Almaatouq, A., Yin, M., &amp; Watts, D.J. (2020). Collective problem-solving of groups across tasks of varying complexity. (PsyArXiv preprint).</text></passage><passage><infon key="fpage">388</infon><infon key="issue">1</infon><infon key="lpage">407</infon><infon key="name_0">surname:Anwyl-Irvine;given-names:AL</infon><infon key="name_1">surname:Massonnié;given-names:J</infon><infon key="name_2">surname:Flitton;given-names:A</infon><infon key="name_3">surname:Kirkham;given-names:N</infon><infon key="name_4">surname:Evershed;given-names:JK</infon><infon key="pub-id_doi">10.3758/s13428-019-01237-x</infon><infon key="pub-id_pmid">31016684</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">52</infon><infon key="year">2020</infon><offset>43543</offset><text>Gorilla in our midst: an online behavioral experiment builder</text></passage><passage><infon key="fpage">99</infon><infon key="issue">1</infon><infon key="lpage">131</infon><infon key="name_0">surname:Arechar;given-names:AA</infon><infon key="name_1">surname:Gächter;given-names:S</infon><infon key="name_2">surname:Molleman;given-names:L</infon><infon key="pub-id_doi">10.1007/s10683-017-9527-2</infon><infon key="pub-id_pmid">29449783</infon><infon key="section_type">REF</infon><infon key="source">Experimental Economics</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2018</infon><offset>43605</offset><text>Conducting interactive experiments online</text></passage><passage><infon key="fpage">59</infon><infon key="issue">7729</infon><infon key="lpage">64</infon><infon key="name_0">surname:Awad;given-names:E</infon><infon key="name_1">surname:Dsouza;given-names:S</infon><infon key="name_2">surname:Kim;given-names:R</infon><infon key="name_3">surname:Schulz;given-names:J</infon><infon key="name_4">surname:Henrich;given-names:J</infon><infon key="name_5">surname:Shariff;given-names:A</infon><infon key="pub-id_doi">10.1038/s41586-018-0637-6</infon><infon key="pub-id_pmid">30356211</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">563</infon><infon key="year">2018</infon><offset>43647</offset><text>The moral machine experiment</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>43676</offset><text>Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.G., &amp; et al. (2020). Botorch: A framework for efficient Monte-Carlo Bayesian optimization. Advances in Neural Information Processing Systems, 33.</text></passage><passage><infon key="fpage">1696</infon><infon key="issue">5</infon><infon key="lpage">1715</infon><infon key="name_0">surname:Balietti;given-names:S</infon><infon key="pub-id_doi">10.3758/s13428-016-0824-z</infon><infon key="pub-id_pmid">27864814</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2017</infon><offset>43894</offset><text>nodegame: Real-time, synchronous, online experiments in the browser</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>43962</offset><text>Balietti, S., Klein, B., &amp; Riedl, C. (2020a). Optimal design of experiments to identify latent behavioral types. Experimental Economics.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>44099</offset><text>Balietti, S., Klein, B., &amp; Riedl, C. (2020b). Optimal design of experiments to identify latent behavioral types. Experimental Economics, pp. 1–28.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>44248</offset><text>Becker, J., Almaatouq, A., &amp; Horvat, A. (2020). Network structures of collective intelligence: The contingent benefits of group discussion. arXiv preprint arXiv:2009.07202.</text></passage><passage><infon key="fpage">E5070</infon><infon key="issue">26</infon><infon key="lpage">E5076</infon><infon key="name_0">surname:Becker;given-names:J</infon><infon key="name_1">surname:Brackbill;given-names:D</infon><infon key="name_2">surname:Centola;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2017</infon><offset>44421</offset><text>Network dynamics of social influence in the wisdom of crowds</text></passage><passage><infon key="fpage">13404</infon><infon key="name_0">surname:Becker;given-names:J</infon><infon key="name_1">surname:Guilbeault;given-names:D</infon><infon key="name_2">surname:Smith;given-names:EB</infon><infon key="pub-id_doi">10.5465/AMBPP.2019.13404abstract</infon><infon key="section_type">REF</infon><infon key="source">Academy of Management Proceedings</infon><infon key="type">ref</infon><infon key="volume">2019</infon><infon key="year">2019</infon><offset>44482</offset><text>The crowd classification problem</text></passage><passage><infon key="fpage">10717</infon><infon key="issue">2</infon><infon key="lpage">10722</infon><infon key="name_0">surname:Becker;given-names:J</infon><infon key="name_1">surname:Porter;given-names:E</infon><infon key="name_2">surname:Centola;given-names:D</infon><infon key="pub-id_doi">10.1073/pnas.1817195116</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">116</infon><infon key="year">2019</infon><offset>44515</offset><text>The wisdom of partisan crowds</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>44545</offset><text>Ben-Kiki, O., Evans, C., &amp; Ingerson, B. (2009). Yaml ain’t markup language (yamlTM) version 1.1. Retrieved from https://yaml.org/spec/cvs/spec.pdf (Working Draft 2008–05).</text></passage><passage><infon key="fpage">351</infon><infon key="issue">3</infon><infon key="lpage">368</infon><infon key="name_0">surname:Berinsky;given-names:AJ</infon><infon key="name_1">surname:Huber;given-names:GA</infon><infon key="name_2">surname:Lenz;given-names:GS</infon><infon key="pub-id_doi">10.1093/pan/mpr057</infon><infon key="section_type">REF</infon><infon key="source">Political Analysis</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2012</infon><offset>44721</offset><text>Evaluating online labor markets for experimental research: Amazon Mechanical Turk</text></passage><passage><infon key="fpage">803</infon><infon key="lpage">832</infon><infon key="name_0">surname:Birnbaum;given-names:MH</infon><infon key="pub-id_doi">10.1146/annurev.psych.55.090902.141601</infon><infon key="section_type">REF</infon><infon key="source">Annual Review of Psychology</infon><infon key="type">ref</infon><infon key="volume">55</infon><infon key="year">2004</infon><offset>44803</offset><text>Human research and data collection via the Internet</text></passage><passage><infon key="fpage">5133</infon><infon key="lpage">5141</infon><infon key="name_0">surname:Bourgin;given-names:DD</infon><infon key="name_1">surname:Peterson;given-names:JC</infon><infon key="name_2">surname:Reichman;given-names:D</infon><infon key="name_3">surname:Russell;given-names:SJ</infon><infon key="name_4">surname:Griffiths;given-names:TL</infon><infon key="section_type">REF</infon><infon key="source">In Proceedings of Machine Learning Research</infon><infon key="type">ref</infon><infon key="volume">97</infon><infon key="year">2019</infon><offset>44855</offset><text>Cognitive model priors for predicting human decisions</text></passage><passage><infon key="fpage">112</infon><infon key="lpage">130</infon><infon key="name_0">surname:Chandler;given-names:J</infon><infon key="name_1">surname:Mueller;given-names:P</infon><infon key="name_2">surname:Paolacci;given-names:G</infon><infon key="pub-id_doi">10.3758/s13428-013-0365-7</infon><infon key="pub-id_pmid">23835650</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2014</infon><offset>44909</offset><text>Nonnaïveté among Amazon Mechanical Turk workers: Consequences and solutions for behavioral researchers</text></passage><passage><infon key="fpage">88</infon><infon key="lpage">97</infon><infon key="name_0">surname:Chen;given-names:DL</infon><infon key="name_1">surname:Schonger;given-names:M</infon><infon key="name_2">surname:Wickens;given-names:C</infon><infon key="pub-id_doi">10.1016/j.jbef.2015.12.001</infon><infon key="section_type">REF</infon><infon key="source">Journal of Behavioral and Experimental Finance</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2016</infon><offset>45014</offset><text>oTree–an open-source platform for laboratory, online, and field experiments</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:de Leeuw;given-names:JR</infon><infon key="pub-id_doi">10.3758/s13428-014-0458-y</infon><infon key="pub-id_pmid">24683129</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>45092</offset><text>jsPsych: a JavaScript library for creating behavioral experiments in a web browser</text></passage><passage><infon key="fpage">369</infon><infon key="issue">4</infon><infon key="lpage">409</infon><infon key="name_0">surname:Erev;given-names:I</infon><infon key="name_1">surname:Ert;given-names:E</infon><infon key="name_2">surname:Plonsky;given-names:O</infon><infon key="name_3">surname:Cohen;given-names:D</infon><infon key="name_4">surname:Cohen;given-names:O</infon><infon key="pub-id_doi">10.1037/rev0000062</infon><infon key="pub-id_pmid">28277716</infon><infon key="section_type">REF</infon><infon key="source">Psychological Review</infon><infon key="type">ref</infon><infon key="volume">124</infon><infon key="year">2017</infon><offset>45175</offset><text>From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45293</offset><text>Fedosejev, A. (2015). React.js essentials. Packt Publishing Ltd.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45358</offset><text>Feng, D. (2020). Towards socially interactive agents: Learning generative models of social interactions via crowdsourcing. Unpublished doctoral dissertation, Northeastern University.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45541</offset><text>Feng, D., Carstensdottir, E., El-Nasr, M.S., &amp; Marsella, S. (2019). Exploring improvisational approaches to social knowledge acquisition. In Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems (pp. 1060–1068).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45794</offset><text>Finger, H., Goeke, C., Diekamp, D., Standvoß, K., &amp; König, P. (2017). Labvanced: A unified JavaScript framework for online studies. In International Conference on Computational Social Science (Cologne).</text></passage><passage><infon key="fpage">1441</infon><infon key="issue">3</infon><infon key="lpage">1453</infon><infon key="name_0">surname:Garaizar;given-names:P</infon><infon key="name_1">surname:Reips;given-names:U-D</infon><infon key="pub-id_doi">10.3758/s13428-018-1126-4</infon><infon key="pub-id_pmid">30276629</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2019</infon><offset>45999</offset><text>Best practices: Two web-browser-based methods for stimulus presentation in behavioral experiments with high-resolution timing requirements</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46138</offset><text>Giamattei, M., Molleman, L., Seyed Yahosseini, K., &amp; Gächter, S. (2019). Lioness lab-a free web-based platform for conducting interactive experiments online. (SSRN preprint).</text></passage><passage><infon key="fpage">213</infon><infon key="issue">3</infon><infon key="lpage">224</infon><infon key="name_0">surname:Goodman;given-names:JK</infon><infon key="name_1">surname:Cryder;given-names:CE</infon><infon key="name_2">surname:Cheema;given-names:A</infon><infon key="pub-id_doi">10.1002/bdm.1753</infon><infon key="section_type">REF</infon><infon key="source">Journal of Behavioral Decision Making</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2013</infon><offset>46314</offset><text>Data collection in a flat world: The strengths and weaknesses of Mechanical Turk samples</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46403</offset><text>Grootswagers, T. (2020). A primer on running human behavioural experiments online. Behavior Research Methods.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46513</offset><text>Guilbeault, D., Woolley, S., &amp; Becker, J. (2020). Probabilistic social learning improves the public’s detection of misinformation.</text></passage><passage><infon key="fpage">1782</infon><infon key="issue">4</infon><infon key="lpage">1803</infon><infon key="name_0">surname:Hartshorne;given-names:JK</infon><infon key="name_1">surname:de Leeuw;given-names:JR</infon><infon key="name_2">surname:Goodman;given-names:ND</infon><infon key="name_3">surname:Jennings;given-names:M</infon><infon key="name_4">surname:O’Donnell;given-names:TJ</infon><infon key="pub-id_doi">10.3758/s13428-018-1155-z</infon><infon key="pub-id_pmid">30746644</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2019</infon><offset>46646</offset><text>A thousand studies for the price of one: Accelerating psychological science with Pushkin</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46735</offset><text>Henninger, F., Shevchenko, Y., Mertens, U., Kieslich, P.J., &amp; Hilbig, B.E. (2019). Lab.js: A free, open, online study builder. (PsyArXiv preprint).</text></passage><passage><infon key="fpage">399</infon><infon key="issue">3</infon><infon key="lpage">425</infon><infon key="name_0">surname:Horton;given-names:JJ</infon><infon key="name_1">surname:Rand;given-names:DG</infon><infon key="name_2">surname:Zeckhauser;given-names:RJ</infon><infon key="pub-id_doi">10.1007/s10683-011-9273-9</infon><infon key="section_type">REF</infon><infon key="source">Experimental Economics</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2011</infon><offset>46883</offset><text>The online laboratory: Conducting experiments in a real labor market</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46952</offset><text>Houghton, J. (2020). Interdependent diffusion: The social contagion of interacting beliefs. Unpublished doctoral dissertation Massachusetts Institute of Technology, Cambridge, MA.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47132</offset><text>Houghton, J.P. (2020). Interdependent diffusion:, The social contagion of interacting beliefs. arXiv preprint arXiv:2010.02188.</text></passage><passage><infon key="fpage">517</infon><infon key="issue">11</infon><infon key="lpage">521</infon><infon key="name_0">surname:Ishowo-Oloko;given-names:F</infon><infon key="name_1">surname:Bonnefon;given-names:J-F</infon><infon key="name_2">surname:Soroye;given-names:Z</infon><infon key="name_3">surname:Crandall;given-names:J</infon><infon key="name_4">surname:Rahwan;given-names:I</infon><infon key="name_5">surname:Rahwan;given-names:T</infon><infon key="pub-id_doi">10.1038/s42256-019-0113-5</infon><infon key="section_type">REF</infon><infon key="source">Nature Machine Intelligence</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2019</infon><offset>47260</offset><text>Behavioural evidence for a transparency–efficiency tradeoff in human–machine cooperation</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47353</offset><text>Jahani, E., Gallagher, N.M., Merhout, F., Cavalli, N., Guilbeault, D., Leng, Y., &amp; et al. (2020). Exposure to common enemies can increase political polarization: Evidence from a cooperation experiment with automated partisans.</text></passage><passage><infon key="fpage">495</infon><infon key="issue">2</infon><infon key="lpage">519</infon><infon key="name_0">surname:Letham;given-names:B</infon><infon key="name_1">surname:Karrer;given-names:B</infon><infon key="name_2">surname:Ottoni;given-names:G</infon><infon key="name_3">surname:Bakshy;given-names:E</infon><infon key="pub-id_doi">10.1214/18-BA1110</infon><infon key="section_type">REF</infon><infon key="source">Bayesian Analysis</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2019</infon><offset>47580</offset><text>Constrained Bayesian optimization with noisy experiments</text></passage><passage><infon key="fpage">433</infon><infon key="issue">2</infon><infon key="lpage">442</infon><infon key="name_0">surname:Litman;given-names:L</infon><infon key="name_1">surname:Robinson;given-names:J</infon><infon key="name_2">surname:Abberbock;given-names:T</infon><infon key="pub-id_doi">10.3758/s13428-016-0727-z</infon><infon key="pub-id_pmid">27071389</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2017</infon><offset>47637</offset><text>Turkprime. com: a versatile crowdsourcing data acquisition platform for the behavioral sciences</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47733</offset><text>Mao, A., Chen, Y., Gajos, K.Z., Parkes, D.C., Procaccia, A.D., &amp; Zhang, H. (2012). Turkserver: Enabling synchronous and longitudinal online experiments. In Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence.</text></passage><passage><infon key="fpage">13800</infon><infon key="name_0">surname:Mao;given-names:A</infon><infon key="name_1">surname:Dworkin;given-names:L</infon><infon key="name_2">surname:Suri;given-names:S</infon><infon key="name_3">surname:Watts;given-names:DJ</infon><infon key="pub-id_doi">10.1038/ncomms13800</infon><infon key="section_type">REF</infon><infon key="source">Nature Communications</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>47963</offset><text>Resilient cooperators stabilize long-run cooperation in the finitely repeated prisoner’s dilemma</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">23</infon><infon key="name_0">surname:Mason;given-names:W</infon><infon key="name_1">surname:Suri;given-names:S</infon><infon key="pub-id_doi">10.3758/s13428-011-0124-6</infon><infon key="pub-id_pmid">21717266</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2012</infon><offset>48062</offset><text>Conducting behavioral research on Amazon Mechanical Turk</text></passage><passage><infon key="fpage">3</infon><infon key="issue">1</infon><infon key="lpage">19</infon><infon key="name_0">surname:McClelland;given-names:GH</infon><infon key="pub-id_doi">10.1037/1082-989X.2.1.3</infon><infon key="section_type">REF</infon><infon key="source">Psychological Methods</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">1997</infon><offset>48119</offset><text>Optimal design in psychological research</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48160</offset><text>McKnight, M.E., &amp; Christakis, N.A. (2016). Breadboard: Software for online social experiments. Retrieved from https://breadboard.yale.edu/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48300</offset><text>Musch, J., &amp; Reips, U.-D. (2000). A brief history of web experimenting. In Psychological Experiments on the Internet (pp. 61–87): Elsevier.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48442</offset><text>Noriega, A., Camacho, D., Meizner, D., Enciso, J., Quiroz-Mercado, H., Morales-Canton, V., &amp; et al. (2020). Screening diabetic retinopathy using an automated retinal image analysis (ARIA) system in Mexico: Independent and assistive use cases. (medRxiv preprint).</text></passage><passage><infon key="fpage">22</infon><infon key="lpage">27</infon><infon key="name_0">surname:Palan;given-names:S</infon><infon key="name_1">surname:Schitter;given-names:C</infon><infon key="pub-id_doi">10.1016/j.jbef.2017.12.004</infon><infon key="section_type">REF</infon><infon key="source">Journal of Behavioral and Experimental Finance</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2018</infon><offset>48705</offset><text>Prolific.ac–a subject pool for online experiments</text></passage><passage><infon key="fpage">411</infon><infon key="issue">5</infon><infon key="lpage">419</infon><infon key="name_0">surname:Paolacci;given-names:G</infon><infon key="name_1">surname:Chandler;given-names:J</infon><infon key="name_2">surname:Ipeirotis;given-names:PG</infon><infon key="section_type">REF</infon><infon key="source">Judgment and Decision Making</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2010</infon><offset>48757</offset><text>Running experiments on Amazon Mechanical Turk</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48803</offset><text>Pescetelli, N., Rutherford, A., Kao, A., &amp; Rahwan, I. (2019). Collective learning in news consumption. (PsyArXiv preprint).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48927</offset><text>Plonsky, O., Apel, R., Ert, E., Tennenholtz, M., Bourgin, D., Peterson, J.C., &amp; et al. (2019). Predicting human decisions with behavioral theories and machine learning. (arXiv preprint arXiv:1904.06866.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49130</offset><text>Reips, U.-D. (2000). The web experiment method: Advantages, disadvantages, and solutions. In Psychological Experiments on the Internet (pp. 89–117): Elsevier.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49291</offset><text>Reips, U.-D. (2012). Using the Internet to collect data. In APA Handbook of Research Methods in Psychology. American Psychological Association, (Vol. 2 pp. 201–310).</text></passage><passage><infon key="fpage">234</infon><infon key="issue">2</infon><infon key="lpage">240</infon><infon key="name_0">surname:Reips;given-names:U-D</infon><infon key="name_1">surname:Neuhaus;given-names:C</infon><infon key="pub-id_doi">10.3758/BF03195449</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods, Instruments, &amp; computers: A Journal of the Psychonomic Society, Inc</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2002</infon><offset>49459</offset><text>Wextor: A web-based tool for generating and visualizing experimental designs and procedures</text></passage><passage><infon key="fpage">854</infon><infon key="issue">5762</infon><infon key="lpage">856</infon><infon key="name_0">surname:Salganik;given-names:MJ</infon><infon key="name_1">surname:Dodds;given-names:PS</infon><infon key="name_2">surname:Watts;given-names:DJ</infon><infon key="pub-id_doi">10.1126/science.1121066</infon><infon key="pub-id_pmid">16469928</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">311</infon><infon key="year">2006</infon><offset>49551</offset><text>Experimental study of inequality and unpredictability in an artificial cultural market</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49638</offset><text>Schelling, T.C. (2006). Micromotives and macrobehavior. WW Norton &amp; Company.</text></passage><passage><infon key="fpage">370</infon><infon key="lpage">374</infon><infon key="name_0">surname:Shirado;given-names:H</infon><infon key="name_1">surname:Christakis;given-names:NA</infon><infon key="pub-id_doi">10.1038/nature22332</infon><infon key="pub-id_pmid">28516927</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">545</infon><infon key="year">2017</infon><offset>49715</offset><text>Locally noisy autonomous agents improve global human coordination in network experiments</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Suchow;given-names:JW</infon><infon key="name_1">surname:Griffiths;given-names:TL</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2016</infon><offset>49804</offset><text>Rethinking experiment design as algorithm design</text></passage><passage><infon key="fpage">80</infon><infon key="issue">6</infon><infon key="lpage">83</infon><infon key="name_0">surname:Tilkov;given-names:S</infon><infon key="name_1">surname:Vinoski;given-names:S</infon><infon key="pub-id_doi">10.1109/MIC.2010.145</infon><infon key="section_type">REF</infon><infon key="source">IEEE Internet Computing</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2010</infon><offset>49853</offset><text>Node.js: Using JavaScript to build high-performance network programs</text></passage><passage><infon key="fpage">6370</infon><infon key="issue">12</infon><infon key="lpage">6375</infon><infon key="name_0">surname:Traeger;given-names:ML</infon><infon key="name_1">surname:Sebo;given-names:SS</infon><infon key="name_2">surname:Jung;given-names:M</infon><infon key="name_3">surname:Scassellati;given-names:B</infon><infon key="name_4">surname:Christakis;given-names:NA</infon><infon key="pub-id_doi">10.1073/pnas.1910402117</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">117</infon><infon key="year">2020</infon><offset>49922</offset><text>Vulnerable robots positively shape human conversational dynamics in a human–robot team</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50011</offset><text>Valentine, M.A., Retelny, D., To, A., Rahmati, N., Doshi, T., &amp; Bernstein, M.S. (2017). Flash organizations: Crowdsourcing complex work by structuring crowds as organizations. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (pp. 3523–3537).</text></passage><passage><infon key="fpage">58</infon><infon key="issue">8</infon><infon key="lpage">67</infon><infon key="name_0">surname:von Ahn;given-names:L</infon><infon key="name_1">surname:Dabbish;given-names:L</infon><infon key="pub-id_doi">10.1145/1378704.1378719</infon><infon key="section_type">REF</infon><infon key="source">Communications of the ACM</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2008</infon><offset>50286</offset><text>Designing games with a purpose</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50317</offset><text>Whiting, M.E., Blaising, A., Barreau, C., Fiuza, L., Marda, N., Valentine, M., &amp; et al. (2019). Did it have to end this way? Understanding the consistency of team fracture. In Proceedings of the ACM on Human–Computer Interaction, 3(CSCW).</text></passage><passage><infon key="fpage">22</infon><infon key="issue">CSCW1</infon><infon key="name_0">surname:Whiting;given-names:ME</infon><infon key="name_1">surname:Gao;given-names:I</infon><infon key="name_2">surname:Xing;given-names:M</infon><infon key="name_3">surname:N’Godjigui;given-names:JD</infon><infon key="name_4">surname:Nguyen;given-names:T</infon><infon key="name_5">surname:Bernstein;given-names:MS</infon><infon key="pub-id_doi">10.1145/3392877</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM on Human–Computer Interaction</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2020</infon><offset>50558</offset><text>Parallel worlds: Repeated initializations of the same team to improve team viability</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50643</offset><text>Whiting, M.E., Hugh, G., &amp; Bernstein, M.S. (2019). Fair work: Crowd work minimum wage with one line of code. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, (Vol. 7 pp. 197–206).</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50854</offset><text>Wieruch, R. (2017). The road to react: Your journey to master plain yet pragmatic react.js. Robin Wieruch.</text></passage></document></collection>
