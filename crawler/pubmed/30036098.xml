<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201220</date><key>pmc.key</key><document><id>6207992</id><infon key="license">CC BY-NC</infon><passage><infon key="article-id_doi">10.1177/0018720818787126</infon><infon key="article-id_pmc">6207992</infon><infon key="article-id_pmid">30036098</infon><infon key="article-id_publisher-id">10.1177_0018720818787126</infon><infon key="fpage">1192</infon><infon key="issue">8</infon><infon key="kwd">crowdsourcing reaction times mental chronometry psychophysics</infon><infon key="license">This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License (http://www.creativecommons.org/licenses/by-nc/4.0/) which permits non-commercial use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).</infon><infon key="lpage">1206</infon><infon key="name_0">surname:Bazilinskyy;given-names:Pavlo</infon><infon key="name_1">surname:de Winter;given-names:Joost</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">60</infon><infon key="year">2018</infon><offset>0</offset><text>Crowdsourced Measurement of Reaction Times to Audiovisual Stimuli With Various Degrees of Asynchrony</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>101</offset><text>Objective:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>112</offset><text>This study was designed to replicate past research concerning reaction times to audiovisual stimuli with different stimulus onset asynchrony (SOA) using a large sample of crowdsourcing respondents.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>310</offset><text>Background:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>322</offset><text>Research has shown that reaction times are fastest when an auditory and a visual stimulus are presented simultaneously and that SOA causes an increase in reaction time, this increase being dependent on stimulus intensity. Research on audiovisual SOA has been conducted with small numbers of participants.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>627</offset><text>Method:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>635</offset><text>Participants (N = 1,823) each performed 176 reaction time trials consisting of 29 SOA levels and three visual intensity levels, using CrowdFlower, with a compensation of US$0.20 per participant. Results were verified with a local Web-in-lab study (N = 34).</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>892</offset><text>Results:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>901</offset><text>The results replicated past research, with a V shape of mean reaction time as a function of SOA, the V shape being stronger for lower-intensity visual stimuli. The level of SOA affected mainly the right side of the reaction time distribution, whereas the fastest 5% was hardly affected. The variability of reaction times was higher for the crowdsourcing study than for the Web-in-lab study.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1292</offset><text>Conclusion:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1304</offset><text>Crowdsourcing is a promising medium for reaction time research that involves small temporal differences in stimulus presentation. The observed effects of SOA can be explained by an independent-channels mechanism and also by some participants not perceiving the auditory or visual stimulus, hardware variability, misinterpretation of the task instructions, or lapses in attention.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1684</offset><text>Application:</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1697</offset><text>The obtained knowledge on the distribution of reaction times may benefit the design of warning systems.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1801</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1814</offset><text>Reaction times are widely used to examine human information-processing mechanisms, such as in studies of cognitive ability, visual search, and memory. In human factors science, reaction times are typically measured for applied purposes, for example, to quantify stimulus-response compatibility of human–machine interfaces and the effectiveness of warning systems. In the design of any warning system, it should be decided whether the warning signal is auditory, visual, vibrotactile, or multimodal. For example, in automated driving, a takeover warning can be a vibrotactile stimulus in the seat, an auditory signal, a visual notification on the dashboard, or a multimodal signal, such as an audiovisual alarm (e.g.,) or a vibrotactile-auditory alarm (e.g.,). The present study is concerned with a new method for large-scale research on reaction times to multimodal stimuli.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>2691</offset><text>Previous Research on the Effect of Stimulus Onset Asynchrony (SOA) on Reaction Times</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2776</offset><text>It is well established that in simple reaction time tasks, multimodal feedback yields faster reaction times than unimodal feedback. However, the timing and intensity of the stimuli have an important effect on reaction times. Literature shows that average reaction times to bimodal stimuli are fastest when the onsets of the stimuli occur at the same moment, with the mean reaction time as a function of SOA exhibiting a V shape (e.g.,). This V shape is illustrated in Figure 1, showing results from our literature survey on reaction times to audiovisual stimuli as a function of SOA. Only studies that used equivalent task conditions were included in this figure (for additional relevant research on SOA, see). Each subfigure shows mean reaction times as a function of SOA, where a negative SOA value means that the onset of the visual stimulus occurred after the onset of the auditory stimulus. The middle and right subfigures concerned studies that focused on manipulating the intensity of the visual and auditory stimuli, as indicated with lowercase (v, a) and uppercase letters (V, A).</text></passage><passage><infon key="file">10.1177_0018720818787126-fig1.jpg</infon><infon key="id">fig1-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>3866</offset><text>Mean reaction times from a selection of literature on stimulus onset asynchrony in audiovisual reaction time tasks. Left = four independent studies; middle = study that manipulated stimulus intensity; right = study that also manipulated stimulus intensity; a (A) = low- (high-) intensity auditory stimulus; v (V) = low- (high-) intensity visual stimulus.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4221</offset><text>It can also be seen in Figure 1 that the degree with which reaction times increase as a function of SOA depends on the intensity of the stimuli (see also). More specifically, if the visual stimulus is difficult to see, then participants are likely to respond to the auditory stimulus, and so the onset of the auditory stimulus will have a dominant effect on the mean reaction time. Conversely, if the stimulus is poorly audible, then the onset of the visual stimulus will determine the reaction time. These interactions between SOA and stimulus intensity were illustrated by; see Figure 1 middle) and; see Figure 1 right). Thus, the relationship between mean reaction time and SOA is asymmetric (i.e., one side of the V shape is flatter than the other) when the auditory stimulus is weak and the visual stimulus is intense (i.e., aV in Figure 1) or when the visual stimulus is weak and the auditory stimulus is intense (Av in Figure 1).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5158</offset><text>Differences in the overall mean reaction time between the experiments shown in Figure 1 are of lesser interest, as these depend on factors such as the participants’ level of experience, outlier removal, overall stimulus intensity, and hardware used during the experiment (e.g.,). For example, in and, the mean reaction times to audiovisual stimuli were in the range of 135 to 155 ms (SOA 0–50 ms) and 98 to 144 ms (SOA 0–85 ms), respectively. These phenomenally fast reaction times may be explained by the fact that participants were highly trained, the use of intensive stimuli, and specialized hardware that records reaction times with little delay.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5816</offset><text>The research on the effect of audiovisual SOA has been conducted with small sample sizes (see the legends in Figure 1) but typically with dozens of trials per stimulus condition. Accordingly, investigations of the distributions of reaction times have been performed within subjects rather than between subjects. For example, in, there were two participants who each completed 40 test blocks over a period of about 1 month, each block consisting of 130 test trials. It would be relevant to examine whether there exist individual differences in susceptibility to SOA effects. Within the human factors community, it has been emphasized that the design of warning systems should not be based on the mean reaction time but that slowly responding participants should be considered, too.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6597</offset><text>The Potential of Crowdsourcing for Performing Reaction Time Research</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6666</offset><text>The Internet is a now well-established medium for experimental psychological research with large sample sizes. Various studies have replicated classical psychological effects using online crowdsourcing methods. For example,, using a JavaScript engine, replicated three reaction time paradigms (Stroop task, attentional blink task, masked priming task) via crowdsourcing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7037</offset><text>A number of studies suggest that online software and hardware can cause small delays compared to regular psychophysics methods. For example, found an additive reaction time delay of 25 ms, and no difference in variance, when using jsPsych (a library for creating behavioral experiments using JavaScript) running in Google Chrome as compared with MATLAB’s Psychophysics Toolbox on the same laptop hardware. described a limitation of JavaScript, in that audio and visual stimuli scheduled to appear on a Web page at the same time are presented with a small temporal offset that can vary up to 40 ms, depending on the type of browser. replicated the Stroop effect online and noted that the online software contributed to additional reaction time variance compared with a controlled lab study. According to simulations by, the effect of technical variance (due to e.g., keyboards, CPU load, operating systems) is negligible compared with individual differences in reaction time, and they argued that “researchers’ preconceptions concerning the unsuitability of web experiments for conducting research using response time as a dependent measure are misguided” (p. 350).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8210</offset><text>However, concerns have also been raised about the validity of online research, especially when small stimulus durations are involved. replicated well-known paradigms (e.g., Stroop test, flanker test) in three settings (classical lab, Web-in-lab, Web), with a total of 147 participants. Although the replication was successful, the mean reaction times in a simple reaction time task were 253 ms, 280 ms, and 318 ms, respectively, for the three settings. That is, the Web-in-lab method caused an additive delay, presumably due to the browser engine and JavaScript, whereas the Web method might be further affected by differences in participants’ hardware and testing environments. provided a review of 10 online research platforms that can be used for measuring reaction times and concluded that the quality of online data is usually high. However, these authors also discussed sources of technical variability in online reaction time research, such as variability in screen brightness, screen color, and volume of auditory stimuli, and they argued that studies that require short stimulus presentation are not well suitable to online research. Similarly, argued that “the smaller the effect, the more problematic the noise introduced by . . . online experimentation” (p. 10).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9491</offset><text>In summary, although the Internet can be used to replicate psychological phenomena concerning reaction times, online research is associated with additive bias and extra sources of variance compared to lab-based research, and it is unknown whether reaction times to small temporal manipulations can be replicated online.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>9811</offset><text>Aim of This Research</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9832</offset><text>Given the knowledge gap, this study was designed to replicate previous research on the effect of SOA and stimulus intensity on audiovisual reaction times using a large sample of participants via crowdsourcing. A replication study of well-established previous findings may contribute to the understanding of the validity of crowdsourcing and yield new knowledge on the relationship between SOA and reaction times.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10245</offset><text>Our analysis was concerned with investigating whether a V shape of mean reaction times (Figure 1) replicates and whether a lower intensity of the visual stimulus causes the slope of the V shape to be steeper. As pointed out earlier, crowdsourcing research can yield a high variance in reaction times. Therefore, in addition to investigating mean reaction times, we examined individual differences in reaction times (percentiles and trial-to-trial correlations). Furthermore, we assessed the sources of variability in reaction times by examining learning curves, by comparing the results with a Web-in-lab study using the same software, and by studying the effects of experimental conditions, such as whether participants were using a keyboard or mobile phone or whether they were indoors or outdoors.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11046</offset><text>Method</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11053</offset><text>This research complied with the American Psychological Association Code of Ethics and was approved by the Human Research Ethics Committee (HREC) at the Delft University of Technology. Informed consent was obtained from each participant.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11290</offset><text>Stimuli</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11298</offset><text>Participants were presented with audiovisual stimuli. The auditory stimuli were single 210-ms-long beeps of 1,840 Hz. The visual stimuli were red circles on a white background. A total of 29 SOA values were used: −1,000, −500, −300, −200, −100, −90, −80, −70, −60, −50, −40, −30, −20, −10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 500, and 1,000 ms. These 29 SOA values have a range that is higher than the ranges of SOA values used in previous research (Figure 1) while offering a higher temporal resolution (10 ms for SOA values between −100 and 100 ms). A negative SOA value means that the onset of the auditory stimulus occurred before the onset of the visual stimulus, and a positive SOA value means that the onset of the auditory stimulus occurred after the visual stimulus (as in Figure 1). Figure 2 shows example timelines of reaction time trials with negative and positive SOA.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig2.jpg</infon><infon key="id">fig2-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12228</offset><text>Timelines for events of a reaction time trial. The top figure concerns stimulus onset asynchrony (SOA) &lt; 0 ms; the bottom figure concerns SOA &gt; 0 ms. The auditory stimulus always had a duration of 210 ms, whereas the visual stimulus disappeared when the participant pressed the response key.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12520</offset><text>If the auditory stimulus was presented at the same moment or after the visual stimulus (SOA ≥ 0), then a .png file was presented together with a .wav file, with the time delay (SOA) encoded in the .wav file. If the visual stimulus was presented after the auditory stimulus (SOA &lt; 0), then an animated .gif file was presented together with a .wav file. The animated .gif (via its graphics control extension) was a practical solution to encode a time delay of the onset of the visual stimulus. The rendering of the stimuli was powered by the jsPsych JavaScript library for running behavioral experiments online.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13132</offset><text>The red circles were uniform, had a diameter of 195 pixels, and had three levels of intensity: low, medium, or high (see Figure 3 for an example). These three intensity levels (i.e., shades of red) were selected to be notably different but in such a way that the low-intensity stimulus was still clearly distinguishable from the white background, as we did not want that participants would fail to detect the visual stimuli. High-intensity stimuli were rendered on the screen as RGB 233-33-53. Low- and medium-intensity .png files were created using 40% and 70% transparency setting, respectively, which translates into rendered stimuli of RGB 246-166-174 and RGB 240-99-113, respectively. Low- and medium-intensity .gif files were RGB 251-211-215 and RGB 242-122-134, respectively. Because of the different RGB rendering of .png and .gif files, the reaction times to low- and medium-intensity stimuli between SOA &lt; 0 and SOA ≥ 0 should not be directly compared. The auditory stimuli were always 210-ms beeps; they were not varied in intensity to keep the total number of conditions manageable.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig3.jpg</infon><infon key="id">fig3-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>14229</offset><text>Visual stimulus in the browser’s full-screen mode (RGB 246-166-174, screen resolution: 1,920 × 1,080 pixels).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14342</offset><text>Crowdsourcing Experiment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14367</offset><text>Participants in the online experiment participated via the crowdsourcing platform CrowdFlower (https://www.crowdflower.com). Participants became aware of this research by logging into one of many channel websites (e.g., https://www.clixsense.com), where they would see our study in the list of other projects available for completion. We allowed contributors from all countries to participate. It was not permitted to complete the study more than once from the same worker ID. A payment of US$0.20 was offered for the completion of the experiment. A total of 2,000 participants completed the experiment, at a total cost of US$480. Our payment was assumed to be high enough to incentivize participants. investigated the effect of payment for a 6-min task among crowdworkers from India and found that a payment of US$0.10 (“above-minimum-wage condition”) yielded higher data quality than a payment of US$0.02 (“below-minimum-wage condition”), whereas a payment of US$1.00 (“far above the minimum wage”) did not improve data quality compared with a payment of US$0.10.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15445</offset><text>Participants first answered a number of questionnaire items. At the beginning of the questionnaire, contact information of the researchers was provided, and the purpose of the upcoming study was described as “to determine reaction times for different types of visual and auditory signals.” Participants were informed that the study would take approximately 8 min. The participants were also informed that they could contact the investigators to ask questions about the study and that they had to be at least 18 years old. Information about anonymity and voluntary participation was provided as well. The questionnaire started with the following questions:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16105</offset><text>“Have you read and understood the above instructions?” (“Yes,” “No”)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16186</offset><text>“What is your gender?” (“Male,” “Female,” “I prefer not to respond”)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16271</offset><text>“What is your age?” (positive integer)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16314</offset><text>“In which type of place are you located now?” (“Indoor, dark”; “Indoor, dim light”; “Indoor, bright light”; “Outdoor, dark”; “Outdoor, dim light”; “Outdoor, bright light”; “Other”; “I prefer not to respond”)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16558</offset><text>“Which input device are you using now?” (“Laptop keyboard,” “Desktop keyboard,” “Tablet on-screen keyboard,” “Mobile phone on-screen keyboard,” “Other,” “I prefer not to respond”)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16766</offset><text>In this experiment, you will hear sounds and see red circles. Please make sure that your audio is on and set your screen to bright. You need to press “F” after hearing a sound OR seeing a red circle (whichever comes first) as fast as possible. Your reaction times will be recorded. After each group of 25 stimuli you will be able to take a small break. Please press any key to start with the first stimulus.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17178</offset><text>Several additional questions were asked about driving habits, which were not used in this study. The participants were then asked to leave the questionnaire by clicking on a link that opened a Web page with the reaction time task. Participants were presented with instructions on how to complete the given task:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17490</offset><text>The participants had to respond to 88 different stimuli in random order. Each stimulus was repeated twice, yielding 176 stimuli for each participant (i.e., 29 SOA values × 3 visual intensity levels × 2 repetitions + 2 repetitions of an audio-only stimulus). There was no upper limit to the reaction times; the next stimulus trial was loaded after the participants pressed the F button. The stimuli were presented in six batches of 25 and one last batch of 26. After a batch, participants were shown the following text: “You have now completed 25 [50, 75, 100, 125, 150] stimuli out of 176. When ready press ‘C’ to proceed to the next batch.” An analysis of the elapsed times showed that participants took a median time of 9 s to press C after the first batch and a median time of 4 s to press C after the sixth batch.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18318</offset><text>After pressing the F key, a new stimulus was presented after a uniform random delay between 1,000 and 3,299 ms, in agreement with. The images and sounds were preloaded to eliminate unwanted delays between the stimuli. Data for each participant were saved in a database after the 176th stimulus. Analyses of the distribution of reaction times per participant showed that the temporal resolution of the reaction time measurements (i.e., the minimum difference that could be detected) differed between participants: For the majority of participants (88%), the temporal resolution was between 2.6 and 3.0 ms. For 6% of the participants, the temporal resolution was between 3 ms and 12 ms, whereas 4% of participants had a temporal resolution of 42.7 ms. These differences in temporal resolution may be due to different platforms and browsers used by the participants.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19182</offset><text>At the end of the experiment, participants were shown a unique code. Participants were asked to note down this code and return to the Web page of the questionnaire. They were required to enter the code on the questionnaire as proof that they completed the experiment and to receive their remuneration.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19484</offset><text>Web-in-Lab Experiment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19506</offset><text>To verify the results of the crowdsourcing experiment in controlled experimental conditions, we launched the same task in a laboratory setting. We collected responses from 42 participants from the university community. All participants completed the task on the same MacBook Air (13-in. screen, 8 GB memory, Intel Core I7 processor, two cores) laptop behind a table in a standard office room of about 3 × 3 m. The blinds were closed to control the lighting conditions; the ceiling lights (fluorescent lamps) were always on. The volume of the laptop was set to 60% (corresponding to a measured sound intensity of 60–65 dBA), and the brightness of the display was 100%. The experimenter started up the task and left the room so that the participant completed the task while being alone in the room. The temporal resolution of the reaction times of the Web-in-lab experiment was 5.8 ms. Participants of the Web-in-lab experiment did not receive remuneration because it is common practice at our institution to not pay participants for a short-lasting experiment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20569</offset><text>Handling of Reaction Time Outliers and Statistical Testing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20628</offset><text>Reaction times less than 0 ms were removed from the analysis, whereas reaction times greater than 1,500 ms were set equal to 1,500 ms. Using this so-called winsorization method, extremely slow reaction times (&gt;1,500 ms) were retained in the analysis (as recommended by), while limiting the skewness and kurtosis of the reaction time distribution. Differences between participants’ conditions (e.g., input device) were compared using an unequal-variance t test after performing an inverse transformation of the reaction times. Effect sizes were assessed using Cohen’s d of the inverse reaction times.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>21232</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21240</offset><text>The responses were collected between March 3, 2017, 12:42 and March 4, 2017, 17:30 (GMT). Two hundred twenty-four participants completed an optional user satisfaction survey offered by CrowdFlower. The study received an overall satisfaction score of 4.4 out of 5.0 on a scale from 1 (very dissatisfied) to 5 (very satisfied). The mean response to the question “How clear were the task instructions and interface?” was 4.6 on a scale from 1 (very unclear) to 5 (very clear), and the mean response to “How would you rate the pay for this task relative to other tasks you’ve completed?” was 4.2 on a scale from 1 (much worse) to 5 (much better).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21893</offset><text>Participant Filtering and Participant Characteristics</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21947</offset><text>Out of 2,000 participants, 177 were removed during data filtering. These were participants for whom no reaction time data were available due to a server/recording error (n = 119), participants with more than 20% negative reaction times (due to pressing the response key before the stimulus was presented; n = 55), or participants who answered “no” to the question whether they had read and understood the task instructions (n = 9). The 20% threshold was assumed to discriminate between participants with genuine anticipatory reaction times (i.e., accidentally pressing the F key too early) and participants cheating the system by repeatedly pressing F.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22604</offset><text>In the group of the remaining 1,823 participants, 1,283 were male, 533 were female, and 7 did not specify their gender. Three participants reported an unrealistic age or an age that was not in agreement with the task instructions (3, 5, and 17 years). Because these ages could be the result of a basic typographical error, and because these three participants did complete the task, they were retained in the analysis. The participants’ mean age for the 1,820 participants of 18 years and older was 33.9 years (SD = 10.1, min = 18, max = 71).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23149</offset><text>The participants were from 83 different countries, with 22 countries having 25 or more respondents and four countries (Spain, Russia, Serbia, Venezuela) having more than 100 respondents.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>23336</offset><text>Learning Curve</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23351</offset><text>Figure 4 shows that the mean reaction times decreased with trial number, that is, the participants showed faster reaction times as the experiment progressed. The spikes in the graph represent the trials that directly followed the breaks after each 25th stimulus. We removed Trials 1 through 5, 26, 51, 76, 101, 126, and 151 from the remaining analysis (except the correlations among trials), because these trials may be invalid as it is likely that some participants were still learning the basics of the task or pressed an incorrect key during these trials.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig4.jpg</infon><infon key="id">fig4-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23910</offset><text>Mean reaction times versus trial number in the crowdsourcing study (N = 1,823). Each data point represents the mean across approximately 1,815 trials (i.e., 1,823 participants minus excluded responses).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24113</offset><text>Effects of SOA and Stimulus Intensity on Reaction Time</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24168</offset><text>Figure 5 shows the mean reaction times as a function of SOA. Note that the results for SOA = −10 ms are not shown in the figures because the animated .gif files showed a delay of 100 ms when programmed with a delay of 10 ms. It can be seen that the lowest reaction times were obtained when the SOA was 0 ms. Furthermore, the visually delayed stimuli (i.e., SOA &lt; 0 ms) yielded a mean reaction time that was about 43 ms higher than the auditorily delayed stimuli (i.e., SOA &gt; 0 ms). It can also be seen that the low-intensity visual stimuli were associated with a stronger increase of the mean reaction time for increasing SOA than the high-intensity visual stimuli, which is consistent with the literature presented in Figure 1.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig5.jpg</infon><infon key="id">fig5-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24899</offset><text>Mean reaction times for 28 levels of stimulus onset asynchrony (SOA) and three levels of visual intensity in the crowdsourcing study. Each data point represents the mean across approximately 3,400 trials (i.e., 1,823 participants × 2 trials per participant minus excluded responses).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25184</offset><text>Individual differences were assessed using percentiles of the observed reaction times, from low (i.e., fast reactions) to high (i.e., slow reactions). Figure 6 shows that the lowest reaction times were hardly affected by SOA, whereas the 95th percentile is strongly sensitive to SOA. In other words, the changes in mean reaction time observed in Figure 5 can be largely attributed to differences in the right tail of the reaction time distribution.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig6.jpg</infon><infon key="id">fig6-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25633</offset><text>Percentiles of the reaction times for 28 levels of stimulus onset asynchrony (SOA) in the crowdsourcing study. Each data point is based on approximately 10,200 trials (i.e., 1,823 participants × 6 trials per participant minus excluded responses). The auditory-only trial (A) is based on 3,397 trials (i.e., 1,823 participants × 2 trials per participant minus 249 excluded responses).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26019</offset><text>Effects of Experimental Conditions on Reaction Time</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26071</offset><text>Figure 7 shows that indoor lighting condition did not have a large impact on the mean reaction times. A Welch’s test showed no significant difference between dark and bright indoor light, t(189.3) = 0.51, p = .608, d = 0.05. However, completing the task outdoors was associated with significantly higher reaction time than completing the task indoors, t(40.7) = 3.32, p = .002, d = 0.55.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig7.jpg</infon><infon key="id">fig7-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26461</offset><text>Median reaction time at the level of trials, per task environment, input device, gender, and age group for the crowdsourcing study and for the crowdsourcing versus the Web-in-lab study. The error bars denote the 25th and 75th percentiles. Also listed is the number of trials with the number of participants in parentheses. Outdoor refers to “Outdoor, dark”; “Outdoor, dim light”; and “Outdoor, bright light” combined. Other refers to “Tablet on-screen keyboard,” “Mobile phone on-screen keyboard,” and “Other” combined. Ages of 20, 26, 32, 40, 53, and 71 years are the 5th, 25th, 50th, 75th, 95th, and 100th percentiles of participants’ ages, respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27144</offset><text>Figure 7 also shows that participants who completed the task with a laptop or desktop keyboard had faster reaction times than participants who used other input devices (e.g., tablets or mobile phones), t(20.4) = 2.73, p = .013, d = 0.62. There were no statistically significant gender differences, t(1020.9) = 0.39, p = .697, d = 0.02, nor age differences in reaction time (Spearman’s correlation between age and mean reaction time: ρ = 0.03, N = 1,820).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>27602</offset><text>Trial-to-Trial Correlations (Stability) of Reaction Times</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27660</offset><text>Finally, we calculated trial-to-trial correlations to obtain an indication of the stability of participants’ reaction times. Figure 8 shows a Spearman correlation matrix among the reaction times per trial number for the crowdsourced participants. A high correlation between a pair of trials means that participants’ reaction times had a similar rank ordering in these two trials, whereas a correlation of zero would be expected if participants were not consistent at all. A clear simplex pattern can be seen, with temporally adjacent trials showing higher correlations than temporally disparate trials (see also). It can also be seen that reaction times stabilized (i.e., higher correlations) in later trials.</text></passage><passage><infon key="file">10.1177_0018720818787126-fig8.jpg</infon><infon key="id">fig8-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28374</offset><text>Heat map of Spearman rank-order correlations of crowdsourced participants’ reaction times between Trials 1 and 176.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>28492</offset><text>Reaction Times From the Web-in-Lab Experiment Compared With the Crowdsourcing Experiment</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28581</offset><text>In the laboratory setting, we retained responses from 34 participants, obtained between March 7, 2017, 10:57, and March 10, 2017, 13:13 (GMT). We removed four participants with incomplete data and four participants who were involved in pilot tests conducted during the design of the study. The participants were six females and 28 males, having a mean age of 27.1 years (SD = 6.6 years, min = 18, max = 56). The reaction times were processed identically to the crowdsourcing experiments.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29069</offset><text>The results in Figure 7 show substantial differences between the international crowdsourcing method and the local lab method, t(34.8) = 10.68, p &lt; .001, d = 1.57. The Web-in-lab method featured a lower mean reaction time and lower variability of reaction time (Figure 9).</text></passage><passage><infon key="file">10.1177_0018720818787126-fig9.jpg</infon><infon key="id">fig9-0018720818787126</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29341</offset><text>The 5th and 95th percentiles for 28 levels of stimulus onset asynchrony (SOA) in the crowdsourcing and Web-in-lab studies. Each data point of the crowdsourcing study is based on approximately 10,200 trials (i.e., 1,823 participants × 6 trials per participant minus excluded responses). Each data point of the Web-in-lab study is based on approximately 191 trials (34 participants × 6 trials per participant minus excluded responses).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>29777</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>29788</offset><text>Replicated Effects</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>29807</offset><text>In this study we aimed to replicate published research regarding the effects of audiovisual SOA and visual stimulus intensity on reaction times with a large sample of crowdsourced participants and to examine sources of variability of mean reaction times (e.g., learning curves, task conditions, comparison with Web-in-lab study).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30137</offset><text>Our findings replicated the V shape as observed in past research, with the mean reaction time being fastest when SOA = 0 ms and increasing monotonically both with increasing and decreasing SOA. The effect of stimulus intensity was also replicated, as evidenced by the higher reaction times for visual stimuli of lower intensity, as well as by the relatively steep slope of mean reaction times for low-intensity visual stimuli when the auditory stimulus was presented after the visual one (SOA &gt; 0). This steep slope could also be seen for low-intensity visual stimuli in Figure 1 (Av condition).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30733</offset><text>Crowdsourcing allows researchers to access a large pool of participants, thereby yielding high statistical power. This can be illustrated with a post hoc power analysis: For a false-positive rate of 1%, a sample size of 1,823, and an effect size for a pair of conditions (dz) of 0.109 (calculated from a mean difference of 20 ms, an observed SD across participants of 179 ms, and an observed correlation between the two groups of 0.47), the achieved statistical power is 98.0%. The results in the figures allowed for a reliable assessment of experimental effects and individual differences results (effect of SOA, stimulus intensity, learning curves, percentiles).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>31398</offset><text>Effects of Experimental Conditions and Comparison Between the Crowdsourcing Experiment and the Web-in-Lab Experiment</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31515</offset><text>Although the expected effects were clearly replicated, there were substantial differences in reaction times between the crowdsourcing study and the Web-in-lab study. The differences between the two methods may be because the Web-in-lab participants used the same high-quality laptop, which displayed the stimuli with the same intensity, whereas it is plausible that at least some crowdsourcing participants had poor or malfunctioning hardware or did not have their audio turned on despite the task instructions. Some of the crowdsourcing participants completed the task outdoors, which was associated with slower reaction times, possibly due to poor lighting conditions or distractive elements in the environment. Also, crowdsourcing participants who used a handheld device had a higher mean reaction time than participants who used a laptop or PC, which may be because the former involves hardware delays or may be hard to use if one’s task is to provide input as quickly as possible. Furthermore, it is possible that the lab participants were concentrated and motivated to perform well, whereas the crowdsourced participants may have taken the task less seriously because they were anonymous. Previous research shows that IQ and reaction time share a negative correlation. The lab participants, who were mostly students at a technical university, may have faster reaction times than the typical international crowdsourcing participant.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32955</offset><text>We did not find a statistically significant correlation between the mean reaction time and the mean age of the crowdsourced participants. This lack of correlation may be because the oldest participant in our study was 71 years old, whereas simple reaction times increase with age especially for people above 70 years old. It is also possible that the relationship between age and reaction time is confounded because CrowdFlower participants from lower-income countries tend to be younger.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33444</offset><text>Another source of difference between the crowdsourcing and Web-in-lab study may concern differences in understanding of the task instructions. In previous CrowdFlower research, we found that participants from English-speaking countries and participants from countries with a higher gross domestic product (GDP) per capita took less time to complete a questionnaire, which may have been due to difficulty in processing English-language text. Similarly, in a supplementary analysis of the present study, we found that participants from countries with a higher GDP per capita had a lower median time to complete the experiment, including the questionnaire (Spearman’s ρ = −0.67, p &lt; .001, based on 22 countries with 25 or more respondents), and had a faster mean reaction time as well (Spearman’s ρ = −0.36, p = .101). (Supplemental material is available at https://doi.org/10.4121/uuid:673c9bbc-bf17-42fa-a23a-3d716e141b1f) In summary, national differences may be a source of heterogeneity in the crowdsourcing study.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34470</offset><text>In our analysis, 55 of 2,000 participants were excluded due to negative reaction times. We aimed to show the variability of reaction times and therefore did not exclude slow-responding participants. However, others who use crowdsourcing and aim for clean data could opt for applying stricter screening criteria.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>34782</offset><text>Learning Curve and Trial-to-Trial Correlations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34829</offset><text>The first trials were associated with slower reaction times as the participants needed time to get used to the system. Also, the participants showed increased reaction times after the breaks, which is presumably because some participants did not have their finger on the keyboard yet or initially pressed an incorrect key. That is, participants had to press C to proceed to the next batch of trials but had to press F after each trial, which may result in initial confusion. We also found that performance became more stable (i.e., higher between-trial correlations) as the experiment progressed. This increase of stability may be caused by the fact that participants learned the nature of the task and entered the autonomous phase of skill learning, in which performance is less susceptible to task-irrelevant distractions.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>35654</offset><text>Individual Differences and Reinterpretation of the Effects of SOA on Reaction Times</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35738</offset><text>We found that SOA hardly affected the fastest reaction times, but it did have a substantial effect on the slowest (e.g., 95th percentile) of the reactions. That is, the hypothesized V shape of mean reaction time as a function of SOA was evident only in the mean reaction time and the higher percentiles of reaction time but was hardly evident from the 5th, 25th, and 50th percentiles of reaction time. These observations suggest that reductions in mean reaction times caused by simultaneous multimodal feedback are not necessarily due to multisensory neural integration, in which auditory and visual information is summed or combined in the central nervous system or at the level of individual neurons. Our findings can be explained using an independent-channels mechanism where the visual and auditory channels operate in parallel (see, for a review). That is, our results can be explained by the notion that participants sometimes do not attend to the auditory or visual stimulus. For example, a participant may be temporarily blinded due to an eye blink (typically lasting 150 ms), as a result of which he or she is more likely to react to the auditory stimulus. Similarly, a participant may have a lapse in hearing (e.g., due to an internal distraction or masking due to external noise), as a result of which he or she is more likely to react to the visual stimulus. Future research could use eye tracking and neurophysiological measures to investigate how reaction times depend on eye blinks and lapses in attention.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37260</offset><text>Participants were not prescreened based on their hearing or visual disabilities or other criteria that could affect their ability to complete this task. The variability in the right tail of the reaction time distribution may also be caused by individual differences in the understanding of the task instructions (i.e., to respond to the first of the two stimuli), in sensory ability, and in computer hardware. People with a hearing disability or with malfunctioning speakers, for example, by definition have to react to the visual stimuli. The auditory-only stimulus caused a relatively high proportion of delayed responses (≥1,500 ms), which suggests that a portion of participants were “waiting” for the visual stimulus to arrive or did not have their sound enabled. More generally, our findings suggest that warning signals should be audiovisual rather audio only or visual only and that the visual and auditory warning should be presented simultaneously (SOA = 0 ms), as was done in an automated driving study by, for example.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>38297</offset><text>Limitations of Crowdsourcing Regarding Temporal Resolution and Timing of Audiovisual Stimuli</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38390</offset><text>Based on their review, argued that “only subset of studies, specifically those requiring short stimulus presentation, are not so well suited to online research” (p. 15). We indeed did have some technical problems in the presentation of the stimuli. First, we observed a limited temporal resolution of the reaction time measurements in the crowdsourcing experiment, being 2.6 to 3 ms for 88% of the participants but 42.7 ms for 4% of the participants. Second, the animated .gifs do not render properly for delays of 10 ms (i.e., SOA = −10 ms), a known issue in computer graphics. Also, the .gif files were associated with an average additive delay of about 43 ms. This additive delay was not observed in the lab study and was hardly present among the faster responses. Thus, it is possible that the 43-ms delay was caused by certain browsers not displaying the animated .gif files properly. Despite the problems observed with animated .gif files, differences in reaction times could be detected even for auditory and visual delays of 10-ms increments, which is noteworthy when considering that a typical screen refresh rate is only 17 ms (see, for further discussion). In summary, we obtained credible experimental effects despite imperfect control of the SOA and despite a limited temporal resolution of the measurements. The robustness of reaction times to noise and temporal resolution is in agreement with simulations by and. Authors of future research could extend our approach by varying not only the intensity of visual stimuli but also the intensity of the auditory stimuli.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>39978</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>39990</offset><text>We conclude that crowdsourcing may allow for large-scale reaction time research, at the expense of a lack of control of the test environment. For example, screen brightness and rendering problems may affect the perception of visual stimuli, whereas hardware volume level can affect the perception of auditory stimuli. The expected effects of SOA were replicated despite the variable test environment, which indicates that crowdsourcing is a powerful tool in reaction time research.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>40472</offset><text>Key Points</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40483</offset><text>A large crowdsourcing study (N = 1,823) on audiovisual reaction times was performed.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40568</offset><text>Stimulus onset asynchrony and visual stimulus intensity were varied.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40637</offset><text>Results replicated past psychophysics research that used comparatively small sample sizes.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40728</offset><text>Crowdsourcing yielded higher variability in reaction times than a local Web-in-lab study.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>40818</offset><text>Crowdsourcing is a promising medium for reaction time research.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>40882</offset><text>ORCID iD: J. C. F. de Winter  https://orcid.org/0000-0002-1281-8200</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>40950</offset><text>Supplemental Material: Supplemental material is available at https://doi.org/10.4121/uuid:673c9bbc-bf17-42fa-a23a-3d716e141b1f</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>41077</offset><text>References</text></passage><passage><infon key="fpage">577</infon><infon key="lpage">586</infon><infon key="name_0">surname:Abe;given-names:G.</infon><infon key="name_1">surname:Richardson;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.apergo.2005.11.001</infon><infon key="pub-id_pmid">16364231</infon><infon key="section_type">REF</infon><infon key="source">Applied Ergonomics</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2006</infon><offset>41088</offset><text>Alarm timing, trust and driver expectation for forward collision warning systems</text></passage><passage><infon key="fpage">3</infon><infon key="lpage">27</infon><infon key="name_0">surname:Ackerman;given-names:P. L.</infon><infon key="pub-id_doi">10.1037/0033-2909.102.1.3</infon><infon key="section_type">REF</infon><infon key="source">Psychological Bulletin</infon><infon key="type">ref</infon><infon key="volume">102</infon><infon key="year">1987</infon><offset>41169</offset><text>Individual differences in skill learning: An integration of psychometric and information processing perspectives</text></passage><passage><infon key="fpage">229</infon><infon key="lpage">240</infon><infon key="name_0">surname:Baddeley;given-names:A. D.</infon><infon key="name_1">surname:Ecob;given-names:R. J.</infon><infon key="pub-id_doi">10.1080/14640747308400342</infon><infon key="section_type">REF</infon><infon key="source">Quarterly Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">1973</infon><offset>41282</offset><text>Reaction time and short-term memory: Implications of repetition effects for the high-speed exhaustive scan hypothesis</text></passage><passage><infon key="fpage">918</infon><infon key="lpage">929</infon><infon key="name_0">surname:Barnhoorn;given-names:J. S.</infon><infon key="name_1">surname:Haasnoot;given-names:E.</infon><infon key="name_2">surname:Bocanegra;given-names:B. R.</infon><infon key="name_3">surname:Van Steenbergen;given-names:H.</infon><infon key="pub-id_doi">10.3758/s13428-014-0530-7</infon><infon key="pub-id_pmid">25407763</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>41400</offset><text>QRTEngine: An easy solution for running online reaction time experiments using Qualtrics</text></passage><passage><infon key="fpage">350</infon><infon key="lpage">357</infon><infon key="name_0">surname:Brand;given-names:A.</infon><infon key="name_1">surname:Bradley;given-names:M. T.</infon><infon key="pub-id_doi">10.1177/0894439311415604</infon><infon key="section_type">REF</infon><infon key="source">Social Science Computer Review</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2012</infon><offset>41489</offset><text>Assessing the effects of technical variance on the statistical outcomes of web experiments measuring response times</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">7</infon><infon key="name_0">surname:Chapanis;given-names:A.</infon><infon key="name_1">surname:Lindenbaum;given-names:L. E.</infon><infon key="pub-id_doi">10.1177/001872085900100401</infon><infon key="section_type">REF</infon><infon key="source">Human Factors</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">1959</infon><offset>41605</offset><text>A reaction time study of four control-display linkages</text></passage><passage><infon key="fpage">e57410</infon><infon key="name_0">surname:Crump;given-names:M. J.</infon><infon key="name_1">surname:McDonnell;given-names:J. V.</infon><infon key="name_2">surname:Gureckis;given-names:T. M.</infon><infon key="pub-id_doi">10.1371/journal.pone.0057410</infon><infon key="pub-id_pmid">23516406</infon><infon key="section_type">REF</infon><infon key="source">PLOS ONE</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2013</infon><offset>41660</offset><text>Evaluating Amazon’s Mechanical Turk as a tool for experimental behavioral research</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:De Leeuw;given-names:J. R</infon><infon key="pub-id_pmid">24683129</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>41745</offset><text>jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:De Leeuw;given-names:J. R.</infon><infon key="name_1">surname:Motz;given-names:B. A</infon><infon key="pub-id_doi">10.3758/s13428-015-0567-2</infon><infon key="pub-id_pmid">25761390</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2016</infon><offset>41828</offset><text>Psychophysics in a Web browser? Comparing response times collected with JavaScript and Psychophysics Toolbox in a visual search task</text></passage><passage><infon key="fpage">62</infon><infon key="lpage">73</infon><infon key="name_0">surname:Der;given-names:G.</infon><infon key="name_1">surname:Deary;given-names:I. J.</infon><infon key="pub-id_doi">10.1037/0882-7974.21.1.62</infon><infon key="pub-id_pmid">16594792</infon><infon key="section_type">REF</infon><infon key="source">Psychology and Aging</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2006</infon><offset>41961</offset><text>Age and sex differences in reaction time in adulthood: Results from the United Kingdom Health and Lifestyle Survey</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">152</infon><infon key="name_0">surname:De Winter;given-names:J. C. F.</infon><infon key="name_1">surname:Dodou;given-names:D</infon><infon key="pub-id_doi">10.1016/j.paid.2016.03.091</infon><infon key="section_type">REF</infon><infon key="source">Personality and Individual Differences</infon><infon key="type">ref</infon><infon key="volume">98</infon><infon key="year">2016</infon><offset>42076</offset><text>National correlates of self-reported traffic violations across 41 countries</text></passage><passage><infon key="fpage">2518</infon><infon key="lpage">2525</infon><infon key="name_0">surname:De Winter;given-names:J. C. F.</infon><infon key="name_1">surname:Kyriakidis;given-names:M.</infon><infon key="name_2">surname:Dodou;given-names:D.</infon><infon key="name_3">surname:Happee;given-names:R</infon><infon key="pub-id_doi">10.1016/j.promfg.2015.07.514</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>42152</offset><text>Using CrowdFlower to study the relationship between self-reported violations and traffic accidents</text></passage><passage><infon key="fpage">1388</infon><infon key="lpage">1404</infon><infon key="name_0">surname:Diederich;given-names:A.</infon><infon key="name_1">surname:Colonius;given-names:H.</infon><infon key="pub-id_doi">10.3758/BF03195006</infon><infon key="pub-id_pmid">15813202</infon><infon key="section_type">REF</infon><infon key="source">Perception &amp; Psychophysics</infon><infon key="type">ref</infon><infon key="volume">66</infon><infon key="year">2004</infon><offset>42251</offset><text>Bimodal and trimodal multisensory enhancement: Effects of stimulus onset and intensity on reaction time</text></passage><passage><infon key="fpage">674</infon><infon key="lpage">687</infon><infon key="name_0">surname:Dodonova;given-names:Y. A.</infon><infon key="name_1">surname:Dodonov;given-names:Y. S.</infon><infon key="pub-id_doi">10.1016/j.intell.2013.09.001</infon><infon key="section_type">REF</infon><infon key="source">Intelligence</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2013</infon><offset>42355</offset><text>Is there any evidence of historical slowing of reaction time? No, unless we compare apples and oranges</text></passage><passage><infon key="fpage">689</infon><infon key="lpage">705</infon><infon key="name_0">surname:Eriksson;given-names:A.</infon><infon key="name_1">surname:Stanton;given-names:N. A.</infon><infon key="pub-id_doi">10.1177/0018720816685832</infon><infon key="pub-id_pmid">28124573</infon><infon key="section_type">REF</infon><infon key="source">Human Factors</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2017</infon><offset>42458</offset><text>Takeover time in highly automated vehicles: Noncritical transitions to and from manual control</text></passage><passage><infon key="fpage">175</infon><infon key="lpage">191</infon><infon key="name_0">surname:Faul;given-names:F.</infon><infon key="name_1">surname:Erdfelder;given-names:E.</infon><infon key="name_2">surname:Lang;given-names:A. G.</infon><infon key="name_3">surname:Buchner;given-names:A.</infon><infon key="pub-id_doi">10.3758/BF03193146</infon><infon key="pub-id_pmid">17695343</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2007</infon><offset>42553</offset><text>G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</text></passage><passage><infon key="name_0">surname:Fitts;given-names:P. M.</infon><infon key="name_1">surname:Posner;given-names:M. I.</infon><infon key="section_type">REF</infon><infon key="source">Human performance</infon><infon key="type">ref</infon><infon key="year">1967</infon><offset>42662</offset></passage><passage><infon key="fpage">199</infon><infon key="lpage">210</infon><infon key="name_0">surname:Fitts;given-names:P. M.</infon><infon key="name_1">surname:Seeger;given-names:C. M.</infon><infon key="pub-id_doi">10.1037/h0062827</infon><infon key="pub-id_pmid">13084867</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">1953</infon><offset>42663</offset><text>SR compatibility: Spatial characteristics of stimulus and response codes</text></passage><passage><infon key="fpage">1497</infon><infon key="lpage">1510</infon><infon key="name_0">surname:Fortenbaugh;given-names:F. C.</infon><infon key="name_1">surname:DeGutis;given-names:J.</infon><infon key="name_2">surname:Germine;given-names:L.</infon><infon key="name_3">surname:Wilmer;given-names:J. B.</infon><infon key="name_4">surname:Grosso;given-names:M.</infon><infon key="name_5">surname:Russo;given-names:K.</infon><infon key="name_6">surname:Esterman;given-names:M.</infon><infon key="pub-id_doi">10.1177/0956797615594896</infon><infon key="pub-id_pmid">26253551</infon><infon key="section_type">REF</infon><infon key="source">Psychological Science</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2015</infon><offset>42736</offset><text>Sustained attention across the life span in a sample of 10,000: Dissociating ability and strategy</text></passage><passage><infon key="fpage">1278</infon><infon key="lpage">1291</infon><infon key="name_0">surname:Giray;given-names:M.</infon><infon key="name_1">surname:Ulrich;given-names:R.</infon><infon key="pub-id_doi">10.1037/0096-1523.19.6.1278</infon><infon key="pub-id_pmid">8294892</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: Human Perception and Performance</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">1993</infon><offset>42834</offset><text>Motor coactivation revealed by response force in divided and focused attention</text></passage><passage><infon key="fpage">1938</infon><infon key="lpage">1942</infon><infon key="name_0">surname:Gold;given-names:C.</infon><infon key="name_1">surname:Damböck;given-names:D.</infon><infon key="name_2">surname:Lorenz;given-names:L.</infon><infon key="name_3">surname:Bengler;given-names:K.</infon><infon key="pub-id_doi">10.1177/1541931213571433</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Human Factors and Ergonomics Society 57th Annual Meeting</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>42913</offset><text>“Take over!” How long does it take to get the driver back into the loop?</text></passage><passage><infon key="fpage">1692</infon><infon key="lpage">1709</infon><infon key="name_0">surname:Gondan;given-names:M.</infon><infon key="name_1">surname:Götze;given-names:C.</infon><infon key="name_2">surname:Greenlee;given-names:M. W.</infon><infon key="pub-id_doi">10.3758/APP.72.6.1692</infon><infon key="section_type">REF</infon><infon key="source">Attention, Perception, &amp; Psychophysics</infon><infon key="type">ref</infon><infon key="volume">72</infon><infon key="year">2010</infon><offset>42990</offset><text>Redundancy gains in simple responses and go/no-go tasks</text></passage><passage><infon key="fpage">723</infon><infon key="lpage">735</infon><infon key="name_0">surname:Gondan;given-names:M.</infon><infon key="name_1">surname:Minakata;given-names:K.</infon><infon key="pub-id_doi">10.3758/s13414-015-1018-y</infon><infon key="section_type">REF</infon><infon key="source">Attention, Perception, &amp; Psychophysics</infon><infon key="type">ref</infon><infon key="volume">78</infon><infon key="year">2016</infon><offset>43046</offset><text>A tutorial on testing the race model inequality</text></passage><passage><infon key="fpage">763</infon><infon key="lpage">775</infon><infon key="name_0">surname:Harrar;given-names:V.</infon><infon key="name_1">surname:Harris;given-names:L. R.</infon><infon key="name_2">surname:Spence;given-names:C.</infon><infon key="pub-id_doi">10.1007/s00221-016-4822-2</infon><infon key="pub-id_pmid">27872958</infon><infon key="section_type">REF</infon><infon key="source">Experimental Brain Research</infon><infon key="type">ref</infon><infon key="volume">235</infon><infon key="year">2017</infon><offset>43094</offset><text>Multisensory integration is independent of perceived simultaneity</text></passage><passage><infon key="fpage">289</infon><infon key="lpage">293</infon><infon key="name_0">surname:Hershenson;given-names:M.</infon><infon key="pub-id_doi">10.1037/h0039516</infon><infon key="pub-id_pmid">13906889</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">1962</infon><offset>43160</offset><text>Reaction time as a measure of intersensory facilitation</text></passage><passage><infon key="fpage">1718</infon><infon key="lpage">1724</infon><infon key="name_0">surname:Hilbig;given-names:B. E.</infon><infon key="pub-id_doi">10.3758/s13428-015-0678-9</infon><infon key="pub-id_pmid">26542972</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2016</infon><offset>43216</offset><text>Reaction time effects in lab-versus Web-based research: Experimental evidence</text></passage><passage><infon key="fpage">467</infon><infon key="lpage">476</infon><infon key="name_0">surname:Ho;given-names:C.</infon><infon key="name_1">surname:Gray;given-names:R.</infon><infon key="name_2">surname:Spence;given-names:C.</infon><infon key="pub-id_doi">10.1007/s00221-013-3522-4</infon><infon key="pub-id_pmid">23604626</infon><infon key="section_type">REF</infon><infon key="source">Experimental Brain Research</infon><infon key="type">ref</infon><infon key="volume">227</infon><infon key="year">2013</infon><offset>43294</offset><text>Role of audiovisual synchrony in driving head orienting responses</text></passage><passage><infon key="name_0">surname:Jensen;given-names:A. R.</infon><infon key="section_type">REF</infon><infon key="source">Clocking the mind: Mental chronometry and individual differences</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>43360</offset></passage><passage><infon key="name_0">surname:Karonen;given-names:I</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>43361</offset><text>Why is this GIF’s animation speed different in Firefox vs. IE? [Answer]</text></passage><passage><infon key="fpage">2784</infon><infon key="lpage">2791</infon><infon key="name_0">surname:Larsson;given-names:P.</infon><infon key="name_1">surname:Johansson;given-names:E.</infon><infon key="name_2">surname:Söderman;given-names:M.</infon><infon key="name_3">surname:Thompson;given-names:D.</infon><infon key="pub-id_doi">10.1016/j.promfg.2015.07.735</infon><infon key="section_type">REF</infon><infon key="source">Procedia Manufacturing</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2015</infon><offset>43435</offset><text>Interaction design for communicating system state and capabilities during automated highway driving</text></passage><passage><infon key="fpage">213</infon><infon key="lpage">228</infon><infon key="name_0">surname:Leone;given-names:L. M.</infon><infon key="name_1">surname:McCourt;given-names:M. E.</infon><infon key="pub-id_doi">10.1068/i0532</infon><infon key="pub-id_pmid">24349682</infon><infon key="section_type">REF</infon><infon key="source">i-Perception</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2013</infon><offset>43535</offset><text>The roles of physical and physiological simultaneity in audiovisual multisensory facilitation</text></passage><passage><infon key="fpage">2915</infon><infon key="lpage">2922</infon><infon key="name_0">surname:Leone;given-names:L. M.</infon><infon key="name_1">surname:McCourt;given-names:M. E.</infon><infon key="pub-id_doi">10.1111/ejn.13087</infon><infon key="pub-id_pmid">26417674</infon><infon key="section_type">REF</infon><infon key="source">European Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2015</infon><offset>43629</offset><text>Dissociation of perception and action in audiovisual multisensory integration</text></passage><passage><infon key="fpage">519</infon><infon key="lpage">528</infon><infon key="name_0">surname:Litman;given-names:L.</infon><infon key="name_1">surname:Robinson;given-names:J.</infon><infon key="name_2">surname:Rosenzweig;given-names:C.</infon><infon key="pub-id_doi">10.3758/s13428-014-0483-x</infon><infon key="pub-id_pmid">24907001</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>43707</offset><text>The relationship between motivation, monetary compensation, and data quality among US- and India-based workers on Mechanical Turk</text></passage><passage><infon key="fpage">157</infon><infon key="lpage">162</infon><infon key="name_0">surname:Madison;given-names:G.</infon><infon key="name_1">surname:Mosing;given-names:M. A.</infon><infon key="name_2">surname:Verweij;given-names:K. J.</infon><infon key="name_3">surname:Pedersen;given-names:N. L.</infon><infon key="name_4">surname:Ullén;given-names:F.</infon><infon key="pub-id_doi">10.1016/j.intell.2016.10.001</infon><infon key="section_type">REF</infon><infon key="source">Intelligence</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2016</infon><offset>43837</offset><text>Common genetic influences on intelligence and auditory simple reaction time in a large Swedish sample</text></passage><passage><infon key="fpage">e71608</infon><infon key="name_0">surname:Mégevand;given-names:P.</infon><infon key="name_1">surname:Molholm;given-names:S.</infon><infon key="name_2">surname:Nayak;given-names:A.</infon><infon key="name_3">surname:Foxe;given-names:J. J.</infon><infon key="pub-id_doi">10.1371/journal.pone.0071608</infon><infon key="pub-id_pmid">23951203</infon><infon key="section_type">REF</infon><infon key="source">PLOS ONE</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2013</infon><offset>43939</offset><text>Recalibration of the multisensory temporal window of integration results from changing task demands</text></passage><passage><infon key="fpage">514</infon><infon key="lpage">521</infon><infon key="name_0">surname:Merat;given-names:N.</infon><infon key="name_1">surname:Jamson;given-names:A. H.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>44039</offset><text>How do drivers behave in a highly automated car</text></passage><passage><infon key="fpage">331</infon><infon key="lpage">343</infon><infon key="name_0">surname:Miller;given-names:J.</infon><infon key="pub-id_doi">10.3758/BF03203025</infon><infon key="pub-id_pmid">3786102</infon><infon key="section_type">REF</infon><infon key="source">Attention, Perception, &amp; Psychophysics</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">1986</infon><offset>44087</offset><text>Timecourse of coactivation in bimodal divided attention</text></passage><passage><infon key="fpage">101</infon><infon key="lpage">151</infon><infon key="name_0">surname:Miller;given-names:J.</infon><infon key="name_1">surname:Ulrich;given-names:R.</infon><infon key="pub-id_doi">10.1016/S0010-0285(02)00517-0</infon><infon key="pub-id_pmid">12643892</infon><infon key="section_type">REF</infon><infon key="source">Cognitive Psychology</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2003</infon><offset>44143</offset><text>Simple reaction time and statistical facilitation: A parallel grains model</text></passage><passage><infon key="fpage">489</infon><infon key="lpage">509</infon><infon key="name_0">surname:Nickerson;given-names:R. S.</infon><infon key="pub-id_doi">10.1037/h0035437</infon><infon key="pub-id_pmid">4757060</infon><infon key="section_type">REF</infon><infon key="source">Psychological Review</infon><infon key="type">ref</infon><infon key="volume">80</infon><infon key="year">1973</infon><offset>44218</offset><text>Intersensory facilitation of reaction time: Energy summation or preparation enhancement?</text></passage><passage><infon key="fpage">204</infon><infon key="lpage">215</infon><infon key="name_0">surname:Petermeijer;given-names:S. M.</infon><infon key="name_1">surname:Bazilinskyy;given-names:P.</infon><infon key="name_2">surname:Bengler;given-names:K. J.</infon><infon key="name_3">surname:De Winter;given-names:J. C. F.</infon><infon key="pub-id_doi">10.1016/j.apergo.2017.02.023</infon><infon key="pub-id_pmid">28411731</infon><infon key="section_type">REF</infon><infon key="source">Applied Ergonomics</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2017</infon><offset>44307</offset><text>Take-over again: Investigating multimodal and directional TORs to get the driver back into the loop</text></passage><passage><infon key="fpage">897</infon><infon key="lpage">907</infon><infon key="name_0">surname:Petermeijer;given-names:S.</infon><infon key="name_1">surname:De Winter;given-names:J. C. F.</infon><infon key="name_2">surname:Bengler;given-names:K.</infon><infon key="pub-id_doi">10.1109/TITS.2015.2494873</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Intelligent Transportation Systems</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2016</infon><offset>44407</offset><text>Vibrotactile displays: A survey with a view on highly automated driving</text></passage><passage><infon key="fpage">510</infon><infon key="lpage">532</infon><infon key="name_0">surname:Ratcliff;given-names:R.</infon><infon key="pub-id_pmid">8272468</infon><infon key="section_type">REF</infon><infon key="source">Psychological Bulletin</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">1993</infon><offset>44479</offset><text>Methods for dealing with reaction time outliers</text></passage><passage><infon key="fpage">309</infon><infon key="lpage">327</infon><infon key="name_0">surname:Reimers;given-names:S.</infon><infon key="name_1">surname:Stewart;given-names:N.</infon><infon key="pub-id_doi">10.3758/s13428-014-0471-1</infon><infon key="pub-id_pmid">24903687</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>44527</offset><text>Presentation and response time accuracy in Adobe Flash and HTML5/Javascript Web experiments</text></passage><passage><infon key="fpage">897</infon><infon key="lpage">908</infon><infon key="name_0">surname:Reimers;given-names:S.</infon><infon key="name_1">surname:Stewart;given-names:N.</infon><infon key="pub-id_doi">10.3758/s13428-016-0758-5</infon><infon key="pub-id_pmid">27421976</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2016</infon><offset>44619</offset><text>Auditory presentation and synchronization in Adobe Flash and HTML5/JavaScript Web experiments</text></passage><passage><infon key="fpage">e67769</infon><infon key="name_0">surname:Schubert;given-names:T. W.</infon><infon key="name_1">surname:Murteira;given-names:C.</infon><infon key="name_2">surname:Collins;given-names:E. C.</infon><infon key="name_3">surname:Lopes;given-names:D.</infon><infon key="pub-id_doi">10.1371/journal.pone.0067769</infon><infon key="pub-id_pmid">23805326</infon><infon key="section_type">REF</infon><infon key="source">PLOS ONE</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2013</infon><offset>44713</offset><text>ScriptingRT: A software library for collecting response latencies in online studies of cognition</text></passage><passage><infon key="fpage">1241</infon><infon key="lpage">1260</infon><infon key="name_0">surname:Semmelmann;given-names:K.</infon><infon key="name_1">surname:Weigelt;given-names:S.</infon><infon key="pub-id_doi">10.3758/s13428-016-0783-4</infon><infon key="pub-id_pmid">27496171</infon><infon key="section_type">REF</infon><infon key="source">Behavior Research Methods</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2017</infon><offset>44810</offset><text>Online psychophysics: Reaction time effects in cognitive experiments</text></passage><passage><infon key="fpage">255</infon><infon key="lpage">266</infon><infon key="name_0">surname:Stein;given-names:B. E.</infon><infon key="name_1">surname:Stanford;given-names:T. R.</infon><infon key="pub-id_doi">10.1038/nrn2331</infon><infon key="pub-id_pmid">18354398</infon><infon key="section_type">REF</infon><infon key="source">Nature Reviews Neuroscience</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2008</infon><offset>44879</offset><text>Multisensory integration: Current issues from the perspective of the single neuron</text></passage><passage><infon key="name_0">surname:Todd;given-names:J. W.</infon><infon key="name_1">surname:Woodworth;given-names:R. S.</infon><infon key="section_type">REF</infon><infon key="source">Archives of psychology, XXI</infon><infon key="type">ref</infon><infon key="year">1912</infon><offset>44962</offset><text>Reaction to multiple stimuli</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:Ulrich;given-names:R.</infon><infon key="name_1">surname:Giray;given-names:M.</infon><infon key="pub-id_doi">10.1111/j.2044-8317.1989.tb01111.x</infon><infon key="section_type">REF</infon><infon key="source">British Journal of Mathematical and Statistical Psychology</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">1989</infon><offset>44991</offset><text>Time resolution of clocks: Effects on reaction time measurement-Good news for bad clocks</text></passage><passage><infon key="fpage">20</infon><infon key="lpage">28</infon><infon key="name_0">surname:Van der Stoep;given-names:N.</infon><infon key="name_1">surname:Spence;given-names:C.</infon><infon key="name_2">surname:Nijboer;given-names:T. C. W.</infon><infon key="name_3">surname:Van der Stigchel;given-names:S</infon><infon key="pub-id_doi">10.1016/j.actpsy.2015.09.010</infon><infon key="pub-id_pmid">26436587</infon><infon key="section_type">REF</infon><infon key="source">Acta Psychologica</infon><infon key="type">ref</infon><infon key="volume">162</infon><infon key="year">2015</infon><offset>45080</offset><text>On the relative contributions of multisensory integration and crossmodal exogenous spatial attention to multisensory response enhancement</text></passage><passage><infon key="fpage">4546</infon><infon key="lpage">4550</infon><infon key="name_0">surname:Wang;given-names:Y.</infon><infon key="name_1">surname:Toor;given-names:S. S.</infon><infon key="name_2">surname:Gautam;given-names:R.</infon><infon key="name_3">surname:Henson;given-names:D. B.</infon><infon key="pub-id_doi">10.1167/iovs.10-6553</infon><infon key="pub-id_pmid">21447676</infon><infon key="section_type">REF</infon><infon key="source">Investigative Ophthalmology &amp; Visual Science</infon><infon key="type">ref</infon><infon key="volume">52</infon><infon key="year">2011</infon><offset>45218</offset><text>Blink frequency and duration during perimetry and their relationship to test-retest threshold variability</text></passage><passage><infon key="fpage">28</infon><infon key="lpage">35</infon><infon key="name_0">surname:Welch;given-names:B. L.</infon><infon key="pub-id_doi">10.1093/biomet/34.1-2.28</infon><infon key="pub-id_pmid">20287819</infon><infon key="section_type">REF</infon><infon key="source">Biometrika</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">1947</infon><offset>45324</offset><text>The generalization of “Student’s” problem when several different population variances are involved</text></passage><passage><infon key="name_0">surname:Wickens;given-names:C. D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>45429</offset><text>Attention to safety and the psychology of surprise</text></passage><passage><infon key="fpage">33</infon><infon key="lpage">39</infon><infon key="name_0">surname:Wolfe;given-names:J. M.</infon><infon key="pub-id_doi">10.1111/1467-9280.00006</infon><infon key="section_type">REF</infon><infon key="source">Psychological Science</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">1998</infon><offset>45480</offset><text>What can 1 million trials tell us about visual search?</text></passage><passage><infon key="fpage">e1058</infon><infon key="name_0">surname:Woods;given-names:A. T.</infon><infon key="name_1">surname:Velasco;given-names:C.</infon><infon key="name_2">surname:Levitan;given-names:C. A.</infon><infon key="name_3">surname:Wan;given-names:X.</infon><infon key="name_4">surname:Spence;given-names:C.</infon><infon key="pub-id_doi">10.7717/peerj.1058</infon><infon key="pub-id_pmid">26244107</infon><infon key="section_type">REF</infon><infon key="source">PeerJ</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2015</infon><offset>45535</offset><text>Conducting perception research over the Internet: A tutorial review</text></passage></document></collection>
