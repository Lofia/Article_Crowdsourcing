<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201219</date><key>pmc.key</key><document><id>5806457</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1186/s13007-018-0278-7</infon><infon key="article-id_pmc">5806457</infon><infon key="article-id_pmid">29449872</infon><infon key="article-id_publisher-id">278</infon><infon key="elocation-id">12</infon><infon key="kwd">Phenotyping Image-based Observer Agreement Variability Crowdsourcing Citizen-science</infon><infon key="license">Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.</infon><infon key="name_0">surname:Giuffrida;given-names:M. Valerio</infon><infon key="name_1">surname:Chen;given-names:Feng</infon><infon key="name_2">surname:Scharr;given-names:Hanno</infon><infon key="name_3">surname:Tsaftaris;given-names:Sotirios A.</infon><infon key="name_4">surname:Tsaftaris;given-names:Sotirios A.</infon><infon key="name_5">surname:Tsaftaris;given-names:Sotirios A.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">14</infon><infon key="year">2018</infon><offset>0</offset><text>Citizen crowds and experts: observer variability in image-based plant phenotyping</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>82</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>93</offset><text>Image-based plant phenotyping has become a powerful tool in unravelling genotype–environment interactions. The utilization of image analysis and machine learning have become paramount in extracting data stemming from phenotyping experiments. Yet we rely on observer (a human expert) input to perform the phenotyping process. We assume such input to be a ‘gold-standard’ and use it to evaluate software and algorithms and to train learning-based algorithms. However, we should consider whether any variability among experienced and non-experienced (including plain citizens) observers exists. Here we design a study that measures such variability in an annotation task of an integer-quantifiable phenotype: the leaf count.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>821</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>829</offset><text>We compare several experienced and non-experienced observers in annotating leaf counts in images of Arabidopsis Thaliana to measure intra- and inter-observer variability in a controlled study using specially designed annotation tools but also citizens using a distributed citizen-powered web-based platform. In the controlled study observers counted leaves by looking at top-view images, which were taken with low and high resolution optics. We assessed whether the utilization of tools specifically designed for this task can help to reduce such variability. We found that the presence of tools helps to reduce intra-observer variability, and that although intra- and inter-observer variability is present it does not have any effect on longitudinal leaf count trend statistical assessments. We compared the variability of citizen provided annotations (from the web-based platform) and found that plain citizens can provide statistically accurate leaf counts. We also compared a recent machine-learning based leaf counting algorithm and found that while close in performance it is still not within inter-observer variability.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1956</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1968</offset><text>While expertise of the observer plays a role, if sufficient statistical power is present, a collection of non-experienced users and even citizens can be included in image-based phenotyping annotation tasks as long they are suitably designed. We hope with these findings that we can re-evaluate the expectations that we have from automated algorithms: as long as they perform within observer variability they can be considered a suitable alternative. In addition, we hope to invigorate an interest in introducing suitably designed tasks on citizen powered platforms not only to obtain useful information (for research) but to help engage the public in this societal important problem.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2652</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2663</offset><text>This community is well aware of the importance of measuring a plant’s phenotype and its modulation due to environmental and genotypic variations. Scientists have been observing plants directly, measuring phenotyping traits manually for years. Whilst this method is labour-intensive and time consuming, it is also prone to errors. Recently, image-based phenotyping by coupling imaging and automation has created a revolution on how we observe (and can potentially quantify) such phenotypic variation, in the hope of reducing the phenotyping bottleneck. Without a doubt this potential has spurred a great interest in the imaging of plants at various levels of scale, above or below ground level, in the optical or hyper-spectral spectrum in 2D or 3D.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3414</offset><text>However, the ability to extract actionable information from image data, that will lead to the full realization of this revolution, is still considered a hard task . It is the complexity of some of the tasks involved that have now created a new bottleneck: lack of appropriate software solutions able to effectively analyze such data . The community has reacted swiftly by placing significant emphasis in the design of new algorithms and the release of software (for example see the collection of http://www.plant-image-analysis.org and). More recently, open datasets have allowed not only the ability of experts within the community to evaluate algorithmic performance on key phenotyping tasks, such as leaf segmentation and counting, but also enabled image computing experts new to plant phenotyping to enter this exciting field. Unsurprisingly, many of the new methods rely on machine learning, a technology that has the potential to transform how phenotyping discovery from images can occur in the future, as also recently demonstrated. Even though its potential is well-known, machine learning algorithms do require data to learn from, which typically need to be annotated by expert observers when domain-specificity is required. The performance of algorithms is bounded to the precision of observers. Naturally this raises the question how precise are the experts on a given task?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4802</offset><text>In the medical community, variability among observers is known to exist and has been accepted. Also experts in plant breeding, diseases, and taxonomy agree that variability exists. For example, several studies have been used as de-facto references for discussing rater disagreement when visually scoring leaf diseases on the basis of scales. At the same time they have become motivating references advocating that image analysis systems can help reduce (rater) variation. They have been also perused in advocating for the use of digital imaging itself as opposed to on site surveys with rating scales. Even the image-based phenotyping literature has been perusing these works. However, an extensive literature review has not found a comparison of raters on visually quantifiable traits or phenotypes.</text></passage><passage><infon key="file">13007_2018_278_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5603</offset><text>Annotation tool. Screenshots of the annotation tool and the web-page seen by users. A Screenshot of the customized, yet simplified, version of the leaf annotation tool in. B An excerpt of the Zooniverse site used here showing annotations and the (single-choice) confidence question</text></passage><passage><infon key="file">13007_2018_278_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5885</offset><text>Intra-observer variability. A Intra-observer variability of experienced (left: A1) or non-experienced (right: A2) observers in RPi. B Influence of the tool in intra-observer measurements in experienced (left: B1) or non-experienced (right: B2) observers in RPi</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6146</offset><text>One such integer-quantifiable phenotype is counting the number of leaves (or fruits, flowers). Leaf count can be used to describe the growth status of a plant, and is obviously closely related to plastochron or phyllochron and can be used to assess plants’ reactions to stress. Herewith lies a key difference: the count as a phenotype has a physical ‘ground truth’ which visual scales are not capturing and are not suited for. To this day, no such direct evaluation of observer agreement in leaf counting exists and to the best of our knowledge in the broader sense of image-based phenotyping of quantifiable phenotypes.  </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6775</offset><text>Clearly, counting objects, here leaves, is a task generally doable even by non-experts without detailed explanations. This may not be true for other, maybe visually harder, phenotyping tasks. However, even though counting plant organs might seem an elementary task, many factors may result in different values among observers, such as severe occlusions, small objects in the scene, low camera resolution, as well as mental fatigue of the annotators.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7225</offset><text>Estimating observer variability is crucial because it primarily allows us to put bounds on effect sizes and devise annotation strategies that minimize annotation effort (e.g. by splitting annotation effort among many observers). At the same time, by evaluating agreement comparing experienced (expert) and non-experienced (non-expert) observers we can evaluate the potential of using non-experts for simple well-defined annotation tasks. In addition, it allows us to put the performance of algorithms in comparison to intra- or inter-observer variation and assess how close we are to achieve human performance. It may even permit us to devise different algorithmic approaches that learn despite the presence of disagreement .</text></passage><passage><infon key="file">13007_2018_278_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>7952</offset><text>Inter-observer and influence of resolution. A Inter-observer variability among experienced (left: A1) or non-experienced (right: A2) observers in RPI; B same as in A but in Canon data; C Variability of experienced (left: C1) or non-experienced (right: C2) observers when comparing counts of the same observer in RPi and Canon data</text></passage><passage><infon key="file">13007_2018_278_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>8283</offset><text>Average longitudinal counts. Average longitudinal count curves (solid) of the two cultivars [red: col-0; blue: pgm] and 1 standard deviation (shaded area), shown in A relying on a single experienced (left: A1) or non-experienced observer (right: B1); B relying on all experienced (left: B1) or non-experienced (right: B2) observers; C relying on all together; and in D relying on the consensus citizen</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8685</offset><text>Equally exciting is the potential to explore how the use of common citizens can be used to not only annotate data for machine learning but as being part of a phenotyping experimental pipeline. The introduction of Amazon Mechanical Turk (AMT, https://www.mturk.com/) that permits the use of humans (via fee) in solving computer based microtasks in combination with annotation frameworks (e.g. LabelMe ) has led to an explosion of the potential use of crowdsourcing—a term was coined by Jeff Howe in 2006. It has been used for a variety of tasks already even for plant research e.g. http://photonynq.org. However, there have been ongoing debates as to how one can control the quality of outcomes because in principle, crowdsourcing allows ‘anyone’ to contribute. More recently, citizen-powered platforms, where volunteers participate to help with a task, as opposed to receiving a reward (a payment in real [AMT] or virtual money [Gamification]), have received particular attention by many researchers. One such popular platform, Zooniverse (http://www.zooniverse.org), allows researchers to build projects to collect data from thousands of people around the world, in order to support corresponding research. Several exciting projects have used the platform already: for example, Arteta et al. used the data from a penguin watch project to automatically count penguins in the wild.  </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10075</offset><text>variations exist between the same observer (intra-observer);</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10136</offset><text>computer-aided counting, using a specifically designed annotation tool, helps to reduce variability compared to straight-forward visual observation;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10285</offset><text>observers differ from each other (inter-observer);</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10336</offset><text>higher resolution reduced observer variability;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10384</offset><text>observer variability has any statistical influence in separating a cultivar of known different leaf growth w.r.t. wild-type;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10509</offset><text>time needed for annotations depends on expertise;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10559</offset><text>we can simulate the effects of randomly sampling from an observer population on statistical inference;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10662</offset><text>counts from a citizen-powered study can be used for phenotyping; and</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10731</offset><text>a recent ML algorithm that predicts leaf count from plant images performs within the variation of observers.</text></passage><passage><infon key="file">13007_2018_278_Fig5_HTML.jpg</infon><infon key="id">Fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>10840</offset><text>Citizen distribution and variability. A Number of images annotated per user (citizen); B Relationship between leaf count variation and average user confidence per plant; C Variability between the consensus citizen and the reference observer; D Variability between the consensus citizen and a random selection of counts (from the 3 available per-plant)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11192</offset><text>In this paper we aim to estimate observer agreement with a simple, yet expertly designed, image-based observational study. We select images of Arabidopsis Thaliana (taken from a dataset in the public domain) and ask several observers to count leaves using a variety of setups in a controlled fashion. At the same time, we included the same images within a larger citizen-powered research project that runs on Zooniverse. Specifically, we aim to assess whether:We address these points one by one in this order in the “Results” section. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11732</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11740</offset><text>We recruited 10 annotators: 5 who have experience with image-based plant phenotyping (shorthanded below as ExP) and 5 who do not have experience with phenotyping but yet have experience with images (shorthanded hereafter as NExP) to annotate a subset of the Arabidopsis dataset in. Specifically, each annotator had a set of different tasks to accomplish using visual tools or simple observation designed to assess the influence of the factors considered in this study (see background above). Details of the approach taken are provided below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12282</offset><text>Employed image data</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>12302</offset><text>Measurement of agreement between experienced and non-experienced observers</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th char=&quot;(&quot; align=&quot;left&quot;&gt;DiC ↓&lt;/th&gt;&lt;th char=&quot;(&quot; align=&quot;left&quot;&gt;|DiC| ↓&lt;/th&gt;&lt;th char=&quot;.&quot; align=&quot;left&quot;&gt;MSE ↓&lt;/th&gt;&lt;th char=&quot;.&quot; align=&quot;left&quot;&gt;&lt;italic&gt;R&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;↑&lt;/th&gt;&lt;th char=&quot;.&quot; align=&quot;left&quot;&gt;Alpha ↑&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Intra-observer (RPi) tool&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced [The reference observer]&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.10 (0.54)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.29 (0.47)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.307&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.980&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.987&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.13 (0.77)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.42 (0.65)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.600&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.960&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.981&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Tool versus visual (RPi)&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.00 (0.64)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.33 (0.55)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.415&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.970&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.986&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.23 (0.82)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.46 (0.71)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.730&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.950&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Inter-observer (RPi) tool&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.07 (0.65)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.37 (0.53)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.423&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.974&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.980&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.49 (0.76)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.60 (0.67)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.815&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.962&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.962&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Inter-observer (Canon) tool&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.55 (0.74)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.63 (0.68)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.861&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.969&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.959&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.23 (0.63)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.37 (0.56)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.450&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.977&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.976&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Intra-observer across resolution (RPi and Canon) tool&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.57 (0.87)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.68 (0.79)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.100&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.950&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.965&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-experienced&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.40 (0.70)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.51 (0.62)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.650&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.973&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; colspan=&quot;6&quot;&gt;
&lt;italic&gt;Citizens inter-observer (RPi) zooniverse&lt;/italic&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced versus consensus (average)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.53 (0.77)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.62 (0.69)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.869&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.962&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.960&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Experienced versus consensus (max)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.08 (0.82)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.45 (0.69)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.684&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.957&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.971&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Consensus (average) versus sing. random&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.00 (0.78)&lt;/td&gt;&lt;td char=&quot;(&quot; align=&quot;char&quot;&gt;0.42 (0.65)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.607&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.960&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.970&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>12377</offset><text>	DiC ↓	|DiC| ↓	MSE ↓	R2↑	Alpha ↑	 	Intra-observer (RPi) tool	 	Experienced [The reference observer]a	0.10 (0.54)	0.29 (0.47)	0.307	0.980	0.987	 	Non-experienced	0.13 (0.77)	0.42 (0.65)	0.600	0.960	0.981	 	Tool versus visual (RPi)	 	Experienced	0.00 (0.64)	0.33 (0.55)	0.415	0.970	0.986	 	Non-experienced	0.23 (0.82)	0.46 (0.71)	0.730	0.950	0.977	 	Inter-observer (RPi) tool	 	Experienced	0.07 (0.65)	0.37 (0.53)	0.423	0.974	0.980	 	Non-experienced	0.49 (0.76)	0.60 (0.67)	0.815	0.962	0.962	 	Inter-observer (Canon) tool	 	Experienced	0.55 (0.74)	0.63 (0.68)	0.861	0.969	0.959	 	Non-experienced	0.23 (0.63)	0.37 (0.56)	0.450	0.977	0.976	 	Intra-observer across resolution (RPi and Canon) tool	 	Experienced	0.57 (0.87)	0.68 (0.79)	1.100	0.950	0.965	 	Non-experienced	0.40 (0.70)	0.51 (0.62)	0.650	0.973	0.977	 	Citizens inter-observer (RPi) zooniverse	 	Experienced versus consensus (average)	0.53 (0.77)	0.62 (0.69)	0.869	0.962	0.960	 	Experienced versus consensus (max)	0.08 (0.82)	0.45 (0.69)	0.684	0.957	0.971	 	Consensus (average) versus sing. random	0.00 (0.78)	0.42 (0.65)	0.607	0.960	0.970	 	</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>13487</offset><text>For shorthand definitions see text. For DiC and |DiC| average and standard deviation are reported. Note that these correspond also to bias and limits of agreement (when standard deviation is multiplied by 1.96) of the Bland–Altman plots reported.  means lower is better, whereas  the opposite</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>13782</offset><text>aThis experienced observer is noted and used as the reference observer for the remaining analysis throughout the paper</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13904</offset><text>The data used in this study have been collected using an affordable imaging setup that used a Raspberry Pi camera, but also an optical zoom camera that offered a higher effective resolution. Images of two cultivars were selected (the wild-type col-0 and pgm), 5 replicates each every other day at 8am (i.e. every 48 h). pgm is known not to be able to accumulate transitory starch due to a mutation in the plastidic isoform of the phosphoglucomutase, which is required for starch synthesis and overall is known to be smaller than the wild-type. Furthermore, pgm was recently shown to produce new leaves at a pace lower than wild-type. Thus, we knew a priori that these cultivars should show differences in a longitudinal assessment of leaf count. The sampling frequency chosen (every 48 h) results in 13 time points per each plant, providing 130 images overall for annotation. This sampling frequency was chosen after statistical power analysis on the sample size of an ANOVA experiment drawing effect sizes reported in. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14927</offset><text>Images were cropped such that a plant appears centered in the field of view. Plant images from the Raspberry Pi camera had an effective resolution of 300 × 300 pixels (hereafter shorthanded as RPi), whereas the ones from the camera with movable optics had 470 × 470 pixels (shorthanded as Canon). In addition, to properly test intra-observer variability eliminating as much as possible effects of visual memory, a copy of all images was created, where images were artificially transformed by random 90°, 180°, 270° rotation or horizontal/vertical flip. These transformed datasets are shorthanded as RPi’ and Canon’. Data within each set were randomized to break temporal consistency and within genotype associations and to satisfy an identically independently distributed (IID) data source design.1 Dataset names were obscured as A (RPi), B (Canon), C (RPi’), and D (Canon’) such that observers were blinded to what the sets meant and reduce possible bias in ratings.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15912</offset><text>Study design</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15925</offset><text>A customized graphical user interface, based on the annotation tool in Phenotiki,2 was specifically designed for this study. The tool prompted the user to select a dataset for annotation (from A, B, C, D) and the selected list of images was automatically loaded. For each image, the observer could place dot annotations marking every leaf they could identify. Critically dots remained visible throughout a plant annotation helping the annotator keep track of visited leaves. When the observer was done, they could proceed to the next plant. Zoom and pan functionality were available to help observers visualize scenarios such as small emerging leaves and occlusions. Annotation timing was recorded but observers were not aware of this fact. Annotation timing (per plant) was calculated as the time elapsed from the first and last leaf annotation for a given plant. An example of the interface seen by users is shown in Fig. 1A.</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16854</offset><text>F and p values for the ANOVA tests corresponding to the plots in Fig. 4</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Sum sq.&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;F&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;p&lt;/italic&gt; value&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;A single ExP&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;47.816&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;43.775&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.000167&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;A single NExP&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;47.170&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;30.017&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.000588&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;All ExP&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;56.264&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;34.661&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.000367&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;All NExP&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;49.533&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;29.116&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.000649&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;All observers&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;53.219&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;32.280&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.000464&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Consensus citizen (average)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;66.923&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;19.044&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.0024&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Consensus citizen (max)&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;76.855&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;23.713&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.0012&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16928</offset><text>	Sum sq.	F	p value	 	A single ExP	47.816	43.775	0.000167	 	A single NExP	47.170	30.017	0.000588	 	All ExP	56.264	34.661	0.000367	 	All NExP	49.533	29.116	0.000649	 	All observers	53.219	32.280	0.000464	 	Consensus citizen (average)	66.923	19.044	0.0024	 	Consensus citizen (max)	76.855	23.713	0.0012	 	</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>17231</offset><text>Only time*cultivar interaction is shown corresponding to the factor of interest (longitudinal trend). Results with ‘All’ and consensus citizen average (or max) across per-plant observations</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17425</offset><text>Experienced (with image-based plant phenotyping) and non-experienced observers were recruited to participate in this observational study. They were provided with a description of the purpose of the study, and were asked to consent to participate in the study. They were shown a guide and an introduction to the annotation tool to ensure a common baseline. Specifically, we showed them examples of good plant annotations, where they were asked to mark leaves at the center of the leaf blade (or the most visible area in case of severe overlap). Each observer was assigned two or more of the datasets to rate and count leaves. The order of the datasets shown was randomized and never of the same orientation (e.g. if one was shown A the next dataset would be C or D) to minimize effects of memory. To further reduce memory effects a 10 min break was enforced between annotation tasks. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18310</offset><text>Some observers were asked to rate the images also without the use of the tool but recorded leaf counts in a spreadsheet after shown an image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18452</offset><text>Time to complete each set was recorded in addition to the times recorded by the tool itself (see annotation timing above).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18575</offset><text>Citizen-powered study</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>18597</offset><text>A simulated citizen-powered experiment. p values corresponding to an ANOVA test randomizing the number of observations available per each plant at a specific time point</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;
&lt;italic&gt;K&lt;/italic&gt;
&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Min&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Max&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Mean&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Std&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Kurtosis&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Any&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00003&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00819&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00124&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00113&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;10.34&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Any&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;2&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00002&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00729&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00120&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00112&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;8.98&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Any&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00010&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00235&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00061&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00032&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;6.49&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00000&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00726&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00102&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00103&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;9.58&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;2&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00004&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00306&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00057&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00040&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;9.29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00008&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00150&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00047&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00021&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;5.35&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;NExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00008&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00378&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00100&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00065&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;5.71&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;NExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;2&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00023&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00174&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00078&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00028&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3.49&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;NExP only&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00033&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00124&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00069&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.00015&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3.19&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>18766</offset><text>	K	Min	Max	Mean	Std	Kurtosis	 	Any	1	0.00003	0.00819	0.00124	0.00113	10.34	 	Any	2	0.00002	0.00729	0.00120	0.00112	8.98	 	Any	3	0.00010	0.00235	0.00061	0.00032	6.49	 	ExP only	1	0.00000	0.00726	0.00102	0.00103	9.58	 	ExP only	2	0.00004	0.00306	0.00057	0.00040	9.29	 	ExP only	3	0.00008	0.00150	0.00047	0.00021	5.35	 	NExP only	1	0.00008	0.00378	0.00100	0.00065	5.71	 	NExP only	2	0.00023	0.00174	0.00078	0.00028	3.49	 	NExP only	3	0.00033	0.00124	0.00069	0.00015	3.19	 	</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>19237</offset><text>Process is repeated sampling from any of the observers (i.e. the sampling may contain a mix of experienced and non-experienced observers) or only from experienced (ExP) or non-experienced (i.e. NExP) ones</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19442</offset><text>The A data (RPi) were included as part of a larger citizen-powered study (“Leaf Targeting”, available at https://www.zooniverse.org/projects/venchen/leaf-targeting) built on Zooniverse (https://www.zooniverse.org/). Using the Zooniverse application programming interface (API), an annotation work-flow was designed that showed an image to a user via a web browser. The users (random visitors) were asked to view a tutorial on how to annotate leaves. The task essentially involved placing a dot annotation on each leaf, thus retaining the characteristics of the interface used in the fully controlled study described previously. Users could as well zoom in and out and delete dot annotations. Users were also asked to answer a question after each plant was annotated as to their confidence in having annotated all leaves (encoded as Yes: 3, Not sure: 2, Missed leaves: 1). An example of an annotated image along with the interface and questions seen by the users are shown in Fig. 1B. We note that the users have the option to log in to the platform and also to comment about images where they can discuss issues related to the image or the task in general. We set the work-flow to repeat the same image 8 times after at least all images have been annotated 3 times; images for annotation are shown at random and thus annotations can be treated as IID and the same image is not rated by the same user. The system exports complete information for each annotated image such as image ID, user name (or unique IP), time, the locations and number of dots, and the response to the confidence question. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21043</offset><text>Statistics and evaluation metrics</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21077</offset><text>Difference in count (DiC) mean and standard deviation of difference between  and . [Zero is best.]</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21176</offset><text>Absolute difference in count (|DiC|) mean and standard deviation of absolute difference between  and . [Zero is best.]</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21295</offset><text>Mean squared error (MSE) squared difference between  and . [Zero is best.]</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21370</offset><text>Coefficient of determination (R2) the proportion of the variance in  that is predictable from . [One is best.]</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21481</offset><text>Krippendorff’s alpha (alpha) a chance-adjusted index of inter-observer agreement. We used the mALPHAK implementation in Matlab treating counts as a ratio scale variable comparing  and . [One is best.]</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21684</offset><text>A variety of descriptive and summary statistics as well as several statistical methods were used to evaluate agreement in the controlled experiment. We note that in the case of discrete counts and heavily zero inflated differences (when comparing counts between observers) many of the common statistics and visualization methods can lead to misinterpretations. Thus, between a reference observer  and one of the other observers , we adopted:The first four metrics were adopted since they have been used to compare counting algorithms on the basis of challenge data.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>22250</offset><text>Algorithmic leaf counting results obtained using the method in </text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Algorithm versus annotator&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Algorithm versus annotator&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Annotator versus reference&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Training error&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Testing error&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Inter-observer error&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;DiC ↓&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00 (1.07)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;−  0.04 (1.31)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.21 (0.75)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;|DiC| ↓&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.61 (0.88)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.88 (0.96)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.46 (0.62)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MSE ↓&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.163&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.700&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.600&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;R&lt;sup&gt;2&lt;/sup&gt;↑&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.933&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.895&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.964&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>22314</offset><text>	Algorithm versus annotator	Algorithm versus annotator	Annotator versus reference	 	Training error	Testing error	Inter-observer error	 	DiC ↓	0.00 (1.07)	−  0.04 (1.31)	0.21 (0.75)	 	|DiC| ↓	0.61 (0.88)	0.88 (0.96)	0.46 (0.62)	 	MSE ↓	1.163	1.700	0.600	 	R2↑	0.933	0.895	0.964	 	</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>22606</offset><text>Four metrics are reported. We first compare between the algorithm and the 728 images in the training set (ie. how well the algorithm learns). Then we compare how well the algorithm predicts counts on a testing set of 130 images (also used in this study) comparing the algorithm with the counts of the annotator (that also was involved in deriving annotations for the training set). Lastly we compare the annotator (the data of which we used to train the algorithm and was not involved in this study) with the reference observer used throughout in this study</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23164</offset><text>To visualize agreement between pairs of observers we used a modified version of the Bland–Altman (BA) plot in conjunction with the histogram of count differences. For the BA plot, we plot color labelled squares with square color varying according to how many points agree on the same coordinates. This is necessary since we observed that in scatter plots of discrete quantities, points will overlap misrepresenting the true distribution of the data. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23617</offset><text>Finally, while evaluating agreement is interesting on its own, we also considered an application-driven measure of agreement by estimating a mixed effect repeated measure two way ANOVA on count data as employed in for the two cultivars. By this, essentially we test whether any observable differences exist in between cultivar longitudinal trends obtaining average counts using a different set of observers. We treated subject ID (i.e. the replicate) as a random effect whilst all other as fixed effects. To not over-inflate degrees of freedom we treated time as a continuous predictor. Of particular interest is the interaction term between time and cultivar (cultivar*time hereafter), since this is the term that tests longitudinal differences between the cultivars.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>24386</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24394</offset><text>Intra-observer variability</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24421</offset><text>We assessed this via a second reading from the same observer using the tool. In Fig. 2A we plot histograms and Bland–Altman (BA) plots for two observers on the datasets A, C (ie. same as A but with geometric changes). Considering also the corresponding rows in Table 1, we can see that intra-observer agreement overall is excellent, with the NExP observer showing slightly higher variation (higher standard deviation) and decreased agreement (alpha) compared to ExP.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24893</offset><text>Variability between tool and spreadsheet based counting</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24949</offset><text>To assess whether the tool contributes to lower variability in intra-observer measurements, in Fig. 2B we show histograms and BA plots comparing counts obtained via the tool or spreadsheet measurements using the same, ExP or NExP, observer, shown respectively left and right. Note that deviation is higher when compared to the intra-observer findings using the tool alone (previous paragraph). It appears that the tool has less effect (smaller deviation) to an ExP, whereas it seems to help reduce variability for NExP. This adheres to comments of NExP observers stating that when leaf numbers are high, and plant structure appears complex, it is hard to keep counting the leaves manually without visual reference resulting in frequent restarts of counting (even 3 times). We note that the tool retains visible the placed dots to precisely help visual memory. The same conclusions can be drawn from the statistical numbers shown in Table 1, however with slightly decreased agreement in the NExP observer.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25957</offset><text>All the results presented in the following refer to tool based annotations.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26033</offset><text>Inter-observer variability</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26060</offset><text>To assess inter-observer variability we selected one experienced observer as a reference and compared against other ExP and NExP observers (a total of 9), which allows us to be concise (e.g. by showing representative comparison pairs instead of all possible combinations). Although this approach does not take into account observation error of the reference observer, the chosen observer had the smallest intra-observer variation (see entry marked with a ‘[Reference observer]a’ in Table 1.)</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26557</offset><text>Figure 3A and B visualize inter-observer agreement in the case of RPi and Canon, whereas Table 1 offers statistics. Overall we see that agreement is excellent independent of experience. At times experienced observers appear to disagree more particularly when resolution is higher. This is likely attributed to how experienced observers appreciate new leaf emergence and particularly if they are trained to see it or not.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26981</offset><text>Influence of resolution on intra-observer variability</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27035</offset><text>This variation among experienced observers becomes also evident when comparing the same observer and their annotations when resolution alters. The ExP observer (who is also the reference) tends to underestimate when resolution is lower. Whereas the NExP observer shows less under-estimation and higher agreement. It appears that NExP observers may miss young leaves independent of resolution (as they are not trained to see them) whereas the ExP observer misses them only on lower resolution.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>27528</offset><text>Influence of observer variation in longitudinal analysis</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27585</offset><text>In Fig. 4 we show per-day average leaf count for each cultivar (i.e. averaging across replicates) when using annotations from different sets (and numbers) of observers for the RPi data. The top row refers to using a single ExP or NExP observer i.e. averaging within the population of each cultivar (panel A); whereas the middle row refers to a group of observers within their expertise, averaging first across observer annotations, and then across replicates (panel B). Panel C is similar to B but averages across all observers. The plots show average leaf count (within the population of each cultivar) and 1 standard deviation (shading) from the mean of the population. It is evident that given the effect size of the chosen cultivars, trends of average leaf count are expected even when using a single observer, albeit the ExP observer shows less variation. When combining observations across a group of observers trends still show even clearer and one may even argue that averaging across NExP tends to perform even better than a single NExP observer (compare panel B and A).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28667</offset><text>In Table 2 the results of the statistical ANOVA experiment are shown focusing only on the interaction term of interest (time*cultivar). We can see that in all cases the interaction is significant (p ≤ 0.05) confirming the visual findings of Fig. 4 and analyzed above. Note that although the smoothing effect is evident in the plots, when using more observers slightly increases the p value (decrease of the F score). This could be attributed to the fact that when using a single observer their behaviour (e.g. tendency to under-estimate) may be considered a fixed effect which is captured in the intercept, whereas using a population of observers (even of the same expertise) this may not be captured by the specification of the ANOVA model.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>29417</offset><text>Time results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29430</offset><text>Overall, we find that on average observers using the tool spent 48 min to annotate 130 plants for an average of 21 s per plant. Observers using the spreadsheet took on average 42 min. These findings were obtained by recording start and stop times of 5 observers in a controlled setting and provide aggregate timing information across an annotation task.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29787</offset><text>On the other hand, by keeping track of time when annotations were placed using the tool, more precise per leaf timing annotations were obtained (see “Methods”). Since this approach assumes that observers continuously label leaves, which may not hold if they take a break whilst labeling a plant, times greater than 200 s were considered outliers and were excluded from analysis.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30171</offset><text>Recording the time required to annotate a plant, we found that there is no statistical difference between experienced and non-experienced observers (p value 0.245). On average, within the 21 s required to annotate a plant, only 8.5s were used to actually complete the task. (In general, an annotator takes 1.10 ± 2.15 s per-leaf). We argue that annotators use the remaining time to assess how to annotate a plant and evaluate the quality of their own work. In fact, several annotators were double-checking their work after they finished to annotate all the leaves. We found this by analysing the timestamps recorded for each annotation. For some plants, the last annotation was placed after 40 min from the first one on the same image. Moreover, we also found no correlation between errors and time. Specifically, comparing the leaf count with the reference expert, the DiC is not affected over time.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>31078</offset><text>Simulating a citizen-powered study</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31113</offset><text>Given the number of available observers on RPi (9 observers) and the a priori knowledge of their experience, it is of interest to explore: (i) the effects of using multiple observers for phenotyping by reducing their load (i.e. not having to annotate all images but a fraction of them) and consequently; (ii) the potential of using citizen-powered research platforms for phenotyping (where experience could be an unknown factor).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31543</offset><text>At first instance we wanted to simulate how many annotations we need to still maintain the phenotyping findings of the previous section: i.e. that there is an effect between time and genotype in the ANOVA setup. For this purpose we set-up a Monte Carlo simulation study that at each trial randomly draws a sampling matrix with K observations per time point. For example, for two observations per time point, this matrix has K = 2 ones per row (a row is an observation) for a total of 260 ones (the rest being zeros). The placement of ones select from which annotator an observation is obtained for this time point. For more than 1 annotation per time point (i.e. plant image), annotations across observers are averaged.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32265</offset><text>We varied K = 1, 2, 3 drawing from all available annotators (n = 9) or only from experienced (n = 5) or non-experienced observers (n = 4) to inspect the influence of mixing experience in annotations in the overall result. At each trial we run the ANOVA experiment and record the p value of the interaction term (time*cultivar). We draw 500 trials for each variation of setup (K and the observer groups) and finally obtain summary statistics of the distribution of the p values among the 500 trials, namely minimum, maximum, mean, standard deviation, and kurtosis (a notion of symmetry and normality).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32874</offset><text>Table 3 reports the findings of this study. Overall we see that at no point, independently of the number of annotations used or the experience of observers, the p value is not statistically significant (the max p value is always below the significance threshold). This is telling since even 1 annotation is enough for the effect size observed in these cultivars. With 1 annotation per time point, with 9 observers this would have an effect of reducing annotation effort per-observer to 11.1% of the dataset (i.e. 14–15 plants per each observer). As expected the more observers the better; but sampling only from experienced observers did not necessarily outperform sampling only from non-experienced ones. Given the leptokurtic characteristic of these distributions (high kurtosis), the distributions are highly peaked around the mean with values concentrating around these. Overall, while the max indicates the worst expected result, results around the mean are to be expected as more typical.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>33872</offset><text>Results from the citizen-powered study</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33911</offset><text>The study was launched on May 1st 2017, and by June 1st, approximately 5000 user annotations were available on a dataset of 1248 images, including the 130 RPi images used in this paper, with each image having at least 3 user annotations. Data were extracted from the Zooniverse database and a similar statistical analysis as to the one outlined above was carried out.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34279</offset><text>Of the 5000 annotations 4 Zooniverse users were responsible for annotating close to 10% of the data, as we can see in Fig. 5A. Most users contribute few annotations (long tail to the right), and not surprisingly most of the users are logged in (shown as black stem line without a marker in Fig. 5A), which implies that they are frequent contributors to the platform.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34648</offset><text>Of particular interest is to explore if the self-reported confidence (answering the question on whether they believe they have annotated all leaves) relates to the spread of leaf counts among users for each plant. Figure 5B shows a two dimensional histogram of the per-plant standard deviation of the reported leaf count among the users with none referring to 0 standard deviation (i.e. annotations agree fully) and the average confidence (averaging the confidence question) for each plant of the 130 used in this study. An average of 3 shows high confidence (y-axis) versus an average of 1 low confidence (y-axis). Color encodes probability of occurrence. Users tend to agree with each other and their self reporting of confidence appears to be consistent with their spread in counting leaves, since the upper left quadrant sums to approximately 70% of occurrences.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35516</offset><text>We then estimated a consensus citizen by averaging counts across the annotated counts for each plant. We compared this consensus against the reference observer (from our controlled study) and a random single selection of counts, which can be seen as selecting one count per plant out of the 3 citizen provided counts (shorthanded as sing. random in Table 1). The results of this analysis are shown in Fig. 5C and D respectively. We see what there is some variability among the reference observer and consensus citizen (Fig. 5C), with the latter underestimating counts (see also related entries of DiC in Table 1). On the other hand variability appears to be smaller within citizens (c.f. Fig. 5D and entries in Table 1).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36243</offset><text>Admittedly of most interest is to see if plain citizens can be used for actual phenotyping. We use the counts of the consensus citizen and plot as previously average (and one standard deviation) per cultivar counts as a function of time in Fig. 4D. We can see that this plot closely resembles the others and particularly the one of using only non-experienced observers in our controlled study. Equally the corresponding ANOVA experiment (last row in Table 2) shows exactly the same findings since using the consensus citizen counts yields a p value still statistically significant, albeit larger compared to the one of the controlled experiment. However, a key difference between the two exists: in our controlled study all observers rated all images, so perhaps fixed effects of each observer may be captured in the intercept. Instead in the citizen experiment all counts come from a large pool of observers. In fact, when we compare the p value of the consensus citizen (p = 0.0014) it is within the min-max bounds we find in our simulated study reported in Table 3.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37318</offset><text>Post-hoc, i.e. knowing that citizens under-estimate, under-estimation reaches 0 if we use the maximum across annotated counts (instead of average), and several other metrics improve including the p value of the ANOVA. In Tables 1 and 2 this is shown as consensus (max).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>37589</offset><text>Variability between algorithmic leaf count and experts</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37644</offset><text>In addition to manual counting, we also tested a well-known leaf counting algorithm to assess whether algorithm error is within (or outside) human variation.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37802</offset><text>For this experiment, we used the plant images in, with annotations performed by experts not involved in other aspects of this study. Overall, this dataset contains 1248 individual images of plants, taken from five different cultivars (col-0, pgm, ein2.1, ctr, and adh1). Specifically, images of ctr, adh1, and ein2.1 cultivars were used as training set (728 images in total), whereas the images of pgm and col-0 cultivars, which were also used in this study, were employed as testing set (130 images in total). From the training images, we learned a plant descriptor that derives image features and the projected leaf area to learn a non-linear model to predict the leaf count. It is noteworthy that the training set contains cultivars not included in the testing set, which makes this learning protocol the most stringent condition as the algorithm has never seen the mutants. After the model was trained, we calculated the evaluation metrics in in the training (728 images) and testing sets (130 images). In addition, since the expert observer that labeled the images used to train the algorithm was not part of this study, we also computed the disagreement between this expert and the reference observer used throughout this study.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39037</offset><text>As shown in Table 4, the algorithm learns well (agreement between algorithm and annotator on the 728 training images the algorithm was trained on). When predicting counts on the 130 test images, the algorithm performs slightly worse when compared with the same annotator involved in labeling the training set (middle column). However, we can see that the algorithm is within inter-observer variability which compares two expert annotators (last column in Table 4). While on average the algorithm predicts the correct leaf count on some images (mean close to zero) it appears that it is over- or under-estimating counts on some, which explains the high standard deviation and high MSE. We note that here the algorithm carries two sources of variation (error): one of the annotator and one of the learning process itself. The latter can be minimized, but the former unfortunately is harder to do so unless a mixture of annotators is used.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>39976</offset><text>Discussion and conclusion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40002</offset><text>In the following, we discuss the findings of our study, where we investigated observer variability for an annotation task being deliberately chosen to be simple to understand and perform for human annotators. Clearly, not all of these findings generalize to all (possible) human annotation tasks. Findings on ‘negative effects’, i.e. factors increasing annotator variability, like fatigue, lack of suitable annotation tools etc. can be expected to be also present for harder annotation tasks being more challenging for humans. They are expected to generalize well. However, ‘positive effects’, e.g. observed discriminative power of human annotations for the investigated task, cannot as easily be generalized to other, especially more difficult tasks.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40762</offset><text>In this study, we showed that intra-observer variability remains low with experienced observers, but non-experienced ones tend to vary more in their second repeat reading using a visualization tool. Our annotation tool helps to retain mental memory and to reduce fatigue overall lessening the potential for errors when plants become larger and have more leaves. At the same time we showed that higher image resolution helps, but not always with the same effect: higher resolution aids the experienced user to find more of the smaller leaves, but non-experienced ones missed them more often independently of resolution. Inter-observer variability is not significantly greater than intra-observer variability. Overall observers tend to be within plus/minus one leaf almost 80% of the time.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>41550</offset><text>This agreement seems appealing but it might be random in nature and we explored if it affects the use of observers in actually identifying group differences in longitudinal counts. Repeat statistical tests showed that, when we use one or more experienced or non-experienced observers, we still come to the same statistical conclusion using an ANOVA test on the same longitudinal cultivar comparison: we find, as expected, differences in trends between col-0 and pgm as reported previously on the same data. Whether we use only experienced or non-experienced observers has minimal effects on the statistical inference of the test.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42180</offset><text>Encouraging are the investigations using simulated and real data from citizen-powered experiments. In real experiments we cannot ensure the composition (in expertise) of the participating users and neither we can assume that the same user will annotate all the data. However, our analysis on simulated data (where we can control the composition) showed that having even 1 annotation per plant can be sufficient to arrive to the same statistical conclusion (differences in cultivar trends) but of course having more is better, reducing variation. These findings held also in the real citizen-powered experiment based on the Zooniverse platform. Leaf counting based on algorithms while showing promise and progress does not yet meet human performance necessitating further investigation in the area; thankfully, collation studies and challenges (e.g. the counting challenge of the CVPPP workshop series https://www.plant-phenotyping.org/CVPPP2017-challenge) on open data will help advance the state-of-the-art.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>43189</offset><text>This paper points to several potential areas for further research. Variability will be present in annotations and we can either obtain a better consensus, learn to ignore this variability, or alter the annotation task to minimize variability. In this study consensus was obtained through averaging across annotations and treating time points independently, but alternative mechanisms can be used to establish more consistent longitudinal counts. For example, one can adopt several other consensus approaches that are data-agnostic or if we assume that leaves always emerge or remain the same in succession of images but cannot disappear, consensus can be derived using a dynamic filtering approach. Alternatively, machine learning algorithms can be used to learn directly from such repeated and imprecise (in machine learning speak: noisy) annotations potentially also obtaining consensus estimates which should also help eliminate observer bias. However, in machine learning much effort has been devoted to noisy annotations in classification tasks  but in regression is a yet unexplored area. A more radical approach, is to alter the design of the annotation task completely: for example, users can be shown pairs of images and can be asked to identify only ‘new’ leaves (if any at all). Irrespective of the design of the annotation task, minimizing the amount of data requiring annotation by selectively displaying (to the observers/annotators) only images that do need annotation is always desirable. This has strong links to active (machine) learning which displays images that are the most informative from a machine learning perspective. Integrating this may be possible within a controlled lab annotation platform (as for example with the CellProfiler software3) but doing so in Zooniverse is not straightforward as images used in the work-flow cannot be altered on the fly and a customized platform would be required.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>45121</offset><text>Considering all these findings we can conclusively argue that while there is some variability among observers it is minimal when evaluating quantitative traits like counting objects, even of very different sizes. For the group (cultivar) effect sizes observed here this variability had no effect in statistical inference. At the same time common citizens, empowered by easy to use platforms, can greatly assist the effort of annotating images; at least, when the overall task is broken down in elementary sub-tasks generally doable even by non-experts without detailed explanations. Then common citizens can be used to provide annotations and drive phenotypic analysis. Such annotations help to develop and evaluate automated algorithms and allow to train machine learning-based solutions. Using such platforms a higher annotation throughput can be met than perhaps available locally in a lab, reducing significantly annotation effort.4 It is time to consider how we can motivate the participation of citizens and design annotation tasks that can provide data of sufficient quality for other phenotyping tasks. This will have not only an effect on phenotyping but also on introducing this societally important problem to the broad public.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>46360</offset><text>This more closely emulates how experts rate data with visual scales in the field since there is an inherent assumption that previous ratings and images of the scene are not used as reference.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>46552</offset><text>More information at http://phenotiki.com.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>46594</offset><text>This is planned to be made available in Phenotiki in mid 2018 for the counting module.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>46681</offset><text>We emphasize that Zooniverse is not an annotation platform per se and any workflow presented should have a strong ethical and reward mechanism to be accepted as a Zooniverse project. For tasks with a demanding rate and purely annotation objective gamification and crowdsourcing should be selected.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>46979</offset><text>References</text></passage><passage><infon key="fpage">578</infon><infon key="issue">3</infon><infon key="lpage">586</infon><infon key="name_0">surname:Weight;given-names:C</infon><infon key="name_1">surname:Parnham;given-names:D</infon><infon key="name_2">surname:Waites;given-names:R</infon><infon key="pub-id_doi">10.1111/j.1365-313X.2007.03330.x</infon><infon key="pub-id_pmid">18028263</infon><infon key="section_type">REF</infon><infon key="source">Plant J</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2008</infon><offset>46990</offset><text>LeafAnalyser: a computational method for rapid and large-scale analyses of leaf shape variation</text></passage><passage><infon key="fpage">376</infon><infon key="lpage">394</infon><infon key="name_0">surname:An;given-names:N</infon><infon key="name_1">surname:Palmer;given-names:CM</infon><infon key="name_2">surname:Baker;given-names:RL</infon><infon key="name_3">surname:Markelz;given-names:RJC</infon><infon key="name_4">surname:Ta;given-names:J</infon><infon key="name_5">surname:Covington;given-names:MF</infon><infon key="name_6">surname:Maloof;given-names:JN</infon><infon key="name_7">surname:Welch;given-names:SM</infon><infon key="name_8">surname:Weinig;given-names:C</infon><infon key="pub-id_doi">10.1016/j.compag.2016.04.002</infon><infon key="section_type">REF</infon><infon key="source">Comput Electron Agric</infon><infon key="type">ref</infon><infon key="volume">127</infon><infon key="year">2016</infon><offset>47086</offset><text>Plant high-throughput phenotyping using photogrammetry and imaging techniques to measure leaf length and rosette area</text></passage><passage><infon key="fpage">267</infon><infon key="issue">1</infon><infon key="lpage">291</infon><infon key="name_0">surname:Fiorani;given-names:F</infon><infon key="name_1">surname:Schurr;given-names:U</infon><infon key="pub-id_doi">10.1146/annurev-arplant-050312-120137</infon><infon key="pub-id_pmid">23451789</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Plant Biol</infon><infon key="type">ref</infon><infon key="volume">64</infon><infon key="year">2013</infon><offset>47204</offset><text>Future scenarios for plant phenotyping</text></passage><passage><infon key="fpage">635</infon><infon key="issue">12</infon><infon key="lpage">644</infon><infon key="name_0">surname:Furbank;given-names:RT</infon><infon key="name_1">surname:Tester;given-names:M</infon><infon key="pub-id_doi">10.1016/j.tplants.2011.09.005</infon><infon key="pub-id_pmid">22074787</infon><infon key="section_type">REF</infon><infon key="source">Trends Plant Sci</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2011</infon><offset>47243</offset><text>Phenomics: technologies to relieve the phenotyping bottleneck</text></passage><passage><infon key="fpage">93</infon><infon key="lpage">99</infon><infon key="name_0">surname:Fahlgren;given-names:N</infon><infon key="name_1">surname:Gehan;given-names:MA</infon><infon key="name_2">surname:Baxter;given-names:I</infon><infon key="pub-id_doi">10.1016/j.pbi.2015.02.006</infon><infon key="pub-id_pmid">25733069</infon><infon key="section_type">REF</infon><infon key="source">Curr Opin Plant Biol</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2015</infon><offset>47305</offset><text>Lights, camera, action: high-throughput plant phenotyping is ready for a close-up</text></passage><passage><infon key="fpage">20078</infon><infon key="issue">11</infon><infon key="lpage">20111</infon><infon key="name_0">surname:Li;given-names:L</infon><infon key="name_1">surname:Zhang;given-names:Q</infon><infon key="name_2">surname:Huang;given-names:D</infon><infon key="pub-id_doi">10.3390/s141120078</infon><infon key="pub-id_pmid">25347588</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2014</infon><offset>47387</offset><text>A review of imaging techniques for plant phenotyping</text></passage><passage><infon key="fpage">618</infon><infon key="issue">5</infon><infon key="name_0">surname:Vazquez-Arellano;given-names:M</infon><infon key="name_1">surname:Griepentrog;given-names:HW</infon><infon key="name_2">surname:Reiser;given-names:D</infon><infon key="name_3">surname:Paraforos;given-names:DS</infon><infon key="pub-id_doi">10.3390/s16050618</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>47440</offset><text>3-d imaging systems for agricultural applications: a review</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47500</offset><text>Rousseau D, Dee H, Pridmore T. Imaging methods for phenotyping of plant traits. In: Kumar J, Pratap A, Kumar S, editors. Phenomics in crop plants: trends, options and limitations. Berlin: Springer; 2015. p. 61–74.</text></passage><passage><infon key="fpage">126</infon><infon key="issue">4</infon><infon key="lpage">131</infon><infon key="name_0">surname:Minervini;given-names:M</infon><infon key="name_1">surname:Scharr;given-names:H</infon><infon key="name_2">surname:Tsaftaris;given-names:SA</infon><infon key="pub-id_doi">10.1109/MSP.2015.2405111</infon><infon key="section_type">REF</infon><infon key="source">IEEE Signal Process Mag</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2015</infon><offset>47716</offset><text>Image analysis: the new bottleneck in plant phenotyping</text></passage><passage><infon key="fpage">38</infon><infon key="issue">1</infon><infon key="name_0">surname:Lobet;given-names:G</infon><infon key="name_1">surname:Draye;given-names:X</infon><infon key="name_2">surname:Périlleux;given-names:C</infon><infon key="pub-id_doi">10.1186/1746-4811-9-38</infon><infon key="pub-id_pmid">24107223</infon><infon key="section_type">REF</infon><infon key="source">Plant Methods</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2013</infon><offset>47772</offset><text>An online database for plant image analysis software tools</text></passage><passage><infon key="fpage">80</infon><infon key="lpage">89</infon><infon key="name_0">surname:Minervini;given-names:M</infon><infon key="name_1">surname:Fischbach;given-names:A</infon><infon key="name_2">surname:Scharr;given-names:H</infon><infon key="name_3">surname:Tsaftaris;given-names:SA</infon><infon key="pub-id_doi">10.1016/j.patrec.2015.10.013</infon><infon key="section_type">REF</infon><infon key="source">Pattern Recognit Lett</infon><infon key="type">ref</infon><infon key="volume">81</infon><infon key="year">2016</infon><offset>47831</offset><text>Finely-grained annotated datasets for image-based plant phenotyping</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47899</offset><text>Bell J, Dee H. Aberystwyth leaf evaluation dataset. 2016.</text></passage><passage><infon key="fpage">735</infon><infon key="issue">5</infon><infon key="lpage">749</infon><infon key="name_0">surname:Cruz;given-names:JA</infon><infon key="name_1">surname:Yin;given-names:X</infon><infon key="name_2">surname:Liu;given-names:X</infon><infon key="name_3">surname:Imran;given-names:SM</infon><infon key="name_4">surname:Morris;given-names:DD</infon><infon key="name_5">surname:Kramer;given-names:DM</infon><infon key="name_6">surname:Chen;given-names:J</infon><infon key="pub-id_doi">10.1007/s00138-015-0734-6</infon><infon key="section_type">REF</infon><infon key="source">Mach Vis Appl</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2016</infon><offset>47957</offset><text>Multi-modality imagery database for plant phenotyping</text></passage><passage><infon key="fpage">585</infon><infon key="issue">4</infon><infon key="lpage">606</infon><infon key="name_0">surname:Scharr;given-names:H</infon><infon key="name_1">surname:Minervini;given-names:M</infon><infon key="name_10">surname:Yin;given-names:X</infon><infon key="name_11">surname:Tsaftaris;given-names:SA</infon><infon key="name_2">surname:French;given-names:AP</infon><infon key="name_3">surname:Klukas;given-names:C</infon><infon key="name_4">surname:Kramer;given-names:DM</infon><infon key="name_5">surname:Liu;given-names:X</infon><infon key="name_6">surname:Luengo;given-names:I</infon><infon key="name_7">surname:Pape;given-names:J-M</infon><infon key="name_8">surname:Polder;given-names:G</infon><infon key="name_9">surname:Vukadinovic;given-names:D</infon><infon key="pub-id_doi">10.1007/s00138-015-0737-3</infon><infon key="section_type">REF</infon><infon key="source">Mach Vis Appl</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2016</infon><offset>48011</offset><text>Leaf segmentation in plant phenotyping: a collation study</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48069</offset><text>Giuffrida MV, Minervini M, Tsaftaris SA. Learning to count leaves in rosette plants. In: Scharr H, Tsaftaris SA, Pridmore T, editors. Proceedings of the computer vision problems in plant phenotyping (CVPPP) workshop. 2015. p. 1–1113.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48305</offset><text>Pape J-M, Klukas C. Utilizing machine learning approaches to improve prediction of leaf counts and individual leaf segmentation of rosette plants. In: Scharr H, Pridmore T, Tsaftaris SA, editors. Proceedings of the computer vision problems in plant phenotyping workshop. 2015.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48582</offset><text>Romera-Paredes B, Torr PHS. Recurrent Instance Segmentation. In: Proceedings of the European conference on computer vision. 2016. p. 312–329.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48726</offset><text>Ren M, Zemel RS. End-to-end instance segmentation and counting with recurrent attention. In: Proceedings of computer vision and pattern recognition conference. 2017.</text></passage><passage><infon key="fpage">989</infon><infon key="issue">12</infon><infon key="lpage">991</infon><infon key="name_0">surname:Tsaftaris;given-names:SA</infon><infon key="name_1">surname:Minervini;given-names:M</infon><infon key="name_2">surname:Scharr;given-names:H</infon><infon key="pub-id_doi">10.1016/j.tplants.2016.10.002</infon><infon key="pub-id_pmid">27810146</infon><infon key="section_type">REF</infon><infon key="source">Trends Plant Sci</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2016</infon><offset>48892</offset><text>Machine learning for plant phenotyping needs image processing</text></passage><passage><infon key="fpage">110</infon><infon key="issue">2</infon><infon key="lpage">124</infon><infon key="name_0">surname:Singh;given-names:A</infon><infon key="name_1">surname:Ganapathysubramanian;given-names:B</infon><infon key="name_2">surname:Singh;given-names:AK</infon><infon key="name_3">surname:Sarkar;given-names:S</infon><infon key="pub-id_doi">10.1016/j.tplants.2015.10.015</infon><infon key="pub-id_pmid">26651918</infon><infon key="section_type">REF</infon><infon key="source">Trends Plant Sci</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2016</infon><offset>48954</offset><text>Machine learning for high-throughput stress phenotyping in plants</text></passage><passage><infon key="fpage">204</infon><infon key="issue">1</infon><infon key="lpage">216</infon><infon key="name_0">surname:Minervini;given-names:M</infon><infon key="name_1">surname:Giuffrida;given-names:MV</infon><infon key="name_2">surname:Perata;given-names:P</infon><infon key="name_3">surname:Tsaftaris;given-names:SA</infon><infon key="pub-id_doi">10.1111/tpj.13472</infon><infon key="pub-id_pmid">28066963</infon><infon key="section_type">REF</infon><infon key="source">Plant J</infon><infon key="type">ref</infon><infon key="volume">90</infon><infon key="year">2017</infon><offset>49020</offset><text>Phenotiki: an open software and hardware platform for affordable and easy image-based phenotyping of rosette-shaped plants</text></passage><passage><infon key="fpage">494</infon><infon key="issue">4</infon><infon key="lpage">502</infon><infon key="name_0">surname:Kellgren;given-names:JH</infon><infon key="name_1">surname:Lawrence;given-names:JS</infon><infon key="pub-id_doi">10.1136/ard.16.4.494</infon><infon key="pub-id_pmid">13498604</infon><infon key="section_type">REF</infon><infon key="source">Ann Rheum Dis</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">1957</infon><offset>49143</offset><text>Radiological assessment of osteo-arthrosis</text></passage><passage><infon key="fpage">15</infon><infon key="issue">1</infon><infon key="lpage">26</infon><infon key="name_0">surname:Hartung;given-names:K</infon><infon key="name_1">surname:Piepho;given-names:H-P</infon><infon key="pub-id_doi">10.1007/s10681-006-9296-z</infon><infon key="section_type">REF</infon><infon key="source">Euphytica</infon><infon key="type">ref</infon><infon key="volume">155</infon><infon key="year">2007</infon><offset>49186</offset><text>Are ordinal rating scales better than percent ratings? a statistical and “psychological” view</text></passage><passage><infon key="fpage">290</infon><infon key="issue">2</infon><infon key="lpage">298</infon><infon key="name_0">surname:Poland;given-names:JA</infon><infon key="name_1">surname:Nelson;given-names:RJ</infon><infon key="pub-id_doi">10.1094/PHYTO-03-10-0087</infon><infon key="section_type">REF</infon><infon key="source">Phytopathology</infon><infon key="type">ref</infon><infon key="volume">101</infon><infon key="year">2010</infon><offset>49284</offset><text>In the eye of the beholder: the effect of rater variability and different rating scales on QTL mapping</text></passage><passage><infon key="fpage">154</infon><infon key="issue">7312</infon><infon key="lpage">155</infon><infon key="name_0">surname:MacLeod;given-names:N</infon><infon key="name_1">surname:Benfield;given-names:M</infon><infon key="name_2">surname:Culverhouse;given-names:P</infon><infon key="pub-id_doi">10.1038/467154a</infon><infon key="pub-id_pmid">20829777</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">467</infon><infon key="year">2010</infon><offset>49387</offset><text>Time to automate identification</text></passage><passage><infon key="fpage">806</infon><infon key="issue">8</infon><infon key="name_0">surname:Nutter;given-names:FW;suffix:Jr</infon><infon key="pub-id_doi">10.1094/Phyto-83-806</infon><infon key="section_type">REF</infon><infon key="source">Phytopathology</infon><infon key="type">ref</infon><infon key="volume">83</infon><infon key="year">1993</infon><offset>49419</offset><text>Assessing the accuracy, intra-rater repeatability, and inter-rater reliability of disease assessment systems</text></passage><passage><infon key="fpage">530</infon><infon key="issue">4</infon><infon key="lpage">541</infon><infon key="name_0">surname:Bock;given-names:CH</infon><infon key="name_1">surname:Parker;given-names:PE</infon><infon key="name_2">surname:Cook;given-names:AZ</infon><infon key="name_3">surname:Gottwald;given-names:TR</infon><infon key="pub-id_doi">10.1094/PDIS-92-4-0530</infon><infon key="section_type">REF</infon><infon key="source">Plant Dis</infon><infon key="type">ref</infon><infon key="volume">92</infon><infon key="year">2008</infon><offset>49528</offset><text>Visual rating and the use of image analysis for assessing different symptoms of citrus canker on grapefruit leaves</text></passage><passage><infon key="fpage">59</infon><infon key="issue">2</infon><infon key="lpage">107</infon><infon key="name_0">surname:Bock;given-names:CH</infon><infon key="name_1">surname:Poole;given-names:GH</infon><infon key="name_2">surname:Parker;given-names:PE</infon><infon key="name_3">surname:Gottwald;given-names:TR</infon><infon key="pub-id_doi">10.1080/07352681003617285</infon><infon key="section_type">REF</infon><infon key="source">Crit Rev Plant Sci</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2010</infon><offset>49643</offset><text>Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging</text></passage><passage><infon key="fpage">734</infon><infon key="name_0">surname:Mutka;given-names:AM</infon><infon key="name_1">surname:Bart;given-names:RS</infon><infon key="pub-id_pmid">25601871</infon><infon key="section_type">REF</infon><infon key="source">Front Plant Sci</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2014</infon><offset>49758</offset><text>Image-based phenotyping of plant disease symptoms</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">9</infon><infon key="name_0">surname:Laskin;given-names:DN</infon><infon key="name_1">surname:McDermid;given-names:GJ</infon><infon key="pub-id_doi">10.1016/j.ecoinf.2016.02.005</infon><infon key="section_type">REF</infon><infon key="source">Ecol Inform</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2016</infon><offset>49808</offset><text>Evaluating the level of agreement between human and time-lapse camera observations of understory plant phenology at multiple scales</text></passage><passage><infon key="fpage">23</infon><infon key="name_0">surname:Naik;given-names:HS</infon><infon key="name_1">surname:Zhang;given-names:J</infon><infon key="name_2">surname:Lofquist;given-names:A</infon><infon key="name_3">surname:Assefa;given-names:T</infon><infon key="name_4">surname:Sarkar;given-names:S</infon><infon key="name_5">surname:Ackerman;given-names:D</infon><infon key="name_6">surname:Singh;given-names:A</infon><infon key="name_7">surname:Singh;given-names:AK</infon><infon key="name_8">surname:Ganapathysubramanian;given-names:B</infon><infon key="pub-id_doi">10.1186/s13007-017-0173-7</infon><infon key="pub-id_pmid">28405214</infon><infon key="section_type">REF</infon><infon key="source">Plant Methods</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2017</infon><offset>49940</offset><text>A real-time phenotyping framework using machine learning for plant stress severity rating in soybean</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50041</offset><text>Cooperation center for coding growth stages in tobacco crops (CORESTA): a scale for coding growth stages in tobacco crops. 2009.</text></passage><passage><infon key="fpage">907</infon><infon key="issue">3</infon><infon key="lpage">914</infon><infon key="name_0">surname:Tsai;given-names:CH</infon><infon key="name_1">surname:Miller;given-names:A</infon><infon key="name_2">surname:Spalding;given-names:M</infon><infon key="name_3">surname:Rodermel;given-names:S</infon><infon key="pub-id_doi">10.1104/pp.115.3.907</infon><infon key="pub-id_pmid">12223853</infon><infon key="section_type">REF</infon><infon key="source">Plant Physiol</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">1997</infon><offset>50170</offset><text>Source strength regulates an early phase transition of tobacco shoot morphogenesis</text></passage><passage><infon key="fpage">1169</infon><infon key="issue">336</infon><infon key="name_0">surname:Walter;given-names:A</infon><infon key="name_1">surname:Schurr;given-names:U</infon><infon key="pub-id_doi">10.1093/jxb/50.336.1169</infon><infon key="section_type">REF</infon><infon key="source">J Exp Bot</infon><infon key="type">ref</infon><infon key="volume">50</infon><infon key="year">1999</infon><offset>50253</offset><text>The modular character of growth in nicotiana tabacum plants under steady-state nutrition</text></passage><passage><infon key="fpage">1470</infon><infon key="issue">6</infon><infon key="lpage">1478</infon><infon key="name_0">surname:Dellen;given-names:B</infon><infon key="name_1">surname:Scharr;given-names:H</infon><infon key="name_2">surname:Torras;given-names:C</infon><infon key="pub-id_doi">10.1109/TCBB.2015.2404810</infon><infon key="pub-id_pmid">26684463</infon><infon key="section_type">REF</infon><infon key="source">IEEE/ACM Trans Comput Biol Bioinform</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2015</infon><offset>50342</offset><text>Growth signatures of rosette plants from time-lapse video</text></passage><passage><infon key="fpage">2405</infon><infon key="issue">5512</infon><infon key="lpage">2407</infon><infon key="name_0">surname:Berardini;given-names:TZ</infon><infon key="name_1">surname:Bollman;given-names:K</infon><infon key="name_2">surname:Sun;given-names:H</infon><infon key="name_3">surname:Scott Poethig;given-names:R</infon><infon key="pub-id_doi">10.1126/science.1057144</infon><infon key="pub-id_pmid">11264535</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">291</infon><infon key="year">2001</infon><offset>50400</offset><text>Regulation of vegetative phase change in arabidopsis thaliana by cyclophilin 40</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50480</offset><text>Sukhbaatar S, Bruna J, Paluri M, Bourdev L, Fergus R. Training convolutional networks with noisy labels. International conference on learning representations, San Diego, 7-9 May 2015. p. 1–11. arXiv:1406.2080v4.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50694</offset><text>Reed SE, Lee H, Anguelov D, Szegedy C, Erhan D, Rabinovich A. Training deep neural networks on noisy labels with bootstrapping (2014). arXiv arXiv:1412–6596</text></passage><passage><infon key="fpage">157</infon><infon key="issue">1–3</infon><infon key="lpage">173</infon><infon key="name_0">surname:Russell;given-names:BC</infon><infon key="name_1">surname:Torralba;given-names:A</infon><infon key="name_2">surname:Murphy;given-names:KP</infon><infon key="name_3">surname:Freeman;given-names:WT</infon><infon key="pub-id_doi">10.1007/s11263-007-0090-8</infon><infon key="section_type">REF</infon><infon key="source">Int J Comput Vis</infon><infon key="type">ref</infon><infon key="volume">77</infon><infon key="year">2008</infon><offset>50853</offset><text>Labelme: a database and web-based tool for image annotation</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50913</offset><text>Howe J. The rise of crowdsourcing (2017). https://www.wired.com/2006/06/crowds/. Accessed 01 Apr 2017.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51016</offset><text>Arteta C, Lempitsky V, Zisserman A. Counting in the wild. In: European conference on computer vision .2016.</text></passage><passage><infon key="fpage">11</infon><infon key="issue">1</infon><infon key="lpage">17</infon><infon key="name_0">surname:Caspar;given-names:T</infon><infon key="name_1">surname:Huber;given-names:SC</infon><infon key="name_2">surname:Somerville;given-names:C</infon><infon key="pub-id_doi">10.1104/pp.79.1.11</infon><infon key="pub-id_pmid">16664354</infon><infon key="section_type">REF</infon><infon key="source">Plant Physiol</infon><infon key="type">ref</infon><infon key="volume">79</infon><infon key="year">1985</infon><offset>51124</offset><text>Alterations in growth, photosynthesis, and respiration in a starchless mutant of arabidopsis thaliana (l.) deficient in chloroplast phosphoglucomutase activity</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51284</offset><text>Lenth RV. Java Applets for power and sample size [Computer software]. http://www.stat.uiowa.edu/~rlenth/Power (2017). Accessed 01 Apr 2017.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51424</offset><text>Minervini M, Giuffrida MV, Tsaftaris SA. An interactive tool for semi-automated leaf annotation. In: Scharr H, Tsaftaris SA, Pridmore T, editors. Proceedings of the computer vision problems in plant phenotyping workshop. 2015. p. 6–1613.</text></passage><passage><infon key="name_0">surname:Gwet;given-names:KL</infon><infon key="section_type">REF</infon><infon key="source">Handbook of inter-rater reliability: the definitive guide to measuring the extent of agreement among raters</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>51664</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51665</offset><text>Girard JM. MATLAB functions for computing inter-observer reliability. https://github.com/jmgirard/mReliability (2017). Accessed 20 May 2017.</text></passage><passage><infon key="fpage">307</infon><infon key="issue">8476</infon><infon key="lpage">310</infon><infon key="name_0">surname:Martin Bland;given-names:J</infon><infon key="name_1">surname:Altman;given-names:D</infon><infon key="pub-id_doi">10.1016/S0140-6736(86)90837-8</infon><infon key="section_type">REF</infon><infon key="source">Lancet</infon><infon key="type">ref</infon><infon key="volume">327</infon><infon key="year">1986</infon><offset>51806</offset><text>Statistical methods for assessing agreement between two methods of clinical measurement</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51894</offset><text>Sheshadri A, Lease M. SQUARE: a benchmark for research on computing crowd consensus. In: Hartman B, Horvitz E, editors. First AAAI conference on human computation and crowdsourcing-HCOMP. 2013.</text></passage><passage><infon key="fpage">1826</infon><infon key="issue">6</infon><infon key="lpage">1831</infon><infon key="name_0">surname:Jones;given-names:TR</infon><infon key="name_1">surname:Carpenter;given-names:AE</infon><infon key="name_10">surname:Sabatini;given-names:DM</infon><infon key="name_2">surname:Lamprecht;given-names:MR</infon><infon key="name_3">surname:Moffat;given-names:J</infon><infon key="name_4">surname:Silver;given-names:SJ</infon><infon key="name_5">surname:Grenier;given-names:JK</infon><infon key="name_6">surname:Castoreno;given-names:AB</infon><infon key="name_7">surname:Eggert;given-names:US</infon><infon key="name_8">surname:Root;given-names:DE</infon><infon key="name_9">surname:Golland;given-names:P</infon><infon key="pub-id_doi">10.1073/pnas.0808843106</infon><infon key="pub-id_pmid">19188593</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci</infon><infon key="type">ref</infon><infon key="volume">106</infon><infon key="year">2009</infon><offset>52088</offset><text>Scoring diverse cellular morphologies in image-based screens with iterative feedback and machine learning</text></passage></document></collection>
