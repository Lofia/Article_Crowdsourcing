<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210607</date><key>pmc.key</key><document><id>8168828</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1016/j.jclinepi.2020.11.003</infon><infon key="article-id_pii">S0895-4356(20)31172-0</infon><infon key="article-id_pmc">8168828</infon><infon key="article-id_pmid">33171275</infon><infon key="fpage">140</infon><infon key="kwd">Machine learning Study classifiers Searching Information retrieval Methods/methodology Randomized controlled trials Systematic reviews Automation Crowdsourcing Cochrane Library</infon><infon key="license">This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</infon><infon key="lpage">151</infon><infon key="name_0">surname:Thomas;given-names:James</infon><infon key="name_1">surname:McDonald;given-names:Steve</infon><infon key="name_2">surname:Noel-Storr;given-names:Anna</infon><infon key="name_3">surname:Shemilt;given-names:Ian</infon><infon key="name_4">surname:Elliott;given-names:Julian</infon><infon key="name_5">surname:Mavergames;given-names:Chris</infon><infon key="name_6">surname:Marshall;given-names:Iain J.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">133</infon><infon key="year">2021</infon><offset>0</offset><text>Machine learning reduced workload with minimal risk of missing studies: development and evaluation of a randomized controlled trial classifier for Cochrane Reviews</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>164</offset><text>Objectives</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>175</offset><text>This study developed, calibrated, and evaluated a machine learning classifier designed to reduce study identification workload in Cochrane for producing systematic reviews.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>348</offset><text>Methods</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>356</offset><text>A machine learning classifier for retrieving randomized controlled trials (RCTs) was developed (the “Cochrane RCT Classifier”), with the algorithm trained using a data set of title–abstract records from Embase, manually labeled by the Cochrane Crowd. The classifier was then calibrated using a further data set of similar records manually labeled by the Clinical Hedges team, aiming for 99% recall. Finally, the recall of the calibrated classifier was evaluated using records of RCTs included in Cochrane Reviews that had abstracts of sufficient length to allow machine classification.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>948</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>956</offset><text>The Cochrane RCT Classifier was trained using 280,620 records (20,454 of which reported RCTs). A classification threshold was set using 49,025 calibration records (1,587 of which reported RCTs), and our bootstrap validation found the classifier had recall of 0.99 (95% confidence interval 0.98–0.99) and precision of 0.08 (95% confidence interval 0.06–0.12) in this data set. The final, calibrated RCT classifier correctly retrieved 43,783 (99.5%) of 44,007 RCTs included in Cochrane Reviews but missed 224 (0.5%). Older records were more likely to be missed than those more recently published.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1555</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1567</offset><text>The Cochrane RCT Classifier can reduce manual study identification workload for Cochrane Reviews, with a very low and acceptable risk of missing eligible RCTs. This classifier now forms part of the Evidence Pipeline, an integrated workflow deployed within Cochrane to help improve the efficiency of the study identification processes that support systematic review production.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1944</offset><text>Highlights</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1955</offset><text>Systematic review processes need to become more efficient.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2014</offset><text>Machine learning is sufficiently mature for real-world use.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2074</offset><text>A machine learning classifier was built using data from Cochrane Crowd.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2146</offset><text>It was calibrated to achieve very high recall.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2193</offset><text>It is now live and in use in Cochrane review production systems.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_caption</infon><offset>2258</offset><text>Key findings</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2271</offset><text>Manual workload can be saved by using a machine learning classifier</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2339</offset><text>The risk of missing eligible studies is accepta bly low.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title</infon><offset>2396</offset><text>What this adds to what was known?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2430</offset><text>Machine learning tools are sufficiently mature to be used in real-world scenarios.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2513</offset><text>It is possible to build a machine learning classifier to identify RCTs that is sufficiently reliable to be deployed in live workflows.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title</infon><offset>2648</offset><text>What is the implication and what should change now?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2700</offset><text>Where possible, systematic reviewers should use this classifier to make their work more efficient.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2799</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2810</offset><text>Cochrane is a leading producer of systematic reviews, with more than 8,000 currently published in the Cochrane Library. These reviews incorporate the results of tens of thousands of randomized controlled trials (RCTs) and other primary studies. The manual effort invested in identifying primary studies eligible for inclusion in these and other systematic reviews is vast. Author teams and information specialists typically search a large number of bibliographic databases to find the comparatively small number of studies eligible to be included. These searches are sensitive to identify as many relevant studies as possible but therefore yield large number of irrelevant records, which are then screened manually by author teams. This is a time-consuming and therefore costly process, especially when all records are checked by at least two people to aid reliability. With the rapidly increasing volume of research being conducted and published, systematic reviews tend to be resource-intensive projects, which can take years to complete. As a consequence, many important research questions are not covered by systematic reviews, and it is increasingly difficult to maintain an up-to-date synthesized evidence base. This is a waste of global investment in research, leading to suboptimal decision-making and poorer health outcomes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4144</offset><text>Automation has been proposed as one possible solution to reduce the manual burden of many systematic review tasks. For example, machine learning classification algorithms (“classifiers”) can “learn” the eligibility criteria of a review through exposure to a manually classified set of documents, thus reducing the human effort required to find relevant studies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4514</offset><text>To date, most automation approaches operate at the level of individual reviews, rather than addressing structural deficiencies in research curation. This paper describes an important component in an alternative approach, which aims to improve the efficiency of study identification across multiple systematic reviews of RCTs. The system comprises (1) database searching, (2) machine learning, and (3) crowdsourcing (via the Cochrane Crowd citizen science project) to populate an existing database of RCTs (CENTRAL). The interlinked system or “workflow” is known as the Cochrane “Evidence Pipeline.” Here we describe the machine learning component of the Pipeline workflow; the other components (the Cochrane Crowd and a Centralised Search Service) are detailed elsewhere. The reason that this is so beneficial for Cochrane Reviews is twofold. First, on the basis that RCT study designs can be ethically implemented to produce results capable of supporting causal claims about the beneficial effects of the large majority of health care interventions evaluated in Cochrane Reviews, approximately 90% of Cochrane Reviews aim to include only RCTs. Thus, the capability to efficiently identify studies with designs at scale will generate large corollary cost savings and efficiency gains in review production and updating systems across thousands of Cochrane Reviews, reducing research waste. Second, searches conducted for Cochrane and non-Cochrane health and nonhealth systematic reviews of RCT evidence also retrieve many records of studies that are not RCTs (often well over 50%). Thus, the capability to automatically exclude non-RCTs from manual checking in such reviews will reduce manual workload (because, even if they are about the right topic, the fact that they are not RCTs means that they are ineligible for inclusion), with corollary cost savings and efficiency gains.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6402</offset><text>We have previously described methods for automatically identifying RCTs from research databases. In that evaluation, we found machine learning classification systems are more accurate than manually crafted Boolean string searches of databases (the current standard practice). Yet, showing higher accuracy in a validation study is not sufficient to ensure new technologies are adopted in practice. We have engaged with the Cochrane Information Retrieval Methods Group (IRMG) with whom we agreed additional requirements for this technology to be adopted by Cochrane. First, the classifier must recall at least 99% of RCTs (a more stringent threshold than we had applied in our previous work). Second, the classifier should provide an indicative probability score to users. Third, an additional assessment should be done on whether the classifier would be at risk of missing any of the studies included in existing Cochrane Reviews. In this article, we describe the development, calibration, and evaluation of a machine learning classifier designed to meet these requirements, which has subsequently been adopted by and deployed within Cochrane.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>7545</offset><text>Materials and methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>7567</offset><text>Cochrane Evidence Pipeline workflow</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7603</offset><text>Cochrane publishes a database of RCTs that are relevant for current or potential future reviews (CENTRAL), with an administrative interface for Cochrane users, known as the Cochrane Register of Studies. Although a rapidly increasing minority of reviews synthesize nonrandomized research designs (including qualitative and quasiexperimental studies), CENTRAL focuses on RCTs, which currently remain the basis of the large majority of published Cochrane Reviews. We likewise focus our efforts on the discovery of RCTs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8120</offset><text>We seek to benefit from efficiencies in two ways. First, current practice is to identify RCTs through searches of bibliographic databases using highly sensitive RCT filters. Such filters have low precision, retrieving as many as 20 non-RCTs for every true RCT. These irrelevant articles then need to be manually screened and removed. Second, the same studies are retrieved and assessed multiple times by different people across the global systematic review workforce. The Pipeline therefore aims to avoid this duplication of effort by facilitating the reuse of previous assessments as to whether a given report describes, or does not describe, an RCT.</text></passage><passage><infon key="file">gr1.jpg</infon><infon key="id">fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>8772</offset><text>The Cochrane Evidence Pipeline workflow, depicting the flow of records from the centralized search service, through machine and crowd classification services to the CENTRAL database.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8955</offset><text>Fig. 1 depicts the role of machine learning within the Pipeline. To populate CENTRAL, Cochrane regularly searches a range of online resources (e.g., biomedical literature databases) through the “Centralised Search Service,” which is described elsewhere. Abstracts of these candidate articles (of which the majority are not RCTs) are “fed” into the top of the Pipeline. (The scope and detail of these searches is described here: https://www.cochranelibrary.com/central/central-creation). The machine learning classifier (described in this article) is used to filter out records that are highly unlikely to be an RCT study report. The remaining articles are then handed over to the Cochrane Crowd, which filters out all further records that do not report an RCT. Finally, the remaining articles (which should all describe RCTs) are stored in CENTRAL. Crowd “labels” are also used to update the machine learning algorithm so that it becomes more accurate at distinguishing between relevant and irrelevant records (see Section 2.7).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9997</offset><text>Data sets and their role in this study</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10036</offset><text>High-quality data sets are vital for the development, calibration, and evaluation of reliable machine learning classifiers. Most evaluations of such classifiers use a single data set, which is split at random between “training” and “test” data (e.g., with 70% of the data reserved for training). The training data are used to estimate the model parameters, and the test data are used to evaluate its performance. However, although a single data set evaluation can provide estimates of classifier performance that have strong internal validity, it cannot tell us how well a classifier will perform in the real world, where data may come from sources that differ in important ways from those used to produce this data set. As outlined by Nevin, it is important to consider the external validity of machine learning models before deployment. Here, we examined external validity in terms of whether the use of our machine learning model would risk missing RCTs included in Cochrane Reviews.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11031</offset><text>Training data, from which the machine learning models are built;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11096</offset><text>Calibration data, on which the threshold for determining the cutoff between “RCT” and “non-RCT” classifications was based; and</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11231</offset><text>Validation data, on which the calibrated classifier was evaluated.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11298</offset><text>We therefore used three distinct data sets in the present study:</text></passage><passage><infon key="file">gr2.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>11363</offset><text>Development and evaluation of the classifier, showing where the various data sets were used in the classifier development process.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11494</offset><text>Fig. 2 summarizes the contribution made by each data set that we now describe in detail.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11584</offset><text>Training data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11598</offset><text>The data set used to train the classifier comprises a corpus of 280,620 title–abstract records retrieved from Embase using a highly sensitive search for RCT (https://www.cochranelibrary.com/central/central-creation). This search has been carried out each month since January 2014 for the purpose of identifying relevant studies for inclusion in CENTRAL (see Section 2). In this study, we used records retrieved between January 2014 and July 2016 inclusive. During this period, any records indexed with the Emtree headings “Randomized controlled trial” or “Controlled clinical study” were automatically marked for inclusion in CENTRAL, without any manual checking, on the basis that this rule produced a false positive rate for identifying reports of RCTs that was judged sufficiently low. To account for the historical use of this rule, records with these specific Emtree headings were also excluded from our training data set. Because obvious RCTs and obvious non-RCTs had already been filtered out of this data set (using Emtree headings and the sensitive search filter for RCTs, respectively) before we used it to train the classifier, the data set therefore comprises records that are, on average, more difficult to classify according to whether or not they report an RCT, compared with an unfiltered sample from the raw database.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12943</offset><text>Next, each record in the training data set was labeled by Cochrane Crowd members according to whether it reported an RCT (n = 20,454) or not (n = 260,166). Each record was labeled by multiple Crowd members, with the final Crowd decision being determined by an agreement algorithm; Noel-Storr et al. report that the Crowd recall and precision for identifying RCTs both exceeded 99%.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13330</offset><text>This data set (“Cochrane Crowd data” in Fig. 2) has characteristics that make it highly suitable for training a machine learning classifier: it is both large—so represents a wide range of instances of both the positive and negative classes (i.e., RCTs and non-RCTs)—and also very accurately labeled. However, it also comprised records added to Embase during a relatively short (&lt;3 years) period, which could limit the generalizability of the resulting machine learning classifier.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13821</offset><text>Calibration data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13838</offset><text>When machine learning classifiers are used for prediction, they output a score (often scaled to be bounded by 0 and 1) that gets assigned to each title–abstract record, with a higher value representing an increased likelihood that the record reports an RCT. However, to use the classifier to reduce manual screening workload, we also needed to set a threshold score below which records (unlikely to be RCTs) are discarded and conversely above which records (possible RCTs) are retained for manual screening. Higher score thresholds can be expected to lead to a higher prevalence of reports of RCTs (true positives) among fewer retained records (i.e., higher precision) but at the expense of having discarded some reports of RCTs (false negatives) with scores below the threshold (i.e., lower recall). Conversely, a lower threshold can be expected to lead to lower precision but higher recall.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14733</offset><text>We sought the advice from the Cochrane IRMG and were advised that the classifier would need to have a threshold score calibrated to retrieve at least 99% of relevant RCT study reports to be adopted for use in Cochrane and also that achieving this high level of recall should be prioritized over any reductions in manual screening workload. These specifications reflect the strong aversion that we have, when conducting systematic reviews, to inadvertently failing to identify studies that should be included.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15242</offset><text>The Clinical Hedges data set (Fig. 2) was built during 2000 and 2001 for the purposes of testing and validating sensitive search filters for RCTs. It contains 49,028 title–abstract records manually identified and selected by information specialists using a combination of hand search and electronic search methods. Corresponding full-text reports of all records were manually checked to ascertain with confidence whether or not each reported an RCT, making this a highly accurate data set for our current purpose. Three records from this data set were no longer available in PubMed, so our final calibration data set comprised 49,025 PubMed title–abstract records, of which 1,587 reported an RCT (and the remaining 47,438 did not report an RCT).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15993</offset><text>It was more demanding to calibrate our RCT classifier on this data set (compared with using a proportion of records held back from the Cochrane Crowd data set) because (1) the records are older and are less likely to have a consistent reporting structure for RCTs because the study reports were published only a few years after the CONSORT statement (and before the latter became widely used) and (2) the Clinical Hedges Team's assessments were based on full-text reports, and there is no indication in some of the corresponding titles and abstracts that they actually report an RCT. We used this data set to identify the threshold for achieving 99% recall and thereby calibrate our classifier, and we also present results concerning the precision with which these records can be identified.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16785</offset><text>Validation data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16801</offset><text>As described previously, this machine learning classifier was primarily designed to identify records of study reports potentially eligible for inclusion in Cochrane Reviews. We therefore validated the classifier using a third data set to determine whether the desired level of 99% recall (calibrated using the Clinical Hedges dataset) could be achieved in practice. This validation data set (“Cochrane Included Studies” in Fig. 2) comprises title and abstract records of all study reports included in Cochrane Reviews in which eligible study designs are restricted to “RCTs only,” published up to April 2017. The data set comprises 94,305 records of 58,283 included studies across 4,296 Cochrane Reviews. Although it could be assumed that the vast majority of these records report an RCT, in practice, we found that some records of included study reports did not report an RCT (e.g., they reported a meta-analysis of RCTs, a related editorial, or personal correspondence). These records were retained in the validation set, as removing them all would have required the manual screening of all records.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17912</offset><text>Excluded data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17926</offset><text>Articles without an abstract (i.e., title-only records) may contain insufficient information for accurate machine (or human) classification. However, title-only records that include the words “randomised controlled trial” (as per CONSORT guidance) should be labeled correctly by a classifier. In consultation with the IRMG and in the light of manual assessment of records with some content in their abstract field (but not a full abstract), we determined that pragmatic cutoffs for including a record in the training, calibration, or validation data sets would be set at 400 characters as a minimum abstract length and 15 characters as a minimum title length. These cutoffs aimed to balance the need for sufficient text to be present in the abstract field for the machine learning to operate while not referring too many records for manual assessment. It is important to note that, in the Cochrane Evidence Pipeline workflow (mentioned previously), all records with title and/or abstract fields that have fewer characters than the minimum cutoff are referred for manual screening by members of the Cochrane Crowd. When the minimum character cutoff is applied to the Cochrane Included Studies data set, the final number of studies in the evaluation falls to 44,007.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19195</offset><text>Machine learning methods for RCT identification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19243</offset><text>Machine learning describes a group of algorithms that seek to “learn” to do a task by exposure to (typically large amounts of) data. The approach we used here can be described as supervised machine learning: meaning that the algorithm is “trained” on articles for which the true label is already known. Although the current state-of-the-art approach for text classification is the use of neural network models, we have previously found that support vector machine (SVM) models (and specifically ensembles [ensembling describes a strategy of using multiple machine learning models together, with the aim of improving performance compared with any model individually] of multiple of SVM models) were similarly accurate for high-sensitivity document recall. SVMs are less computationally intensive than neural models and therefore can run quickly and without the need for any special computer hardware. The final Cochrane RCT Classifier model also needed to be deployed in a live Web service that might need to cope with heavy user demand. For these reasons, we chose SVMs for the present study. We refer the interested reader to a detailed description of machine learning methods as applied to abstract classification. In our previous work, we incorporated metadata from the database describing study design into our models (the Publication Type tag in MEDLINE, which is manually added by MEDLINE staff, often several months after publication). However, as the Evidence Pipeline retrieves mainly very new records, which usually lack this metadata, we used a model that uses titles and abstract text without additional metadata.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20877</offset><text>We used the bag-of-words approach, in which each title–abstract record is represented as a vector of 0s and 1s, depending on the presence or absence of each unique word from the article set vocabulary. These vector representations are then used to “train” (i.e., find optimal parameters for) an SVM model. We preprocessed the records to remove commonly used words (e.g., “and,” “the”) that appear on the PubMed “stopword” list. During our initial development phase, we found that an ensemble of two SVM models with minor differences resulted in greatest accuracy when evaluated on the training data. We therefore selected an ensemble of two SVM variants for use in the study (see Fig. 2). The first classifier (SVM1) represented the texts as individual words, pairs of words, and triplets of words (uni-, bi-, and tri-grams). This accounts for situations in which adjacent words affect document class (e.g., the text “randomized controlled trial” might be more strongly indicative of an RCT than any word individually). The second classifier (SVM2) used a unigram model (i.e., each word is considered individually) and used a strategy of oversampling. This strategy aims to reduce the likelihood of missing a “rare class” (here the “rare class” is RCTs, which account for ~5% of the data set) by artificially increasing the number of RCTs in the training data set by random sampling with replacement (this process is not repeated with the calibration or validation data sets, which are left in their original state).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22425</offset><text>The source code for building SVM1 is available at https://github.com/alan-turing-institute/DSSG19-Cochrane/blob/dev/analyses/partner_baseline/create_model.py.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22584</offset><text>The source code for building SVM2 is available at https://github.com/ijmarshall/robotsearch.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22677</offset><text>Generating calibrated probability estimates</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22721</offset><text>SVMs estimate the distance between a given record and a “hyperplane”, which, in the current use, separates RCTs (the positive class) from non-RCTs (the negative class). The hyperplane distance metric is not readily interpretable (in our data set, this metric had a numeric value approximately between −1 and +8), and we therefore sought to add probability estimates to meet the needs of Cochrane users, who have found this feature to be particularly useful in understanding the output. To achieve this, we calibrated the ensemble SVM scores on the Clinical Hedges data set using a logistic regression model (known as Platt scaling). This generated a score for each “unseen” record in the calibration or validation data sets that is bounded by 0 and 1. These scores are closer to representing the true probability that a given record reports an RCT; as such they are readily interpretable, with higher scores representing a higher likelihood that the record reports an RCT (and vice versa). When viewed graphically, the distribution of scores is often U-shaped, with most records being assigned either a high (close to 1) or low (close to 0) probability score, and a smaller number of records in the middle that are more ambiguous in terms of class membership.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23991</offset><text>Evaluation metrics</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24010</offset><text>In this article, we use the conventional information retrieval terminology recall and precision, which are synonymous with sensitivity and positive predictive value, respectively. As outlined previously, the recall statistic is of primary concern in the current use scenario—that is, that eligible study reports are not incorrectly discarded from the Evidence Pipeline workflow. Cochrane required the system to have at least 99% recall. Recall is calculated as the proportion of relevant items (i.e., records describing an RCT) that are correctly identified by the Evidence Pipeline workflow compared with the total number of records genuinely reporting an RCT that should have been identified.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24707</offset><text>To evaluate the discriminative performance and quality of calibration of our machine learning strategy on the Clinical Hedges data, we used bootstrap sampling as described by Steyerberg et al.. In short, a series of artificial new data sets were “bootstrapped” by random sampling with replacement from the Clinical Hedges dataset. Logistic regression models (which served the dual purposes of ensembling the individual SVM models and producing calibrated probability outputs) were trained on each sampled data set and evaluated on the original data set. This process was repeated 5,000 times and used to estimate performance metrics with 95% confidence intervals. Although the primary use of the system is for binary classification, a key secondary use is providing indicative probability scores to users. We evaluate the quality of the probabilities via a calibration plot and by calculation of the Brier score and C statistics.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25642</offset><text>Common practice in Cochrane Reviews is to find, use, and cite all published (and unpublished) reports of each included study (“study reports”). Many studies included in Cochrane Reviews comprised multiple study reports. This means that if the classifier “misses” one of several study reports of the same RCT, this does not necessarily mean the RCT study has been “lost.” We therefore adopted the following approach. We first classified all study reports in Cochrane Reviews of RCTs using the machine learning classifier and then we considered a study to be “lost” only if all reports of that study fell below the threshold. As such, the “study” is our unit of analysis rather than the “study report.” We made this decision because we found many secondary citations in reviews referred to indirectly related non-RCT studies and also because we would expect the retrieval of a single article would alert the review team to the existence of the trial.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26615</offset><text>Precision is also a metric of interest because it can be used to compute the number of articles requiring manual screening by Cochrane Crowd in the Evidence Pipeline workflow. Here, we were concerned with the number of irrelevant records (i.e., records not reporting an RCT) that are incorrectly classified by machine learning as relevant (i.e., records with an assigned probability score above the identified threshold score), which must then be filtered out manually by the Cochrane Crowd. Precision is calculated as the proportion of retrieved records, which genuinely report an RCT.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27202</offset><text>2 × 2 table from which precision and recall are calculated</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;&lt;th&gt;RCTs (gold standard)&lt;/th&gt;&lt;th&gt;Non-RCTs (gold standard)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Machine learning classed RCTs&lt;/td&gt;&lt;td&gt;True positives&lt;/td&gt;&lt;td&gt;False positives&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Machine learning classed non-RCTs&lt;/td&gt;&lt;td&gt;False negatives&lt;/td&gt;&lt;td&gt;True negatives&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27262</offset><text>	RCTs (gold standard)	Non-RCTs (gold standard)	 	Machine learning classed RCTs	True positives	False positives	 	Machine learning classed non-RCTs	False negatives	True negatives	 	</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>27442</offset><text>Abbreviation: RCT, randomized controlled trial.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27490</offset><text>Recall and precision were calculated from a 2 × 2 table representing positive/negative (relevant/irrelevant) classes and whether they were correctly or incorrectly classified (Table 1).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27678</offset><text>Formulas used to calculate precision and recall are as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27742</offset><text>We computed statistics for precision at 99% recall against the Clinical Hedges calibration data set. As specified previously, recall was set at 99% by the IRMG. As the Cochrane Reviews we examined contain only RCTs (and the non-RCTs excluded during searches are not usually comprehensively recorded), we were not able to calculate precision on the Cochrane Reviews dataset and report recall only.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28139</offset><text>For the primary analysis, the denominator was all articles in Cochrane Reviews meeting the minimum character length criteria described previously (i.e., very short titles and abstracts were excluded). We assume that manual assessment will yield 100% recall of these records. We also report results on the full data set, without removing articles with small or nonexistent abstracts, as a secondary analysis. The first figure can be interpreted as the recall of the overall workflow because it takes account of our decision to remove records with insufficient information for machine classification from the workflow.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28756</offset><text>We present absolute values of the total number of eligible studies “lost” to Cochrane Reviews. Finally, we also present the distribution of “lost” study reports according to the year of publication because we hypothesize that the classifier may perform less well on older study reports because (1) it has been trained on newer reports and (2) trial reporting may have improved as a result of the CONSORT statement.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>29179</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29187</offset><text>The machine learning classifier for identifying reports of randomized trials (Cochrane RCT Classifier) was built as per the previously mentioned methods from the screening of 280,620 Embase records (January 2014 to July 2016) by Cochrane Crowd. Of these, 20,454 (7.3%) were deemed to be RCTs.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>29480</offset><text>Threshold setting and binary classification performance</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>29536</offset><text>Bootstrap estimates of model performance on Clinical Hedges data set, with 95% confidence intervals</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Validation precision&lt;/th&gt;&lt;th&gt;Validation recall&lt;/th&gt;&lt;th&gt;Validation specificity&lt;/th&gt;&lt;th&gt;C statistic&lt;/th&gt;&lt;th&gt;Brier score&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;0.08 (0.06, 0.12)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;0.99 (0.98, 0.99)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;0.63 (0.48, 0.76)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;0.98 (0.98, 0.98)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;0.05 (0.05, 0.05)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29636</offset><text>Validation precision	Validation recall	Validation specificity	C statistic	Brier score	 	0.08 (0.06, 0.12)	0.99 (0.98, 0.99)	0.63 (0.48, 0.76)	0.98 (0.98, 0.98)	0.05 (0.05, 0.05)	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29817</offset><text>The 49,025 records from the Clinical Hedges data set were scored by the machine learning classifier. The records were ordered according to classifier score, and precision and recall statistics were calculated for every record in sequence. The classifier probability, which corresponded with 99% recall, was recorded and used as the classification threshold for the later validation (and the deployed system). The discriminative and calibrative performance of this strategy, estimated using bootstrap sampling is presented in Table 2. We estimate that precision was 8.3%, meaning that one in every 12 records retrieved described an RCT. Setting the classifier at this level of recall resulted in 58% of records in this data set being automatically discarded as highly unlikely to be reporting a randomized trial.</text></passage><passage><infon key="file">gr3.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30630</offset><text>Calibration plot showing bootstrap estimates of predicted vs. observed probabilities of an article being an RCT in Clinical Hedges dataset (each blue point represents an estimate of a model generated from one bootstrap sample) and the performance of the final model (orange). (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)</text></passage><passage><infon key="file">gr4.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>31036</offset><text>Distribution of classification scores for RCTs and non-RCTs in Clinical Hedges data set. RCT, randomized controlled trials.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31160</offset><text>Estimates of the C statistic and Brier score were 0.978 and 0.048, respectively, indicating excellent discriminative performance. We present a calibration plot showing point estimates from the bootstrap evaluation and the final model (trained on the whole dataset) in Fig. 3. We show how the predicted scores are distributed for RCTs and non-RCTs in Fig. 4.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>31520</offset><text>Validating the classifier recall on studies included in Cochrane Reviews</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31593</offset><text>The title and abstract records of 58,283 studies included in 4,296 Cochrane Reviews were fed through the classifier. Records with a score equal to or above the threshold identified in the previous step were automatically classified as potentially reporting an RCT; those scoring below this threshold were automatically classified as not reporting an RCT.</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>31948</offset><text>Number of included studies in Cochrane Reviews classified as RCTs</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;&lt;th&gt;RCTS correctly identified by the classifier (recall)&lt;/th&gt;&lt;th&gt;RCTS not identified by the classifier&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;All studies (&lt;italic&gt;N&lt;/italic&gt; = 58,283)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;54,683 (93.8%)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;3,600 (6.2%)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Studies with sufficient information for machine classification (&lt;italic&gt;N&lt;/italic&gt; = 44,007 studies)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;43,783 (99.5%)&lt;/td&gt;&lt;td align=&quot;char&quot;&gt;224 (0.5%)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>32014</offset><text>	RCTS correctly identified by the classifier (recall)	RCTS not identified by the classifier	 	All studies (N = 58,283)	54,683 (93.8%)	3,600 (6.2%)	 	Studies with sufficient information for machine classification (N = 44,007 studies)	43,783 (99.5%)	224 (0.5%)	 	</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>32276</offset><text>Abbreviation: RCT: randomized controlled trial.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32324</offset><text>Table 3 summarizes the number of eligible studies that are “lost” to reviews as a result of all of their corresponding study reports scoring lower than the threshold. When records that contain insufficient information for machine classification are excluded from machine classification and assumed to be manually assessed (see Section 2), the classifier correctly identifies 99.5% (43,783 out of 44,007) of studies.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32745</offset><text>In our secondary analysis, when we include for machine classification data for all studies (including the subset of studies, which contain insufficient information for accurate machine classification [see Section 2]), we find that 3,396 studies would be potentially “lost” to reviews (compared with 224 studies when only those with sufficient information are included).</text></passage><passage><infon key="file">gr5.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>33119</offset><text>RCTs “lost” by the classifier per 1,000 published, by year of publication, showing that the risk of “losing” a publication decreases over time.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33271</offset><text>Fig. 5 shows the 224 randomized trials “lost” by the classifier per 1,000 published, by year of publication, for all but one of the publications (the age of one publication could not be ascertained). These results show that older reports are much more likely to be misclassified by the machine learning classifier.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>33591</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>33602</offset><text>Summary of findings</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33622</offset><text>We conducted a three-stage study that involved training, calibrating, and evaluating a machine learning classifier designed to distinguish between bibliographic title–abstract records that report an RCT and those that do not. Recall falls to an unacceptably low level (94%) if records with limited information in their titles and/or abstracts are submitted for machine classification. However, when these records are excluded, the classifier exceeds the standard required by Cochrane with recall at 99.5% of all those records scored. It should be noted that this means that some records are unsuitable for machine learning and so must necessarily be checked manually; however, this mirrors current practice, whereby records with limited information in their titles and abstracts are retained for further assessment on the basis of their corresponding full-text reports.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34494</offset><text>We deem the recall level as “acceptable” for use in “live” reviews on the basis that (1) this exceeds the recall of validated RCT search filters that have been used in systematic review production for many years and (b) this threshold was agreed by methodologists in Cochrane for use in Cochrane Reviews.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34807</offset><text>Although the precision of 8% estimated against the Clinical Hedges dataset appears low, this is partly because of the age of that data set and relatively low prevalence of RCTs. In the Cochrane Evidence Pipeline workflow (Fig. 1), the classifier saved Cochrane Crowd from needing to check 185,039 records manually (out of a total of 449,480) during the 2018 calendar year, a very large saving in manual workload.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35221</offset><text>Systematic reviews are frequently used to support decision-making processes for both policymakers and practitioners and are also key sources of evidence in drug licensing regulation. Reviews need to be accurate representations of the state of current knowledge, as decisions that are based on their findings can affect people's lives. Reviews also need to be demonstrably correct, as the way in which evidence is synthesized can have implications, for example, for drug licensing, and can therefore be open to legal challenge. These joint imperatives—for systematic reviews to be correct and to be seen to be correct—generate the normative expectation that they should contain all relevant research evidence and the corollary concern that review findings based on bodies of evidence that inadvertently exclude some eligible studies are potentially unreliable. To this end, our study provides data demonstrating the reliability of implementing what could be seen as a major innovation in study identification methods for systematic reviews, the automatic eligibility assessment of study reports, and the exclusion of a portion without any manual checking by humans, rolled out at scale across Cochrane: the largest producer of systematic reviews globally and an organization committed to minimizing the risk of bias in review production through methodological and editorial rigor. We note that the recall threshold set by Cochrane (99%) exceeds the performance of conventional search methods (e.g., the Cochrane Highly Sensitive Search Strategy was found to have recall of 98.5% by the Clinical Hedges team), and we have demonstrated in previous work that our machine learning approach can exceed the precision achieved by conventional search filters.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36976</offset><text>Although our results indicated that 0.5% of studies could have been “lost” to Cochrane Reviews if authors had used this classifier (affecting 178 reviews, leaving 4,118 reviews unaffected), this is almost certainly an overestimate when considering the prospective use of this classifier to support the identification of newly published RCTs for new, updated, and/or living systematic reviews. First, other means of finding studies are routinely used in Cochrane Reviews alongside conventional electronic searching—such as checking reference lists and citing records or contacting researchers who are active in the topic area—so some of these “lost” studies would likely be found using these complementary search methods. Second, studies that are potentially lost are overwhelmingly older reports. Although we do not dismiss the potential importance of identifying older trials for consideration in systematic reviews, it is reassuring that more recent studies (relevant especially for newer treatments and review updates) are far less likely to be missed. One reason the classifier performs better for more recent studies could be improvement in the reporting of RCTs over time, for example, in response to the CONSORT statement. Trialists are now widely expected to detail trial methodology in the report's abstract and to include the fact that they are reporting an RCT in its title.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>38374</offset><text>Strengths and weaknesses of this evaluation</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38418</offset><text>In this article, we have described a robust evaluation of the performance of an RCT classifier in a large data set of systematic reviews. We were fortunate in having three large, independently generated, high-quality data sets available to train, calibrate, and validate the classifier. This is an unusual position to be in, and there are probably few study designs other than RCTs with comparably high-quality data sets available. We note that this may limit the potential to evaluate the performance of similar workflows, created to identify other types of study design, using the same three-stage process.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39027</offset><text>The current classifier has been trained almost exclusively on records published in English, so it does not necessarily generalize to other languages. However, this important limitation is, in principle, surmountable, as machine learning technology is language agnostic and would therefore be capable of modeling any language, so long as sufficient training data were available.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39405</offset><text>The focus of this work has been to build a machine learning classifier for deployment in a specific workflow. The machine learning classifier we have developed meets required levels of recall but inevitably results in some studies being “lost” to reviews. This study does not attempt to ascertain the impact of these losses on the affected reviews' statistical and narrative results and findings, and a future extension of this study will investigate this important question. We also note that only 178 of 4,296 reviews were affected, leaving results unchanged in at least 96% of the reviews.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>40002</offset><text>Next steps: the “Screen4Me” service</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40042</offset><text>We are currently piloting an extension to the Evidence Pipeline for use with individual Cochrane Reviews. Authors using this service will compile their set of potentially eligible records from searches of multiple databases (including CENTRAL) as is typical for any systematic review. Given that the RCT classifier and Cochrane Crowd have already classified more than 800,000 study records (and increasing by &gt;10,000 per month), it is likely that a proportion of the records retrieved and uploaded to the Classifier by authors have already been classified according to whether they report an RCT or not. Where this is the case, the records that are already known not to describe RCTs will be removed from the workflow. The remaining studies will then be sent to the RCT classifier, and those records classified as not reporting an RCT (i.e., that fall below the 99% recall threshold) will be discarded. Finally, the records classified as potentially reporting an RCT will be screened by Cochrane Crowd. The review team is then left with a much smaller pool of records to examine, containing only RCTs. In early pilots, this new workflow reduced manual screening workload by between 40% and 70% depending on the prevalence of RCTs in the search results of individual reviews.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>41317</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>41329</offset><text>The Cochrane RCT Classifier is now deployed by Cochrane for reducing screening workload in review production. As part of a wider workflow that includes prospective database searches and crowdsourcing to build a comprehensive database of RCTs, machine learning can reduce the manual screening burden associated with research synthesis while ensuring a very high level of recall that is acceptable for an organization, which depends on having comprehensive access to the published research that falls within health care topics relevant to its scope.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>41877</offset><text>References</text></passage><passage><infon key="comment">Available at</infon><infon key="element-citation">https://www.cochranelibrary.com/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>41888</offset><text>Cochrane Library</text></passage><passage><infon key="fpage">67</infon><infon key="lpage">99</infon><infon key="name_0">surname:Lefebvre;given-names:C.</infon><infon key="name_1">surname:Glanville;given-names:J.</infon><infon key="name_10">surname:Li;given-names:T.</infon><infon key="name_11">surname:Page;given-names:M.</infon><infon key="name_2">surname:Briscoe;given-names:S.</infon><infon key="name_3">surname:Littlewood;given-names:A.</infon><infon key="name_4">surname:Marshall;given-names:C.</infon><infon key="name_5">surname:Metzendorf;given-names:M.</infon><infon key="name_6">surname:Higgins;given-names:J.</infon><infon key="name_7">surname:Thomas;given-names:J.</infon><infon key="name_8">surname:Chandler;given-names:J.</infon><infon key="name_9">surname:Cumpston;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Cochrane Handbook for Systematic Reviews of Interventions</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>41905</offset></passage><passage><infon key="fpage">e1000326</infon><infon key="issue">9</infon><infon key="name_0">surname:Bastian;given-names:H.</infon><infon key="name_1">surname:Glasziou;given-names:P.</infon><infon key="name_2">surname:Chalmers;given-names:I.</infon><infon key="pub-id_pmid">20877712</infon><infon key="section_type">REF</infon><infon key="source">PLoS Med</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2010</infon><offset>41906</offset><text>Seventy-five trials and eleven systematic reviews a day: how will we ever keep up?</text></passage><passage><infon key="fpage">224</infon><infon key="lpage">233</infon><infon key="name_0">surname:Shojania;given-names:K.G.</infon><infon key="name_1">surname:Sampson;given-names:M.</infon><infon key="name_2">surname:Ansari;given-names:M.T.</infon><infon key="name_3">surname:Ji;given-names:J.</infon><infon key="name_4">surname:Doucette;given-names:S.</infon><infon key="pub-id_pmid">17638714</infon><infon key="section_type">REF</infon><infon key="source">Ann Intern Med</infon><infon key="type">ref</infon><infon key="volume">147</infon><infon key="year">2007</infon><offset>41989</offset><text>How quickly do systematic reviews go out of date? A survival analysis</text></passage><passage><infon key="fpage">101</infon><infon key="lpage">104</infon><infon key="name_0">surname:Macleod;given-names:M.R.</infon><infon key="name_1">surname:Michie;given-names:S.</infon><infon key="name_2">surname:Roberts;given-names:I.</infon><infon key="name_3">surname:Dirnagl;given-names:U.</infon><infon key="name_4">surname:Chalmers;given-names:I.</infon><infon key="name_5">surname:Ioannidis;given-names:J.P.A.</infon><infon key="pub-id_pmid">24411643</infon><infon key="section_type">REF</infon><infon key="source">Lancet</infon><infon key="type">ref</infon><infon key="volume">383</infon><infon key="year">2014</infon><offset>42059</offset><text>Biomedical research: increasing value, reducing waste</text></passage><passage><infon key="fpage">f139</infon><infon key="issue">1</infon><infon key="name_0">surname:Tsafnat;given-names:G.</infon><infon key="name_1">surname:Dunn;given-names:A.</infon><infon key="name_2">surname:Glasziou;given-names:P.</infon><infon key="name_3">surname:Coiera;given-names:E.</infon><infon key="pub-id_pmid">23305843</infon><infon key="section_type">REF</infon><infon key="source">BMJ</infon><infon key="type">ref</infon><infon key="volume">346</infon><infon key="year">2013</infon><offset>42113</offset><text>The automation of systematic reviews</text></passage><passage><infon key="fpage">5</infon><infon key="issue">1</infon><infon key="name_0">surname:O’Mara-Eves;given-names:A.</infon><infon key="name_1">surname:Thomas;given-names:J.</infon><infon key="name_2">surname:McNaught;given-names:J.</infon><infon key="name_3">surname:Miwa;given-names:M.</infon><infon key="name_4">surname:Ananiadou;given-names:S.</infon><infon key="pub-id_pmid">25588314</infon><infon key="section_type">REF</infon><infon key="source">Syst Rev</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2015</infon><offset>42150</offset><text>Using text mining for study identification in systematic reviews: a systematic review of current approaches</text></passage><passage><infon key="fpage">602</infon><infon key="lpage">614</infon><infon key="name_0">surname:Marshall;given-names:I.</infon><infon key="name_1">surname:Storr;given-names:A.N.</infon><infon key="name_2">surname:Kuiper;given-names:J.</infon><infon key="name_3">surname:Thomas;given-names:J.</infon><infon key="name_4">surname:Wallace;given-names:B.C.</infon><infon key="pub-id_pmid">29314757</infon><infon key="section_type">REF</infon><infon key="source">Res Synth Methods</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2018</infon><offset>42258</offset><text>Machine learning for identifying randomized controlled trials: an evaluation and practitioner’s guide</text></passage><passage><infon key="fpage">1165</infon><infon key="issue">6</infon><infon key="lpage">1168</infon><infon key="name_0">surname:Wallace;given-names:B.C.</infon><infon key="name_1">surname:Noel-Storr;given-names:A.</infon><infon key="name_2">surname:Marshall;given-names:I.J.</infon><infon key="name_3">surname:Cohen;given-names:A.M.</infon><infon key="name_4">surname:Smalheiser;given-names:N.R.</infon><infon key="name_5">surname:Thomas;given-names:J.</infon><infon key="pub-id_pmid">28541493</infon><infon key="section_type">REF</infon><infon key="source">J Am Med Inform Assoc</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2017</infon><offset>42362</offset><text>Identifying reports of randomized controlled trials (RCTs) via a hybrid machine learning and crowdsourcing approach</text></passage><passage><infon key="fpage">31</infon><infon key="lpage">37</infon><infon key="name_0">surname:Thomas;given-names:J.</infon><infon key="name_1">surname:Noel-Storr;given-names:A.</infon><infon key="name_2">surname:Marshall;given-names:I.</infon><infon key="name_3">surname:Wallace;given-names:B.</infon><infon key="name_4">surname:McDonald;given-names:S.</infon><infon key="name_5">surname:Mavergames;given-names:C.</infon><infon key="pub-id_pmid">28912003</infon><infon key="section_type">REF</infon><infon key="source">J Clin Epidemiol</infon><infon key="type">ref</infon><infon key="volume">91</infon><infon key="year">2017</infon><offset>42478</offset><text>Living systematic reviews: 2. Combining human and machine effort</text></passage><passage><infon key="comment">Available at</infon><infon key="element-citation">https://community.cochrane.org/help/tools-and-software/crs-cochrane-register-studies/about-crs</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>42543</offset><text>About the CRS (Cochrane Register of Studies). Cochrane Community</text></passage><passage><infon key="fpage">142</infon><infon key="lpage">150</infon><infon key="name_0">surname:Noel-Storr;given-names:A.</infon><infon key="name_1">surname:Dooley;given-names:G.</infon><infon key="name_2">surname:Wisneiwski;given-names:S.</infon><infon key="name_3">surname:Glanville;given-names:J.</infon><infon key="name_4">surname:Thomas;given-names:J.</infon><infon key="name_5">surname:Cox;given-names:S.</infon><infon key="pub-id_pmid">32798713</infon><infon key="section_type">REF</infon><infon key="source">J Clin Epidemiol</infon><infon key="type">ref</infon><infon key="volume">127</infon><infon key="year">2020</infon><offset>42608</offset><text>Cochrane Centralised Search Service showed high sensitivity identifying randomized controlled trials: a retrospective analysis</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>42735</offset><text>Noel-Storr A, Dooley G, Elliott J, Steele E, Shemilt I, Mavergames C, et al. An evaluation of Cochrane Crowd finds that crowdsourcing can help to address the challenge of information overload in evidence synthesis. J Clin Epidemiol.</text></passage><passage><infon key="fpage">4</infon><infon key="issue">11</infon><infon key="lpage">7</infon><infon key="name_0">surname:Nevin;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">PLoS Med</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2018</infon><offset>42969</offset><text>Advancing the beneficial use of machine learning in health care and medicine: toward a community understanding</text></passage><passage><infon key="fpage">1</infon><infon key="issue">20</infon><infon key="lpage">15</infon><infon key="name_0">surname:Wilczynski;given-names:N.L.</infon><infon key="name_1">surname:Douglas;given-names:M.</infon><infon key="name_2">surname:Haynes;given-names:R.B.</infon><infon key="pub-id_pmid">15638940</infon><infon key="section_type">REF</infon><infon key="source">BMC Med Inform Decis Mak</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2005</infon><offset>43080</offset><text>An overview of the design and methods for retrieving high-quality studies for clinical care</text></passage><passage><infon key="fpage">1</infon><infon key="issue">18</infon><infon key="lpage">9</infon><infon key="name_0">surname:Schulz;given-names:K.F.</infon><infon key="name_1">surname:Altman;given-names:D.C.</infon><infon key="name_2">surname:Moher;given-names:D.</infon><infon key="pub-id_pmid">20051100</infon><infon key="section_type">REF</infon><infon key="source">BMC Med</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2010</infon><offset>43172</offset><text>CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials</text></passage><passage><infon key="fpage">131</infon><infon key="name_0">surname:Sain;given-names:S.R.</infon><infon key="name_1">surname:Vapnik;given-names:V.N.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>43262</offset></passage><passage><infon key="name_0">surname:Platt;given-names:J.C.</infon><infon key="section_type">REF</infon><infon key="source">Adv Large Margin Classif</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>43263</offset><text>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</text></passage><passage><infon key="name_0">surname:Steyerberg;given-names:E.W.</infon><infon key="name_1">surname:Harrell;given-names:F.E.</infon><infon key="name_2">surname:Borsboom;given-names:G.J.J.M.</infon><infon key="name_3">surname:Eijkemans;given-names:M.J.C.</infon><infon key="name_4">surname:Vergouwe;given-names:Y.</infon><infon key="name_5">surname:Habbema;given-names:J.D.F.</infon><infon key="section_type">REF</infon><infon key="source">J Clin Epidemiol</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>43363</offset><text>Internal validation of predictive models: efficiency of some procedures for logistic regression analysis</text></passage><passage><infon key="name_0">surname:Brier;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Mon Weather Rev</infon><infon key="type">ref</infon><infon key="year">1950</infon><offset>43468</offset><text>Verification of forecasts expressed in terms of probability</text></passage><passage><infon key="fpage">187</infon><infon key="lpage">202</infon><infon key="name_0">surname:McKibbon;given-names:K.A.</infon><infon key="name_1">surname:Lou;given-names:W.N.</infon><infon key="name_2">surname:Haynes;given-names:R.B.</infon><infon key="pub-id_pmid">19712211</infon><infon key="section_type">REF</infon><infon key="source">Health Info Libr J</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2009</infon><offset>43528</offset><text>Retrieving randomized controlled trials from MEDLINE: a comparison of 38 published search filters</text></passage><passage><infon key="fpage">60</infon><infon key="issue">1</infon><infon key="name_0">surname:Turner;given-names:L.</infon><infon key="name_1">surname:Shamseer;given-names:L.</infon><infon key="name_2">surname:Altman;given-names:D.G.</infon><infon key="name_3">surname:Schulz;given-names:K.F.</infon><infon key="name_4">surname:Moher;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Syst Rev</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2012</infon><offset>43626</offset><text>Does use of the CONSORT statement impact the completeness of reporting of randomised controlled trials published in medical journals? A Cochrane review</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>43778</offset><text>Funding: This work received funding from Cochrane via Project Transform; Australian  (Partnership Project grant APP1114605);  (Award 2R01-LM012086-05); I.J.M. was supported by a  (UK) fellowship (MR/N015185/1). A portion of James Thomas's time was supported by the  (NIHR) Collaboration for Leadership in Applied Health Research and Care North Thames at Barts Health NHS Trust. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>44277</offset><text>Ethics approval and consent to participate: Not applicable: study does not involve human subjects.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>44376</offset><text>Conflict of interest: The authors declare that they have no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>44457</offset><text>Authors’ contributions: J.T., A.N.S., S.M., and I.J.M. designed the study. J.T. and I.J.M. built the classifiers and calibration models. A.N.S. and S.M. worked on evaluation data sets. C.M. provided overall Cochrane direction and governance. I.S. and J.E. provided methodological input throughout. All authors read and approved the final article.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>44806</offset><text>Consent for publication: Not applicable: no participant data presented.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>44878</offset><text>Availability of data and materials: Hyperlinks to source code repositories are supplied in the text. Cochrane's CENTRAL database is referenced and is available at https://www.cochranelibrary.com/central. All Cochrane Crowd labels are open and available at http://crowd.cochrane.org/DownloadData.php.</text></passage></document></collection>
