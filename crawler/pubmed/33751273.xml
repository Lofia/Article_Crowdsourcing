<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220218</date><key>pmc.key</key><document><id>8360862</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1007/s10071-021-01490-8</infon><infon key="article-id_pmc">8360862</infon><infon key="article-id_pmid">33751273</infon><infon key="article-id_publisher-id">1490</infon><infon key="fpage">947</infon><infon key="issue">5</infon><infon key="kwd">Analysing behaviour Animal behaviour metrics Rating behaviour Coding behaviour Crowd-sourcing data analysis Measuring behaviour</infon><infon key="license">Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.</infon><infon key="lpage">956</infon><infon key="name_0">surname:Root-Gutteridge;given-names:Holly</infon><infon key="name_1">surname:Brown;given-names:Louise P.</infon><infon key="name_2">surname:Forman;given-names:Jemma</infon><infon key="name_3">surname:Korzeniowska;given-names:Anna T.</infon><infon key="name_4">surname:Simner;given-names:Julia</infon><infon key="name_5">surname:Reby;given-names:David</infon><infon key="name_6">surname:Reby;given-names:David</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">24</infon><infon key="year">2021</infon><offset>0</offset><text>Using a new video rating tool to crowd-source analysis of behavioural reaction to stimuli</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>90</offset><text>Quantifying the intensity of animals’ reaction to stimuli is notoriously difficult as classic unidimensional measures of responses such as latency or duration of looking can fail to capture the overall strength of behavioural responses. More holistic rating can be useful but have the inherent risks of subjective bias and lack of repeatability. Here, we explored whether crowdsourcing could be used to efficiently and reliably overcome these potential flaws. A total of 396 participants watched online videos of dogs reacting to auditory stimuli and provided 23,248 ratings of the strength of the dogs’ responses from zero (default) to 100 using an online survey form. We found that raters achieved very high inter-rater reliability across multiple datasets (although their responses were affected by their sex, age, and attitude towards animals) and that as few as 10 raters could be used to achieve a reliable result. A linear mixed model applied to PCA components of behaviours discovered that the dogs’ facial expressions and head orientation influenced the strength of behaviour ratings the most. Further linear mixed models showed that that strength of behaviour ratings was moderately correlated to the duration of dogs’ reactions but not to dogs’ reaction latency (from the stimulus onset). This suggests that observers’ ratings captured consistent dimensions of animals’ responses that are not fully represented by more classic unidimensional metrics. Finally, we report that overall participants strongly enjoyed the experience. Thus, we suggest that using crowdsourcing can offer a useful, repeatable tool to assess behavioural intensity in experimental or observational studies where unidimensional coding may miss nuance, or where coding multiple dimensions may be too time-consuming.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1903</offset><text>Supplementary Information</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1929</offset><text>The online version contains supplementary material available at 10.1007/s10071-021-01490-8.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2021</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2034</offset><text>Developing accurate and unbiased measures of behavioural responses to stimuli is critical to the study of animal behaviour (Banks; Meagher). The human brain remains one of the most effective tools for analysing data, encompassing a wide range of features and achieving complex and high-level perceptual categorisations in milliseconds (Marois and Ivanoff). Observation can therefore be a powerful method for characterising animal behaviour. However, the use of subjective assessments of behaviour, personality, and emotional state has attracted strong criticism due to biases resulting from prior experience, preconceptions, and observer-gender (Marsh and Hanlon; Tuyttens et al.). Here we use the term subjective to describe metrics based on “an individual’s perception and judgement, and can therefore be influenced by experience or personal views” (Meagher). Therefore, much effort has been expended on developing robust, discrete coding systems to quantify distinct behaviours. A common example of this is the ethogram, which is a list of species-specific behaviours coded into behavioural units which represent discrete actions, coded with duration, latency, and binary occurrence (Banks). However, ethograms require the choice of behaviours to be made a priori, and can fail to capture subtle responses (e.g. small ear movements), or complex interactions of many different elements (e.g. the combination of ear, eye, and mouth movements) (Meagher; Waller and Micheletta). Objective assessments may also fail to fully describe the gestalt as it is perceived by the observer (Meagher). Observers can identify emotional valence and arousal from gestalt impressions and may use fleeting or tiny movements to guide their observations (Tami and Gallagher; Wan et al.; Scheumann et al.; Maréchal et al.; Kelly et al.; Guo et al.). Moreover, the lack of blinding during analysis has proven to be problematic, especially when experimenters analyse their own work (Tuyttens et al.). The problems associated with rating behaviours have resulted in the understandable perception that quantitative coding is more objective and reliable, and therefore superior, and coding is used to validate rating results (see Vazire et al. for a review). Here we explore whether observer bias can be reduced by simply increasing the number of observers (e.g. by using a crowdsourcing approach to recruit naïve observers), and whether this can result in reliable metrics of one feature of animal behaviour, i.e. reaction strength.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4551</offset><text>Although used only rarely by behavioural scientists, crowdsourcing has been used for over a century by biologists (Droege). This has produced large-scale datasets which could not otherwise be generated (e.g. surveying bird species distributions at national or international levels by using local reports by ornithologists) (Desell et al.). More recently with the expansion of the internet, researchers have adopted crowdsourcing approaches for mass data analysis tasks which are time-consuming or cannot be automated easily (Cox et al.). These have been found to contribute positively to both public engagement levels and science generally, with high publication rates and large cost savings relative to employing a worker full-time for years (Cox et al.). Canine behaviour studies have previously used crowdsourcing for both data collection (Stewart et al.; Worsley and O’Hara) and analysis (Mirkó et al.; Bloom and Friedman), and while this has not been widely adopted by researchers, it has shown that even naïve observers can form correct assessments of dog behavioural responses (Mirkó et al.) and emotions (Wan et al.; Bloom and Friedman). Furthermore, humans can successfully judge a domestic dog’s emotions from its facial expressions in photographs, independent of their knowledge of dogs, suggesting that personal experience is not required (Bloom and Friedman) and that naïve and inexperienced observers can still offer valuable analyses.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6009</offset><text>Crowdsourcing represents a feasible option for the mass-analysis of data despite trade-offs between the time-saved and the loss of expertise, especially as the importance of expertise depends on the ambiguity of the data being presented (Law et al.). While individuals may make mistakes, errors are minimised by drawing data from many different people, with results collated and reviewed, so the advantages of crowd-sourced data analysis typically outweigh the errors (Bonter and Cooper; Gardiner et al.). The reliability of observers can be quantified using measures of correlation between individuals, referred to as inter-observer reliability, such as Cronbach’s alpha, and the measures accepted if the agreement is high enough (Bland and Altman; Koo and Li). Thus, we suggest that greater objectivity can be achieved by increasing the number of observers to reduce the influence of any single observer on the results and that as the observers are blind to the aim of the experiment, the potential subjectivity in ratings can be reduced as there is no bias towards desired outcomes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7097</offset><text>In this study, we explored whether crowd-sourcing analysis by large groups of naïve participants can produce widely-agreed-upon assessments of the behaviour of dogs, and we also investigate how many observers are required to achieve a reliable result. We follow a model detailed by Hecht and Spicer Rice, where researchers provide the data content which is to be assessed and naive observers analyse them. Observers were presented with videos of domestic dogs responding to acoustic playback trials and asked to rate the strength of the dog’s reaction to the stimulus in each video. Raters were not given instructions as to which behaviours should be considered, or other criteria for response, and were asked for their naïve judgements only. Finally, as crowdsourcing relies on the willingness of participants to perform the task, the participants were asked to rate their enjoyment of the study and the likelihood of participating again in future.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>8051</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8059</offset><text>Participants</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8072</offset><text>Three hundred and ninety-six people (56 men and 340 women) participated in the study, with a mean age of 19.92 years old, standard deviation (SD) = 4.0 years, and the oldest participant was 67. 16 people were known to have rated more than one set of videos. The participants were recruited by word of mouth and via the University of Sussex student body (contacted via online advertising on an internal website). Students were rewarded with course credit through the School of Psychology’s system while no reward was offered to other participants. The results for the studies were pooled. 358 (90.4%) participants agreed with the statement (1) “are you an animal lover?” and 199 participants (50.3%) agreed with statement (2) “Have you ever owned a dog?” Overall, the participants had owned dogs from 96 breeds and identified breed-mixes (see Electronic Supplementary Materials for list of breeds owned).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8993</offset><text>Videos: dog behavioural reactions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9027</offset><text>Each rated video featured a single dog listening to a short stimulus sound, with all dogs recorded in the same location. All video data were collected as part of the BBSRC funded project ‘How Dogs Hear Us’. The dogs were accompanied by their owners to a testing room on the UoS Falmer campus where they each heard six stimuli sound in a habituation-dishabituation experiment [see Root-Gutteridge et al. for details]. None of the stimuli was distressing to the dogs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9497</offset><text>Each video was clipped using the video editing software iMovie (Apple Inc., 2016) or Sony Vegas Pro (version 9: Sony Creative Software, 2009; version 13: Sony Creative Software, 2013; version 14: Sony Creative Software, 2014) to feature a single dog’s response to one trial. Soundtracks were muted, with the stimulus sounds replaced by a champagne cork pop sound effect (see electronic supplementary material (ESM) for sample video). This replacement was to avoid bias in the raters’ responses, which can be an issue in studies without blinding (Tuyttens et al.). The videos were converted to MP4 format using Adobe Media Encoder CC (Adobe 2018).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10148</offset><text>A total of 258 videos were presented in three separate datasets (wave 1:36 videos, wave 2:78 videos, wave 3:144 videos, where wave indicates a dataset). These videos were selected to represent a range of behavioural reactions from low to high activity within the context of the experiment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>10438</offset><text>Human ratings</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10452</offset><text>Rater participants were logged on our in-house testing platform (www.syntoolkit.org), which allows video uploads alongside surveys, and questionnaires. The first task for participants was to fill in a short questionnaire about their age, gender, whether they had ever been a dog owner, and, if so, which breeds they had owned. Then participants were shown four sample videos of different dogs’ reactions to demonstrate the range of possible dog reactions from low activity (only the dog’s eyebrows moved) to high activity (the dog’s entire face and body moved). Participants were not asked to rate these videos and were not informed how these videos had been rated by the researchers, to avoid biasing their assessments, but were told that they represented the range of reactions.</text></passage><passage><infon key="file">10071_2021_1490_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>11239</offset><text>Example screen from the survey website for rating dogs’ responses. The video started when the participant clicked play. Dogs had heard the sound stimulus approximately 1 s into the video (see Electronic Supplementary Material for example video) but the sound was replaced within the playback video by a “pop”</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11555</offset><text>Next, participants were asked to watch dog videos to rate the strength of the dogs’ reaction to stimuli using a slider bar running from 0 (no reaction) to 100 (strongest reaction), with the slider bar pre-set to zero to avoid priming the participants’ responses. Videos were presented with one per page (see Fig. 1). Participants were asked to watch each video before rating the dog’s reaction to the sound and could watch the video as many times as they chose. The website randomised the order of presentation and recorded the order of presentation along with the ratings. Videos only played when clicked and could be rewound or paused as desired. A tally displayed how many videos had been completed so far. Results were saved after each video by pressing the “Confirm” button and videos could not be viewed again once “Confirm” was selected.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12415</offset><text>Video ratings: pilot study</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12442</offset><text>We piloted the study and website with an initial presentation of 36 videos of 6 dogs, with 6 videos per dog. These were initially shown to 10 naïve participants. In addition to rating the dogs, participants were asked to rate the website’s instructions, ease of use, and presentation, and to comment on any issues they experienced. Following this, we adjusted the slider bar for ease-of-use, added additional instructions, and included a check-box to indicate if the video had not worked as expected.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12946</offset><text>Video ratings: pooled results from waves 1–3</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12993</offset><text>Following the pilot, the study was rolled out through the University of Sussex internal study recruitment website. 346 additional participants were recruited to rate three separate sets of videos, hereafter referred to as waves 1, 2 and 3. The pooled 216 participants rated a total of 258 videos: 29 participants, including the initial 10, rated the 36 videos of 6 dogs originally used in the pilot (wave 1), 153 participants rated 78 videos of 13 dogs (wave 2), and 34 participants rated 144 videos of 24 dogs (wave 3). The videos were chosen to represent a range of reactions by the dogs including those which moved only slightly and dogs which produced complex movements in response to the stimulus. One dog provided twelve videos presented across two waves, while all other dogs provided 6 videos to one wave. The videos in wave 3 represent all the videos recorded for one habituation-dishabituation test condition in Root-Gutteridge et al., which were not used in that study as too many of the dogs were distracted, looking at their owners, or did not reach habituation. Thus, the wave 3 videos represent a complete set of experimental videos.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14142</offset><text>Post-study questionnaire</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14167</offset><text>Several months after the completion of the ratings for waves 1 and 2, participants were sent an invitation to participate in a questionnaire on their experience of rating dog behaviour videos, hosted on the site Survey Monkey. For wave 3, this questionnaire was added to the SynToolkit website and integrated into the task. Seven questions were asked (supplementary Table 1) and results were collated.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14569</offset><text>Statistics</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14580</offset><text>All statistics were performed in the statistical program SPSS 25 (IBM Corp. Released, 2013).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14673</offset><text>Assessing inter-rater-reliability and required numbers for representation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14747</offset><text>Rater reliability is a measurement of how well different raters agree with each other and measures the homogeneity of assessments by different observers. Low inter-rater reliability suggests that raters do not assess the data using the same criteria. The probability of their agreement is assessed using the SPSS function ‘Reliability Analysis’. This uses the intra-class correlation coefficient (ICC) calculated using a two-way mixed model (Koo and Li). This results in a Cronbach’s alpha score for the rating reliability, where &gt; 0.8 is considered to be highly reliable (Bland and Altman). It was calculated for each of the three waves independently as different raters had participated in each wave.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15460</offset><text>Following this, we estimated the number of raters required to achieve a similar level of inter-rater agreement as found in this study, i.e. &gt; 0.8 following (Bland and Altman), to inform how many raters might be required for similar studies. The number of raters was estimated from r = 2/cv, where r = required number of raters, cv = standard error of percent agreement/percent agreement across each video (Gwet).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15889</offset><text>Statistics for rating results</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15919</offset><text>All rating results were pooled. Participants were identified using anonymised codes, which linked to the results from the questionnaires to allow their demographic information to be added to statistical models. The rating results were then compared in a single mixed effects linear model with sex, age, and agreement with statements (1) “Are you an animal lover?” and (2) “Have you ever owned a dog? “ as fixed effects, with individual rater ID and dog ID as random effects.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16402</offset><text>Assessing behaviours performed in videos</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16443</offset><text>Description of behaviours performed by dogs in videos in response to stimuli and percent of videos where dogs performed the behaviours</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Behaviour&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Description&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Videos where behaviour was performed (%)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Change in breathing&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog showed altered breathing&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;8.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Down&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog lay down from sit or stand&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;4.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Ears moved&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog changed ear position&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;50.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Eyebrow movement&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog moved its eyebrows&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;54.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Eyes turned&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog moved eyes independent of head movement&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;56.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Facial expression&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog changed facial expression&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;16.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Freeze&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog stopped any movement&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Head tilt&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog tilted its head from centre to left or right&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;17.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Head turn&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog moved its head in the direction of the speaker&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;58.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Look at speaker&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog looked towards the speaker&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;54.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Mouth moved&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog opened or closed mouth&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;14.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Nostrils flared&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog flared nostrils&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;11.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Retreat&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog moved away from speaker&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sit&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog moved to sit from down or stand&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Stand&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Dog stood up from sit or down&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;3.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16578</offset><text>Behaviour	Description	Videos where behaviour was performed (%)	 	Change in breathing	Dog showed altered breathing	8.1	 	Downa	Dog lay down from sit or stand	4.7	 	Ears moved	Dog changed ear position	50.0	 	Eyebrow movement	Dog moved its eyebrows	54.7	 	Eyes turned	Dog moved eyes independent of head movement	56.2	 	Facial expression	Dog changed facial expression	16.7	 	Freezea	Dog stopped any movement	1.9	 	Head tilt	Dog tilted its head from centre to left or right	17.1	 	Head turn	Dog moved its head in the direction of the speaker	58.5	 	Look at speaker	Dog looked towards the speaker	54.4	 	Mouth moved	Dog opened or closed mouth	14.0	 	Nostrils flared	Dog flared nostrils	11.2	 	Retreata	Dog moved away from speaker	0.8	 	Sita	Dog moved to sit from down or stand	1.9	 	Standa	Dog stood up from sit or down	3.9	 	</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>17399</offset><text>aThese measurements were removed from further analyses as they were observed in &lt; 5% of videos</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17498</offset><text>When all the videos had been rated by more than 30 participants, the behaviours the dogs presented in each video were first listed (Table 1) and then assessed as a binary response (e.g. head turn? Y/N) for each video. Behaviours from all videos were coded independently by three researchers (co-authors JF, LB, and HRG). Where the agreement was not reached unanimously, the majority vote ruled. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17894</offset><text>A principal components analysis (PCA) was performed to reduce the listed behaviours to a set of components (varimax rotation) in SPSS. These were loaded with behaviours that were highly correlated to strength as rated by participants. A linear regression model was then used to assess which components contributed to the reaction strength-ratings, Bonferroni adjusted p = 0.0125.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18278</offset><text>For each video, the latency and duration of the dog’s overall response were coded by the authors. Attention to the stimulus was defined as the dog performing one or more active behaviours (see Table 1). Latency was defined as the time between the stimulus onset and the start of the dog’s reaction. The duration was defined as the start of any of the behaviours and the time that the dog stopped visibly responding, or the beginning of the next trial. Therefore, duration was capped at 7 s as this was the sum of the duration of the original stimulus sound and the six-second habituation time. Lack of response was coded as duration equals zero. We used linear regression to explore the potential correlation between reaction strength and the duration or latency of response (adjusted p = 0.025).</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>19084</offset><text>Definitions for ordinal scores of dogs’ strength of reaction in response to stimuli</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Ordinal scale&lt;/th&gt;&lt;th align=&quot;left&quot; colspan=&quot;5&quot;&gt;Response is seen as change in&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Eyes/ears orientation&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Breathing&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Facial expression&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Head position&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Body posture&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N or Slight&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N or Slight&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Slow or slight&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Fast or large&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;N&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Y&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>19170</offset><text>Ordinal scale	Response is seen as change in	 	Eyes/ears orientation	Breathing	Facial expression	Head position	Body posture	 	0	N	N	N	N	N	 	1	Y	N or Slight	N or Slight	N	N	 	2	Y	Y	Y	Slow or slight	N	 	3	Y	Y	Y	Fast or large	N	 	4	Y	Y	Y	Y	Y	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19411</offset><text>Finally, an ordinal scale of behavioural responses was created, and each video was given a score from 0 to 4 by HRG, with 10% second coded by ATK. The operational definitions are given in Table 2 with 0 equalling the weakest reaction (e.g. no visible change in expression, demeanour, body posture etc.) and 4 equalling the strongest. We used linear regression to explore the potential correlation between the ordinal score of response and reaction strength.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>19869</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>19877</offset><text>Participants</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19890</offset><text>For wave 1, participants rated between 1 and 36 out of 36 total videos (mean = 34.2, SD = 7.2). For wave 2, participants rated between 2 and 78 out of 78 total videos (mean = 72.4, SD = 18.1). For wave 3, participants rated between 10 and 144 out of 144 total videos (mean = 136.3, SD = 27.7). To remove unrepresentative outliers, which were often due in wave 1 to the programme not playing the video and raters thus giving inaccurate scores of “0” because they could not play the video, rating scores were retained if they were within 2 SD of the mean for the stimulus video, calculated in SPSS using the ‘Descriptives’ function.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>20554</offset><text>Assessing inter-rater-reliability and required numbers</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>20609</offset><text>Within each wave, there was a strong average agreement on ratings as assessed using the ICC metric (Cronbach’s alpha): wave 1 ICC = 0.785, wave 2 = 0.949, and wave 3 ICC = 0.992. When we removed ratings that fell outside of two SD from the mean, to account for videos where the video failed and the rating defaulted to zero, agreement increased for all studies: wave 1 ICC = 0.990, wave 2 ICC = 0.995, and wave 3 ICC = 0.998.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21061</offset><text>The estimated number of raters required to reach a similar level of agreement differed across studies: for wave 1 required n = 16 raters, for wave 2 = 17, and wave 3 = 10, depending on the standard error of the original ratings.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21302</offset><text>Participant characteristics affecting rating strength</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21356</offset><text>The linear mixed effects model of all ratings showed that the participants’ ratings of dogs’ reaction strength were affected by participants’ sex (F1,3762 = 60.153, p &lt; 0.001), with men giving higher average ratings than women, age (F1,3766 = 7.646, p &lt; 0.001), and age (F15,3765 = 7.646, p &lt; 0.001), and whether they agreed with the statement that they were “animal lovers” (F1,3801 = 37.084, p &lt; 0.001), with disagreeing participants giving higher ratings. However, as participants were recruited using a School of Psychology internal system at the University of Sussex, 97% of participants were under the age of 25, 85.4% of the raters were female and 90.4% of participants agreed they were “animal lovers”, making the result for those variables potentially unreliable. There was no effect of the rater’s previous experience of dog ownership (F1,3637 = 1.127, p = 0.289, 52% were dog owners).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>22301</offset><text>PCA results</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>22313</offset><text>Rotated component matrix from PCA analysis. Loadings &gt; 0.5 are marked in bold. 68.2% of data variance was explained</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Variable&lt;/th&gt;&lt;th align=&quot;left&quot; colspan=&quot;4&quot;&gt;Component&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Facial expression&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Ears and eyes&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Orientation&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Head tilt&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Change in breathing&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.814&lt;/bold&gt;&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.071&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;− 0.177&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Ears moved&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.147&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.633&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.363&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.142&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Eyes turned&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.06&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.746&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.219&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.171&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Eyebrows moved&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.031&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.792&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;− 0.283&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.062&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Facial expression changed&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.87&lt;/bold&gt;&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.095&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.142&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.005&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Head tilt&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.083&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.252&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.046&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.699&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Head turn&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.072&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.173&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.834&lt;/bold&gt;&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.079&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Looked at speaker&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.048&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.036&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.788&lt;/bold&gt;&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.204&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Mouth moved&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.845&lt;/bold&gt;&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.041&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.21&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.07&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Nostrils flared&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.095&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.054&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.044&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.819&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>22433</offset><text>Variable	Component	 	Facial expression	Ears and eyes	Orientation	Head tilt	 	Change in breathing	0.814	0.071	− 0.177	0.08	 	Ears moved	0.147	0.633	0.363	− 0.142	 	Eyes turned	0.06	0.746	0.219	− 0.171	 	Eyebrows moved	0.031	0.792	− 0.283	0.062	 	Facial expression changed	0.87	0.095	0.142	0.005	 	Head tilt	− 0.083	− 0.252	0.046	0.699	 	Head turn	0.072	0.173	0.834	− 0.079	 	Looked at speaker	0.048	− 0.036	0.788	0.204	 	Mouth moved	0.845	0.041	0.21	− 0.07	 	Nostrils flared	0.095	0.054	0.044	0.819	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22961</offset><text>The Principal Component Analysis (PCA) was used to reduce the dimensions of the behavioural descriptive variables listed in Table 1 to a smaller set. PCA achieves this by converting observations of possibly correlated variables (e.g. “head turn” and “look at speaker” or “eyes turned” and “eyebrows moved”) to a set of uncorrelated variables called principal components which explain the largest variance in the data. The correlation matrix for these are shown in Table 3. Components with Eigenvalues of &gt;  = 1 were retained. Thus, the first four components were retained and explained 68.2% of the variance. Components 1–4 explained 25.9%, 16.8%, 15.2%, and 10.4% of the data variance respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>23687</offset><text>PCA components correlated with ratings reaction strength: LMM results</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>23757</offset><text>Linear model results for PCA components of behaviours on rating scores</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Fixed effect&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Estimate&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Std error&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;d.f&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;t&lt;/italic&gt; value&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;p&lt;/italic&gt; value&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Facial expression&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.7244&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.7236&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;257&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;2.383&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.018&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Ears and eyes&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 1.0675&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.7564&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;256&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 1.411&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.159&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Orientation&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;5.8869&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.7824&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;254&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;7.524&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Head tilt&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.0498&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.7008&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;257&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;1.498&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.136&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>23828</offset><text>Fixed effect	Estimate	Std error	d.f	t value	p value	 	Facial expression	1.7244	0.7236	257	2.383	0.018	 	Ears and eyes	− 1.0675	0.7564	256	− 1.411	0.159	 	Orientation	5.8869	0.7824	254	7.524	 &lt; 0.001	 	Head tilt	1.0498	0.7008	257	1.498	0.136	 	</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>24082</offset><text>Significant results are marked in bold</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24121</offset><text>In this section we investigated what behaviours performed by the dogs were predictors of the reaction strength ratings. The linear regression model of the effect of observed behaviours on dog reaction strength determined that two PCA components had a significant effect on rating strength (Table 4), adjusted R-squared = 0.205 and F4,253 = 17.6. These were Facial expression (loaded with the behaviours Change in breathing, Facial expression changed, and Mouth moved) and Orientation (loaded with the behaviours Head turn and Looked at the speaker).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24679</offset><text>Comparing reaction strength to classic metrics and ordinal scores</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>24745</offset><text>Results for linear regression between mean rating strength and duration or latency for each wave and all results together, and for mean rating strength and ordinal score</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Variable&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Wave&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;N&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;r&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;p&lt;/italic&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;4&quot;&gt;Duration&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.332&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.048&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;78&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.229&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt;0.044&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;144&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.430&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Pooled 1–3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;258&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.389&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;4&quot;&gt;Latency&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.019&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.913&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;78&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.127&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.279&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;144&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.065&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.456&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Pooled 1–3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;258&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;− 0.115&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.073&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;4&quot;&gt;Ordinal score&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.795&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;78&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.712&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;144&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.719&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Pooled 1–3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;258&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;0.716&lt;/td&gt;&lt;td char=&quot;.&quot; align=&quot;char&quot;&gt;&lt;bold&gt; &amp;lt; 0.001&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>24915</offset><text>Variable	Wave	N	r	p	 	Duration	1	36	0.332	0.048	 	2	78	0.229	0.044	 	3	144	0.430	 &lt; 0.001	 	Pooled 1–3	258	0.389	 &lt; 0.001	 	Latency	1	36	− 0.019	0.913	 	2	78	− 0.127	0.279	 	3	144	− 0.065	0.456	 	Pooled 1–3	258	− 0.115	0.073	 	Ordinal score	1	36	0.795	 &lt; 0.001	 	2	78	0.712	 &lt; 0.001	 	3	144	0.719	 &lt; 0.001	 	Pooled 1–3	258	0.716	 &lt; 0.001	 	</text></passage><passage><infon key="file">10071_2021_1490_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25298</offset><text>Boxplot of the mean rating of each of the 258 videos against the ordinal score of the intensity of reaction. Linear regression showed a correlation of score to rating at r = 0.716, p &lt; 0.001</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25497</offset><text>The linear regression of reaction strength to duration and latency of reactions was calculated for each wave and all waves together (Table 5). The duration was correlated to mean rating strength at p &lt; 0.05 for all waves, though not at adjusted p value &lt; 0.025 for waves 1 and 2, while latency was not correlated with reaction strength in any wave. However, latency was heavily skewed to the first 0.5 s after the stimulus began which meant it had limited variance. The linear regression of reaction strength was also calculated for ordinal scores. This was significant at p &lt; 0.001 for all 3 waves and all waves together (Table 5, Fig. 2).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26151</offset><text>Questionnaire results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26173</offset><text>107 participants returned the post-hoc survey questionnaire. Their results were collated and used to inform future directions (see Electronic Supplementary Material Table 1). 68.3% of people enjoyed the experience a moderate amount to a great deal (only 4% did not enjoy at all), while 82.3% agreed that they would participate in the same type of study again. 90.6% found the study easy to complete, and 92.5% felt it was appropriate for scientists to use citizen-science recruits for data analysis. The most frequent comments were for fewer videos and a greater range of dogs.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>26751</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>26762</offset><text>We found that crowdsourced observations of animal behaviour could produce consistent ratings of response strength, with the high agreement between raters across more than 258 videos. As raters were not aware of the scores being given by others, the independence of their scores and the consistency between scores suggest that reliable metrics were achieved. Rated reaction strength related to several different behavioural responses in dogs, including head, eye, and mouth movements. As the ten binary variables describing the dogs’ behaviour in the video explained only 68.2% of the data variance in the Principal Component analysis, we determined that the dogs’ responses were not fully described by them, however, as binary coding of the presence of different behaviours such as head-turning and ear movement only captured 68.2% of the data variance and rating strength did not correlate strongly to either duration of or latency to reactions. Furthermore, the ordinal scores correlated well with the average ratings (r = 0.716, p &lt; 0.001), but did not capture the full variability of the data. While the scores worked well for the lower intensity reactions, more complex behaviours were harder to capture using simple ordinal scores. Therefore, we suggest that ratings could potentially capture nuance and encompass a gestalt that is not captured by classic metrics alone, supporting Meagher, and that crowd-sourcing ratings could offer a potentially powerful tool for analysing animal behaviour. However, classic metrics can be used to validate ratings against a widely accepted standard method of analysing behaviour.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>28398</offset><text>We also determined that inter-rater agreement was high enough to overcome potential subjectivity by individual raters. Inter-rater agreement was high with a result of Cronbach’s alpha &gt; 0.9 for most comparisons, well above the suggested threshold for excellent reliability of &gt; 0.8 (Bland and Altman). We estimated how many individuals raters are required to produce representative results from the standard error of the mean, which resulted in a range from 10 to 17. This is much smaller than the number who participated, which were between 34 and 216 raters per wave, and suggests that fewer than 20 people are required per video for reliable assessments. The higher number may also have reflected technical difficulties that the first two waves experienced where zeros were recorded when videos failed to work properly. However, we believe that the number of raters required to achieve consistency would be easy to achieve.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>29335</offset><text>We suggest that crowdsourcing offers a useful metric for rating animal behaviour that is holistic and less liable to subjective bias than ratings done by one or two observers, and may require only a small group of individuals. There are several advantages to rating through crowdsourcing, as while there was an initial time investment in the preparation of the files and the removal of the soundtrack, neither recruitment nor post-hoc analysis of results were time consuming compared to the large amount of data provided by the raters. Also, while individual results are indeed subjective, we find that the collective response has a high level of agreement and that this could potentially be obtained with as few as 10 raters. These results are in line with previous studies where observers formed correct assessments of dogs’ behavioural responses (Mirkó et al.) and emotions (Wan et al.; Bloom and Friedman). This also did not require expertise in dog behaviour on the part of the raters as dog ownership did not predict ratings. While this new method still requires validation against a range of established behavioural and physiological variables as well as testing its performance in different experimental set-ups and in other species before crowd-sourced rating is generalised, we believe that we have shown here that the raters can come to robust agreements about simple metrics of attention and that these provide reliable and useful metrics. In future, the validity of this method will rely on the question being posed correctly and metrics properly applied, (e.g. asking “how strongly does the dog react?” vs “the dog reacts strongly, what is your rating of their intensity?”) but high levels of agreement across multiple raters and correlation with more classically accepted methods suggest that ratings are a powerful tool.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31183</offset><text>Crowdsourcing relies on the goodwill participation of the crowd and is only sustainable if sufficient interest in it can be generated. Therefore, we asked our participants how they felt about the study in a post-hoc survey. Most participants had a positive attitude to the experience and expressed willingness to participate in similar studies in future. While this should be explored with a larger sample of the general population, the popularity of citizen science sites such as Zooniverse.org and Dognition.com suggest that it is possible to recruit large numbers of people for such studies and therefore that our method is repeatable. We suggest it is likely that there is a general willingness to participate in such behavioural research and that it can engage the public interest, as for other crowd sourced science projects (Desell et al.; Law et al.). As our participants were mostly undergraduate psychology students, there may have been a bias towards participants who were potentially more painstaking than average in their responses as they were being rewarded with course credit. However, previous research has shown that crowdsourcing can be a reliable method of data analysis (review: Bonney et al.), and future studies could incorporate a greater range of participants to test the effects of demographics, including age, educational level, and experience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32555</offset><text>A clear next step for this methodology is testing it with a broader range of experimental data, including more complex or difficult to rate behaviours, and testing it with other species. Here, dogs were used as the focal species and the results may reflect raters’ familiarity with the species compared to other, less familiar animals. Half of our participants currently or had previously owned dogs, however, this had no effect on their ratings of behaviour, suggesting that prior experience of living with the focal species was not important. Animal lovers did score the reactions lower than non-animal lovers, which may reflect that the degree of interest in animal behaviour influences ratings. Thus, exploring ratings of the behaviour of a diverse range of species performing a range of different behaviours and a broader cross-section of participants could determine the usefulness of crowdsourced ratings. In particular, we suggest that studies investigating the effectiveness of crowd sourcing ratings of attention and emotional valence would be particularly valuable as these are notoriously difficult to categorise using classic methods.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33705</offset><text>In conclusion, we suggest that crowdsourcing can offer reliable assessments of the strength of response to stimuli and that it is a useful tool which could also benefit other behavioural observation studies.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>33913</offset><text>Supplementary Information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>33939</offset><text>Below is the link to the electronic supplementary material.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>33999</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>34016</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>34135</offset><text>Author contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>34156</offset><text>All authors contributed to the design of the study, HRG, LB, JF, and ATK collected and analysed the data, HRG performed the statistical analysis and wrote the manuscript, all authors edited and contributed to the final draft.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>34382</offset><text>Data availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>34400</offset><text>The videos are available via the Dryad depository under the title ‘Using a new video rating tool to crowd source analysis of the behavioural reaction to stimuli’: https://doi.org/10.5061/dryad.rbnzs7h95</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>34607</offset><text>Compliance with ethical standards</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>34641</offset><text>Conflicts of interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>34663</offset><text>The authors have no conflicts of interest or competing interests.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>34729</offset><text>Ethics approval</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>34745</offset><text>The data for the study was originally collected for a separate project, published as Root-Gutteridge et al.. Ethics were granted by the Animal Welfare Ethical Review Board (AWERB) at the University of Sussex under permit number ARG/04/04 for the video collection. Approval to use human participants to rate data was also obtained from the University of Sussex Life Sciences and Psychology Cluster based Research Ethics Committee at the University of Sussex under permit numbers HR/236/3 and HR/236/7 for the rating of the videos.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>35275</offset><text>Consent for publication</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>35299</offset><text>All authors consent to publication and have approved the final version of the manuscript.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>35389</offset><text>References</text></passage><passage><infon key="fpage">434</infon><infon key="lpage">446</infon><infon key="name_0">surname:Banks;given-names:EM</infon><infon key="pub-id_doi">10.2527/jas1982.542434x</infon><infon key="pub-id_pmid">7076599</infon><infon key="section_type">REF</infon><infon key="source">J Anim Sci</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">1982</infon><offset>35400</offset><text>Behavioral research to answer questions about animal welfare</text></passage><passage><infon key="name_0">surname:Bland;given-names:JM</infon><infon key="name_1">surname:Altman;given-names:DG</infon><infon key="pub-id_doi">10.1136/bmj.314.7080.572</infon><infon key="pub-id_pmid">9302962</infon><infon key="section_type">REF</infon><infon key="source">BMJ</infon><infon key="type">ref</infon><infon key="year">1997</infon><offset>35461</offset><text>Statistics notes: Cronbach’s alpha</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Bloom;given-names:T</infon><infon key="name_1">surname:Friedman;given-names:H</infon><infon key="pub-id_doi">10.1016/j.beproc.2013.02.010</infon><infon key="pub-id_pmid">23485925</infon><infon key="section_type">REF</infon><infon key="source">Behav Processes</infon><infon key="type">ref</infon><infon key="volume">96</infon><infon key="year">2013</infon><offset>35498</offset><text>Classifying dogs’ (Canis familiaris) facial expressions from photographs</text></passage><passage><infon key="fpage">1436</infon><infon key="lpage">1437</infon><infon key="name_0">surname:Bonney;given-names:R</infon><infon key="name_1">surname:Shirk;given-names:JL</infon><infon key="name_2">surname:Phillips;given-names:TB</infon><infon key="pub-id_doi">10.1126/science.1251554</infon><infon key="pub-id_pmid">24675940</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">343</infon><infon key="year">2014</infon><offset>35573</offset><text>Next steps for citizen science</text></passage><passage><infon key="fpage">305</infon><infon key="lpage">307</infon><infon key="name_0">surname:Bonter;given-names:DN</infon><infon key="name_1">surname:Cooper;given-names:CE</infon><infon key="pub-id_doi">10.1890/110273</infon><infon key="section_type">REF</infon><infon key="source">Front Ecol Environ</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2012</infon><offset>35604</offset><text>Data validation in citizen science: a case study from Project FeederWatch</text></passage><passage><infon key="fpage">28</infon><infon key="lpage">41</infon><infon key="name_0">surname:Cox;given-names:J</infon><infon key="name_1">surname:Oh;given-names:EY</infon><infon key="name_2">surname:Simmons;given-names:B</infon><infon key="pub-id_doi">10.1109/MCSE.2015.65</infon><infon key="section_type">REF</infon><infon key="source">Comput Sci Eng</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2015</infon><offset>35678</offset><text>Defining and measuring success in online citizen science: a case study of zooniverse projects</text></passage><passage><infon key="fpage">384</infon><infon key="lpage">393</infon><infon key="name_0">surname:Desell;given-names:T</infon><infon key="name_1">surname:Goehner;given-names:K</infon><infon key="name_2">surname:Andes;given-names:A</infon><infon key="pub-id_doi">10.1016/j.procs.2015.05.258</infon><infon key="section_type">REF</infon><infon key="source">Procedia Comput Sci</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2015</infon><offset>35772</offset><text>On the effectiveness of crowd sourcing avian nesting video analysis at Wildlife@Home</text></passage><passage><infon key="name_0">surname:Droege;given-names:S</infon><infon key="pub-id_doi">10.2190/PM.49.1.b</infon><infon key="section_type">REF</infon><infon key="source">Citiz Sci Toolkit Conf</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>35857</offset><text>Just because you paid them doesn’t mean their data are better</text></passage><passage><infon key="fpage">471</infon><infon key="lpage">476</infon><infon key="name_0">surname:Gardiner;given-names:MM</infon><infon key="name_1">surname:Allee;given-names:LL</infon><infon key="name_2">surname:Brown;given-names:PM</infon><infon key="pub-id_doi">10.1890/110185</infon><infon key="section_type">REF</infon><infon key="source">Front Ecol Environ</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2012</infon><offset>35921</offset><text>Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs</text></passage><passage><infon key="name_0">surname:Guo;given-names:K</infon><infon key="name_1">surname:Li;given-names:Z</infon><infon key="name_2">surname:Yan;given-names:Y</infon><infon key="name_3">surname:Li;given-names:W</infon><infon key="pub-id_doi">10.1007/s00221-019-05574-3</infon><infon key="pub-id_pmid">31646349</infon><infon key="section_type">REF</infon><infon key="source">Exp Brain Res</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>36016</offset><text>Viewing heterospecific facial expressions: an eye-tracking study of human and monkey viewers</text></passage><passage><infon key="name_0">surname:Gwet;given-names:KL</infon><infon key="section_type">REF</infon><infon key="source">Handbook of inter-rater reliability: the definitive guide to measuring the extent of agreement among raters</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>36109</offset></passage><passage><infon key="name_0">surname:Hecht;given-names:J</infon><infon key="name_1">surname:Spicer Rice;given-names:E</infon><infon key="pub-id_doi">10.1016/j.beproc.2014.10.014</infon><infon key="pub-id_pmid">25444773</infon><infon key="section_type">REF</infon><infon key="source">Behav Processes</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>36110</offset><text>Citizen science: a new direction in canine behavior research</text></passage><passage><infon key="fpage">919</infon><infon key="lpage">930</infon><infon key="name_0">surname:Kelly;given-names:T</infon><infon key="name_1">surname:Reby;given-names:D</infon><infon key="name_2">surname:Levréro;given-names:F</infon><infon key="pub-id_doi">10.1093/biolinnean/blw016</infon><infon key="section_type">REF</infon><infon key="source">Biol J Linn Soc</infon><infon key="type">ref</infon><infon key="volume">120</infon><infon key="year">2017</infon><offset>36171</offset><text>Adult human perception of distress in the cries of Bonobo, chimpanzee, and human infants</text></passage><passage><infon key="fpage">155</infon><infon key="lpage">163</infon><infon key="name_0">surname:Koo;given-names:TK</infon><infon key="name_1">surname:Li;given-names:MY</infon><infon key="pub-id_doi">10.1016/j.jcm.2016.02.012</infon><infon key="pub-id_pmid">27330520</infon><infon key="section_type">REF</infon><infon key="source">J Chiropr Med</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2016</infon><offset>36260</offset><text>A guideline of selecting and reporting intraclass correlation coefficients for reliability research</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>36360</offset><text>Law E, Gajos KZ, Wiggins A, et al (2017) Crowdsourcing as a tool for research. In: proceedings of the 2017 ACM conference on computer supported cooperative work and social computing: CSCW ’17. ACM Press, New York, USA 1544–1561</text></passage><passage><infon key="name_0">surname:Maréchal;given-names:L</infon><infon key="name_1">surname:Levy;given-names:X</infon><infon key="name_2">surname:Meints;given-names:K</infon><infon key="name_3">surname:Majolo;given-names:B</infon><infon key="pub-id_doi">10.7717/peerj.3413</infon><infon key="pub-id_pmid">28584731</infon><infon key="section_type">REF</infon><infon key="source">PeerJ</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>36592</offset><text>Experience-based human perception of facial expressions in Barbary macaques (Macaca sylvanus )</text></passage><passage><infon key="fpage">296</infon><infon key="lpage">305</infon><infon key="name_0">surname:Marois;given-names:R</infon><infon key="name_1">surname:Ivanoff;given-names:J</infon><infon key="pub-id_doi">10.1016/j.tics.2005.04.010</infon><infon key="pub-id_pmid">15925809</infon><infon key="section_type">REF</infon><infon key="source">Trends Cogn Sci</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2005</infon><offset>36687</offset><text>Capacity limits of information processing in the brain</text></passage><passage><infon key="fpage">1425</infon><infon key="lpage">1433</infon><infon key="name_0">surname:Marsh;given-names:DM</infon><infon key="name_1">surname:Hanlon;given-names:TJ</infon><infon key="pub-id_doi">10.1016/j.anbehav.2004.02.017</infon><infon key="section_type">REF</infon><infon key="source">Anim Behav</infon><infon key="type">ref</infon><infon key="volume">68</infon><infon key="year">2004</infon><offset>36742</offset><text>Observer gender and observation bias in animal behaviour research: experimental tests with red-backed salamanders</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">14</infon><infon key="name_0">surname:Meagher;given-names:RK</infon><infon key="pub-id_doi">10.1016/j.applanim.2009.02.026</infon><infon key="section_type">REF</infon><infon key="source">Appl Anim Behav Sci</infon><infon key="type">ref</infon><infon key="volume">119</infon><infon key="year">2009</infon><offset>36856</offset><text>Observer ratings: validity and value as a tool for animal welfare research</text></passage><passage><infon key="fpage">88</infon><infon key="lpage">98</infon><infon key="name_0">surname:Mirkó;given-names:E</infon><infon key="name_1">surname:Kubinyi;given-names:E</infon><infon key="name_2">surname:Gácsi;given-names:M</infon><infon key="name_3">surname:Miklósi;given-names:Á</infon><infon key="pub-id_doi">10.1016/J.APPLANIM.2012.02.016</infon><infon key="section_type">REF</infon><infon key="source">Appl Anim Behav Sci</infon><infon key="type">ref</infon><infon key="volume">138</infon><infon key="year">2012</infon><offset>36931</offset><text>Preliminary analysis of an adjective-based dog personality questionnaire developed to measure some aspects of personality in the domestic dog (Canis familiaris)</text></passage><passage><infon key="fpage">20190555</infon><infon key="name_0">surname:Root-Gutteridge;given-names:H</infon><infon key="name_1">surname:Ratcliffe;given-names:VF</infon><infon key="name_2">surname:Korzeniowska;given-names:AT</infon><infon key="name_3">surname:Reby;given-names:D</infon><infon key="pub-id_doi">10.1098/rsbl.2019.0555</infon><infon key="pub-id_pmid">31795850</infon><infon key="section_type">REF</infon><infon key="source">Biol Lett</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2019</infon><offset>37092</offset><text>Dogs perceive and spontaneously normalize formant-related speaker and vowel differences in human speech sounds</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Scheumann;given-names:M</infon><infon key="name_1">surname:Hasting;given-names:AS</infon><infon key="name_2">surname:Kotz;given-names:SA</infon><infon key="name_3">surname:Zimmermann;given-names:E</infon><infon key="pub-id_doi">10.1371/journal.pone.0091192</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>37203</offset><text>The voice of emotion across species: how do human listeners recognize animals’ affective states?</text></passage><passage><infon key="name_0">surname:Stewart;given-names:L</infon><infon key="name_1">surname:MacLean;given-names:EL</infon><infon key="name_2">surname:Ivy;given-names:D</infon><infon key="pub-id_doi">10.1371/journal.pone.0135176</infon><infon key="pub-id_pmid">26716874</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>37302</offset><text>Citizen science as a new tool in dog cognition research</text></passage><passage><infon key="fpage">159</infon><infon key="lpage">169</infon><infon key="name_0">surname:Tami;given-names:G</infon><infon key="name_1">surname:Gallagher;given-names:A</infon><infon key="pub-id_doi">10.1016/j.applanim.2009.06.009</infon><infon key="section_type">REF</infon><infon key="source">Appl Anim Behav Sci</infon><infon key="type">ref</infon><infon key="volume">120</infon><infon key="year">2009</infon><offset>37358</offset><text>Description of the behaviour of domestic dog (Canis familiaris) by experienced and inexperienced people</text></passage><passage><infon key="fpage">273</infon><infon key="lpage">280</infon><infon key="name_0">surname:Tuyttens;given-names:FAMAM</infon><infon key="name_1">surname:de Graaf;given-names:S</infon><infon key="name_2">surname:Heerkens;given-names:JLTLT</infon><infon key="pub-id_doi">10.1016/j.anbehav.2014.02.007</infon><infon key="section_type">REF</infon><infon key="source">Anim Behav</infon><infon key="type">ref</infon><infon key="volume">90</infon><infon key="year">2014</infon><offset>37462</offset><text>Observer bias in animal behaviour research: can we believe what we score, if we score what we believe?</text></passage><passage><infon key="fpage">190</infon><infon key="lpage">206</infon><infon key="name_0">surname:Vazire;given-names:S</infon><infon key="name_1">surname:Gosling;given-names:SD</infon><infon key="name_2">surname:Dickey;given-names:AS</infon><infon key="name_3">surname:Schapiro;given-names:SJ</infon><infon key="section_type">REF</infon><infon key="source">Handbook of Research Methods in Personality Psychology</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>37565</offset><text>Measuring personality in nonhuman animals</text></passage><passage><infon key="fpage">54</infon><infon key="lpage">59</infon><infon key="name_0">surname:Waller;given-names:BM</infon><infon key="name_1">surname:Micheletta;given-names:J</infon><infon key="pub-id_doi">10.1177/1754073912451503</infon><infon key="section_type">REF</infon><infon key="source">Emot Rev</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2013</infon><offset>37607</offset><text>Facial expression in nonhuman animals</text></passage><passage><infon key="name_0">surname:Wan;given-names:M</infon><infon key="name_1">surname:Bolger;given-names:N</infon><infon key="name_2">surname:Champagne;given-names:FA</infon><infon key="pub-id_doi">10.1371/journal.pone.0051775</infon><infon key="pub-id_pmid">23284925</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>37645</offset><text>Human perception of fear in dogs varies according to experience with dogs</text></passage><passage><infon key="fpage">457</infon><infon key="lpage">465</infon><infon key="name_0">surname:Worsley;given-names:HK</infon><infon key="name_1">surname:O’Hara;given-names:SJ</infon><infon key="pub-id_doi">10.1007/s10071-018-1181-3</infon><infon key="pub-id_pmid">29713846</infon><infon key="section_type">REF</infon><infon key="source">Anim Cogn</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2018</infon><offset>37719</offset><text>Cross-species referential signalling events in domestic dogs (Canis familiaris)</text></passage></document></collection>
