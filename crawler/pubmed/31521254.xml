<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201223</date><key>pmc.key</key><document><id>6855300</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1016/j.artmed.2019.06.004</infon><infon key="article-id_pmc">6855300</infon><infon key="article-id_pmid">31521254</infon><infon key="article-id_publisher-id">S0933-3657(18)30259-8</infon><infon key="fpage">77</infon><infon key="kwd">Mobile games Computer vision Autism Emotion Emotion classification</infon><infon key="license">This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</infon><infon key="lpage">86</infon><infon key="name_0">surname:Kalantarian;given-names:Haik</infon><infon key="name_1">surname:Jedoui;given-names:Khaled</infon><infon key="name_2">surname:Washington;given-names:Peter</infon><infon key="name_3">surname:Tariq;given-names:Qandeel</infon><infon key="name_4">surname:Dunlap;given-names:Kaiti</infon><infon key="name_5">surname:Schwartz;given-names:Jessey</infon><infon key="name_6">surname:Wall;given-names:Dennis P.</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">98</infon><infon key="year">2019</infon><offset>0</offset><text>Labeling images with facial emotion and the potential for pediatric healthcare</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>79</offset><text>Highlights</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>90</offset><text>Autism spectrum disorder (ASD) affects 750,000 American Children under the age of 10.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>176</offset><text>Emotion classifiers integrated into mobile solutions can be used for screening and therapy.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>268</offset><text>Emotion classifiers do not generalize well to children due to a lack of labeled training data.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>363</offset><text>We propose a method of aggregating emotive video through a mobile game.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>435</offset><text>We demonstrate that several algorithms can automatically label frames from video derived from the game.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>539</offset><text>Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by repetitive behaviors, narrow interests, and deficits in social interaction and communication ability. An increasing emphasis is being placed on the development of innovative digital and mobile systems for their potential in therapeutic applications outside of clinical environments. Due to recent advances in the field of computer vision, various emotion classifiers have been developed, which have potential to play a significant role in mobile screening and therapy for developmental delays that impair emotion recognition and expression. However, these classifiers are trained on datasets of predominantly neurotypical adults and can sometimes fail to generalize to children with autism. The need to improve existing classifiers and develop new systems that overcome these limitations necessitates novel methods to crowdsource labeled emotion data from children. In this paper, we present a mobile charades-style game, Guess What?, from which we derive egocentric video with a high density of varied emotion from a 90-second game session. We then present a framework for semi-automatic labeled frame extraction from these videos using meta information from the game session coupled with classification confidence scores. Results show that 94%, 81%, 92%, and 56% of frames were automatically labeled correctly for categories disgust, neutral, surprise, and scared respectively, though performance for angry and happy did not improve significantly from the baseline.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2088</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2101</offset><text>Autism spectrum disorder (ASD) is a neurodevelopmental disorder affecting an individual's ability to communicate and interact with their peers. While symptoms vary, this condition is generally characterized by stereotyped and repetitive behaviors as well as deficits in social interaction and communication ability such as difficulty recognizing facial expressions, making eye contact, and engaging in social activities with peers. In recent years, the incidence of autism has increased; it is now estimated that one in 40 children in the United States are affected by this condition. While there is no cure, an abundance of evidence has demonstrated the positive impact of early intervention on communication skills and language ability.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2840</offset><text>Common approaches to autism therapy include the Early Start Denver Model (ESDM) and Applied Behavior Analysis (ABA). ESDM therapy supports the development of core social skills through interactions with a licensed behavioral therapist with an emphasis on interpersonal exchange and joint activities. Similarly, ABA therapy is an intervention customized by a trained behavioral analyst to specifically suit the learner's skills and deficits. This program is based on a series of structured activities that emphasize the development of transferable skills to the real world. While both treatments have been shown to be safe and effective, early intervention is essential to maximize the benefits of these programs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3553</offset><text>Despite significant progress in recent years, imbalances in coverage and barriers to diagnosis and treatment remain. Within the United States, it has been observed that children in rural cities receive diagnosis approximately five months later than those in cities. Moreover, children from families near the poverty line receive diagnosis almost a full year later than those from higher-income families. These delays can defer intervention during times of development considered crucial for maximizing the effectiveness of subsequent behavioral interventions. Alternative solutions that can ameliorate some of these challenges can come from digital and mobile tools, many of which are reliant on computer vision technology that has found increasing application in real-time social support and therapy.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4355</offset><text>Emotion classification is an area of computer vision that emphasizes the development of algorithms that produce an emotion label such as happy or sad given a photo or video frame containing a face using machine-learning techniques. Our prior work, the Superpower Glass Project, has demonstrated the efficacy of real-time emotion classification to autism therapy via the augmented reality wearable, Google Glass. The Glass unit relays emotion cues in real-time to the child, enabling facial engagement and social reciprocity. Others have also explored the use of wearable systems and affective computing as companion tools for social-emotional learning and the use of the recorded videos for defining a process to collect, segment, label, and use video clips from everyday conversations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5142</offset><text>We present a mobile charades-style game, Guess What?, designed for a young audience, including those with ASD, from which we can scalably acquire egocentric video with a high density of varied emotion.</text></passage><passage><infon key="file">gr1.jpg</infon><infon key="id">fig0005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5344</offset><text>The proposed system is a method to aggregate labeled emotion data from videos derived from a mobile game using classification confidence values and contextual meta-information.</text></passage><passage><infon key="file">gr1.jpg</infon><infon key="id">fig0005</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>5521</offset><text>Fig. 1</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5528</offset><text>We present a framework for semi-automatic labeled frame extraction from videos derived from Guess What? using meta information from the game session coupled with classification confidence scores, shown in Fig. 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5741</offset><text>We present a search algorithm which aims to simultaneously optimize the aggregate number of frames retained as well as the percentage of relevant frames.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5895</offset><text>A number of emotion classifiers have been developed in recent years by major providers of cloud services including Microsoft Azure Cognitive Services API, Amazon Rekognition, Google Cloud Vision, and others. These algorithms, which typically label an image based on some variation of the seven Ekman emotions, are trained on large databases of labeled images such as CIFAR-100 and ImageNet. Datasets specific to facial emotion are also available, such as the Cohn-Kanade Database and Belfast-Induced Natural Emotion Databases. These datasets suffer from a variety of limitations, among which is a lack of generalizability to children: a population significantly underrepresented in these sources. This problem is exacerbated within the domain of autism research, as children with this condition struggle with facial affect and may express themselves in ways that do not closely resemble that of their peers. These variances are unaccounted for in most datasets, rendering some state-of-the-art emotion classifiers unsuitable for vision-based autism research and the development of therapies and assistive solutions derived from these tools. This motivates the development of new approaches for scalable aggregation of emotive frames from children that can be used to design future classifiers and augment existing ones. The primary contributions of this paper are as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>7271</offset><text>Related work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7284</offset><text>Our primary aim is to develop methods to crowdsource facial-emotion labeled data from children with ASD, with the greater goal of training classifiers suitable for the pediatric population for use as outcome measures, therapies, and screening tools. These systems fall within the scope of affective computing: a field that broadly covers the development and application of methods to give computers the ability to recognize and express emotions. An overview of this area was provided in, in which Picard described emerging trends in emotion recognition research using electrodermal activity, speech, motion, facial expression, and other sensing paradigms. Picard outlined a vision for future affective computing research that partners psychologists with engineers to interweave emotion detection into everyday life.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8100</offset><text>Various research efforts have explored whether children with ASD differ in their ability to emote compared to their neurotypical peers. For example, Brewer et al. investigated if individuals with and without ASD can correctly identify emotional facial expressions. The results indicated that regardless of the status of the recognizer, emotions produced by individuals with ASD were more poorly recognized compared to their typically developing peers. By contrast, Faso et al. conducted a study in which 38 observers evaluate the expressions of individuals with and without ASD and showed that ASD expressions were identified with greater accuracy, though they were rated as less natural and more intense compared to those from typically developing individuals. In another study, Capps et al. explored parents’ perceptions of the emotional expressiveness of their children. The findings of this study contradict older studies which suggest an absence of emotional reactions from children with ASD. In fact, the results demonstrated that older children with ASD displayed more facial affect than typically developing children. Other research efforts examine facial muscle movements associated with emotion expression in children with ASD based on videotapes from semi-structured play sessions. This study found that children with autism exhibited reduced muscle movements in certain facial regions compared to typically developing peers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9539</offset><text>While several systems have been developed to help children recognize and express facial emotion, other studies focused on improving the ability of neurotypical children and adults to interact with individuals with ASD. For example, Tang et al. described an IoT-based play environment designed to allow neurotypical children to better understand the emotions of their peers with autism using a variety of sensors including pressure, temperature, humidity, and a Kinect camera. The authors later conducted a computational study in which they evaluated children's facial expressions during naturalistic tasks in which the children view cartoons while being recorded by a Kinect camera. As before, the aim of this preliminary study was to develop tools to assist typically developing individuals in understanding the emotions of children with autism.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10386</offset><text>More broadly, Aztiria et al. provided an overview of the field of affect aware ambient intelligence. The authors describe the various forms of affect that can be characterized using wearable and ambient sensors, including voice, body language, posture, and physiological signals such as EEG and EMG. This work provided a broad overview of these techniques as well as several relevant applications such as intelligent tutoring services (ITS)-systems capable of recognizing student affect to assist in the student's learning process. Further work by Karyotis et al. proposed a computational methodology for incorporating emotion into intelligence system design, validated through multiple simulations. The authors proposed a fuzzy emotion representation framework, and demonstrated its utility in big data applications such as social networks, data queries, and sentiment analysis. The work by Maniak et al. proposed a deep neural network model for hierarchical feature extraction to model human reasoning within the context of sound classification.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11434</offset><text>In recent years, computer vision-based systems have received increasing interest in ASD research. In, Marcu et al. proposed a system in which wearable cameras are affixed to children for understanding their needs and preferences while improving their engagement. In, Picard et al. provided an overview of methods to automatically detect autonomic nervous-system activation (ASM) in children with ASD to identify and avoid incidents of cognitive overload. Another mobile assistance technology, MOSOC, was presented by Escobedo et al. in. Here, the authors developed a tool that provides visual support of a validated curriculum to help children with ASD practice social skills in real-life situations. These systems are indicative of a general transition from traditional healthcare practices to modern mobile and digital solutions that leverage recent advances in computer vision, augmented reality, robotics, and artificial intelligence. This trend motivates an investigation of methods to augment existing datasets to train new classifiers that generalize to children with ASD.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12514</offset><text>Several methods of crowdsourced labeled data acquisition have been proposed in recent years. In, Barsoum et al. proposed a deep convolutional neural network architecture to evaluate four different labeling techniques. Specifically, the authors explored techniques to combine scores from ten raters into a final label for each image while minimizing errors. Other research efforts have also explored the efficacy of multi-class labels for each image to mitigate the impact of ambiguities on data labeling. In, Yu et al. demonstrated that an ensemble of deep learning classifiers can significantly outperform a single classifier for facial emotion recognition. This approach is similar to our own ensemble method, though our technique fuses minimum likelihood with game meta information rather than assigning the label with the maximum probability. This technique, which used variations in probability scores to search for relevant frames and regions within time-series data are inspired partially by prior work on time-series segmentation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>13553</offset><text>System architecture</text></passage><passage><infon key="file">gr2.jpg</infon><infon key="id">fig0010</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>13573</offset><text>The mechanism for crowdsourcing emotion-labeled frames is a mobile charades game available for Android devices.</text></passage><passage><infon key="file">gr2.jpg</infon><infon key="id">fig0010</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>13685</offset><text>Fig. 2</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13692</offset><text>Guess What?  is a mobile Android application modeled after the popular charades game, Heads Up. This social gaming activity is shared between the child, who attempts to act out the prompt shown on the screen, and the parent, who holds the phone up to record the child and attempts to guess the word associated with the prompt. This interplay is shown in Fig. 2: the parent positions the phone with the screen facing outward for the entirety of the 90 s game session, as the front camera records the child tasked with representing the prompt using a combination of gestures and facial expressions. The prompt consists of an image with an associated word displayed at the bottom. While several categories of prompt are supported, the two most germane to emotion recognition and expression are emoji, which shows exaggerated cartoon representations of emotive faces, and faces, which shows real photos of children.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14606</offset><text>The parent can change the prompt by tilting the phone forward, which awards a point. This occurs when the child acknowledges the correct guess, or in some cases, when the parent makes the determination that the prompt has been represented correctly based on a priori knowledge about the image shown. By tilting the phone backward, the prompt is skipped without awarding a point. Immediately thereafter, a new prompt is randomly selected until the 90 s have elapsed. After the game session, parents can review the footage and elect to share the data by uploading the video to an IRB-approved secure Amazon S3 bucket that is fully compliant with Stanford University's High-Risk Application security standards. Meta information is included with the video, which describes the prompts shown, timing data, and the number of points awarded. Using this method of crowdsourced at-home video acquisition, we are developing a database of children with ASD as well as neurotypical children as they express themselves in response to various stimuli.</text></passage><passage><infon key="file">gr3.jpg</infon><infon key="id">fig0015</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15646</offset><text>A game session of Guess What? in which the child is recorded while acting out the prompt shown on the screen.</text></passage><passage><infon key="file">gr3.jpg</infon><infon key="id">fig0015</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>15756</offset><text>Fig. 3</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15763</offset><text>An example of the main game screen is shown in Fig. 3: the prompt is shown in the center, with the amount of time remaining displayed on the left and the number of points awarded on the right. This particular prompt is associated with the faces category, which is among the most efficacious at deriving emotive facial expressions from children. By contrast, the animals category emphasizes vocalizations and sports is associated with gestures.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>16207</offset><text>Algorithms</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16218</offset><text>Videos derived from Guess What? can be analyzed frame-by-frame by manual raters to assign emotion labels to each image. However, this approach is tedious and presents an impediment to the scalability of a crowdsource-based system for aggregation of emotive video. In this section, we present several strategies for scalable aggregation of labeled frames from Guess What? game sessions using automatic or semi-automatic techniques that leverage both the video and the accompanying meta information from the game session.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>16738</offset><text>Boundary-based segmentation</text></passage><passage><infon key="file">gr4.jpg</infon><infon key="id">fig0020</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16766</offset><text>The structure of a single video is characterized by its boundary points, Bi through Bk, which identify the times at which various prompts were shown to the child.</text></passage><passage><infon key="file">gr4.jpg</infon><infon key="id">fig0020</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>16929</offset><text>Fig. 4</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16936</offset><text>The structure of a Guess What? video is shown in Fig. 4. Meta information uploaded after each game session delineates the video into regions at which various prompts were shown. For example, frames associated with times at which Prompt 2 was displayed to the child can be found between timestamps B2 and B3. If Prompt 2 is an emotion-related image, this approach is a reasonable starting point to automatically obtain labeled frames associated with this emotion. More formally, for each emotion we are interested in every frame f between a boundary point bf and the subsequent boundary, bf+1, at which the emotion of the boundary, e(bf), matches the emotion we wish to extract, label. These conditions are expressed in Eq. (1), where t is a function that returns the time associated with a frame or boundary point.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17751</offset><text>While regions contain a preponderance of emotive frames associated with the prompt shown during this interval, it is unlikely that young children will consistently emote the appropriate emotion during the entirety of the game session. This is particularly true for children with developmental delays who may struggle to recognize, interpret, and convey emotion. Moreover, children may misunderstand their parent's instructions or lose interest in continuing the game session. This motivates additional optimizations to further increase the percentage of retrieved frames that match the emotion of interest. Notwithstanding the possibility of further refinements, this approach in its current form will generally suffice for semi-automatic labeling approaches: scenarios in which the algorithm retrieves a set of likely frames and manual raters filter out incorrect matches.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18625</offset><text>Sub-bound analysis</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18644</offset><text>While the representation of a video's structure shown in Fig. 4 provides a rudimentary method of identifying high-density regions of various emotions, this model is too simplistic. In practice, there is an interval α between the time when the prompt changes and the child's face adjusts accordingly. During this interpretation period, a child will analyze the provided prompt as their face transitions from a typically neutral or happy expression to one associated with the prompt. In theory, complex prompts will require more time for interpretation than the simpler ones: this parameter varies both between subjects and prompts.</text></passage><passage><infon key="file">gr5.jpg</infon><infon key="id">fig0025</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19279</offset><text>The density of emotion within the video is highest if the leading and trailing frames of the boundary region, α and β, are cropped.</text></passage><passage><infon key="file">gr5.jpg</infon><infon key="id">fig0025</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>19418</offset><text>Fig. 5</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19425</offset><text>If the child has correctly represented the prompt, there is a time period β before the beginning of the next prompt when the frames are of little use. There are two possible reasons why these frames are best excluded from our analysis. First, the child's face may naturally return to a resting pose in anticipation of the next prompt. Second, the game mechanics of Guess What? require the parents to tilt the phone in acknowledgement of a correct guess. In practice, the act of tilting may cause the child's face to briefly leave the frame. The video structure that considers these α and β parameters is shown in Fig. 5.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20056</offset><text>Boundary search algorithm.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20083</offset><text>Unlike the previous scenario, we are now interested in every frame f between a boundary point bf + α and the subsequent boundary, bf+1 − β, at which the emotion of the prompt shown in the region, e(bf), matches the emotion of the frame we wish to extract, label. These conditions are expressed in Eq. (2), where as before, t is a function that returns the time associated with a frame or boundary point.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20499</offset><text>However, increasing the α and β parameters excessively has the potential to discard potentially relevant frames while offering only marginal improvements to emotion density. We have devised an algorithm to account for these two tradeoffs, which is shown in simplified form in Algorithm 1. The algorithm is initialized with default values, α = 0 and β = 0. During each step, we evaluate the effects of incrementing α and β on the increase in percentage of relevant frames and decrease in total number of available frames, the ratio of these two parameters being denoted by k. A value of k = 1 indicates that the accuracy improved from the baseline by the same margin that the number of frames decreased, which for our application is an acceptable tradeoff. A value of less than 1 suggests marginal improvements to accuracy or perhaps a regression, which is the terminating condition for this algorithm. It should be noted that this algorithm is run on a class-by-class basis to determine optimal α and β values for each prompt. This decision is motivated by the observation that more complex prompts will require more time to interpret and correctly emote.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21683</offset><text>Minimum confidence</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21702</offset><text>While the sub-bounds search technique outlined previously can further increase the percentage of frames that match the prompt by filtering out those in the periphery of the region, it remains unlikely that the remaining frames will be associated with the same category as some children may fail to correctly interpret or represent the prompt even within the center of region. This is particularly true for non-trivial prompts that are challenging for children with developmental delays. To further filter out incorrect prompts within the highest-density region with limited manual burden would require an automatic system that can determine if a frame matches the prompt shown to the child. Clearly, no such system exists, due to the lack of labeled data that motivates this work.</text></passage><passage><infon key="file">gr6.jpg</infon><infon key="id">fig0030</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>22483</offset><text>To further improve the percentage of correctly labeled frames, we retain all frames within the region of interest that have a minimum classification confidence of λ.</text></passage><passage><infon key="file">gr6.jpg</infon><infon key="id">fig0030</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>22654</offset><text>Fig. 6</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22661</offset><text>To overcome the limitations of existing emotion classifiers while still leveraging their capabilities, we propose a system in which the classification confidence of the emotion associated with the currently shown prompt acts as a filtering mechanism to eliminate irrelevant frames within the region of interest. While the performance of the classifier is insufficient for us to exclude a frame in which the emotion with the highest classification confidence is discordant with our a priori knowledge of the displayed prompt, an extremely low confidence score may still be sufficient grounds for exclusion. This approach is shown in Fig. 6; frames are retained only when located within the highest density region, and when the emotion classifier indicates that the probability of agreement between the emotion in the frame and that of the region exceeds λ. Using the same notation as before, Eq. (3) formalizes our approach for retaining frames associated with a specific category, label.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23654</offset><text>Min. confidence algorithm.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23681</offset><text>To obtain Pr(f = e(bf)), the probability that the frame matches the emotion of the prompt shown within this region, we use the Azure Faces API provided by Microsoft Azure Cognitive Services. Given an image transmitted via HTTP request, this API returns an HTTP response containing JSON formatted information about the classification confidences associated with each supported emotion, between 0 and 1. It is important to individually determine λ for each class, as classifier sensitivities may be carefully tuned to account for class priors in naturalistic settings that do not generalize to mobile gameplay. This approach, shown in Algorithm 2, is similar to the optimization problem for α and β; as before, we attempt to optimize the density of relevant frames within the region while avoiding significant decreases in the total number of relevant frames by using the ratio of these two parameters as the terminating condition for the iterative algorithm that returns the final λ for each emotion class.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>24704</offset><text>Ensemble classification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24728</offset><text>A limitation of the previous method is that this technique is too tightly coupled to the nuances of a particular classifier. While indication of a non-zero likelihood of a certain emotion within a frame can be efficacious for making a determination to filter or retain a frame, it is also possible that a classifier reports a 0% likelihood for an emotion that is clearly within the frame. By using classification confidence scores from multiple classifiers, the impact of these anomalies can be mitigated; each classifier's unique nuances can be effectively averaged out to improve the robustness of our filtering algorithm.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25353</offset><text>Max: Selecting the maximum classification confidence from all three classifiers for the emotion of interest is a viable choice for classifiers tuned for high precision and low recall.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25537</offset><text>Min: Selecting the minimum classification confidence from all three classifiers for the emotion of interest is suitable for classifiers tuned for high recall and low precision.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25714</offset><text>Average: A non-weighted average would be suitable to smooth out the precision/recall biases without requiring careful characterization of their performance.</text></passage><passage><infon key="file">gr7.jpg</infon><infon key="id">fig0035</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25871</offset><text>Architecture of the ensemble classification approach.</text></passage><passage><infon key="file">gr7.jpg</infon><infon key="id">fig0035</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>25925</offset><text>Fig. 7</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25932</offset><text>This ensemble-based approach, which also leverages the sub-bounds search algorithm described previously, is shown in Fig. 7. In addition to AWS, confidence scores are derived from two additional classifiers: Sighthound and Amazon Rekognition. Given three sets of classification confidence scores that are normalized between 0 (minimum confidence) and 1 (maximum confidence), several simple methods can be employed to combine this information into a single value that will be compared to λ to make a final filtering decision.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26462</offset><text>Regardless of the approach used, the combined confidence score for each of these three techniques would be compared to a class-specific λ value.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>26612</offset><text>Experimental methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26633</offset><text>While our long-term objective is to deploy Guess What? as a system for crowdsourcing video, an in-lab study provided the data necessary to validate our framework for automatic labeled data extraction. In this section, we describe our methods to obtain the video that formed the basis of our experiments.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>26937</offset><text>Data collection</text></passage><passage><infon key="file">tbl0015.xml</infon><infon key="id">tbl0015</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>26953</offset><text>List of subjects.</text></passage><passage><infon key="file">tbl0015.xml</infon><infon key="id">tbl0015</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Subject ID&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Age&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Gender&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Diagnosis&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;12&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26971</offset><text>Table 3	 	</text></passage><passage><infon key="file">tbl0015.xml</infon><infon key="id">tbl0015</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Subject ID&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Age&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Gender&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Diagnosis&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;12&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Male&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ASD&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26982</offset><text>Subject ID	Age	Gender	Diagnosis	 	1	9	Male	ASD	 	2	7	Male	ASD	 	3	6	Male	ASD	 	4	8	Male	ASD	 	5	8	Male	ASD	 	6	12	Male	ASD	 	7	10	Male	ASD	 	8	8	Male	ASD	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27139</offset><text>The dataset used in our experiments was derived from a prior study which included eight children with a prior diagnosis of ASD. The children each played several Guess What? games in a single session administered by the same member of our research staff. The average age of participating children with ASD was 8.5 years ±1.85, as shown in Table 3. Due to the non-uniform incidence of autism between genders and small sample size, all participants in this study were boys. During each session, the participant played up to five games with the following decks in no particular order: emoji, faces, animals, sports, and jobs. However, we focus this study on the category most strongly correlated with facial affect, faces, which produced a total of 1080 frames.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>27898</offset><text>Data processing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27914</offset><text>Two raters annotated frames to establish a ground truth to evaluate our automatic labeling algorithms: one student (age 23) and one Postdoctoral Researcher (age 29). Both raters were male, and neither had received any relevant clinical training at the time. An important design decision made during this study was to use non-expert raters: those without clinical experience. The motivation for this decision was twofold. First, prior literature has demonstrated that there may be fundamental differences in how children with autism express emotions, which could affect the ability of individuals to recognize and perceive facial emotion from children with developmental delay. Building a dataset of emotion-labeled frames understandable to clinicians but not by the general population could be detrimental to our long-term objective of building AI-enabled systems to help children develop their ability to communicate with their peers-rather than those with clinical training. Additional factors that motivated this decision were the conclusion drawn from our prior work, which demonstrated that raters without clinical expertise are capable of annotating videos from children with developmental delay with high sensitivity and specificity. These findings are corroborated by the high inter-rater reliability scores between the two raters used in this study, as shown in Fig. 10.</text></passage><passage><infon key="file">tbl0010.xml</infon><infon key="id">tbl0010</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>29294</offset><text>Total frames per category.</text></passage><passage><infon key="file">tbl0010.xml</infon><infon key="id">tbl0010</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Category&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Frames&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Total&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1080&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;506&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;574&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;167&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;127&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;118&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29321</offset><text>Table 2	 	</text></passage><passage><infon key="file">tbl0010.xml</infon><infon key="id">tbl0010</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Category&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Frames&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Total&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1080&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;506&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Non-neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;574&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;167&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;127&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;118&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29332</offset><text>Category	Frames	 	Total	1080	 	Neutral	506	 	Non-neutral	574	 	Happy	167	 	Sad	104	 	Surprised	127	 	Scared	28	 	Disgusted	118	 	Angry	30	 	</text></passage><passage><infon key="file">tbl0010.xml</infon><infon key="id">tbl0010</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>29473</offset><text>The number of frames both manual raters assigned to the same category, for the dataset used in our experiments.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29585</offset><text>The raters manually assigned emotion labels to each frame in the selected videos based on the seven Ekman universal emotions with the addition of a neutral class. In cases when no face could be located within the frame, or the frame was too blurry to discern, reviewers did not assign a label. To simplify annotation and establish a format consistent with commercial emotion classification APIs, the anger and contempt emotions were merged into a single category. Furthermore, the confusion emotion was ignored as not every emotion classifier supported it and no related prompts were shown during these game sessions. A total of 1350 frames were manually labeled by the two raters. Frames were discarded in cases when the raters disagreed or did not assign a label. This produced a total of 1080 frames from the original 1350, distributed between emotions as shown in Table 2.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>30462</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30470</offset><text>In this section, we describe the accuracy of our proposed automatic labeling techniques as well as the inter-rater reliability for the manual annotation that served as the ground truth of our experiments.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>30675</offset><text>Inter-rater reliability</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30699</offset><text>From a total of 1350 frames, 1185 were flagged as valid: frames which both raters agreed were of sufficiently high quality to assign an emotion label. From these 1185 valid frames, the raters assigned the same emotion to 1080 (91%). The Cohen's Kappa statistic for inter-rater reliability, a metric which accounts for agreements due to chance, was 0.10. This indicates a high level of reliability between the two manual raters.</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>31127</offset><text>Abbreviations</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Emotion&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;HP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;AG&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;DG&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;NT&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SC&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31141</offset><text>Table 1	 	</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Emotion&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;HP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;AG&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;DG&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;NT&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SC&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31152</offset><text>	Emotion	 	HP	Happy	 	SD	Sad	 	AG	Angry	 	DG	Disgusted	 	NT	Neutral	 	SC	Scared	 	SP	Surprised	 	</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>31250</offset><text>Abbreviations for emotions used throughout this paper.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31305</offset><text>Fig. 10 shows the distribution of frames between the manual raters, for all valid frames. Most misclassified frames were between the happy-neutral and sad-neutral categories. The abbreviations used in this figure are defined in Table 1.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>31542</offset><text>Distribution of frames</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31565</offset><text>Table 2 shows the total number of frames in each category from all three videos, omitting those frames in which the manual raters disagreed on the label. Frames that are designated as non-neutral refer to those valid frames which have a label other than the neutral class. From the 1080 total frames, 46.8% were neutral compared to 53.1% non-neutral frames. The most represented emotion was happy, with 167 frames, followed by surprised and disgusted with 127 and 118 frames respectively. The two least represented emotions were scared, with 28 frames, and angry, with 30.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>32138</offset><text>Baseline: boundary analysis</text></passage><passage><infon key="file">gr8.jpg</infon><infon key="id">fig0040</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32166</offset><text>A much higher percentage of frames for a given emotion can be found during the times in which the associated prompt was shown on the screen, compared to their prevalence throughout the entire video. This is particularly true for prompts that are otherwise sparse, such as angry and scared.</text></passage><passage><infon key="file">gr8.jpg</infon><infon key="id">fig0040</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>32456</offset><text>Fig. 8</text></passage><passage><infon key="file">gr9.jpg</infon><infon key="id">fig0045</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32463</offset><text>(a) Parameter α refers to the number of frames (at 5 frames per second) skipped at the beginning of the window, while β refers to the number of frames omitted before the end of the window. (b) As parameters α and β are tuned to increase the percentage of correct frames within the boundary, the total number of frames may decrease.</text></passage><passage><infon key="file">gr9.jpg</infon><infon key="id">fig0045</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>32809</offset><text>Fig. 9</text></passage><passage><infon key="file">gr10.jpg</infon><infon key="id">fig0050</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32816</offset><text>The confusion matrix of the two raters assignments of frames into emotion categories.</text></passage><passage><infon key="file">gr10.jpg</infon><infon key="id">fig0050</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>32902</offset><text>Fig. 10</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32910</offset><text>Fig. 8 provides a visualization of the percentage of frames within the boundary region that matched the emotive prompt shown during these times, based on three 90-second video sessions from three children subsampled to five frames per second. While the majority of frames within the disgust and neutral region matched the prompt, performance was poor for happy and scared. As shown in Fig. 9, regions contained a much higher percentage of relevant emotions compared to the prevalence of these emotions throughout the entire video. Moreover, the videos derived from Guess What? contained a reasonable diversity of emotive frames from various categories as shown in Table 2. Naturally, some emotions were more sparsely represented than others; scared and angry were associated with 28 and 30 frames, respectively. However, these disparities can be rectified by modifying the composition of prompts to emphasize these less common emotions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>33847</offset><text>Sub-bound analysis</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33866</offset><text>Results suggest that the central region of the boundary generally has a higher density of relevant frames. Fig. 9A shows the percentage of frames which match the emotion associated with the region as a function of α and β, when optimizing globally rather than on a per-emotion basis. Baseline accuracy was approximately 35%, but increased to 40% with α and β values of 0.8 s and 1.2 s, respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34280</offset><text>Fig. 9B shows the raw number of relevant frames retained within a region that matched the boundary as a function of these two parameters. It is important to carefully consider the possibility of loss of frames when tuning these parameters. For example, choosing a β value of 2.0 s and an α value of 1.6 s reduces the number of relevant frames by over 50%, with only marginal improvements to accuracy.</text></passage><passage><infon key="file">tbl0020.xml</infon><infon key="id">tbl0020</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>34688</offset><text>Optimal parameters per emotion.</text></passage><passage><infon key="file">tbl0020.xml</infon><infon key="id">tbl0020</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Category&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;α&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;β&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;λ&lt;/italic&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2.2 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.02&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.0 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.0 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.8 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>34720</offset><text>Table 4	 	</text></passage><passage><infon key="file">tbl0020.xml</infon><infon key="id">tbl0020</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Category&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;α&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;β&lt;/italic&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;&lt;italic&gt;λ&lt;/italic&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2.2 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.02&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Happy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Surprised&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Scared&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.0 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.0 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disgusted&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1.8 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Angry&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.4 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.6 s&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>34731</offset><text>Category	α	β	λ	 	Neutral	2.2 s	0	0.02	 	Happy	0	0	0.00	 	Sad	0.4 s	0.4 s	0.00	 	Surprised	0.4 s	1.6 s	0.10	 	Scared	1.0 s	1.0 s	0.00	 	Disgusted	0.6 s	1.8 s	0.01	 	Angry	0.4 s	0.6 s	0.00	 	</text></passage><passage><infon key="file">tbl0020.xml</infon><infon key="id">tbl0020</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>34946</offset><text>The number of frames skipped at the beginning and end of the window, α and β, varied per prompt, as did the minimum classification confidence used to filter frames, λ.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35126</offset><text>After optimizing on a per-class basis using Algorithm 1, the value of these parameters is shown in Table 4, and varies widely between prompts. For instance, the happy prompt did not require any trimming. This is likely because many children were smiling throughout the game session, irrespective of the prompt shown. The large α time associated with the neutral class could be caused by the uncertainty a non-emotive class introduced as most other prompts had a clear and perhaps exaggerated emotion associated with them. The large trailing times for disgusted and surprised might be explained by the relative discomfort of maintaining these exaggerated facial expressions for extended periods, though a much larger dataset is necessary to draw definitive conclusions.</text></passage><passage><infon key="file">gr11.jpg</infon><infon key="id">fig0055</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35899</offset><text>Adjusting the α and β parameters did not improve the percentage of correctly classified frames for every prompt, but improved accuracy for scared, neutral and surprised.</text></passage><passage><infon key="file">gr11.jpg</infon><infon key="id">fig0055</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36076</offset><text>Fig. 11</text></passage><passage><infon key="file">gr12.jpg</infon><infon key="id">fig0060</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>36084</offset><text>(a) Retaining only frames which the classifier reports to match the emotion associated with the boundary can dramatically reduce the number of remaining frames for various classes. (b) Retaining only frames which the classifier reports to match the class associated with the boundary region can increase the percentage of relevant frames for some emotions.</text></passage><passage><infon key="file">gr12.jpg</infon><infon key="id">fig0060</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36441</offset><text>Fig. 12</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36449</offset><text>Fig. 11 shows the percentage of matching frames using the sub-bound approach on a per-class basis, with results from this technique denoted by black bars. For several categories, disgust, neutral, and surprise, the percentage of matching frames increased significantly. The improvement was most pronounced for disgust, which increased from 58% to 75%. However, the percentage of relevant frames remained constant for happy and improved only marginally for angry and sad (Fig. 12).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>36930</offset><text>Sub-bound + minimum confidence</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36965</offset><text>The optimal minimum confidence score, λ is shown in Table 4 based on results obtained using the Microsoft Azure Cognitive Services API using the search approach shown in Algorithm 2. Recall that λ represents the minimum required classification confidence of the the emotion associated with the region in which a frame is found for it to be retained by the filtering algorithm.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37352</offset><text>The requisite λ was very small for every class, ranging from 0.00 (no filtering) for happy to 0.10 (10%) for surprise. The improvement derived from this method is likely because the classification confidence reported by the classifier may be too conservative when contextual knowledge indicates that the frame was derived in a region that matches the class associated with the prompt shown. Results for this approach are denoted by the white bars in Fig. 11. The classes that improved from the baseline method to the sub-bound approach increased further using the minimum confidence method: disgust increased from 75% to 94%, neutral increased from 70% to 81%, and surprise increased from 59% to 92%. However, no substantial improvements were found for the other categories.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>38132</offset><text>Sub-bound and ensemble</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38155</offset><text>Max: Selecting the maximum classification confidence from all three classifiers did not improve performance from the baseline for any emotion. This is likely because the evaluated emotion classifiers provided generally very high confidence scores, even for frames that did not match the desired emotion. The results shown in this figure are associated with a λ = 0: no filtering.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38540</offset><text>Min: When filtering based on the minimum classification confidence score between all three classifiers, the percentage of matching frames within a region increased considerably for disgust, scared, and surprised.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38753</offset><text>Average: Averaging the confidence score from all three classifiers provided the best overall accuracy, though improvement in the happy category was marginal and nonexistent in the case of angry.</text></passage><passage><infon key="file">gr13.jpg</infon><infon key="id">fig0065</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38948</offset><text>A comparison of three different methods of combining multiple classification confidence scores to make a filtering decision demonstrates that averaging the scores was generally the best technique.</text></passage><passage><infon key="file">gr13.jpg</infon><infon key="id">fig0065</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>39145</offset><text>Fig. 13</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39153</offset><text>Fig. 13 shows the percentage of frames correctly identified within a region when filtering using an ensemble-based technique that combines classification confidence scores from multiple classifiers using three different methods: minimum, maximum, and average, and comparing the result to a predefined threshold, λ. It should be noted that in some cases, the best ensemble-based technique was still outperformed by the minimum-confidence technique using a single-classifier.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>39632</offset><text>Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39643</offset><text>Entire video: A baseline method that evaluates the percentage of frames in a video that match a particular class of emotion.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39768</offset><text>Within boundary: Aggregating frames from regions within the video where the prompt related to the emotion of interest are shown.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39897</offset><text>Within sub-bound: Searching within the boundary but filtering out leading and trailing frames and limiting the search to the center of the region.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40044</offset><text>Sub-bound + minimum confidence Searching within the center of the region, and further filtering frames in which the classification confidence of the emotion of interest did not exceed a predefined threshold.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40256</offset><text>Sub-bound + ensemble Like before, but using multiple classifiers, combining their classification confidences, and comparing the result to a predefined threshold to make a filtering decision.</text></passage><passage><infon key="file">gr14.jpg</infon><infon key="id">fig0070</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40451</offset><text>A comparison of the methods described in this work shows that a hybrid minimum-confidence ensemble technique that uses the optimal sub-bound for a region is able to make a correct filtering decision for the majority of frames for four out of the seven evaluated emotions.</text></passage><passage><infon key="file">gr14.jpg</infon><infon key="id">fig0070</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>40723</offset><text>Fig. 14</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40731</offset><text>Fig. 14 provides a direct comparison of the five techniques used to obtain labeled emotion data in this work, which we briefly summarize here.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40874</offset><text>Results indicate that a high percentage of frames associated with disgust, scared, neutral, and surprised can be derived using these techniques (94%, 56%, 81%, and 92% respectively), with the ensemble method producing the strongest results overall. This suggests that the provided framework is sufficient for automatic aggregation of labeled frames from some emotions, and semi-automatic labeling of others. However, results for the angry and happy categories remained poor across all techniques. This shortcoming could be caused in part by few subjects or limited manual raters. Given the ambiguity of exactly when a face transitions from neutral to happy, the manual raters could have made labeling decisions that were incongruous with the classifier's definition of a happy face. Regardless, additional experimentation and novel techniques are necessary to bridge this gap and provide methods to derive emotive frames from all categories in structured video.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>41836</offset><text>Limitations and future work</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>41864</offset><text>In this work, we propose a method of crowdsourcing emotion-labeled frames from children with Autism Spectrum Disorder using a mobile application and various automatic labeling algorithms. Future work will validate this approach on a larger, more varied dataset. Moreover, we will include a ground truth of manually annotated frames derived from a greater number of raters with clinical experience to determine if there are appreciable accuracy improvements compared to labels from the two raters used in this study. Subsequently, a deep neural network model will be trained using a transfer-learning approach to validate our hypothesis that the limitations of existing systems arise from a lack of relevant training data.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42586</offset><text>The ecological validity of novel interventions for ASD is an important concern. A conference organized by a multidisciplinary panel of researchers of developmental disabilities developed a list of best practices for screening and early identification of autism in October of 2010. A significant conclusion drawn from this conference was that intervention research should integrate culturally and socially diverse populations to evaluate factors that influence both the participation and outcomes of therapeutic approaches. Therefore, it is crucial for data collection efforts of follow-up studies to consider cultural contexts outside the United States and to represent a more diverse cohort of children.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>43291</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>43302</offset><text>We present a system for deriving emotive video from children with ASD through a charades-style game, and several algorithms that can be used to extract semi-labeled frames from these videos using classification confidence scores and game meta information. We demonstrate three techniques: Sub-Bound Analysis, Minimum Confidence, and Ensemble Classification, that we compare to a baseline method on the basis of their efficacy in correctly labeling frames from videos derived from Guess What? game sessions. Results show that 94%, 81%, 92%, and 56% of frames were automatically labeled correctly for categories disgust, neutral, surprise, and scared respectively, though performance for angry and happy did not improve significantly from the baseline. Once additional video data are available, these methods will be employed to generate a large labeled dataset that will be used to train convolutional neural network classifiers for emotion recognition that are robust across differences in age and developmental delay.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>44321</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>44332</offset><text>Autism Society. What is autism? http://www.autism-society.org/what-is/ [accessed: 2017-010-30].</text></passage><passage><infon key="name_0">surname:Association;given-names:A.P.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>44428</offset></passage><passage><infon key="fpage">e20174161</infon><infon key="issue">Dec. (6)</infon><infon key="name_0">surname:Kogan;given-names:M.D.</infon><infon key="name_1">surname:Vladutiu;given-names:C.J.</infon><infon key="name_10">surname:Lu;given-names:M.C.</infon><infon key="name_2">surname:Schieve;given-names:L.A.</infon><infon key="name_3">surname:Ghandour;given-names:R.M.</infon><infon key="name_4">surname:Blumberg;given-names:S.J.</infon><infon key="name_5">surname:Zablotsky;given-names:B.</infon><infon key="name_6">surname:Perrin;given-names:J.M.</infon><infon key="name_7">surname:Shattuck;given-names:P.</infon><infon key="name_8">surname:Kuhlthau;given-names:K.A.</infon><infon key="name_9">surname:Harwood;given-names:R.L.</infon><infon key="pub-id_pmid">30478241</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">142</infon><infon key="year">2018</infon><offset>44429</offset><text>The prevalence of parent-reported autism spectrum disorder among US children</text></passage><passage><infon key="fpage">775</infon><infon key="issue">3</infon><infon key="lpage">803</infon><infon key="name_0">surname:Dawson;given-names:G.</infon><infon key="pub-id_pmid">18606031</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychopathol</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2008</infon><offset>44506</offset><text>Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder</text></passage><passage><infon key="fpage">e17</infon><infon key="issue">1</infon><infon key="lpage">e23</infon><infon key="name_0">surname:Dawson;given-names:G.</infon><infon key="name_1">surname:Rogers;given-names:S.</infon><infon key="name_2">surname:Munson;given-names:J.</infon><infon key="name_3">surname:Smith;given-names:M.</infon><infon key="name_4">surname:Winter;given-names:J.</infon><infon key="name_5">surname:Greenson;given-names:J.</infon><infon key="name_6">surname:Donaldson;given-names:A.</infon><infon key="name_7">surname:Varley;given-names:J.</infon><infon key="pub-id_pmid">19948568</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">125</infon><infon key="year">2010</infon><offset>44602</offset><text>Randomized, controlled trial of an intervention for toddlers with autism: the early start Denver model</text></passage><passage><infon key="name_0">surname:Cooper;given-names:J.O.</infon><infon key="name_1">surname:Heron;given-names:T.E.</infon><infon key="name_2">surname:Heward;given-names:W.L.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>44705</offset></passage><passage><infon key="fpage">1150</infon><infon key="issue">11</infon><infon key="lpage">1159</infon><infon key="name_0">surname:Dawson;given-names:G.</infon><infon key="name_1">surname:Jones;given-names:E.J.</infon><infon key="name_2">surname:Merkle;given-names:K.</infon><infon key="name_3">surname:Venema;given-names:K.</infon><infon key="name_4">surname:Lowy;given-names:R.</infon><infon key="name_5">surname:Faja;given-names:S.</infon><infon key="pub-id_pmid">23101741</infon><infon key="section_type">REF</infon><infon key="source">J Am Acad Child Adolesc Psychiatry</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2012</infon><offset>44706</offset><text>Early behavioral intervention is associated with normalized brain activity in young children with autism</text></passage><passage><infon key="fpage">1480</infon><infon key="issue">6</infon><infon key="lpage">1486</infon><infon key="name_0">surname:Mandell;given-names:D.S.</infon><infon key="name_1">surname:Novak;given-names:M.M.</infon><infon key="name_2">surname:Zubritsky;given-names:C.D.</infon><infon key="pub-id_pmid">16322174</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">116</infon><infon key="year">2005</infon><offset>44811</offset><text>Factors associated with age of diagnosis among children with autism spectrum disorders</text></passage><passage><infon key="fpage">117</infon><infon key="issue">2</infon><infon key="lpage">127</infon><infon key="name_0">surname:Porayska-Pomsta;given-names:K.</infon><infon key="name_1">surname:Frauenberger;given-names:C.</infon><infon key="name_2">surname:Pain;given-names:H.</infon><infon key="name_3">surname:Rajendran;given-names:G.</infon><infon key="name_4">surname:Smith;given-names:T.</infon><infon key="name_5">surname:Menzies;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Pers Ubiquit Comput</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2012</infon><offset>44898</offset><text>Developing technology for autism: an interdisciplinary approach</text></passage><passage><infon key="fpage">2589</infon><infon key="lpage">2598</infon><infon key="name_0">surname:Escobedo;given-names:L.</infon><infon key="name_1">surname:Nguyen;given-names:D.H.</infon><infon key="name_2">surname:Boyd;given-names:L.</infon><infon key="name_3">surname:Hirano;given-names:S.</infon><infon key="name_4">surname:Rangel;given-names:A.</infon><infon key="name_5">surname:Garcia-Rosas;given-names:D.</infon><infon key="name_6">surname:Tentori;given-names:M.</infon><infon key="name_7">surname:Hayes;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Mosoco: a mobile assistive tool to support children with autism practicing social skills in real-life situations</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>44962</offset><text>Proceedings of the SIGCHI conference on human factors in computing systems, ACM</text></passage><passage><infon key="fpage">38</infon><infon key="issue">1</infon><infon key="lpage">46</infon><infon key="name_0">surname:Escobedo;given-names:L.</infon><infon key="name_1">surname:Tentori;given-names:M.</infon><infon key="name_2">surname:Quintana;given-names:E.</infon><infon key="name_3">surname:Favela;given-names:J.</infon><infon key="name_4">surname:Garcia-Rosas;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Pervas Comput</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2014</infon><offset>45042</offset><text>Using augmented reality to help children with autism stay focused</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">24</infon><infon key="name_0">surname:Kalantarian;given-names:H.</infon><infon key="name_1">surname:Washington;given-names:P.</infon><infon key="name_2">surname:Schwartz;given-names:J.</infon><infon key="name_3">surname:Daniels;given-names:J.</infon><infon key="name_4">surname:Haber;given-names:N.</infon><infon key="name_5">surname:Wall;given-names:D.P.</infon><infon key="section_type">REF</infon><infon key="source">J Healthc Inf Res</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>45108</offset><text>Guess what?</text></passage><passage><infon key="fpage">112</infon><infon key="issue">3</infon><infon key="name_0">surname:Washington;given-names:P.</infon><infon key="name_1">surname:Voss;given-names:C.</infon><infon key="name_2">surname:Kline;given-names:A.</infon><infon key="name_3">surname:Haber;given-names:N.</infon><infon key="name_4">surname:Daniels;given-names:J.</infon><infon key="name_5">surname:Fazel;given-names:A.</infon><infon key="name_6">surname:De;given-names:T.</infon><infon key="name_7">surname:Feinstein;given-names:C.</infon><infon key="name_8">surname:Winograd;given-names:T.</infon><infon key="name_9">surname:Wall;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2017</infon><offset>45120</offset><text>Superpowerglass: a wearable aid for the at-home therapy of children with autism</text></passage><passage><infon key="fpage">S257</infon><infon key="issue">10</infon><infon key="name_0">surname:Daniels;given-names:J.</infon><infon key="name_1">surname:Schwartz;given-names:J.</infon><infon key="name_2">surname:Haber;given-names:N.</infon><infon key="name_3">surname:Voss;given-names:C.</infon><infon key="name_4">surname:Kline;given-names:A.</infon><infon key="name_5">surname:Fazel;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">J Am Acad Child Adolesc Psychiatry</infon><infon key="type">ref</infon><infon key="volume">56</infon><infon key="year">2017</infon><offset>45200</offset><text>5.13 Design and efficacy of a wearable device for social affective learning in children with autism</text></passage><passage><infon key="fpage">1218</infon><infon key="lpage">1226</infon><infon key="name_0">surname:Voss;given-names:C.</infon><infon key="name_1">surname:Washington;given-names:P.</infon><infon key="name_2">surname:Haber;given-names:N.</infon><infon key="name_3">surname:Kline;given-names:A.</infon><infon key="name_4">surname:Daniels;given-names:J.</infon><infon key="name_5">surname:Fazel;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM international joint conference on pervasive and ubiquitous computing: adjunct, ACM</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>45300</offset><text>Superpower glass: delivering unobtrusive real-time social cues in wearable systems</text></passage><passage><infon key="name_0">surname:Teeters;given-names:A.C.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>45383</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45384</offset><text>Azure. https://azure.microsoft.com/en-us/services/cognitive-services/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45455</offset><text>Anon. https://aws.amazon.com/rekognition/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45498</offset><text>https://cloud.google.com/vision/.</text></passage><passage><infon key="fpage">712</infon><infon key="issue">4</infon><infon key="name_0">surname:Ekman;given-names:P.</infon><infon key="name_1">surname:Friesen;given-names:W.V.</infon><infon key="name_2">surname:O'sullivan;given-names:M.</infon><infon key="name_3">surname:Chan;given-names:A.</infon><infon key="name_4">surname:Diacoyanni-Tarlatzis;given-names:I.</infon><infon key="name_5">surname:Heider;given-names:K.</infon><infon key="pub-id_pmid">3681648</infon><infon key="section_type">REF</infon><infon key="source">J Pers Soc Psychol</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">1987</infon><offset>45532</offset><text>Universals and cultural differences in the judgments of facial expressions of emotion</text></passage><passage><infon key="fpage">770</infon><infon key="lpage">778</infon><infon key="name_0">surname:He;given-names:K.</infon><infon key="name_1">surname:Zhang;given-names:X.</infon><infon key="name_2">surname:Ren;given-names:S.</infon><infon key="name_3">surname:Sun;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE conference on computer vision and pattern recognition</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>45618</offset><text>Deep residual learning for image recognition</text></passage><passage><infon key="name_0">surname:Cohn;given-names:J.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>45663</offset></passage><passage><infon key="name_0">surname:Douglas-Cowie;given-names:E.</infon><infon key="name_1">surname:Cowie;given-names:R.</infon><infon key="name_2">surname:Schröder;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">ISCA tutorial and research workshop (ITRW) on speech and emotion</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>45664</offset><text>A new emotion database: considerations, sources and scope</text></passage><passage><infon key="name_0">surname:Picard;given-names:R.W.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>45722</offset></passage><passage><infon key="fpage">250</infon><infon key="issue">3</infon><infon key="lpage">254</infon><infon key="name_0">surname:Picard;given-names:R.W.</infon><infon key="section_type">REF</infon><infon key="source">Emot Rev</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2010</infon><offset>45723</offset><text>Emotion research by the people, for the people</text></passage><passage><infon key="fpage">262</infon><infon key="issue">2</infon><infon key="lpage">271</infon><infon key="name_0">surname:Brewer;given-names:R.</infon><infon key="name_1">surname:Biotti;given-names:F.</infon><infon key="name_2">surname:Catmur;given-names:C.</infon><infon key="name_3">surname:Press;given-names:C.</infon><infon key="name_4">surname:Happé;given-names:F.</infon><infon key="name_5">surname:Cook;given-names:R.</infon><infon key="name_6">surname:Bird;given-names:G.</infon><infon key="pub-id_pmid">26053037</infon><infon key="section_type">REF</infon><infon key="source">Autism Res</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2016</infon><offset>45770</offset><text>Can neurotypical individuals read autistic facial expressions? Atypical production of emotional facial expressions in autism spectrum disorders</text></passage><passage><infon key="fpage">75</infon><infon key="issue">1</infon><infon key="lpage">89</infon><infon key="name_0">surname:Faso;given-names:D.J.</infon><infon key="name_1">surname:Sasson;given-names:N.J.</infon><infon key="name_2">surname:Pinkham;given-names:A.E.</infon><infon key="section_type">REF</infon><infon key="source">J Autism Dev Disorders</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2015</infon><offset>45914</offset><text>Evaluating posed and evoked facial expressions of emotion from adults with autism spectrum disorder</text></passage><passage><infon key="fpage">475</infon><infon key="issue">3</infon><infon key="name_0">surname:Capps;given-names:L.</infon><infon key="name_1">surname:Kasari;given-names:C.</infon><infon key="name_2">surname:Yirmiya;given-names:N.</infon><infon key="name_3">surname:Sigman;given-names:M.</infon><infon key="pub-id_pmid">8326050</infon><infon key="section_type">REF</infon><infon key="source">J Consult Clin Psychol</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">1993</infon><offset>46014</offset><text>Parental perception of emotional expressiveness in children with autism</text></passage><passage><infon key="fpage">177</infon><infon key="issue">2</infon><infon key="lpage">179</infon><infon key="name_0">surname:Czapinski;given-names:P.</infon><infon key="name_1">surname:Bryson;given-names:S.E.</infon><infon key="section_type">REF</infon><infon key="source">Brain Cognit</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2003</infon><offset>46086</offset><text>9. Reduced facial muscle movements in autism: evidence for dysfunction in the neuromuscular pathway?</text></passage><passage><infon key="fpage">350</infon><infon key="lpage">352</infon><infon key="name_0">surname:Kalantarian;given-names:H.</infon><infon key="name_1">surname:Washington;given-names:P.</infon><infon key="name_2">surname:Schwartz;given-names:J.</infon><infon key="name_3">surname:Daniels;given-names:J.</infon><infon key="name_4">surname:Haber;given-names:N.</infon><infon key="name_5">surname:Wall;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">2018 IEEE international conference on healthcare informatics (ICHI)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>46187</offset><text>A gamified mobile system for crowdsourcing video for autism research</text></passage><passage><infon key="fpage">666</infon><infon key="lpage">671</infon><infon key="name_0">surname:Tang;given-names:T.Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 15th international conference on interaction design and children, ACM</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>46256</offset><text>Helping neuro-typical individuals to read the emotion of children with autism spectrum disorder: an internet-of-things approach</text></passage><passage><infon key="fpage">533</infon><infon key="lpage">539</infon><infon key="name_0">surname:Tang;given-names:T.Y.</infon><infon key="name_1">surname:Winoto;given-names:P.</infon><infon key="name_2">surname:Chen;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 conference on interaction design and children, ACM</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>46384</offset><text>Emotion recognition via face tracking with realsense (tm) 3d camera for children with autism</text></passage><passage><infon key="name_0">surname:Aztiria;given-names:A.</infon><infon key="name_1">surname:Augusto;given-names:J.C.</infon><infon key="name_2">surname:Orlandini;given-names:A.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>46477</offset></passage><passage><infon key="fpage">448</infon><infon key="lpage">463</infon><infon key="name_0">surname:Karyotis;given-names:C.</infon><infon key="name_1">surname:Doctor;given-names:F.</infon><infon key="name_2">surname:Iqbal;given-names:R.</infon><infon key="name_3">surname:James;given-names:A.</infon><infon key="name_4">surname:Chang;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">Inf Sci</infon><infon key="type">ref</infon><infon key="volume">433</infon><infon key="year">2018</infon><offset>46478</offset><text>A fuzzy computational model of emotion for cloud based sentiment analysis</text></passage><passage><infon key="fpage">600</infon><infon key="lpage">611</infon><infon key="name_0">surname:Maniak;given-names:T.</infon><infon key="name_1">surname:Jayne;given-names:C.</infon><infon key="name_2">surname:Iqbal;given-names:R.</infon><infon key="name_3">surname:Doctor;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">Inf Sci</infon><infon key="type">ref</infon><infon key="volume">294</infon><infon key="year">2015</infon><offset>46552</offset><text>Automated intelligent system for sound signalling device quality assurance</text></passage><passage><infon key="fpage">401</infon><infon key="lpage">410</infon><infon key="name_0">surname:Marcu;given-names:G.</infon><infon key="name_1">surname:Dey;given-names:A.K.</infon><infon key="name_2">surname:Kiesler;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2012 ACM conference on ubiquitous computing, ACM</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>46627</offset><text>Parent-driven use of wearable cameras for autism support: a field study with families</text></passage><passage><infon key="fpage">3575</infon><infon key="issue">1535</infon><infon key="lpage">3584</infon><infon key="name_0">surname:Picard;given-names:R.W.</infon><infon key="section_type">REF</infon><infon key="source">Philos Trans R Soc B: Biol Sci</infon><infon key="type">ref</infon><infon key="volume">364</infon><infon key="year">2009</infon><offset>46713</offset><text>Future affective technology for autism and emotion communication</text></passage><passage><infon key="fpage">279</infon><infon key="lpage">283</infon><infon key="name_0">surname:Barsoum;given-names:E.</infon><infon key="name_1">surname:Zhang;given-names:C.</infon><infon key="name_2">surname:Ferrer;given-names:C.C.</infon><infon key="name_3">surname:Zhang;given-names:Z.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th ACM international conference on multimodal interaction, ACM</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>46778</offset><text>Training deep networks for facial expression recognition with crowd-sourced label distribution</text></passage><passage><infon key="fpage">1247</infon><infon key="lpage">1250</infon><infon key="name_0">surname:Zhou;given-names:Y.</infon><infon key="name_1">surname:Xue;given-names:H.</infon><infon key="name_2">surname:Geng;given-names:X.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd ACM international conference on Multimedia, ACM</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>46873</offset><text>Emotion distribution recognition from facial expressions</text></passage><passage><infon key="fpage">435</infon><infon key="lpage">442</infon><infon key="name_0">surname:Yu;given-names:Z.</infon><infon key="name_1">surname:Zhang;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2015 ACM on international conference on multimodal interaction, ACM</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>46930</offset><text>Image based static facial expression recognition with multiple deep network learning</text></passage><passage><infon key="name_0">surname:Kalantarian;given-names:H.</infon><infon key="name_1">surname:Mortazavi;given-names:B.</infon><infon key="name_2">surname:Pourhomayoun;given-names:M.</infon><infon key="name_3">surname:Alshurafa;given-names:N.</infon><infon key="name_4">surname:Sarrafzadeh;given-names:M.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>47015</offset></passage><passage><infon key="fpage">397</infon><infon key="lpage">412</infon><infon key="name_0">surname:Kalantarian;given-names:H.</infon><infon key="name_1">surname:Sarrafzadeh;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Pervas Mobile Comput</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2017</infon><offset>47016</offset><text>Probabilistic time-series segmentation</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47055</offset><text>Anon. https://www.sighthound.com/products/cloud.</text></passage><passage><infon key="fpage">1455</infon><infon key="issue">4pt2</infon><infon key="lpage">1472</infon><infon key="name_0">surname:Dawson;given-names:G.</infon><infon key="name_1">surname:Bernier;given-names:R.</infon><infon key="pub-id_pmid">24342850</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychopathol</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2013</infon><offset>47104</offset><text>A quarter century of progress on the early detection and treatment of autism spectrum disorder</text></passage><passage><infon key="fpage">e1002705</infon><infon key="issue">11</infon><infon key="name_0">surname:Tariq;given-names:Q.</infon><infon key="name_1">surname:Daniels;given-names:J.</infon><infon key="name_2">surname:Schwartz;given-names:J.N.</infon><infon key="name_3">surname:Washington;given-names:P.</infon><infon key="name_4">surname:Kalantarian;given-names:H.</infon><infon key="name_5">surname:Wall;given-names:D.P.</infon><infon key="pub-id_pmid">30481180</infon><infon key="section_type">REF</infon><infon key="source">PLoS Med</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2018</infon><offset>47199</offset><text>Mobile detection of autism through machine learning on home video: a development and prospective validation study</text></passage><passage><infon key="fpage">S1</infon><infon key="issue">Supplement 1</infon><infon key="lpage">S9</infon><infon key="name_0">surname:Zwaigenbaum;given-names:L.</infon><infon key="name_1">surname:Bauman;given-names:M.L.</infon><infon key="name_2">surname:Choueiri;given-names:R.</infon><infon key="name_3">surname:Fein;given-names:D.</infon><infon key="name_4">surname:Kasari;given-names:C.</infon><infon key="name_5">surname:Pierce;given-names:K.</infon><infon key="pub-id_pmid">26430167</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">136</infon><infon key="year">2015</infon><offset>47313</offset><text>Early identification and interventions for autism spectrum disorder: executive summary</text></passage></document></collection>
