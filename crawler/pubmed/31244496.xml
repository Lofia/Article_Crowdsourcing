<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201219</date><key>pmc.key</key><document><id>6582699</id><infon key="license">CC BY-NC</infon><passage><infon key="alt-title">KENNETH BENOIT, KEVIN MUNGER, AND ARTHUR SPIRLING MEASURING TEXTUAL COMPLEXITY</infon><infon key="article-id_doi">10.1111/ajps.12423</infon><infon key="article-id_pmc">6582699</infon><infon key="article-id_pmid">31244496</infon><infon key="article-id_publisher-id">AJPS12423</infon><infon key="fn">This research was partly supported by the European Research Council grant ERC‐2011‐StG283794‐QUANTESS. The authors are grateful to audiences at the Midwest Political Science Association annual meeting, at the European Political Science Association annual meeting, and at the New Directions in Analyzing Text as Data meeting. Jacob Montgomery provided very helpful feedback on an earlier draft. Three anonymous referees and the editor at the American Journal of Political Science provided excellent comments that improved the paper considerably.</infon><infon key="fpage">491</infon><infon key="issue">2</infon><infon key="license">This is an open access article under the terms of the http://creativecommons.org/licenses/by-nc/4.0/ License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.</infon><infon key="lpage">508</infon><infon key="name_0">surname:Benoit;given-names:Kenneth</infon><infon key="name_1">surname:Munger;given-names:Kevin</infon><infon key="name_2">surname:Spirling;given-names:Arthur</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">63</infon><infon key="year">2019</infon><offset>0</offset><text>Measuring and Explaining Political Sophistication through Textual Complexity</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>77</offset><text>Abstract</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>86</offset><text>Political scientists lack domain‐specific measures for the purpose of measuring the sophistication of political communication. We systematically review the shortcomings of existing approaches, before developing a new and better method along with software tools to apply it. We use crowdsourcing to perform thousands of pairwise comparisons of text snippets and incorporate these results into a statistical model of sophistication. This includes previously excluded features such as parts of speech and a measure of word rarity derived from dynamic term frequencies in the Google Books data set. Our technique not only shows which features are appropriate to the political domain and how, but also provides a measure easily applied and rescaled to political texts in a way that facilitates probabilistic comparisons. We reanalyze the State of the Union corpus to demonstrate how conclusions differ when using our improved approach, including the ability to compare complexity as a function of covariates.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>1093</offset><text>Replication Materials</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>1115</offset><text>The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network, at: https://doi.org/10.7910/DVN/9SF3TI.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>1358</offset><text>A key concern in the study of politics is how the nature of political communication has changed. At the same time that the challenges of governing have grown in complexity, the sophistication of political speech, by many measures, appears to have declined. Typically as part of a broader discussion concerning “dumbing down” (Gatto 2002), scholars have applied measures of textual complexity from educational fields to find that the sophistication of political language has steadily decreased over the past 200 years (e.g., Lim 2008). Such concerns are echoed in popular presentations, and it is not uncommon to see media analysis assessing political speeches in terms of the (purported lower) school grade level required to understand them.1 </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>2106</offset><text>By contrast, and with more optimistic conclusions, other social science studies have used measures of textual complexity to link linguistic sophistication to outcomes, with a focus on the concrete benefits of clarity. Jansen (2011), for instance, studies the reading level of communications from four central banks, equating lower reading levels of bank communication with greater clarity, which they link to positive effects on the volatility of financial market returns. Likewise, Owens and Wedeking (2011) and Spriggs (1996) examine the complexity of Supreme Court decisions, pointing to the importance of clarity in court opinions. In the context of the British parliament, Spirling (2016) applies readability measures to document the democratizing effects of franchise reform on elite speeches. Studying postwar Austrian and German elections, Bischof and Senninger (2018) find that simpler manifestos make for better‐informed voters. Finally, as a meta‐analysis to defend against charges of elitism and jargon (e.g., Kristof 2014), Cann, Goelzhauser, and Johnson (2014) show that while the reading ease of articles in the top political science journals has declined since 1910, the typical political science article requires less reading ability than the average article in Time Magazine or Reader's Digest.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>3423</offset><text>These applications share one trait: They equate important substantive characteristics of political, economic, or legal communication—such as clarity or sophistication—with indexes such as the Flesch Reading Ease (FRE) score (Flesch 1948). These measures, however, were developed decades earlier in entirely different contexts, namely, educational research and applied psychology, and their applicability to contemporary political speech remains untested. Consequently, we are uncertain as to the true direction of change for specifically political communication. More importantly perhaps, because our current measurement strategies are weak, we find it hard to disentangle changes to texts that are normatively positive (“clearer”) versus negative (“dumbing down”). For example, the fact that we might communicate the same complex content, but in shorter words and sentences that require less processing effort by the reader, is almost certainly a good thing. Yet, as we will see, traditional measures imply such changes are in line with appealing to a less educated audience and thus deemed a source of concern.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>4549</offset><text>To address such problems, here we systematically review the properties and statistical performance of current measures of textual difficulty and develop a new measure designed specifically for political language. Our approach uses experimental data based on human pairwise comparisons of short extracts of political speech (e.g., Lowe and Benoit 2013; Montgomery and Carlson 2017), which we then use to scale linguistic sophistication using a scaling approach developed by Bradley and Terry (1952) to measure latent “ability” from pairwise contests, treating the reading ease of a text as equivalent to ability. This approach permits more direct statements about uncertainty and inference, including the probability that a given text is easier or harder, either to one another or to a known benchmark, such as a fifth‐grade reading level. Rather than relying on static estimates fit to data from a nonpolitical context, our approach allows flexible determination of the components of textual sophistication, as well as their appropriate weights, in a manner that can be adapted to any domain but that is fit here to texts from the U.S. State of the Union (SOTU) corpus to provide a specifically political measure of textual sophistication. Generalizing beyond this corpus, our contribution is to set out clear principles for measuring linguistic sophistication in the political domain, demonstrate the methodological superiority of our approach, and outline a new method for fitting appropriate measures to any context.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>6075</offset><text>Measuring the Sophistication of Political Communication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6131</offset><text>We first define terms and review previous efforts.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>6183</offset><text>Textual Sophistication, Complexity, and Difficulty</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6234</offset><text>As applied to text, we use the terms sophistication, difficulty, and complexity somewhat interchangeably, reflecting ambiguity in existing use of these terms.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6394</offset><text>For Luskin (1990), to give a political communication example, sophistication is a property of individuals rather than messages, and it pertains to how elaborate is the individuals' political belief system. Thus, “[a] person is politically sophisticated to the extent to which his or her political cognitions are numerous, cut a wide substantive swath, and are highly organized, or ‘constrained”’ (Luskin 1990, 332). In that world, measurement focuses on the interest that a citizen has in politics, her educational level, and exposure to current events and related variables. Of course, this conception does not lend itself naturally to a measure for texts themselves. For example, it is unclear whether a document written in a simple way about an obscure (but wholly political) issue ought to be considered more or less sophisticated than one written about a well‐known subject that requires some prior education to appreciate fully.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7340</offset><text>In linguistics, complexity is a characteristic of a text, but there are multiple measures and thus multiple implied definitions in practice. For example, social scientists might well agree with Gibson (1998, 2) that complexity is the “quantity of computational resources …[that documents] require to process.” But they might disagree with his focus on ambiguity as to whether a transitive verb refers to the object or subject, the presence of particular types of nested clauses, and the distance between certain elements in sentences. That is, a “sophisticated” political sentence is not merely confusing or hard to follow.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7974</offset><text>Perhaps the simplest way to conceptualize the measurement problem comes from education research, in which the concern is to match learning materials to students based on their age and cognitive ability (for an overview, see Klare 1963). There, the emphasis is on the “readability” of a document and the intuitive notion that one text may be relatively more “difficult” than another in terms of some downstream comprehension task (e.g., a school test about the passage or book in question). In this vein, textual difficulty embodies some mix of the concepts above. If the message in a text is subtle and can only really be understood or appreciated by a well‐educated person, it is both difficult and sophisticated.2 Meanwhile, if a document is written with an unusual or archaic (but nonetheless correct) grammatical structure, it is both difficult and complex.3 Although these concepts are not exactly equivalent, their ready application to texts based on empirically established markers has encouraged their widespread adoption in educational research to measure the reading difficulty of texts, a usage whose application has spread to other fields. For this reason, and because these formulations are so straightforward, we focus our efforts here on improving these measures as they apply to political texts.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9296</offset><text>Traditional Measures of Textual Difficulty</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9339</offset><text>Measuring the difficulty of educational texts is not new (e.g., Sherman 1893), and there are now a large number of indexes for this task—indeed, Michalke (2017) references and implements no fewer than 27 of them—but the various Flesch‐based metrics (Flesch 1948, 1949; Kincaid et al. 1975) have dominated.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9653</offset><text>In terms of technical details, for a given document, the traditional measures of reading difficulty take into account some combination of (average) sentence length (e.g., Flesch 1948, 1949; Fry 1968; Gunning 1952; Kincaid et al. 1975), the (average) number of syllables per word (e.g., Flesch 1948, 1949; Fry 1968; Gunning 1952; Kincaid et al. 1975; Wheeler and Smith 1954), the parts of speech represented in the document (e.g., Coleman and Liau 1975), and the (average) familiarity of the terms used (e.g., Dale and Chall 1948; Spache 1953).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10199</offset><text>Flesch's (1948) pioneering work focused on the reading comprehension of schoolchildren: in particular, the average grade of students who could correctly answer at least 75% of some multiple‐choice questions regarding a few select texts. This dependent variable was subsequently transformed to a 0–100 scale and regressed on a constant and two predictors (average sentence length and average number of syllables per word). This yielded the following formula for scoring documents:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10685</offset><text>Known as the Flesch Reading Ease score, this measure had the intended range “for almost all samples taken from ordinary prose” (Flesch 1948, 225).4 Subsequently, Kincaid et al. (1975) introduced a mechanical conversion of the formula that yields values roughly equivalent to the U.S. grade school level required to understand a text.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11025</offset><text>Other than indirectly through syllable counts, the Flesch formula does not take into account the actual familiarity of the words used in a text. An example of an approach that does is the Dale‐Chall formula (Dale and Chall 1948), whose key difference from the Flesch index was its replacement of the word length input by a text's percentage of “difficult” words, specified as any word not included in a list of 763 words deemed to be known by 80% of fourth‐grade children (in 1948).5 </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11518</offset><text>Improving Measures of Textual Sophistication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11563</offset><text>While political scientists have not ignored the measurement of readability (e.g., Cann, Goelzhauser, and Johnson 2014), there has not been especially great interest in adapting measures to specifically political contexts. This gives rise to two broad sets of issues that give us pause: first, theory‐based concerns related to what using such measures implies about the elements that determine textual sophistication and their appropriate weights; and second, a general lack of desirability from a statistical perspective.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12088</offset><text>Empirically Determining the Indicators of Textual Sophistication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12153</offset><text>Traditional measures of readability use different combinations of indicators and weights. Since the 1970s, however, such measures have been criticized as atheoretical, at least in terms of the way that educational researchers thought about cognition (Kintsch and Vipond 1979). Consequently, scholars have treated them with increasing caution because their performance was found wanting in a series of studies (e.g., Bruce, Rubin, and Starr 1981; Smith 1986). Since none of those contexts were political, furthermore, the arbitrary choice of indicators from studies in nonpolitical domains makes the case for fitting a specifically political measure of textual sophistication all the more compelling. In particular, the schoolchildren studied in most previous approaches may not be representative of the adult citizens we care about for political science cases. And while what makes a political text difficult may be somewhat similar to the factors that make educational passages harder, this remains an empirical question to be examined. For a statistical model of textual easiness, such as the one we develop below, this means fitting the model to a large set of potential determinants of sophistication, within the context of domain‐relevant texts. We now lay out our priors about what will matter, and why.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13466</offset><text>First, we expect greater use of longer words to indicate a higher degree of sophistication. As in education, longer words are assumed to make things harder for political audiences, whether this length occurs in the form of characters or syllables. Use of the noun plebiscite, for instance, signals greater textual sophistication than use of its synonym, vote. Because political text is usually designed explicitly to deliver an ideological message, such deliberate choices may matter even more than similar indicators in school texts, in which the goal is to educate and entertain.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14049</offset><text>Next, we expect that greater use of relatively uncommon words will indicate higher sophistication than use of their more commonplace synonyms. Not only do relatively rare words require a larger vocabulary, and hence a more widely read and more literate audience, but also rarer words typically mark more precise, domain‐specific language that is the hallmark of expertise. In political text, this can translate into more sophisticated content, as well as style.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14514</offset><text>Traditional measures of readability have captured word rarity in a static fashion, in the form of lists of “easy words” (e.g., Dale and Chall 1948) or some difficulty measure attached to each word (e.g., Bonsall et al. 2017).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14745</offset><text>Word rarity is not static, however, especially with respect to changing lexicons over time. The term husbandry (the cultivation and breeding of crops and animals, respectively) was used much more often in the 1790s than in current times, and therefore its inclusion in a list of easy or difficult words today may be misleading for the prior period. Thus, we need to model contemporary understandings differently from more historical ones.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15185</offset><text> Longer sentences also reflect greater sophistication, whether measured in words or characters, since these not only reflect more complex ideas but also require more attention to absorb, in the linguistic sense we mentioned above. For this reason, nearly every previous measure of reading difficulty takes sentence length into account in some form.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15535</offset><text>Finally, more sentences with more complex syntactic and grammatical structures indicate greater sophistication. Beyond length, structural complexity in the form of multiple or subordinate clauses indicates that more complex ideas are being communicated. This may also take the form of greater reliance on particular parts of speech, such as nouns or adjectives. We know that politicians use stories or anecdotes as a rhetorical device to exemplify a given policy or reform (Charteris‐Black 2011). In this light, we can imagine that, per Flesch (1948), more “compelling” political texts—that invoke human interest via noun usage (over other parts of speech)—are deemed easier to understand. Such content should be modeled, but presumably it was not in previous measures due primarily to the lack of reliable, automatic natural‐language processing (NLP) tools to parse dependency structures or tag grammar. In our application below, we use modern techniques in this area to capture the role of both grammar and syntax as they affect textual sophistication.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16603</offset><text>To capture the varieties of these potential determinants of political sophistication and to determine their appropriate weights for our context, we include 22 possible indicators (described below). As with all previous investigations into the appropriate indicators of reading difficulty, we do not purport to outline a full human linguistic model of the “data‐generating process” of textual sophistication. However, our approach is able to consider a comprehensive set of domain‐specific candidate inputs, including the fixed set or fixed weights of those used in traditional measures. We leave the weight of each input's contribution as an empirical question to be tested in context, and not one whose answer can be determined from theory or from findings derived in different settings.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17401</offset><text>Improving the Statistical Properties of Sophistication Measures</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17465</offset><text>Even if we have managed to fit a “correct” set of variables to measure textual sophistication, statistical issues remain. Traditional measures are simply weighted sums—they are not fit to anything other than the original data and so do not maximize a well‐defined objective function. The immediate consequence is that we cannot know whether a given measure is performing well or not, statistically, on new texts. Thus, we cannot naturally compare different measures on the same data. Perhaps unsurprisingly given the lack of an objective function, there are also no uncertainty estimates associated with document scores. Yet surely (in the sense of Lowe and Benoit 2013) we think that, holding the indicators constant, a text with greater values of indicators related positively to sophistication provides more evidence of a given level of (latent) difficulty than a text with lower values on those indicators. Finally, using a static index approach means that fine‐grained differences in scores have essentially no useful interpretation. There are two elements to this issue: First, continuous estimates from measures like the FRE only really apply to the original schoolchildren in Flesch's study. In light of this, it is unclear what it means to say one State of the Union speech is a 70 and another is a 75 in the year 2018. Second, there is no way to convert numbers like 70 and 75 into a framework that allows probabilistic inference—we mean this both in terms of the interpretation of the point estimates and in terms of the confidence intervals around them.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19044</offset><text>In what follows, we address this problem by providing an approach generated from pairwise comparisons and ideally suited to direct, probabilistic comparisons of difficulty, either between two texts or between a text and a known baseline. We demonstrate that this key feature, combined with uncertainty estimates, provides a far more useful comparative measure for political and social science than previous approaches.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>19464</offset><text>Methodology for Fitting a Domain‐Specific Measure of Textual Sophistication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19542</offset><text>We have two broad sets of problems to solve: first, determining the appropriate inputs, and their weights, for a model of textual sophistication that fits the political context better than the simple mechanical formulas derived from education research; and second, formulating this in an explicitly statistical framework that enables the direct, probabilistic statements needed for social scientific measurement and comparison.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19971</offset><text>Get human judgments of relative textual easiness for specifically political texts. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20056</offset><text>Sample pairs of short, appropriate text segments (“snippets”) that form a minimally connected set.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20159</offset><text>Get large numbers of human judgments as to which text segment is easier for each pair, using crowdsourcing.6 </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20269</offset><text> Fit an unstructured Bradley‐Terry (Bradley and Terry 1952) model for pairwise comparisons to the judgment data from Step 1, in order to estimate a measure of latent “easiness” as equivalent to the “ability” parameter in the Bradley‐Terry framework.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20532</offset><text> Using the set of potential determinants of relative textual easiness, estimate the best predictors of the textual easiness from Step 2 using the random forests algorithm.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20705</offset><text> Using only the most highly predictive variables from Step 3, fit a structured Bradley‐Terry model using the data from Step 1.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20836</offset><text> Use the fitted model from Step 4 to “predict” the easiness parameter for a given new text, including </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20944</offset><text>Using the comparative formulation to estimate the relative probability that one new text is easier than another text, or a baseline text; and</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21086</offset><text>Using nonparametric bootstrapping of the new texts to represent uncertainty in the predicted point estimates.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21196</offset><text>Our workflow involves the following steps:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21241</offset><text>Step 5 is similar to having reengineered a classical difficulty measure, but with improved properties as a statistical estimator. In software to accompany this article, we provide this fitted model along with functions to apply it to any new text. By detailing the earlier steps, we provide not only full transparency as to how the new measure was produced, but also a reproducible workflow to enable this approach to be fit to new contexts. In the remainder of this section, we detail each of these steps.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21750</offset><text>Obtaining Human Judgments of Relative Textual Easiness</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21805</offset><text>Our measurement assumption is that human interpretation provides the “gold standard” for judging the relative sophistication of political text (or any text). Because there is no absolute metric of textual difficulty, we view this as a fundamentally comparative problem: What factors make one text more sophisticated than another? Our first step was to produce data consisting of roughly comparable, short segments of text, drawn from the political domain of interest, and obtain large numbers of human judgments as to which was easier than the other.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22361</offset><text>For data, we extracted a series of short texts of one or two sentences each—“snippets”—to be given to human coders to compare, pairwise. The coders tell us which of the two snippets is easier to understand, and they do this multiple times for various combinations of different snippets. In principle, we could have had the coders rate each snippet on some predefined scale, but experience demonstrates that humans find it considerably easier to do pairwise comparisons with respect to a trait (Montgomery and Carlson 2017; Thurstone 1927). Snippets are only segments of the original documents, of course, but asking raters to compare entire documents is infeasible. In addition, previous work based on coding document components (e.g., Benoit et al. 2016) indicates that, especially when the segments are of comparable length, this approach works well for recovering document characteristics.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23264</offset><text>Your task is to read two short passages of text, and to judge which you think would be easier for a native English speaker to read and understand. An easier text is one that takes a reader less time to comprehend fully, requires less re‐reading, and can be more easily understood by someone with a lower level of education and language ability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23611</offset><text>To obtain the pairwise judgments, we recruited large numbers of nonexperts to provide judgments in a fast, reproducible manner using a crowdsourcing platform. Crowdsourcing is a means of getting a large‐scale task completed by dividing it into many small pieces and outsourcing the pieces in random order to a distributed, anonymous worker pool known as the “crowd.” By reassembling the returned microtasks, the overall job is completed quickly and inexpensively by a pool of workers whose effort and attention adapt flexibly to the job requirements and their own willingness and availability. Our tasks used the Figure Eight crowdsourcing platform.7 The task was labeled as “Identify Which of Two Text Segments Contains Easier Language.” Upon accepting the task, the workers were shown a number of example comparisons, with one option correctly labeled as more complex, as well as a qualifying test of 10 questions from which six had to be answered correctly in order to qualify for production tasks. The specific instructions provided to each worker were as follows:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24692</offset><text>The snippets served for comparison were two‐sentence segments drawn from the 70 State of the Union addresses (SOTUs) delivered after 1950. We used these texts because the purpose of the SOTU addresses has remained relatively unchanged in the postwar period, and because of the attention these speeches have received in previous examinations of readability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25052</offset><text>Some preprocessing of the addresses prior to creating snippets was required; we removed some organizational nonsentence pieces of text (mostly referring to the medium by which the address was delivered). Once cut down for comparison, we disqualified some snippets from consideration. We dropped those which were outside the 0–121 range of the FRE, as a simple way to remove unusual texts that were much harder than an adult reader would typically encounter (note that 121 is the maximum easiness possible for the Flesch scale). We also removed snippets that contained more than two numeric years, had large numbers, or began with the title of a document section. These restrictions were put in place to avoid comparisons being made on dimensions that are not strictly connected to the regular textual content of a message.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25878</offset><text>We constrained the snippets drawn for comparison to three bands of approximately equal lengths—345–60, 360–75, and 375–90 words—to avoid comparisons in which deciding on the “easier” snippet encourages coders to simply select the one noticeably shorter than the other. From this set, we randomly selected 2,000 pairs of snippets for direct comparison. To produce the estimates from the Bradley‐Terry scaling, we also needed the pairs to be “connected” in the sense that every snippet must meet at least one snippet in a contest that meets others (there can be no “islands” of snippets that only meet each other).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26515</offset><text>The snippet pairs were assigned randomly to participants, in individual tasks consisting of 10 comparisons each. Each pair in our data set was rated at least twice by different coders.8 Coders judged a median and mean of 18 and 33 pairs, respectively.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26768</offset><text>We took standard steps (e.g., Benoit et al. 2016; Berinsky, Margolis, and Sances 2014) to stop coders from generating low‐quality comparisons due to lack of effort, fatigue, or a skill level below the task requirements. Tasks were interspersed with “gold standard” pairs, in which one snippet is unambiguously easier than the other, at a rate of 1 in 10. To create the gold standard test questions, we selected some snippet pairs with the largest disparity in FRE scores, verified through inspection. Prior to being accepted for the task, a crowd worker had to pass a qualification test consisting entirely of test questions, answering at least 7 of 10 correctly. Following successful qualification, a coder performed job lots of 10 pairwise comparisons, in which one of these was a test question. Workers who did not maintain an overall accuracy rate of 70% correct on the test questions were removed from the pool of workers and their answers dropped from the data set. To the 2,000 pairs, and the gold standard pairs, we also added another 5% of special gold “screener” questions, designed to ensure simply that the coders were paying full attention and reading each snippet completely. These screener tasks were those whose answer from the text itself was not as obvious as with the regular gold questions, but which contained explicit, embedded instructions such as “Disregard the text and code this snippet as EASIER.”</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28208</offset><text>After removing duplicates, our snippet data set consisted of 7,236 total pairings for comparison, including 836 “gold” questions, of which 310 were screeners. We crowdsourced the comparisons using a minimum of three coders per pair, yielding 19,810 total comparisons, of which 13,430 did not involve screeners or test questions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>28542</offset><text>Using the Pairwise Data to Estimate Underlying Textual Easiness</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28606</offset><text>With the human pairwise judgment data from the crowdsourcing, we were able to fit a model to estimate the latent dimension on which these texts differed, using the model for pairwise comparisons provided by Bradley and Terry (1952). This model has been applied elsewhere in political science for similar tasks (Loewen, Rubenson, and Spirling 2012; Lowe and Benoit 2013), and we therefore give only an expedited description here following the notation of Turner and Firth (2012).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29085</offset><text>The input data are the result of our human coders' having declared winners in the large number of “easiness contests” between snippets. For a given contest, crowd workers must decide which of two snippets i and j is easier to comprehend (no ties are allowed). If the easiness of these snippets is  and , respectively, then the odds that snippet i is deemed easier than j may be written as .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29480</offset><text>Defining , the regression model can be rewritten in logit form:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29546</offset><text>Subject to specifying a particular snippet as a “reference snippet” (whose easiness is set to zero), this setup allows for maximum likelihood estimation of each snippet's easiness (technically, the logarithm of the easiness).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29776</offset><text>This unstructured Bradley‐Terry model rests on several assumptions. First, we posit the outcomes of the contests are (statistically) independent of one another: that the result of the kth contest does not affect the result of the th contest. Here, of course, the players (the snippets) are inanimate objects, so there are, for example, no “experience” effects from winning or losing. Still, it could be the case that coders see a snippet early on and deem it to have a general quality which then biases their assessment of it later (in whatever contests it appears). We are not overly concerned. For one thing, in 84% of the contests in our main data, the coders involved only saw a given snippet once in the entirety of their work for us. So any effects are likely to be small.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30562</offset><text>Second, we made no allowance for variability between snippets through any sort of random effects, either in this unconditional model or in the structured version used below, for snippets which have otherwise identical covariate values. That is, we are not using any kind of random effects for the snippets themselves. This is because we want other researchers to use our technique to model their data (which presumably does not contain the same exact snippets)—that is, we care about the external portability of the work rather than the best possible local model fitting.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31137</offset><text>As a result of fitting Equation (1) to our pairwise data, we obtained estimates of  for each text snippet, as an unconditional estimate of that text's relative easiness.9 Our task in the next step was to determine the predictors of this outcome using a separate model.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>31408</offset><text>Selecting Predictors Using Random Forests</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31450</offset><text>We have a large number of potential determinants of textual sophistication whose relative contribution to textual sophistication needs to be tested and fitted empirically—the 22 variables listed in Table 1. We have grouped the variables in terms of whether they refer to longer words, rarer words, longer sentences, or more difficult content. We also list the variable names associated with each factor and indicate any traditional readability measures that include them as inputs. Those based on a corpus baseline of rarity, such as the Google and Brown word rarity measures, or the parts of speech and dependent clause measures, are novel to our approach.</text></passage><passage><infon key="file">ajps12423-tbl-0001.xml</infon><infon key="id">ajps12423-tbl-0001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>32112</offset><text>Determinants of Textual Complexity</text></passage><passage><infon key="file">ajps12423-tbl-0001.xml</infon><infon key="id">ajps12423-tbl-0001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr style=&quot;border-bottom:solid 1px #000000&quot;&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Source of Complexity&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Variable Name&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Used by These Measures&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Long Words&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean characters per word&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanWordChars&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ARI; Bormuth; Coleman‐Liau&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with at least 7 characters&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W7C&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LIX&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with at least 6 characters&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W6C&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Harrison‐Jacobson&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean syllables per word&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanWordSyllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Flesch; Flesch‐Kincaid; Tuldava&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with at least 3 syllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W3Sy&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FOG; SMOG&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with fewer than 3 syllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Wlt3Sy&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FOG‐NRI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with 2 syllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W2Sy&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ELF; Wheeler‐Smith&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words with 1 syllable&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W_1Sy&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FJP; FORCAST&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Rare Words&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google Books baseline usage&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;google_min&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;google_mean&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Brown corpus baseline usage&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;brown_mean&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;brown_min&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words in the Dale‐Chall list&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;W_wl.Dale.Chall&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dale‐Chall; Bormuth; Spache&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Long Sentences&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean characters per sentence&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanSentenceChars&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Danielson‐Bryan&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean sentence length in words&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanSentenceLength&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Flesch; Flesch‐Kincaid; ARI; Bormuth&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dale‐Chall; FJP; FOG; Spache; LIX;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Tuldava; Wheeler‐Smith; Harrison‐Jacobson&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of sentences per character&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_sentence&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Coleman‐Liau&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean sentence length in syllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanSentenceSyllables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Strain&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Complex Content&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proportion of nouns&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_noun&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proportion of verbs&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_verb&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proportion of adjectives&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_adjective&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proportion of adverbs&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_adverb&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Average subordinate clauses&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_clause&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(new)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>32147</offset><text>Source of Complexity	Variable Name	Used by These Measures	 	Long Words			 	Mean characters per word	meanWordChars	ARI; Bormuth; Coleman‐Liau	 	Words with at least 7 characters	W7C	LIX	 	Words with at least 6 characters	W6C	Harrison‐Jacobson	 	Mean syllables per word	meanWordSyllables	Flesch; Flesch‐Kincaid; Tuldava	 	Words with at least 3 syllables	W3Sy	FOG; SMOG	 	Words with fewer than 3 syllables	Wlt3Sy	FOG‐NRI	 	Words with 2 syllables	W2Sy	ELF; Wheeler‐Smith	 	Words with 1 syllable	W_1Sy	FJP; FORCAST	 	Rare Words			 	Google Books baseline usage	google_min	(new)	 		google_mean	(new)	 	Brown corpus baseline usage	brown_mean	(new)	 		brown_min	(new)	 	Words in the Dale‐Chall list	W_wl.Dale.Chall	Dale‐Chall; Bormuth; Spache	 	Long Sentences			 	Mean characters per sentence	meanSentenceChars	Danielson‐Bryan	 	Mean sentence length in words	meanSentenceLength	Flesch; Flesch‐Kincaid; ARI; Bormuth	 			Dale‐Chall; FJP; FOG; Spache; LIX;	 			Tuldava; Wheeler‐Smith; Harrison‐Jacobson	 	Number of sentences per character	pr_sentence	Coleman‐Liau	 	Mean sentence length in syllables	meanSentenceSyllables	Strain	 	Complex Content			 	Proportion of nouns	pr_noun	(new)	 	Proportion of verbs	pr_verb	(new)	 	Proportion of adjectives	pr_adjective	(new)	 	Proportion of adverbs	pr_adverb	(new)	 	Average subordinate clauses	pr_clause	(new)	 	</text></passage><passage><infon key="file">ajps12423-tbl-0001.xml</infon><infon key="id">ajps12423-tbl-0001</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>33516</offset><text>
Note: Summary of existing measures is taken from Michalke (2017).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>33583</offset><text>Longer Words</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33596</offset><text>We have various measures of word length: count of words with more than one (W_1Sy), two (W2Sy), and three syllables (W3Sy); count of words with fewer than three syllables (Wlt3Sy); count of words with at least six (W6C) or at least seven (W7C) letters; mean number of syllables per word (meanWordSyllables); and mean number of characters per word (meanWordChars).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>33960</offset><text>Rarer Words</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33972</offset><text>We have various measures of word rarity, including membership in the Chall and Dale (1995) word list (W_wl.Dale.Chall). But for measuring more helpful usage rates, we drew on the frequencies of words relative to the frequency of the most common word in the English language—the—from two large baseline corpora: the Brown corpus (Francis and Kucera 1964) and the Google Books corpus (Michel et al. 2011). For each baseline corpus, we computed a measure of its average word's relative frequency (brown_mean and google_mean) and its least frequent word's relative frequency (brown_min and google_min).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34576</offset><text>The Google Book corpus offers one key advantage over the Brown corpus: It consists of unigram term frequencies specific to the year in which they were written, ranging from 1505 to 2008 (whereas the Brown corpus was collected at one point of time in the 1960s).10 We thus obtain a relative term frequency that is specific to each year. Normalizing relative to the frequency of the term the provided a temporally grounded benchmark because its relative frequency has remained relatively unchanged in several hundred years. This allowed us to compare the relative frequencies of terms without being affected by changes in overall word quantities or transcription accuracies (which vary significantly over the time in the data set). After filtering out tokens that occurred fewer than five times or that did not match a dictionary of 133,000 English words and word forms, we ended up with a table of frequencies for 82,558 unique word types from the total Google corpus. To smooth out yearly idiosyncracies, we aggregated the term frequencies by decade.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35627</offset><text>By comparing the frequencies of SOTU addresses to their baseline frequencies in the decade they were written, we were able to distinguish between words that appear to be difficult today but were not in the decade they were written, and words that were genuinely difficult because they were rare even when written. To use our example above, the inclusion of the word husbandry in a contemporary speech should be considered as “harder” for a contemporary audience (e.g., our crowd coders) than it was in the nineteenth century when its use was relatively common. In Supporting Information A, we give an intuitive example of how rarity “works” for comparing speech snippets.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>36309</offset><text>Longer Sentences</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36326</offset><text>We have various measures of sentence length: mean sentence length (meanSentenceLength), mean sentence syllables (meanSentenceSyllables), and mean sentence characters (meanSentenceChars). As a final measure, we divide the number of sentences by the number of characters in the snippet (pr_sentence).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>36625</offset><text>More Complex Content</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36646</offset><text>We measure this complexity in two ways: first, by computing the relative balance of different grammatical forms, represented by parts of speech; and second, by assessing the structure of clause dependencies using a dependency parser. It is possible that different types of words make a text more or less difficult, so for each snippet, we record the proportion of nouns (pr_noun), verbs (pr_verb), adjectives (pr_adjective), and adverbs (pr_adverb). Parts of speech were identified using the spaCy NLP library (Honnibal and Montani 2019). We give more details in SI Appendix C. To measure structural complexity, we also used spaCy to count the number of independent clauses a text contains, normalized by its length in characters (pr_clause).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37389</offset><text>To estimate the relationship of each variable to our Bradley‐Terry estimates of a snippet's easiness, we used random forests (Breiman 2001). This allowed us to confront two closely related estimation problems: having a large number of variables, and having very high correlations among these variables at the snippet level. Given that we want a formula that is both parsimonious and general, we need to reduce the dimensions of the problem significantly, while also having interpretable estimates. Random forests produced estimates of the relative importance of each variable to predicting the outcome, which we can then use to select the most helpful predictors. This approach's main advantages for our problem are accuracy, efficiency, an “automatic” importance measure, and a low risk of overfitting relative to other tree‐based classifiers (for discussion, see Montgomery and Olivella, 2018). Other scholars, such as Montgomery and Carlson (2017), have used item response models for similar tasks. We chose random forests because we did not wish to estimate coder effects, but focused instead on producing an importance ranking of predictors. Of course, other algorithms such as support vector machines allow feature ranking, but typically these require more tuning from the analyst and their task performance is not as good (see Caruana, Karampatziakis, and Yessenalina 2008 for evaluation of various methods for high‐dimensional data).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>38841</offset><text>Using the Selected Predictors to Fit a Structured Bradley‐Terry Model</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>38913</offset><text>With the selected predictors from the previous stage, we refit the Bradley‐Terry model to the pairwise contests, but in a structured form using the selected predictors as covariates. This made the easiness of the snippets conditional on a set of covariates , reparameterizing the easiness  of a given snippet as  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39229</offset><text>From this structured Bradley‐Terry model, the estimated  coefficients tell us the marginal effect of each x‐variable on the perceived (relative) easiness of the snippets. Once the s are obtained, we can used these to predict a  for any (new) text for which we can compute the necessary covariate values.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39538</offset><text>In fitting the structured model, we did not include so‐called “contest‐specific predictors” either indirectly—such as effects for (the proclivities of) given human coders—or directly by allowing for consequences of the order in which the snippets were presented to the subjects who judged them. Not only are such effects hard to estimate in a world in which the median coder only performs 18 comparisons, but also experience shows that once filtered through the minimum quality threshold, no relevant coder characteristics remain that correlate with coding decisions in a way that materially affects the quality of the resulting data (for discussion, see Benoit et al. 2016).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>40227</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40235</offset><text>With the unstructured Bradley‐Terry estimates of each snippet's textual “easiness,” we were then in a position to determine which of our potential set of 22 predictors best predicts easiness, and to what extent, in our context and to compare this with more traditional measures. Prior to this, we compared the traditional approaches with one another as described in SI Appendix E: They do essentially equally well, but we focus on comparisons with the FRE, because it is most familiar, in what follows.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>40745</offset><text>Fitting the Structured Model to the Training Data</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40795</offset><text>To gauge the contribution of our candidate predictors on the unstructured “easiness” measures, we compared the random forests results in terms of model fit and percent correctly predicted.11 </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40991</offset><text>Our initial supervised model suggested the key predictors of easiness were the time‐specific rarity of the least frequently used word (google_min), the average sentence length measured in characters (meanSentenceChars), and the proportion of nouns (pr_noun). These variables collectively were both small in number (allowing for a simple formula) and most “important” in the random forests–specific sense discussed in SI Appendix F. In relation to the classical measures, sentence length has long been a common element of readability indexes, but our inclusion of word rarity and the measure of nouns is novel.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41610</offset><text>To assess the performance of this model in predicting the pairwise contests, and to compare it to the most common classical measures, we constructed a baseline model that uses the FRE as its (only) covariate content. We did this in two ways. First, we include the FRE of the snippet using the weights from Flesch's (1948) original formula. Second, we include the variables Flesch (1948) includes but allow the model to calculate the optimal weights for our political data. In Table 2, we report the findings from those models, in the two leftmost columns. For the “FRE baseline” model (original weights), we see that the Akaike information criterion (AIC) is 26267.79, and the augmented proportion (of contests in the data) correctly predicted (PCP) is 0.719. When we allow the weights on the relevant variables to adjust to local conditions (column 2), we see a commensurately better model fit—the AIC falls to 25910.29, and the proportion correctly predicted rises to 0.737. This is in line with our thinking above: in particular, that models work best when fit to relevant data. Column 3 represents our basic three‐variable model as discussed above. Clearly, it does better than the Flesch model with the original weights, but—perhaps surprisingly—not as well as the reweighted version (AIC is higher).</text></passage><passage><infon key="file">ajps12423-tbl-0002.xml</infon><infon key="id">ajps12423-tbl-0002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>42929</offset><text>Comparing the Performance of the Structured Models</text></passage><passage><infon key="file">ajps12423-tbl-0002.xml</infon><infon key="id">ajps12423-tbl-0002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;right&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;right&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;right&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;right&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr style=&quot;border-bottom:solid 1px #000000&quot;&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FRE Baseline&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FRE Reweight&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Basic RF Model&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Best Model&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FRE&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.02&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.00)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanSentenceLength&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.06&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.00)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanWordSyllables&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−1.79&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.07)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;google_min&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1298.14&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1318.65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(153.07)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(155.64)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanSentenceChars&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.01&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.00)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.00)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_noun&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.43&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.17)&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.17)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;meanWordChars&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;(0.02)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19,430&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19,430&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19,430&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19,430&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AIC&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26267.79&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25910.29&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25915.01&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25740.25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Prop correctly predicted&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.719&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.737&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.738&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.741&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;[95% CI]&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;[0.710, 0.727]&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;[0.728, 0.747]&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;[0.729, 0.748]&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;[0.733, 0.751]&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>42980</offset><text>	FRE Baseline	FRE Reweight	Basic RF Model	Best Model	 	FRE	0.02				 		(0.00)				 	meanSentenceLength		−0.06			 			(0.00)			 	meanWordSyllables		−1.79			 			(0.07)			 	google_min			1298.14	1318.65	 				(153.07)	(155.64)	 	meanSentenceChars			−0.01	−0.01	 				(0.00)	(0.00)	 	pr_noun			0.43	0.31	 				(0.17)	(0.17)	 	meanWordChars				−0.31	 					(0.02)	 	N	19,430	19,430	19,430	19,430	 	AIC	26267.79	25910.29	25915.01	25740.25	 	Prop correctly predicted	0.719	0.737	0.738	0.741	 	[95% CI]	[0.710, 0.727]	[0.728, 0.747]	[0.729, 0.748]	[0.733, 0.751]	 	</text></passage><passage><infon key="file">ajps12423-tbl-0002.xml</infon><infon key="id">ajps12423-tbl-0002</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>43540</offset><text>
Note: Standard errors are in parentheses.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43584</offset><text>This model (“Basic RF Model”) did not include a measure of word length, despite this feature's being one of the two core components of the FRE. From the results of variable importance (presented graphically in SI Appendix E), the most important measure of word length that predicts easiness is the average number of characters per word (MeanWordChars). We added this variable to our machine learning model and refit the structured Bradley‐Terry model, shown in the fourth column of Table 2 (headed “Best Model”). This model outperforms every other version, with the lowest AIC (25740.25) and the highest PCP (0.741). In an effort to ascertain the robustness of this model, we dropped the parts‐of‐speech variable (pr_noun) and added the next highest rated one (pr_adjective). The fit of the model was essentially identical. In what follows, we work with the one that uses the part of speech—nouns, in this case—that the learner preferred in importance terms.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44563</offset><text>While full details of the random forest models that we ran on the unstructured abilities, along with variable importance plots, are provided only in SI Appendix F, we note that all variable effects were both in the expected directions and statistically significant at conventional levels. The higher the relative frequency of the least frequent word (relative to the), the easier the snippet was to understand. Snippets that contained longer sentences and longer words were both judged to be less easy to understand. Finally, increasing the proportion of nouns was also associated with increased easiness.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45170</offset><text>I speak to you not just as a President, but as a father, when I say that responsibility for our children's education must begin at home.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45308</offset><text>To gauge the significance of the differences in accuracy of the reported models, we provide a bootstrapped 95% confidence interval on the percent correctly predicted, based on 500 sentence‐level resamples. Our key observation is that the confidence interval for the fit of the final model does not overlap with the FRE model, implying that it is indeed better in a statistical sense. On what types of data, exactly, does our model perform better? Unsurprisingly, it performs best when two documents are similar other than the proportion of nouns they contain, or the rarity of their words. In the contests for which our model outperforms the Flesch version to the greatest extent, it is the word rarity input that matters most. To get a sense of this, compare these two snippets. The first is from Obama's 2009 address and has an FRE of around 50:  </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46160</offset><text>The first cession was made by the State of New York, and the largest, which in area exceeded all the others, by the State of Virginia.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46296</offset><text>The second is from Cleveland's 1889 effort,12 which has an FRE of approximately 67:  </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46382</offset><text>The FRE model predicts this to be a relatively straightforward win for Cleveland's speech. Our model, of course, penalizes the estimate of its simplicity due to the presence of the relatively rare term cession (along with there being slightly fewer nouns in the second document). Indeed, the frequency of the least common term in Obama's speech is over three orders of magnitude larger than that of Cleveland's speech—something that our approach clearly captures but that traditional indices cannot.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46885</offset><text>It is helpful to be candid about several issues pertaining to our results. First, clearly, while we are outperforming the most widely used measure of readability, our gains are not huge in an absolute sense. The largest gains in predictive accuracy come from refitting the Flesch model appropriately to the data rather than using its usual “off‐the‐shelf” mode. Nonetheless, these gains are reasonable in a relative sense. The baseline Flesch predictive accuracy was 71.9%—about 22 percentage points better than chance. Our final model is 24.3 percentage points better than chance, a relative increase of around 11%. But this increase is “real” per our discussion of the bootstrap results above. Third, whether or not one uses our specification, the general approach—of training on relevant data and providing model‐based estimates—is preferable for the reasons given above. Even if one simply wanted to use the Flesch setup (in terms of its component variables), based on Table 2 we would recommend fitting to domain data for that purpose.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>47948</offset><text>Applying Probabilistic Comparisons</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>47983</offset><text>Using the fitted four‐covariate “Best Model” from Table 2, we can estimate a fitted easiness score for any text. There are two ways of applying this model. First, given Equations (1) and (2), we can obtain a (point) estimate of the probability that any given text i is easier (or conversely, more difficult) than any other text j by calculating  </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48338</offset><text>If we do these things—end social promotion; turn around failing schools; build modern ones; support qualified teachers; promote innovation, competition and discipline—then we will begin to meet our generation's historic responsibility to create 21st century schools. Now, we also have to do more to support the millions of parents who give their all every day at home and at work.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48724</offset><text>To see how this works, consider two snippets; the first one is from Clinton (1999):  </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48810</offset><text>And the victory of freedom in Iraq will strengthen a new ally in the war on terror, inspire democratic reformers from Damascus to Tehran, bring more hope and progress to a troubled region, and thereby lift a terrible threat from the lives of our children and grandchildren. We will succeed because the Iraqi people value their own liberty—as they showed the world last Sunday.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>49190</offset><text>The second one comes from George W. Bush (2005):  </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>49241</offset><text>For each of these snippets, Table 3 gives the relevant covariate values for our best model above. Using the coefficients from Table 2, it is a simple matter of matrix multiplication to form the  values and to compute the probability that the Clinton text is easier than the Bush text.13  </text></passage><passage><infon key="file">ajps12423-tbl-0003.xml</infon><infon key="id">ajps12423-tbl-0003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>49532</offset><text>Examples of Covariates from Two Snippets in the Data</text></passage><passage><infon key="file">ajps12423-tbl-0003.xml</infon><infon key="id">ajps12423-tbl-0003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;char&quot; char=&quot;.&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;char&quot; char=&quot;.&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr style=&quot;border-bottom:solid 1px #000000&quot;&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Variable&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Clinton&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bush&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;google_min rarity when speech given&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.65e‐04&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.40e‐08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MeanSentenceChars&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;155.50&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;153.50&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;pr_noun&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.30&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MeanWordChars&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.94&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;mml:math id=&quot;nlm-math-18&quot;&gt;&lt;mml:msub&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;
&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−2.64&lt;/td&gt;&lt;td align=&quot;char&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−2.93&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>49585</offset><text>Variable	Clinton	Bush	 	google_min rarity when speech given	2.65e‐04	1.40e‐08	 	MeanSentenceChars	155.50	153.50	 	pr_noun	0.30	0.23	 	MeanWordChars	4.94	4.72	 		−2.64	−2.93	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>49769</offset><text>We can also compare each text to a common baseline text, for instance, to a corpus of texts judged to be at a fifth‐grade reading level. We obtained examples of such texts from a university education department14 and estimated the relevant λ to be −2.184507. Thus, the probability that the Clinton text is easier than a fifth‐grade text is estimated to be 0.259, and the probability that the Bush text is easier to follow than the fifth‐grade works is 0.209.15 We can place confidence intervals around the point prediction by bootstrapping the sentences in the texts (in the sense of Lowe and Benoit 2013), where each replicate produces a new computation of the covariate values and then is used to compute fitted values given the estimated model. Note that the differences between texts mean something extremely well defined here: We can make concrete statements about how much easier one document is relative to another, and the quantity refers back to a sensible model. This is quite unlike the FRE, for which a difference of 5 points on the scale has no natural, cardinal interpretation.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>50871</offset><text>Along with model‐based estimates, researchers may also want a quantity analogous to the continuous 0–100 scores from the Flesch (1948) (regression) formula. In Figure 1, we have rescaled the λs (i.e., the s, without applying the exponential function) such that texts measured to be at the fifth‐grade level receive a value of 100 and those at the postcollege level a value of 0.16 </text></passage><passage><infon key="file">AJPS-63-491-g002.jpg</infon><infon key="id">ajps12423-fig-0001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>51261</offset><text>Comparing Our Rescaled Measure to the FRE of the Snippets</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>51319</offset><text>Experimenting with the continuous measure on the SOTU snippet corpus performs well in the sense that it returns point estimates on a roughly 0–100 scale commensurate (but not identical) to the FRE equivalents. This works because it replaces a logit‐style calculation that is not linear in the predictors with a linear sum (i.e., ), exactly like the regression‐based formula for the FRE. In Figure 1, we provide a scatterplot of our measure for the snippets (y‐axis) relative to the FRE for the same data (x‐axis), along with the line of linear fit. The correlation over the full range of points (∼0.7) is reasonably large and positive. Within the (theoretical) minimum and maximum of the FRE range of 0–100, however, the correspondence is even higher. This implies that for the great majority of documents for which the FRE is used, our measure—preferred on theoretical grounds—is a good choice that will behave as expected. Outside the 0–100 range, particularly to the bottom left of the plot, our measure tends to assign a considerably harder score for the hardest texts.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>52415</offset><text>Reanalyzing the State of the Union Addresses</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>52460</offset><text>Just as we demonstrated how to apply the fitted model to other short texts to estimate their easiness level, we can also apply this to each SOTU address in its entirety. Using our model‐based probability measure—here, with a fifth‐grade text as a baseline for comparison—Figure 2 plots the relevant point estimates and 95% (simulated) confidence intervals (y‐axis) plotted against the date of the relevant text. The probability estimates are drifting upward over time, but generally stay below 0.50. But because we are using a well‐defined statistical model, we can say more about the data. In particular, the confidence intervals allow us to make comments about sampling uncertainty. Note that there is considerable overlap between the intervals for the postwar period (e.g., example, some of the speeches in the early 2000s are not so different from those in the early 1950s). This implies that statements about the simplification of language may be correct in some aggregate sense if we consider the entire period since the founding of the Republic, but less clear for modern times specifically.</text></passage><passage><infon key="file">AJPS-63-491-g003.jpg</infon><infon key="id">ajps12423-fig-0002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>53572</offset><text>Probability That a State of the Union Address Is Easier to Understand Than a Fifth Grade Text Baseline, Compared to FRE</text></passage><passage><infon key="file">AJPS-63-491-g003.jpg</infon><infon key="id">ajps12423-fig-0002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>53692</offset><text>
Note: The points and associated vertical lines are probability estimates and 95% confidence intervals for our measure. The blue line is the loess fit of half the ratio of the FRE for the SOTU to the FRE of the fifth‐grade text corpus.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>53930</offset><text>For the closest equivalent to a direct comparison with more traditional approaches, Figure 2 plots the ratio of the FRE for each SOTU speech compared to a corpus of texts designated to be at the fifth‐grade reading level,17 and shown by the smoothed loess line. The measures agree in terms of general direction—addresses become easier over time—but differ in terms of magnitude. In particular, our measure has the speeches prior to around 1910 being considerably more difficult to understand than the FRE claims they were. Post‐1910, our measure tends to have the estimated ease of understanding the passages as higher than the FRE. To the extent that one believes new technology, such as the radio and the television, leads to speeches that are easier to follow after the first decade of the twentieth century, this makes sense. And, to reiterate, our model is actually trained on appropriate, political data with local, decade‐specific, word rarity measures.18 </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>54905</offset><text>Comparing on a Dimension of Political Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>54952</offset><text>A pleasing feature of our approach is that it facilitates direct comparison of texts that differ with respect to some metadata or covariates of interest to produce probabilistic statements about those differences. To demonstrate this, we compare how the complexity of written SOTU addresses differs from that of spoken ones. Because the former medium of delivery was historically much more prevalent and the latter is the norm now, meaningful comparisons are difficult because there is obvious confounding over time and across authors. In 1945, 1956, 1972, and 1974 and from 1978 to 1980, however, each president delivered two SOTU speeches, one spoken and one delivered in writing to Congress, on the same day, and on the same topics. Since all else was generally equal except the medium of communication, this allows us to compare directly the degree of textual sophistication for written versus spoken texts.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>55865</offset><text>Figure 3 plots the results, showing the probability that the spoken address was easier than its written counterpart. Across the set of seven paired addresses, the probability was between about 0.54 and 0.64 that the spoken address was easier. Speculatively, this may help to explain recent trends toward easier and easier addresses by presidents: They are giving them as speeches rather than as written text. Our framework makes this comparison possible using explicit probability statements</text></passage><passage><infon key="file">AJPS-63-491-g004.jpg</infon><infon key="id">ajps12423-fig-0003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56359</offset><text>Probability That a Spoken SOTU Address Was Easier to Understand Than Its Written Counterpart</text></passage><passage><infon key="file">AJPS-63-491-g004.jpg</infon><infon key="id">ajps12423-fig-0003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56452</offset><text>
Note: The lines represent the 95% confidence intervals from bootstrapping.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>56528</offset><text>Summary and Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>56551</offset><text>The nature of the messages that political actors send one another is of key interest to political science, whether it be in American politics, in international relations, or from a comparative perspective. Yet a curious gulf has emerged in our studies. On the one hand, we have plenty of theory and empirical evidence that such communication matters: whether it be “dog whistle” in nature (Albertson 2015), rhetorical (Riker 1996), vague (Lo, Proksch, and Slapin 2016), or more explicitly designed to appeal to certain types of agents. On the other hand, the discipline has been slow to adopt textual complexity measures in any context. This is despite the fact that the various readability measures are easy to use and scale in a straightforward way, which is important given the sheer amount of textual data now available to scholars. Presumably, part of this reticence is lack of familiarity with such approaches. But part of it is likely a very reasonable skepticism about the merits of these educational measures—a concern echoed in other fields of social science (e.g., Loughran and McDonald 2014; Sirico 2008) and, indeed, increasingly in education itself (Ardoin et al. 2005).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>57743</offset><text>Rather than attempt to rehabilitate the indices, here we focused on producing something better, considering all possibly relevant inputs, using a statistical method for determining which inputs explain textual complexity in our context and how, and using an explicitly comparative framework built on pairwise comparisons. In Table 4, we summarize our contribution relative to problems in using traditional readability measures to estimate the textual sophistication of political text.</text></passage><passage><infon key="file">ajps12423-tbl-0004.xml</infon><infon key="id">ajps12423-tbl-0004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>58230</offset><text>Summary of Our Approach as a Solution to a Series of Problems with Traditional Approaches</text></passage><passage><infon key="file">ajps12423-tbl-0004.xml</infon><infon key="id">ajps12423-tbl-0004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr style=&quot;border-bottom:solid 1px #000000&quot;&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dimension&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Traditional Approach&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Our Approach&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Development context&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Education research&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Political text&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Test subjects&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Schoolchildren&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Adults&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Temporal context&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Readers in 1940s/50s, not easily updated&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Contemporary readers, easy to update (via crowdsourcing)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Assessing model fit&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cannot assess quality/fit of predictions for documents&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Straightforward to assess absolute model fit (in training set) via usual metrics like percent correctly predicted&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Comparison of different measures&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cannot compare models of different forms&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Straightforward to assess relative model fit (in training set) via usual metrics like AIC, BIC&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Interpreting differences&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cannot interpret fine‐grained differences in document scores&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Natural model‐based interpretation of document estimates (via Bradley‐Terry model)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Uncertainty accounting&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No uncertainty around estimates&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Uncertainty estimates available both for variables in model and on document scores (via bootstrapping)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Selecting inputs and assigning weights&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Composite indices/aggregate form hides changes in input variables&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Straightforward to examine all changes to component parts&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rarity of term usage&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rarity of terms accounted for in ad hoc, inflexible way, if at all&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rarity of terms systematically derived from large corpus, and available for any period of interest in past 200 years.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>58320</offset><text>Dimension	Traditional Approach	Our Approach	 	Development context	Education research	Political text	 	Test subjects	Schoolchildren	Adults	 	Temporal context	Readers in 1940s/50s, not easily updated	Contemporary readers, easy to update (via crowdsourcing)	 	Assessing model fit	Cannot assess quality/fit of predictions for documents	Straightforward to assess absolute model fit (in training set) via usual metrics like percent correctly predicted	 	Comparison of different measures	Cannot compare models of different forms	Straightforward to assess relative model fit (in training set) via usual metrics like AIC, BIC	 	Interpreting differences	Cannot interpret fine‐grained differences in document scores	Natural model‐based interpretation of document estimates (via Bradley‐Terry model)	 	Uncertainty accounting	No uncertainty around estimates	Uncertainty estimates available both for variables in model and on document scores (via bootstrapping)	 	Selecting inputs and assigning weights	Composite indices/aggregate form hides changes in input variables	Straightforward to examine all changes to component parts	 	Rarity of term usage	Rarity of terms accounted for in ad hoc, inflexible way, if at all	Rarity of terms systematically derived from large corpus, and available for any period of interest in past 200 years.	 	</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59650</offset><text>To get the pairwise comparisons needed to fuel our context‐based estimates, we used human coders (via the crowd) to provide relative assessments of short texts, and from there we built a well‐defined statistical model. That model uses variables that differ from standard approaches, including word rarity and parts‐of‐speech information. The final version performs better in fit terms too, although precisely because the approach is on much firmer probabilistic grounds it is hard to compare directly to previous approaches. Fundamentally, then, we have improved practice here: The approach is transparent, sensible, and model‐based and trained on relevant domain data. It is also flexible, in the sense that the workflow and software we have designed allow end users to calibrate the method to their specific problems.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>60480</offset><text>On the question raised in our introduction—“is discourse being dumbed down?”—our purpose here is less to provide a decisive answer as to provide tools for more accurately answering this question. Certainly, the State of the Union addresses have become easier to comprehend in the modern era. The actual political sophistication of a political message, however, depends more on the content of the message. Traditional measures based on static indexes of readability are unable to capture this directly; for example, shorter sentences may be a good or bad thing, depending on the context. And that context is more likely to be captured via local fitting (to the type of text at hand), measuring the grammatical structure of the documents, and the rarity of the terms they use. These are precisely the things our approach can model. By outlining a flexible approach to the problem, furthermore, we facilitate comparisons of different inputs' effect on textual sophistication, allowing more precise answers to the sources and nature of the trend to growing sophistication in political speech. Finally, we note that prior to our efforts here that use historical benchmarks for familiarity, we had very little idea about whether the documents in question were unusual relative to what readers would have experienced at the time. That is, although not the focus of our work here, we can get some sense of how similar the structure of, say, the SOTU of 1815 was to other readings on offer that year. Put crudely, if the SOTU from that time is much more erudite than the one in 2015, but simultaneously much harder to understand than the average (Google Books) text in 1815, it gives claims about dumbing down a very different complexion.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>62219</offset><text>While our contribution will be helpful for those interested in characterizing political communication, it is hardly the last word on the matter. We have provided a statistical machinery, and variables, for thinking more carefully about the measurement of sophistication or clarity in texts. What we have not done is produced a straightforward way to distinguish between more subtle understandings of such concepts. For example, one can imagine a politician—a president of the United States even—who uses relatively common terms in simple sentence constructions but it is not especially clear. By contrast, great academic writers might be able to describe extremely complicated ideas in straightforward ways for popular audiences. Our approach would generally be better than previous ones, but it is still unlikely to place these two extremes correctly on the same scale. This is, of course, because a sophisticated idea (like democracy, or inclusivity or conservatism) need not be complicated in expression, and vice versa. More attempts should be made—not least at the coding/crowdsourcing level—to iron out these differences, possibly by introducing different dimensions of complexity at the point of testing or modeling. A related next step would be to make all of the variables dynamic, for instance, measuring the proportion of nouns in a text relative to a baseline noun usage from the time the document was written. We leave such efforts for future work.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>63689</offset><text>Supporting information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>63712</offset><text>For instance, see “Trump Speaks at Fourth‐Grade Level, Lowest of Last 15 U.S. Presidents, New Analysis Finds,” Newsweek, January 8, 2018. http://www.newsweek.com/trump‐fire‐and‐fury‐smart‐genius‐obama‐774169. See also “The State of Our Union Is … Dumber: How the Linguistic Standard of the Presidential Address Has Declined,” The Guardian, February 12, 2013. http://www.theguardian.com/world/interactive/2013/feb/12/state‐of‐the‐union‐reading‐level </text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>64198</offset><text>But perhaps not complex for them: For example, a statistics text might use the terms moment and distribution in a way that is not ambiguous to a political methodologist.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>64369</offset><text>But perhaps not sophisticated: For example, reading a noncommissioned officer's diary from the American Civil War might be hard work for a modern reader, but not because it is discussing abstruse themes.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>64574</offset><text>In practice, the statistic is bounded at an upper “ease” limit of 121.22 for texts consisting of one‐syllable, one‐word sentences and bounded from below only by an offset of the average word and sentence length.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>64795</offset><text>This was later expanded to around 3,000 words. The formula has also been adjusted over time (Chall and Dale 1995), but originally, it was 0.1579 (percentage of difficult words) + 0.0496 .</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>64983</offset><text>Because our pretests indicated that it was more straightforward to ask raters which text was easier, our subsequent discussion is about relative easiness rather than difficulty (similar to the original Flesch scale, in which higher values indicated easier texts).</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>65247</offset><text>During our data collection, this company was known as Crowdflower. See Appendix B in the supporting information (SI) for details.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>65378</offset><text>In some very rare cases, less than 1% of all contests, a coder would see a particular pair more than once.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>65486</offset><text>In practice, it is occasionally the case in our sample that a snippet never wins or never loses. The usual consequence of this kind of data separation would be infinite ability estimates. In one run of the model, we simply deleted those missing values, and in another we used the bias‐reduction technique of Firth (1993) to ameliorate this problem. The results, in terms of the variable importance order, are essentially identical.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>65921</offset><text>See http://storage.googleapis.com/books/ngrams/books/datasetsv2.html.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>65991</offset><text>Interpreting our results requires dealing with a subtle problem in calculating the denominator of the model proportion correctly predicted, which stems from the fact that the coders do not always agree on a “correct”' answer. We adjust for this in a way described in SI Appendix D.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66279</offset><text>This snippet appears per our discussion in SI Appendix B about including some older texts from an earlier pilot study.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66399</offset><text>Just for presentational sanity here, we are rounding all values. This has inevitable precision loss, and values produced by our software will differ in practice for such examples.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66580</offset><text>See https://projects.ncsu.edu/project/lancet/fifth.htm.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66636</offset><text>Here, we are using computer precision for our calculations.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66697</offset><text>We used the collection of fifth‐grade texts we mentioned above for the easy end of the scale, and the most difficult snippet (which had an FRE of around 3) for the “hard” end.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>66880</offset><text>This is divided by 2 to ensure normalization in the sense that two texts of equal difficulty should have probability 0.50 of beating each other.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>67026</offset><text>In SI Appendix G, we look at the way our “dynamic” adjustments affect our aggregate estimates: The differences are not huge, but they are in the expected direction.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>67196</offset><text>References</text></passage><passage><infon key="fpage">3</infon><infon key="issue">1</infon><infon key="lpage">26</infon><infon key="section_type">REF</infon><infon key="source">Political Behavior</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2015</infon><offset>67207</offset><text>Dog‐Whistle Politics: Multivocal Communication and Religious Appeals</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">22</infon><infon key="section_type">REF</infon><infon key="source">School Psychology Quarterly</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2005</infon><offset>67278</offset><text>Accuracy of Readability Estimates' Predictions of CBM Performance</text></passage><passage><infon key="fpage">278</infon><infon key="issue">2</infon><infon key="lpage">95</infon><infon key="section_type">REF</infon><infon key="source">American Political Science Review</infon><infon key="type">ref</infon><infon key="volume">110</infon><infon key="year">2016</infon><offset>67344</offset><text>Crowd‐Sourced Text Analysis: Reproducible and Agile Production of Political Data</text></passage><passage><infon key="fpage">739</infon><infon key="issue">3</infon><infon key="lpage">53</infon><infon key="section_type">REF</infon><infon key="source">American Journal of Political Science</infon><infon key="type">ref</infon><infon key="volume">58</infon><infon key="year">2014</infon><offset>67427</offset><text>Separating the Shirkers from the Workers? Making Sure Respondents Pay Attention on Self‐Administered Surveys</text></passage><passage><infon key="fpage">473</infon><infon key="issue">2</infon><infon key="lpage">95</infon><infon key="section_type">REF</infon><infon key="source">European Journal of Political Research</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2018</infon><offset>67538</offset><text>Simple Politics for the People? Complexity in Campaign Messages and Political Knowledge</text></passage><passage><infon key="fpage">329</infon><infon key="issue">2–3</infon><infon key="lpage">57</infon><infon key="section_type">REF</infon><infon key="source">Journal of Accounting and Economics</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">2017</infon><offset>67626</offset><text>A Plain English Measure of Financial Reporting Readability</text></passage><passage><infon key="fpage">324</infon><infon key="issue">3/4</infon><infon key="lpage">45</infon><infon key="section_type">REF</infon><infon key="source">Biometrika</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">1952</infon><offset>67685</offset><text>Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons</text></passage><passage><infon key="fpage">5</infon><infon key="issue">1</infon><infon key="lpage">32</infon><infon key="section_type">REF</infon><infon key="source">Machine Learning</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2001</infon><offset>67764</offset><text>Random Forests</text></passage><passage><infon key="fpage">50</infon><infon key="issue">1</infon><infon key="lpage">52</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Professional Communication</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">1981</infon><offset>67779</offset><text>Why Readability Formulas Fail</text></passage><passage><infon key="fpage">663</infon><infon key="issue">3</infon><infon key="lpage">66</infon><infon key="section_type">REF</infon><infon key="source">PS: Political Science &amp; Politics</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2014</infon><offset>67809</offset><text>Analyzing Text Complexity in Political Science Research</text></passage><passage><infon key="fpage">96</infon><infon key="lpage">103</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 25th International Conference on Machine Learning</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>67865</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Readability Revisited: The New Dale‐Chall Readability Formula</infon><infon key="type">ref</infon><infon key="year">1995</infon><offset>67866</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Politicians and Rhetoric: The Persuasive Power of Metaphor</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>67867</offset></passage><passage><infon key="fpage">283</infon><infon key="issue">2</infon><infon key="lpage">84</infon><infon key="section_type">REF</infon><infon key="source">Journal of Applied Psychology</infon><infon key="type">ref</infon><infon key="volume">60</infon><infon key="year">1975</infon><offset>67868</offset><text>A Computer Readability Formula Designed for Machine Scoring</text></passage><passage><infon key="fpage">11</infon><infon key="issue">1</infon><infon key="lpage">20</infon><infon key="section_type">REF</infon><infon key="source">Educational Research Bulletin</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">1948</infon><offset>67928</offset><text>A Formula for Predicting Readability</text></passage><passage><infon key="fpage">27</infon><infon key="issue">1</infon><infon key="lpage">38</infon><infon key="section_type">REF</infon><infon key="source">Biometrika</infon><infon key="type">ref</infon><infon key="volume">80</infon><infon key="year">1993</infon><offset>67965</offset><text>Bias Reduction of Maximum Likelihood Estimates</text></passage><passage><infon key="fpage">221</infon><infon key="issue">3</infon><infon key="lpage">33</infon><infon key="pub-id_pmid">18867058</infon><infon key="section_type">REF</infon><infon key="source">Journal of Applied Psychology</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">1948</infon><offset>68012</offset><text>A New Readability Yardstick</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The Art of Readable Writing</infon><infon key="type">ref</infon><infon key="year">1949</infon><offset>68040</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">A Standard Corpus of Present‐Day Edited American English, for Use with Digital Computers (Brown)</infon><infon key="type">ref</infon><infon key="year">1964</infon><offset>68041</offset></passage><passage><infon key="fpage">513</infon><infon key="issue">7</infon><infon key="lpage">78</infon><infon key="section_type">REF</infon><infon key="source">Journal of Reading</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1968</infon><offset>68042</offset><text>A Readability Formula That Saves Time</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Dumbing Us Down: The Hidden Curriculum of Compulsory Schooling</infon><infon key="type">ref</infon><infon key="year">2002</infon><offset>68080</offset></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">76</infon><infon key="pub-id_pmid">9775516</infon><infon key="section_type">REF</infon><infon key="source">Cognition</infon><infon key="type">ref</infon><infon key="volume">68</infon><infon key="year">1998</infon><offset>68081</offset><text>Linguistic complexity: locality of syntactic dependencies</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The Technique of Clear Writing</infon><infon key="type">ref</infon><infon key="year">1952</infon><offset>68139</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>68140</offset><text>spaCy: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing</text></passage><passage><infon key="fpage">494</infon><infon key="issue">4</infon><infon key="lpage">509</infon><infon key="section_type">REF</infon><infon key="source">Contemporary Economic Policy</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2011</infon><offset>68255</offset><text>Does the Clarity of Central Bank Communication Affect Volatility in Financial Markets? Evidence from Humphrey‐Hawkins Testimonies</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Derivation of New Readability Formulas (Automated Readability Index, Fog Count, and Flesch Reading Ease Formula) For Navy Enlisted Personnel</infon><infon key="type">ref</infon><infon key="year">1975</infon><offset>68387</offset></passage><passage><infon key="fpage">329</infon><infon key="lpage">65</infon><infon key="name_0">surname:Nilsson;given-names:Lars‐Göran</infon><infon key="section_type">REF</infon><infon key="source">Perspectives on Memory Research</infon><infon key="type">ref</infon><infon key="year">1979</infon><offset>68388</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">The Measurement of Readability</infon><infon key="type">ref</infon><infon key="year">1963</infon><offset>68389</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>68390</offset><text>Professors, We Need You!</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The Anti‐Intellectual Presidency</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>68415</offset></passage><passage><infon key="fpage">591</infon><infon key="issue">3</infon><infon key="lpage">610</infon><infon key="section_type">REF</infon><infon key="source">British Journal of Political Science</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2016</infon><offset>68416</offset><text>Ideological Clarity in Multiparty Competition: A New Measure and Test Using Election Manifestos</text></passage><passage><infon key="fpage">212</infon><infon key="issue">1</infon><infon key="lpage">21</infon><infon key="section_type">REF</infon><infon key="source">Electoral Studies</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2012</infon><offset>68512</offset><text>Testing the Power of Arguments in Referendums: A Bradley–Terry Approach</text></passage><passage><infon key="fpage">1643</infon><infon key="issue">4</infon><infon key="lpage">71</infon><infon key="section_type">REF</infon><infon key="source">Journal of Finance</infon><infon key="type">ref</infon><infon key="volume">69</infon><infon key="year">2014</infon><offset>68586</offset><text>Measuring Readability in Financial Disclosures</text></passage><passage><infon key="fpage">298</infon><infon key="issue">3</infon><infon key="lpage">313</infon><infon key="section_type">REF</infon><infon key="source">Political Analysis</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2013</infon><offset>68633</offset><text>Validating Estimates of Latent Traits from Textual Data Using Human Judgment as a Benchmark</text></passage><passage><infon key="fpage">331</infon><infon key="issue">4</infon><infon key="lpage">61</infon><infon key="section_type">REF</infon><infon key="source">Political Behavior</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">1990</infon><offset>68725</offset><text>Explaining Political Sophistication</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>68761</offset><text> koRpus: An R Package for Text Analysis </text></passage><passage><infon key="fpage">176</infon><infon key="issue">6014</infon><infon key="lpage">82</infon><infon key="pub-id_pmid">21163965</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">331</infon><infon key="year">2011</infon><offset>68802</offset><text>Quantitative Analysis of Culture Using Millions of Digitized Books</text></passage><passage><infon key="fpage">835</infon><infon key="issue">4</infon><infon key="lpage">43</infon><infon key="section_type">REF</infon><infon key="source">American Political Science Review</infon><infon key="type">ref</infon><infon key="volume">111</infon><infon key="year">2017</infon><offset>68869</offset><text>A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts</text></passage><passage><infon key="fpage">729</infon><infon key="issue">3</infon><infon key="lpage">44</infon><infon key="section_type">REF</infon><infon key="source">American Journal of Political Science</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2018</infon><offset>68966</offset><text>Tree‐Based Models for Political Science Data</text></passage><passage><infon key="fpage">1027</infon><infon key="issue">4</infon><infon key="lpage">61</infon><infon key="section_type">REF</infon><infon key="source">Law &amp; Society Review</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2011</infon><offset>69013</offset><text>Justices and Legal Clarity: Analyzing the Complexity of Supreme Court Opinions</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The Strategy of Rhetoric: Campaigning for the American Constitution</infon><infon key="type">ref</infon><infon key="year">1996</infon><offset>69092</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Analytics of Literature: A Manual for the Objective Study of English Prose and Poetry</infon><infon key="type">ref</infon><infon key="year">1893</infon><offset>69093</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>69094</offset><text>Readability Studies: How Technocentrism Can Compromise Research and Legal Determinations</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Understanding Reading: A Psycholinguistic Analysis of Reading and Learning to Read</infon><infon key="type">ref</infon><infon key="year">1986</infon><offset>69183</offset></passage><passage><infon key="fpage">410</infon><infon key="issue">7</infon><infon key="lpage">13</infon><infon key="section_type">REF</infon><infon key="source">Elementary School Journal</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">1953</infon><offset>69184</offset><text>A New Readability Formula for Primary‐Grade Reading Materials</text></passage><passage><infon key="fpage">120</infon><infon key="issue">1</infon><infon key="lpage">36</infon><infon key="section_type">REF</infon><infon key="source">Journal of Politics</infon><infon key="type">ref</infon><infon key="volume">78</infon><infon key="year">2016</infon><offset>69248</offset><text>Democratization of Linguistic Complexity: The Effect of Franchise Extension on Parliamentary Discourse, 1832–1915</text></passage><passage><infon key="fpage">1122</infon><infon key="lpage">51</infon><infon key="section_type">REF</infon><infon key="source">American Journal of Political Science</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">1996</infon><offset>69364</offset><text>The Supreme Court and Federal Administrative Agencies: A Resource‐Based Theory and Analysis of Judicial Impact</text></passage><passage><infon key="fpage">273</infon><infon key="issue">4</infon><infon key="lpage">86</infon><infon key="section_type">REF</infon><infon key="source">Psychological Review</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">1927</infon><offset>69477</offset><text>A Law of Comparative Judgment</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">21</infon><infon key="section_type">REF</infon><infon key="source">Journal of Statistical Software</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2012</infon><offset>69507</offset><text>Bradley‐Terry Models in R: The BradleyTerry2 Package</text></passage><passage><infon key="fpage">397</infon><infon key="issue">7</infon><infon key="lpage">99</infon><infon key="section_type">REF</infon><infon key="source">Elementary English</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">1954</infon><offset>69562</offset><text>A Practical Readability Formula for the Classroom Teacher in the Primary Grades</text></passage></document></collection>
