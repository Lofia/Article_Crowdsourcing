<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201216</date><key>pmc.key</key><document><id>7160704</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.2196/13174</infon><infon key="article-id_pmc">7160704</infon><infon key="article-id_pmid">32234701</infon><infon key="article-id_publisher-id">v7i4e13174</infon><infon key="elocation-id">e13174</infon><infon key="issue">4</infon><infon key="kwd">mobile phone emotion autism digital data mobile app mHealth affect machine learning artificial intelligence digital health</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited. The complete bibliographic information, a link to the original publication on http://mental.jmir.org/, as well as this copyright and license information must be included.</infon><infon key="name_0">surname:Eysenbach;given-names:Gunther</infon><infon key="name_1">surname:Leo;given-names:Marco</infon><infon key="name_10">surname:Ning;given-names:Michael</infon><infon key="name_11">surname:Kline;given-names:Aaron</infon><infon key="name_12">surname:Wall;given-names:Dennis Paul</infon><infon key="name_2">surname:Sahin;given-names:Ned</infon><infon key="name_3">surname:Kalantarian;given-names:Haik</infon><infon key="name_4">surname:Jedoui;given-names:Khaled</infon><infon key="name_5">surname:Dunlap;given-names:Kaitlyn</infon><infon key="name_6">surname:Schwartz;given-names:Jessey</infon><infon key="name_7">surname:Washington;given-names:Peter</infon><infon key="name_8">surname:Husic;given-names:Arman</infon><infon key="name_9">surname:Tariq;given-names:Qandeel</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">7</infon><infon key="year">2020</infon><offset>0</offset><text>The Performance of Emotion Classifiers for Children With Parent-Reported Autism: Quantitative Feasibility Study</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>112</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>123</offset><text>Autism spectrum disorder (ASD) is a developmental disorder characterized by deficits in social communication and interaction, and restricted and repetitive behaviors and interests. The incidence of ASD has increased in recent years; it is now estimated that approximately 1 in 40 children in the United States are affected. Due in part to increasing prevalence, access to treatment has become constrained. Hope lies in mobile solutions that provide therapy through artificial intelligence (AI) approaches, including facial and emotion detection AI models developed by mainstream cloud providers, available directly to consumers. However, these solutions may not be sufficiently trained for use in pediatric populations.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>843</offset><text>Objective</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>853</offset><text>Emotion classifiers available off-the-shelf to the general public through Microsoft, Amazon, Google, and Sighthound are well-suited to the pediatric population, and could be used for developing mobile therapies targeting aspects of social communication and interaction, perhaps accelerating innovation in this space. This study aimed to test these classifiers directly with image data from children with parent-reported ASD recruited through crowdsourcing.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1310</offset><text>Methods</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1318</offset><text>We used a mobile game called Guess What? that challenges a child to act out a series of prompts displayed on the screen of the smartphone held on the forehead of his or her care provider. The game is intended to be a fun and engaging way for the child and parent to interact socially, for example, the parent attempting to guess what emotion the child is acting out (eg, surprised, scared, or disgusted). During a 90-second game session, as many as 50 prompts are shown while the child acts, and the video records the actions and expressions of the child. Due in part to the fun nature of the game, it is a viable way to remotely engage pediatric populations, including the autism population through crowdsourcing. We recruited 21 children with ASD to play the game and gathered 2602 emotive frames following their game sessions. These data were used to evaluate the accuracy and performance of four state-of-the-art facial emotion classifiers to develop an understanding of the feasibility of these platforms for pediatric research.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>2352</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2360</offset><text>All classifiers performed poorly for every evaluated emotion except happy. None of the classifiers correctly labeled over 60.18% (1566/2602) of the evaluated frames. Moreover, none of the classifiers correctly identified more than 11% (6/51) of the angry frames and 14% (10/69) of the disgust frames.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>2661</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2673</offset><text>The findings suggest that commercial emotion classifiers may be insufficiently trained for use in digital approaches to autism treatment and treatment tracking. Secure, privacy-preserving methods to increase labeled training data are needed to boost the models’ performance before they can be used in AI-enabled approaches to social therapy of the kind that is common in autism treatments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>3065</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>3078</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3089</offset><text>Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by stereotyped and repetitive behaviors and interests as well as deficits in social interaction and communication. In addition, autistic children struggle with facial affect and may express themselves in ways that do not closely resemble those of their peers. The incidence of ASD has increased in recent years; it is now estimated that approximately 1 in 40 children in the United States is affected by this condition. Although autism has no cure, there is strong evidence that suggests early intervention can improve speech and communication skills.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3719</offset><text>Common approaches to autism therapy include applied behavior analysis (ABA) and the early start Denver model (ESDM). In ABA therapy, the intervention is customized by a trained behavioral analyst to specifically suit the learner’s skills and deficits. The basis of this program is a series of structured activities that emphasize the development of transferable skills to the real world. Similarly, naturalistic developmental behavioral interventions such as ESDM support the development of core social skills through interactions with a licensed behavioral therapist while emphasizing joint activities and interpersonal exchange. Both treatment types have been shown to be safe and effective, with their greatest impact potential occurring during early intervention at younger ages.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4505</offset><text>Despite significant progress in understanding this condition in recent years, imbalances in coverage and barriers to diagnosis and treatment remain. In developing countries, studies have noted a lack of trained health professionals, inconsistent treatments, and an unclear pathway from diagnosis to intervention. Within the United States, research has shown that children in rural areas receive diagnoses approximately 5 months later than children living in cities. Moreover, it has been observed that children from families near the poverty line receive diagnoses almost a full year later than those from higher-income families. Data-driven approaches have estimated that over 80% of US counties contain no diagnostic autism resources. Even months of delayed access to therapy can limit the effectiveness of subsequent behavioral interventions. Alternative solutions that can ameliorate some of these challenges could be derived from digital and mobile tools. For example, we developed a wearable system using Google Glass that leverages emotion classification algorithms to recognize the facial emotion of a child’s conversation partner for real-time feedback and social support and showed treatment efficacy in a randomized clinical trial.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5750</offset><text>Various cloud-based emotion classifiers may help the value and reach of mobile tools and solutions. These include four commercially available systems: Microsoft Azure Emotion application programming interface (API), Amazon Rekognition, Google Cloud Vision, and Sighthound. Whereas most implementations of these emotion recognition APIs are proprietary, these algorithms are typically trained using large facial emotion datasets such as the Cohn-Kanade database and Belfast-Induced Natural Emotion Database, which have few examples of children. Due to this bias in labeled examples, it is possible that these models do not generalize well to the pediatric population, including children with developmental delays such as autism, which is evaluated in this study. This study puts the disparity to test. To do so, we use our mobile game Guess What?. This game (native to Android and iOS platforms) fosters engagement between the child and their social partner, such as a parent, through charades-like games while building a database of facial image data enriched for a range of emotions exhibited by the child during the game sessions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6883</offset><text>The primary contributions of this study are as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6939</offset><text>We present a mobile charades game, Guess What?, to crowdsource emotive video from its players. This framework has utility both as a mechanism for the evaluation of existing emotion classifiers and for the development of novel systems that appropriately generalize to the population of interest.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7234</offset><text>We present a study in which 2602 emotive frames are derived from 21 children with a parent-reported diagnosis of autism using data from the Guess What mobile game collected in a variety of heterogeneous environments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7451</offset><text>The data were used to evaluate the accuracy and performance of several state-of-the-art classifiers using the workflow shown in Figure 1, to develop an understanding of the feasibility of using these APIs in future mobile therapy approaches.</text></passage><passage><infon key="file">mental_v7i4e13174_fig1.jpg</infon><infon key="id">figure1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>7693</offset><text>A mobile charades game played between caregiver and child is used to crowdsource emotive video, subsampled and categorized by both manual raters and automatic classifiers. Frames from these videos form the basis of our dataset to evaluate several emotion classifiers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>7961</offset><text>Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7974</offset><text>To the best of our knowledge, this is the first work to date that benchmarks public emotion recognition APIs on children with developmental delays. However, a number of interesting apps have been proposed in recent years, which employ vision-based tools or affective computing solutions as an aid for children with autism. The emergence of these approaches motivates a careful investigation of the feasibility of commercial emotion classification algorithms for the pediatric population.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8462</offset><text>Motivated by the fact that children with autism can experience cognitive or emotional overload, which may compromise their communication skills and learning experience, Picard et al provided an overview of technological advances for sensing autonomic nervous system activation in real-time, including wearable electrodermal activity sensors. A more general overview of the role of affective computing in autism is provided by Kalioby et al, with the motivating examples of using technology to help individuals better navigate the socioemotional landscape of their daily lives. Among the enumerated devices include those developed at the Massachusetts Institute of Technology media laboratory, such as expression glasses that discriminate between several emotions, skin conductance-sensing gloves for stress detection, and a pressure-sensitive mouse to infer affective state from how individuals interact with the device. Devices made by industry include the SenseWear Pro2 armband, which includes a variety of wearable sensors that can be repurposed for stress and productivity detection, smart gloves that can detect breathing rate and blood pressure, and wireless heart-rate monitors that can be analyzed in the context of environmental stressors.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9712</offset><text>Prior research conducted by us has demonstrated the efficacy of mobile video phenotyping approaches for children with ASD in general and via the use of emotion classifiers integrated with the Google Glass platform to provide real-time behavioral support to children with ASD. In addition, other studies have confirmed the usability, acceptance, and overall positive impact on families of Google Glass–based systems that use emotion recognition technology to aid social-emotional communication and interaction for autistic children. In addition to these efforts, a variety of other smart-glass devices have been proposed. For example, the SenseGlass is among the earliest works that propose leveraging the Google Glass platform to capture and process real-time affective information using a variety of sensors. The authors proposed apps, including the development of affect-based user interfaces, and empowering wearers toward behavioral change through emotion management interventions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10700</offset><text>Glass-based affect recognition that predates the Google Glass platform has also been proposed. Scheirer et al used piezoelectric sensors to detect expressions such as confusion and interest, which were detected with an accuracy of 74%. A more recent work proposes a device called Empathy Glasses in which users can see, hear, and feel from the perspective of another individual. The system consists of wearable hardware to transmit the wearers’ gaze and facial expression and a remote interface where visual feedback is provided, and data are viewed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11253</offset><text>The research for smart-glass–based interventions is further supported by other technological systems that have been developed and examined within the context of developmental delays, including the use of augmented reality for object discrimination training, assistive robotics for therapy, and mobile assistive technologies for real-time social skill learning. Furthermore, the use of computer vision and gamified systems to both detect and teach emotions continues to progress. A computational approach to detect facial expressions optimized for mobile platforms was proposed, which demonstrated an accuracy of 95% from a 6-class set of expressions. Leo et al proposed a computational approach to assess the ability of children with ASD to produce facial expressions using computer vision, validated by three expert raters. Their findings demonstrated the feasibility of a human-in-the-loop computer vision system for analyzing facial data from children with ASD. Similar to this study, which utilizes Guess What?, a charades-style mobile game to collect emotional face data, Park and colleagues proposed six game design methods for the development of game-driven frameworks in teaching emotions to children with ASD, of which include: observation, understanding, mimicking, and generalization, and supports the use of game play to produce data of value to computer vision approaches for children with autism.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12666</offset><text>Although not all of the aforementioned research studies employ emotion recognition models directly, they are indicative of a general transition from traditional health care practices to modern mobile and digital solutions that leverage recent advances in computer vision, augmented reality, robotics, and artificial intelligence. Thus, the trend motivates our investigation of the efficacy of state-of-the-art vision models on populations with developmental delay.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>13131</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13139</offset><text>Overview</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13148</offset><text>In this section, we describe the architecture of Guess What? followed by a description of the methods employed to obtain test data and processing the frames therein to evaluate the performance of several major emotion classifiers. Although dozens of APIs are available, we limit our analysis to some of the most popular systems from major providers of cloud services as a fair representation of the state-of-the-art in publicly available emotion recognition APIs. The systems evaluated in this work were Microsoft Azure Emotion API (Azure), Amazon AWS Rekognition (AWS), Google Cloud Vision API (Google), and Sighthound (SH).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13774</offset><text>System Architecture</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13794</offset><text>The evaluation of the state-of-the-art in public emotion classification APIs on children with ASD requires a dataset derived from subjects from the relevant population group with a fair amount of consistency in its format and structure. Moreover, as data are limited, it is critical that the video contains a high density of emotive frames to simplify the manual annotation process when establishing a ground truth. Therefore, we have developed and launched an educational mobile game on the Google Play Store and iOS App Store, Guess What?, from which we derive emotive video.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14372</offset><text>In this game, parents hold the phone such that the front camera and screen are facing outward toward the child. When the game session begins, the child is shown a prompt that the caregiver must guess based on the child’s gestures and facial expressions. After a correct guess is acknowledged, the parent tilts the phone forward, indicating that a point should be awarded. At this time, another prompt is shown. If the one holding the phone cannot make a guess, he/she will tilt the phone backward to skip the frame and automatically proceed to the next. This process repeats until the 90-second game session has elapsed. Meta information is generated for each game session that indicates the times at which various prompts are shown and when the correct guesses occur.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15143</offset><text>Although a number of varied prompts are available, the two that are most germane to facial affect recognition and emotion recognition are emojis and faces, as shown in Figures 2 and 3, respectively. After the game session is complete, caregivers can elect to share their files and associated metadata to an institution review board-approved secure Amazon S3 bucket that is fully compliant with the Stanford University’s high-risk application security standards. A more detailed discussion of the mechanics and applications of Guess What? is described in.</text></passage><passage><infon key="file">mental_v7i4e13174_fig2.jpg</infon><infon key="id">figure2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15700</offset><text>Prompts from the emoji category are caricatures, but many are still associated with the classic Ekman universal emotions.</text></passage><passage><infon key="file">mental_v7i4e13174_fig3.jpg</infon><infon key="id">figure3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15822</offset><text>Prompts from the faces category are derived from real photos of children over a solid background.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15920</offset><text>The structure of a video is shown in Figure 4. Each uploaded video yields n video frames, delineated by k boundary points, B1-Bk, where each boundary point represents the time at which a new prompt is shown to the user. To obtain frames associated with a particular emotion, one should first identify the boundary point associated with that emotion through the game meta information, i. Having identified this boundary point, frames between Bi and Bi+1 can be associated with this prompt. However, two additional factors remain. It typically takes some time, α, for the child to react after the prompt is shown. Moreover, there is often a time period, β, between the child’s acknowledgment of the parents’ guess and phone tilt by the parent, during which time the child may adopt a neutral facial expression. Therefore, the frames of interest are those that lie between Bi+α and Bi+1−β.</text></passage><passage><infon key="file">mental_v7i4e13174_fig4.jpg</infon><infon key="id">figure4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16821</offset><text>The structure of a single video is characterized by its boundary points, which identify the times at which various prompts were shown to the child.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16969</offset><text>The proposed system is centered on two key aims. First, this mechanism facilitates the acquisition of structured emotive videos from children in a manner that challenges their ability to express facial emotion. Whereas other forms of video capture could be employed, a gamified system encourages repeated use and has the potential to contain a much higher density of emotive frames than a typical home video structured around nongaming activities. As manual annotation is employed as a ground truth for evaluating emotion classification, a high concentration of emotive frames within a short time period is essential to the simplification and reduction of the burden associated with this process. A second aim is to potentially facilitate the aggregation of labeled emotive videos from children using a crowdsourcing mechanism. This can be used to augment existing datasets with labeled images or create new ones for the development of novel deep-learning-based emotion classifiers that can potentially overcome the limitations of existing methods.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18018</offset><text>Data Acquisition</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18035</offset><text>A total of 46 videos from 21 subjects were analyzed in this study. These data were collected over 1 year. Ten videos were collected in a laboratory environment from six subjects with ASD who played several games in a single session administered by a member of the research staff. An additional 36 videos were acquired through crowdsourcing from 15 remote participants. Diagnosis of any form of developmental disorder was provided by the caregiver through self-report during the registration process, along with demographic information (gender, age, ethnicity). The collected information included diagnoses of autistic disorder (autism), ASD, Asperger's syndrome, pervasive developmental disorder (not otherwise specified), childhood disintegrative disorder, no diagnosis, no diagnosis but suspicious, and social communication (pragmatic) disorder. Additionally, a free-text field was available for parents to specify additional conditions. The videos were evaluated by a clinical professional using the Diagnostic and Statistical Manual of Mental Disorders-V criteria before inclusion. Caregivers of all children who participated in the study selected the autism spectrum disorder option.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19224</offset><text>The format of a Guess What? gameplay session generally enforces a structure on the derived video: the device is held in landscape mode, the child’s face is contained within the frame, and the distance between the child and camera is typically between 2 and 10 feet. Nevertheless, these videos were carefully screened by members of the research staff to ensure the reliability and quality of the data therein; videos that did not include children, were corrupt, filmed under poor lighting conditions, or did not include plausible demographic information were excluded from the analysis. The average age of the participating children was 7.3 (1.76) years. Due to the small sample size and nonuniform incidence of autism between genders, 18 of the 21 participants were male. Although participants explored a variety of game mechanics, all analyzed videos were derived from the two categories most useful for the study of facial affect: faces and emojis. After each game session, the videos were automatically uploaded to an Amazon S3 bucket through an Android background process.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20303</offset><text>Data Processing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20319</offset><text>Most emotion classification APIs charge users per an http request, rendering the processing of every frame in a video prohibitive in terms of both time and cost. To simplify our evaluation, we subsampled each video at a rate of two frames per second. These frames formed the basis of our experiments. To obtain ground truth, two raters manually assigned an emotion label to each frame based on the seven Ekman universal emotions, with the addition of a neutral label. Some frames were discarded when there was no face, or the quality was too poor to make an assessment. A classifier’s performance on a frame was evaluated only under the conditions that the frame was valid (of sufficient quality), and the two manual raters agreed on the emotion label associated with the frame. Frames were considered of insufficient quality if: (1) the frame was too blurry to discern, (2) the child was not in the frame, (3) the image or video was corrupt, or (4) there were multiple individuals within the frame.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21321</offset><text>From a total of 5418 reviewed frames, 718 were discarded due to a lack of agreement between the manual raters. An additional 2123 frames were discarded because at least one rater assigned the not applicable (N/A) label, indicating that the frame was of insufficient quality. This was due to a variety of factors but generally caused by motion artifacts or the child leaving the frame due to the phone being tilted in acknowledgment of a correct guess. The total number of analyzed frames was 2602 divided between the categories shown in Table 1.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21867</offset><text>As shown, most frames were neutral, with a preponderance of happy frames in the nonneutral category. Owing to the limited number of scared and confused frames, this emotion was omitted from our analysis. We also merged the contempt and anger categories due to their similarity of affect and streamline analysis.</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>22179</offset><text>The distribution of frames per category (N=2602).</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Emotion&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frames, n&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1393&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Emotive&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1209&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happy&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;864&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sad&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Surprised&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;165&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disgusted&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Angry&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>22229</offset><text>Emotion	Frames, n	 	Neutral	1393	 	Emotive	1209	 	Happy	864	 	Sad	60	 	Surprised	165	 	Disgusted	69	 	Angry	51	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22343</offset><text>As not all emotion classifiers represented their outputs in a consistent format, some further simplifications were made in our analysis. First, it was necessary to make minor corrections to the format of the outputted data. For example, happy and happiness were considered identical. In the case of AWS, the confused class was ignored, as many other classifiers did not support it. Moreover, calm was renamed neutral. As AWS, Azure, and Sighthound returned probabilities rather than a single label, a frame in which no emotion class was associated with a probability of over 70% was considered a failure. For Google Vision, classification confidence was associated with a categorical label rather than a percentage. In this case, frames did not receive an emotion classification as likely or very likely were considered failures. It is also worth noting that this platform, unlike all the others, does not contain disgust or neutral classes. The final emotions evaluated in this study were happy, sad, surprise, anger, disgust, and neutral, with the latter two omitted for Google Cloud Vision.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23437</offset><text>As real-time use is an important aspect of mobile therapies and aids, we evaluated the performance of each classifier by calculating the number of seconds required to process each 90-second video subsampled to one frame per second. This evaluation was performed on a Wi-Fi network tested with an average download speed of 51 Mbps and an average upload speed of 62.5 Mbps. For each classifier, this experiment was repeated 10 times to obtain the average amount of time required to process the subsampled video.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>23947</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>23955</offset><text>Overview</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23964</offset><text>In this section, we present the results of our evaluation of Guess What? as well as the performance of the evaluated classifiers: Microsoft Azure Emotion API (Azure), AWS, Google Cloud Vision API (Google), and SH. Abbreviations for emotions described within this section can be found in Textbox 1.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2_caption</infon><offset>24262</offset><text>Abbreviations for emotions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24290</offset><text>HP: Happy</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24300</offset><text>CF: Confused</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24313</offset><text>N/A: Not applicable</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24333</offset><text>SC: Scared</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24344</offset><text>SP: Surprised</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24358</offset><text>DG: Disgusted</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24372</offset><text>AG: Angry</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24382</offset><text>Classifier Accuracy</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>24402</offset><text>Comparison With Ground Truth (Classifiers)</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24445</offset><text>Table 2 shows the performance of each classifier calculated by the percentage of correctly identified frames compared with the ground truth for categories neutral, emotive, and all. A neutral frame is one in which the face is recognized, and the neutral label is assigned high confidence. Any other frame within the categories of happy, sad, surprised, disgusted, and angry, are considered emotive frames. A more detailed breakdown of performance by emotion is shown in Table 3. Note that as before, Google’s API does not support the neutral and disgust categories.</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25013</offset><text>Percentage of frames correctly identified by classifier: Azure (Azure Cognitive Services), AWS (Amazon Web Services), SH (Sighthound), and Google (Google Cloud Vision). These results only include frames in which there was a face, and the two manual raters agreed on the class. Google Vision API does not support the neutral label.</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;280&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;280&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;230&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;210&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classifier&lt;/td&gt;&lt;td colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;Frame type&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Emotive (n=1209), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral (n=1393), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All (n=2602), n (%)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Azure&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;798 (66.00)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;744 (53.40)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1542 (59.26)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AWS&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;829 (68.56)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;679 (48.74)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1508 (57.95)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;785 (64.92)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;sup&gt;b&lt;/sup&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sighthound&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;664 (54.92)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;902 (64.75)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1566 (60.18)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>25344</offset><text>Classifier	Frame type	 		Emotive (n=1209), n (%)	Neutral (n=1393), n (%)	All (n=2602), n (%)	 	Azure	798 (66.00)	744 (53.40)	1542 (59.26)	 	AWSa	829 (68.56)	679 (48.74)	1508 (57.95)	 	Google	785 (64.92)	N/Ab	N/A	 	Sighthound	664 (54.92)	902 (64.75)	1566 (60.18)	 	</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>25609</offset><text>aAWS: Amazon AWS Rekognition.</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>25639</offset><text>bN/A: not applicable.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25661</offset><text>Percentage of frames correctly identified by emotion type by each classifier: Azure (Azure Cognitive Services), AWS (Amazon Web Services), SH (Sighthound), and Google (Google Cloud Vision). These results only include frames in which there was a face, and the two manual raters agreed on the class. Note: Google Vision API does not support the neutral or disgust labels.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;90&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;180&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;160&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;150&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classifier&lt;/td&gt;&lt;td colspan=&quot;6&quot; rowspan=&quot;1&quot;&gt;Frame type&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral (n=1394), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happy (n=864), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sad (n=60), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Surprised (n=165), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disgusted (n=69), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Angry (n=51), n (%)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AWS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;679 (48.74)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;709 (82.0)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19 (31)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94 (56.9)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4 (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 (5)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sighthound&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;902 (64.75)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;545 (63.0)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13 (21)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90 (54.5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10 (14)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6 (11)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Azure&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;744 (53.41)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;695 (80.4)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20 (33)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80 (48.4)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0 (0)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 (5)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;676 (78.2)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10 (16)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93 (56.3)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6 (11)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26031</offset><text>Classifier	Frame type	 		Neutral (n=1394), n (%)	Happy (n=864), n (%)	Sad (n=60), n (%)	Surprised (n=165), n (%)	Disgusted (n=69), n (%)	Angry (n=51), n (%)	 	AWS	679 (48.74)	709 (82.0)	19 (31)	94 (56.9)	4 (5)	3 (5)	 	Sighthound	902 (64.75)	545 (63.0)	13 (21)	90 (54.5)	10 (14)	6 (11)	 	Azure	744 (53.41)	695 (80.4)	20 (33)	80 (48.4)	0 (0)	3 (5)	 	Google	N/Aa	676 (78.2)	10 (16)	93 (56.3)	N/A	6 (11)	 	</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>26434</offset><text>aN/A: not applicable.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>26456</offset><text>Interrater Reliability (Classifiers)</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26493</offset><text>The Cohen kappa statistic is a measure of interrater reliability that factors in the percentage of agreement due to chance; an important consideration when the possible classes are few in number. Figure 5 shows the agreement between every pair of classifiers based on their Cohen kappa score calculated based on every evaluated frame, in which a score of 1 indicates perfect agreement. The results reflect low agreement between most combinations of classifiers. This is particularly true for the lack of agreement between Google and Sighthound, with a Cohen kappa score of 0.2. This is likely because of differences in how the classifiers are tuned for precision and recall; Sighthound correctly identified more neutral frames than the others, but performance was lower for the most predominant emotive label: happy.</text></passage><passage><infon key="file">mental_v7i4e13174_fig5.jpg</infon><infon key="id">figure5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27310</offset><text>The Cohen’s Kappa Score is a measure of agreement between two raters, and was calculated for all four evaluated classifiers: Azure (Azure Cognitive Services), AWS (Amazon Web Services), SH (Sighthound), and Google (Google Cloud Vision). Results indicate weak agreement between all pairs of classifiers.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>27615</offset><text>Interrater Reliability (Human Raters)</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27653</offset><text>The Cohen kappa coefficient for agreement between the two manual raters was 0.74, which was higher than any combination of automatic classifiers evaluated in this study. Although this indicates substantial agreement, it is worth exploring the characteristics of frames in which there was disagreement between the two raters. The full confusion matrix can be seen in Figure 6, which shows the distribution of all frames evaluated by the raters. The results indicate that most discrepancies were between happy and neutral. These discrepancies were likely subtle differences in how the raters perceived a happy face due to the inherent subjectivity of this process. A lack of agreement can also be seen between the disgust-anger categories.</text></passage><passage><infon key="file">mental_v7i4e13174_fig6.jpg</infon><infon key="id">figure6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28391</offset><text>The distribution of frames between the two human raters for each emotion: HP (Happy), SD (Sad), AG (Angry), DG (Disgust), NT (Neutral), and SC (Scared).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>28544</offset><text>Classifier Speed</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28561</offset><text>Wearable and mobile solutions for autism generally require efficient classification performance to provide real-time feedback to users. In some cases, this may be environmental feedback, as in the Autism Glass, which uses the outward-facing camera of Google Glass to read the emotions of those around the child and provide real-time social queues. In the case of Guess What? the phone’s front camera is used to read the expression of the child, which can be analyzed to determine if the facial expression matches the prompt displayed at that time.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29111</offset><text>To determine if real-time classification performance is feasible with computation offloaded implementations of emotion classifiers, we measured the amount of time required to process a 90-second video recorded at 30 frames per second and subsampled to one frame per second, yielding a total of 90 frames. For each classifier, this experiment was repeated 10 times to obtain the average number of seconds required to process the subsampled video. Table 4 shows the speed of the API-based classifiers used in this study. The values shown in this table represent the amount of time necessary to send each frame to the Web service via an http post request and receive an http response with the emotion label. These frames were processed sequentially, with no overlap between http requests.</text></passage><passage><infon key="file">table4.xml</infon><infon key="id">table4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>29897</offset><text>Speed of the evaluated classifiers.</text></passage><passage><infon key="file">table4.xml</infon><infon key="id">table4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;500&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classifier&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Time (seconds)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Azure&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28.6&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AWS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.6&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Google&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.9&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sighthound&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41.1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29933</offset><text>Classifier	Time (seconds)	 	Azure	28.6	 	AWS	90.6	 	Google	55.9	 	Sighthound	41.1	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30018</offset><text>The findings indicated that the fastest classifier was Azure, processing all 90 frames in a total of 28.6 seconds. Using Azure with a fast internet connection, it may be possible to obtain semi real-time emotion classification performance, a time of 28.6 seconds corresponds to 3.14 frames per second, which is within the bounds of what could be considered real time. The slowest classifier was AWS, which processed these 90 frames in 90.6 seconds. This corresponds to a frame rate of 0.99 frames per second. In summary, real-time or semi–real-time performance is possible with Web-based emotion classifiers on fast Wi-Fi internet connections. For cellular connections or apps that require frame rates beyond three frames per second, these approaches may be insufficient.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>30792</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>30803</offset><text>Classifier Performance</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30826</offset><text>Results indicate that Google and AWS produced the highest percentage of correctly classified emotive frames, whereas Sighthound produced the highest percentage of correctly identified neutral frames. Google’s API did not provide a neutral label and, therefore, could not be evaluated. The best system in terms of overall classification accuracy was Sighthound by a small margin, with 60.18% (1566/2602) of the frames correctly identified. Further results indicate that none of the classifiers performed well for any category besides happy, which was the emotion most represented in the dataset, as shown in Table 1. In addition, there appears to be a systematic bias toward high recall and low precision for the happy category: those classifiers that identified most of the happy frames performed worse for those in the neutral category.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31666</offset><text>In summary, the data suggest that although a frame with a smile will be correctly identified in most cases, the ability of the evaluated classifiers to identify other expressions for children with ASD is dubious and presents an obstacle in the design of emotion-based mobile and wearable outcome measures, screening tools, and therapies.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>32004</offset><text>Analysis of Frames</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32023</offset><text>Figure 7 shows six frames from one study participant, reproduced with permission from the child’s parents. The top of each frame lists the gold-standard annotation in which both raters agreed on a suitable label for the frame. The bottom of each frame enumerates the labels assigned from each classifier in order: Amazon Rekognition, Sighthound, Azure Cognitive Services, and Google Cloud Vision AI. It should be noted that, as before, these labels are normalized for comparison because each classifier outputs data in a particular format. For example, N/A from one classifier could be compared with a blank field in another, whereas some such as Google Cloud explicitly state Not Sure; for our purposes, all three of these scenarios were labeled as N/A during analysis.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32796</offset><text>Frame A shows a frame that was labeled as neutral by the raters, although each classifier provided a different label: confused, disgusted, happy, and N/A. This is an example of a false-positive, detecting an emotion in a neutral frame. A similar example is shown in frame F; most classifiers failed to identify the neutral label. Such false positives are particularly problematic as the neutral label is the most prevalent, as shown in Table 1.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33241</offset><text>In contrast, frames B and C are examples in which the labels assigned by each classifier matched the labels assigned by the manual raters; all classifiers correctly identified the happy label. As shown in Table 1, happy was the most common nonneutral emotion by a considerable margin, and most classifiers performed quite well in this category; AWS, Azure, and Sighthound all correctly identified between 78.2% (676/864) and 82.0% (709/864) of these frames, although at the expense of increased false-positives such as those shown in frames A and F. An example of a happy frame that was identified as such by the human raters but incorrectly by most classifiers is frame D. It is possible that the child’s hands covering part of her face contribute to this error, as the frame is otherwise quite similar to frame B. Finally, frame E is an example of a frame that was processed by the classifiers but not included in our experimental results because the human raters flagged the frame as insufficient due to motion artifacts. In this case, all four classifiers correctly determined that the frame could not be processed.</text></passage><passage><infon key="file">mental_v7i4e13174_fig7.jpg</infon><infon key="id">figure7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34363</offset><text>A comparison of the performance of each classifier on a set of frames highlights scenarios that may lead to discrepancies in the classifier outputs for various emotions: HP (Happy), CF (Confused), DG (Disgust), N/A (Not Applicable), AG (Angry), SC (Scared). Ground truth manual labels are shown on top, with labels derived from each classifier on the bottom.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>34722</offset><text>Limitations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34734</offset><text>There are several limitations associated with this study, which will be addressed in future work. First, we analyzed only a subset of existing emotion classifiers, emphasizing those from providers of major cloud services. Future efforts will extend this evaluation to include those that are less prolific and require paid licenses. A second limitation is the use of parent-reported diagnoses, which may not always be factual. A third limitation is that although we ruled out some comorbid conditions, we did not rule out all comorbid conditions, including Attention-Deficit/Hyperactivity Disorder, which has been shown to impact emotional processing and function in children. A fourth limitation stems from the lack of neurotypical children. Finally, the dataset we used included an unequal distribution of frames across emotion categories. In the future, we will investigate ways to gather equal numbers of frames, and if this distribution may be related to social deficits associated with autism, or increased prevalence of happy and neutral due to the inherent nature of gameplay. Although our results support the conclusion that the commercial emotion classifiers tested here are not yet at a level needed for use with autistic children, it remains unclear how these models will perform with a larger, more diverse, and stratified sample.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>36077</offset><text>Conclusions</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36089</offset><text>In this feasibility study, we evaluated the performance of four emotion recognition classifiers on children with ASD: Google Cloud Vision, Amazon Rekognition, Microsoft Azure Emotion API, and Sighthound. The average percentage of correctly identified emotive and neutral frames for all classifiers combined was 63.60% (769/1209) and 55.63% (775/1393), respectively, varying greatly between classifiers based on how their sensitivity and specificity were tuned. The results also demonstrated that while most classifiers were able to consistently identify happy frames, the performance for sad, disgust, and anger was poor: no classifier identified more than one-third of the frames from either of these categories. We conclude that the performance of the evaluated classifiers is not yet at the level for use in mobile and/or wearable therapy solutions for autistic children, necessitating the development of larger training datasets from these populations to develop more domain-specific models.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>37085</offset><text>Conflicts of Interest: DW is the founder of Cognoa. This company is developing digital solutions for pediatric behavioral health, including neurodevelopmental conditions such as autism that are detected and treated using techniques, including emotion classification. AK works as a part-time consultant for Cognoa. All other authors declare no competing interests.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>37449</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37463</offset><text>ABA</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37467</offset><text>applied behavior analysis</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37493</offset><text>AI</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37496</offset><text>artificial intelligence</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37520</offset><text>API</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37524</offset><text>application programming interface</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37558</offset><text>ASD</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37562</offset><text>autism spectrum disorder</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37587</offset><text>ESDM</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37592</offset><text>early start Denver model</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37617</offset><text>SH</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>37620</offset><text>Sighthound</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Diagnostic and Statistical Manual of Mental Disorders. Fifth Edition</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>37631</offset></passage><passage><infon key="fpage">933</infon><infon key="issue">4 Pt 1</infon><infon key="lpage">45</infon><infon key="name_0">surname:Lozier;given-names:LM</infon><infon key="name_1">surname:Vanmeter;given-names:JW</infon><infon key="name_2">surname:Marsh;given-names:AA</infon><infon key="pub-id_doi">10.1017/S0954579414000479</infon><infon key="pub-id_medline">24915526</infon><infon key="pub-id_pmid">24915526</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychopathol</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2014</infon><offset>37632</offset><text>Impairments in facial affect recognition associated with autism spectrum disorders: a meta-analysis</text></passage><passage><infon key="fpage">7</infon><infon key="name_0">surname:Loth;given-names:E</infon><infon key="name_1">surname:Garrido;given-names:L</infon><infon key="name_2">surname:Ahmad;given-names:J</infon><infon key="name_3">surname:Watson;given-names:E</infon><infon key="name_4">surname:Duff;given-names:A</infon><infon key="name_5">surname:Duchaine;given-names:B</infon><infon key="pub-id_doi">10.1186/s13229-018-0187-7</infon><infon key="pub-id_medline">29423133</infon><infon key="pub-id_pmid">29423133</infon><infon key="section_type">REF</infon><infon key="source">Mol Autism</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2018</infon><offset>37732</offset><text>Facial expression recognition as a candidate marker for autism spectrum disorder: how frequent and severe are deficits?</text></passage><passage><infon key="fpage">52</infon><infon key="name_0">surname:Fridenson-Hayo;given-names:S</infon><infon key="name_1">surname:Berggren;given-names:S</infon><infon key="name_2">surname:Lassalle;given-names:A</infon><infon key="name_3">surname:Tal;given-names:S</infon><infon key="name_4">surname:Pigat;given-names:D</infon><infon key="name_5">surname:Bölte;given-names:S</infon><infon key="name_6">surname:Baron-Cohen;given-names:S</infon><infon key="name_7">surname:Golan;given-names:O</infon><infon key="pub-id_doi">10.1186/s13229-016-0113-9</infon><infon key="pub-id_medline">28018573</infon><infon key="pub-id_pmid">28018573</infon><infon key="section_type">REF</infon><infon key="source">Mol Autism</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2016</infon><offset>37852</offset><text>Basic and complex emotion recognition in children with autism: cross-cultural findings</text></passage><passage><infon key="issue">6</infon><infon key="name_0">surname:Kogan;given-names:MD</infon><infon key="name_1">surname:Vladutiu;given-names:CJ</infon><infon key="name_10">surname:Lu;given-names:MC</infon><infon key="name_2">surname:Schieve;given-names:LA</infon><infon key="name_3">surname:Ghandour;given-names:RM</infon><infon key="name_4">surname:Blumberg;given-names:SJ</infon><infon key="name_5">surname:Zablotsky;given-names:B</infon><infon key="name_6">surname:Perrin;given-names:JM</infon><infon key="name_7">surname:Shattuck;given-names:P</infon><infon key="name_8">surname:Kuhlthau;given-names:KA</infon><infon key="name_9">surname:Harwood;given-names:RL</infon><infon key="pub-id_doi">10.1542/peds.2017-4161</infon><infon key="pub-id_medline">30478241</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">142</infon><infon key="year">2018</infon><offset>37939</offset><text>The prevalence of parent-reported autism spectrum disorder among US children</text></passage><passage><infon key="fpage">243</infon><infon key="issue">2</infon><infon key="lpage">6</infon><infon key="name_0">surname:Rogers;given-names:SJ</infon><infon key="pub-id_doi">10.1007/bf02172020</infon><infon key="pub-id_medline">8744493</infon><infon key="pub-id_pmid">8744493</infon><infon key="section_type">REF</infon><infon key="source">J Autism Dev Disord</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">1996</infon><offset>38016</offset><text>Brief report: early intervention in autism</text></passage><passage><infon key="name_0">surname:Cooper;given-names:JO</infon><infon key="name_1">surname:Heron;given-names:TE</infon><infon key="name_2">surname:Heward;given-names:WL</infon><infon key="section_type">REF</infon><infon key="source">Applied Behavior Analysis</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>38059</offset></passage><passage><infon key="fpage">e17</infon><infon key="issue">1</infon><infon key="lpage">23</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="name_1">surname:Rogers;given-names:S</infon><infon key="name_2">surname:Munson;given-names:J</infon><infon key="name_3">surname:Smith;given-names:M</infon><infon key="name_4">surname:Winter;given-names:J</infon><infon key="name_5">surname:Greenson;given-names:J</infon><infon key="name_6">surname:Donaldson;given-names:A</infon><infon key="name_7">surname:Varley;given-names:J</infon><infon key="pub-id_doi">10.1542/peds.2009-0958</infon><infon key="pub-id_medline">19948568</infon><infon key="pub-id_pmid">19948568</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">125</infon><infon key="year">2010</infon><offset>38060</offset><text>Randomized, controlled trial of an intervention for toddlers with autism: the Early Start Denver Model</text></passage><passage><infon key="fpage">775</infon><infon key="issue">3</infon><infon key="lpage">803</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="pub-id_doi">10.1017/S0954579408000370</infon><infon key="pub-id_medline">18606031</infon><infon key="pub-id_pmid">18606031</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychopathol</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2008</infon><offset>38163</offset><text>Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder</text></passage><passage><infon key="fpage">1150</infon><infon key="issue">11</infon><infon key="lpage">9</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="name_1">surname:Jones;given-names:EJ</infon><infon key="name_10">surname:Smith;given-names:M</infon><infon key="name_11">surname:Rogers;given-names:SJ</infon><infon key="name_12">surname:Webb;given-names:SJ</infon><infon key="name_2">surname:Merkle;given-names:K</infon><infon key="name_3">surname:Venema;given-names:K</infon><infon key="name_4">surname:Lowy;given-names:R</infon><infon key="name_5">surname:Faja;given-names:S</infon><infon key="name_6">surname:Kamara;given-names:D</infon><infon key="name_7">surname:Murias;given-names:M</infon><infon key="name_8">surname:Greenson;given-names:J</infon><infon key="name_9">surname:Winter;given-names:J</infon><infon key="pub-id_doi">10.1016/j.jaac.2012.08.018</infon><infon key="pub-id_medline">23101741</infon><infon key="pub-id_pmid">23101741</infon><infon key="section_type">REF</infon><infon key="source">J Am Acad Child Adolesc Psychiatry</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2012</infon><offset>38259</offset><text>Early behavioral intervention is associated with normalized brain activity in young children with autism</text></passage><passage><infon key="fpage">1455</infon><infon key="issue">4 Pt 2</infon><infon key="lpage">72</infon><infon key="name_0">surname:Dawson;given-names:G</infon><infon key="name_1">surname:Bernier;given-names:R</infon><infon key="pub-id_doi">10.1017/S0954579413000710</infon><infon key="pub-id_medline">24342850</infon><infon key="pub-id_pmid">24342850</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychopathol</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2013</infon><offset>38364</offset><text>A quarter century of progress on the early detection and treatment of autism spectrum disorder</text></passage><passage><infon key="fpage">1079</infon><infon key="issue">12</infon><infon key="lpage">84</infon><infon key="name_0">surname:Scherzer;given-names:AL</infon><infon key="name_1">surname:Chhagan;given-names:M</infon><infon key="name_2">surname:Kauchali;given-names:S</infon><infon key="name_3">surname:Susser;given-names:E</infon><infon key="pub-id_doi">10.1111/j.1469-8749.2012.04348.x</infon><infon key="pub-id_medline">22803576</infon><infon key="pub-id_pmid">22803576</infon><infon key="section_type">REF</infon><infon key="source">Dev Med Child Neurol</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2012</infon><offset>38459</offset><text>Global perspective on early diagnosis and intervention for children with developmental delays and disabilities</text></passage><passage><infon key="fpage">191</infon><infon key="issue">3</infon><infon key="lpage">200</infon><infon key="name_0">surname:van Cong;given-names:Tran</infon><infon key="name_1">surname:Weiss;given-names:B</infon><infon key="name_2">surname:Toan;given-names:KN</infon><infon key="name_3">surname:Le Thu;given-names:TT</infon><infon key="name_4">surname:Trang;given-names:NT</infon><infon key="name_5">surname:Hoa;given-names:NT</infon><infon key="name_6">surname:Thuy;given-names:DT</infon><infon key="pub-id_doi">10.5114/hpr.2015.53125</infon><infon key="pub-id_medline">27088123</infon><infon key="pub-id_pmid">27088123</infon><infon key="section_type">REF</infon><infon key="source">Health Psychol Rep</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2015</infon><offset>38570</offset><text>Early identification and intervention services for children with autism in Vietnam</text></passage><passage><infon key="fpage">30</infon><infon key="issue">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Samms-Vaughan;given-names:ME</infon><infon key="pub-id_doi">10.3109/17549507.2013.866271</infon><infon key="pub-id_medline">24397842</infon><infon key="pub-id_pmid">24397842</infon><infon key="section_type">REF</infon><infon key="source">Int J Speech Lang Pathol</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2014</infon><offset>38653</offset><text>The status of early identification and early intervention in autism spectrum disorders in lower- and middle-income countries</text></passage><passage><infon key="fpage">1480</infon><infon key="issue">6</infon><infon key="lpage">6</infon><infon key="name_0">surname:Mandell;given-names:DS</infon><infon key="name_1">surname:Novak;given-names:MM</infon><infon key="name_2">surname:Zubritsky;given-names:CD</infon><infon key="pub-id_doi">10.1542/peds.2005-0185</infon><infon key="pub-id_medline">16322174</infon><infon key="pub-id_pmid">16322174</infon><infon key="section_type">REF</infon><infon key="source">Pediatrics</infon><infon key="type">ref</infon><infon key="volume">116</infon><infon key="year">2005</infon><offset>38778</offset><text>Factors associated with age of diagnosis among children with autism spectrum disorders</text></passage><passage><infon key="fpage">e13094</infon><infon key="issue">7</infon><infon key="name_0">surname:Ning;given-names:M</infon><infon key="name_1">surname:Daniels;given-names:J</infon><infon key="name_2">surname:Schwartz;given-names:J</infon><infon key="name_3">surname:Dunlap;given-names:K</infon><infon key="name_4">surname:Washington;given-names:P</infon><infon key="name_5">surname:Kalantarian;given-names:H</infon><infon key="name_6">surname:Du;given-names:M</infon><infon key="name_7">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.2196/13094</infon><infon key="pub-id_medline">31293243</infon><infon key="pub-id_pmid">31293243</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>38865</offset><text>Identification and quantification of gaps in access to autism resources in the United States: an infodemiological study</text></passage><passage><infon key="name_0">surname:Washington;given-names:P</infon><infon key="name_1">surname:Wall;given-names:D</infon><infon key="name_2">surname:Voss;given-names:C</infon><infon key="name_3">surname:Kline;given-names:A</infon><infon key="name_4">surname:Haber;given-names:N</infon><infon key="name_5">surname:Daniels;given-names:J</infon><infon key="name_6">surname:Fazel;given-names:A</infon><infon key="name_7">surname:De;given-names:T</infon><infon key="name_8">surname:Feinstein;given-names:C</infon><infon key="name_9">surname:Winograd;given-names:T</infon><infon key="pub-id_doi">10.1145/3130977</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>38985</offset><text>SuperpowerGlass: A Wearable Aid for the At-Home Therapy of Children with Autism</text></passage><passage><infon key="fpage">2348</infon><infon key="lpage">54</infon><infon key="name_0">surname:Washington;given-names:P</infon><infon key="name_1">surname:Catalin;given-names:V</infon><infon key="name_2">surname:Nick;given-names:H</infon><infon key="name_3">surname:Serena;given-names:T</infon><infon key="name_4">surname:Jena;given-names:D</infon><infon key="name_5">surname:Carl;given-names:F</infon><infon key="name_6">surname:Terry;given-names:W</infon><infon key="name_7">surname:Dennis;given-names:W</infon><infon key="pub-id_doi">10.1145/2851581.2892282</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>39065</offset><text>A Wearable Social Interaction Aid for Children with Autism</text></passage><passage><infon key="fpage">S257</infon><infon key="issue">10</infon><infon key="name_0">surname:Daniels;given-names:J</infon><infon key="name_1">surname:Schwartz;given-names:J</infon><infon key="name_10">surname:Wall;given-names:D</infon><infon key="name_2">surname:Haber;given-names:N</infon><infon key="name_3">surname:Voss;given-names:C</infon><infon key="name_4">surname:Kline;given-names:A</infon><infon key="name_5">surname:Fazel;given-names:A</infon><infon key="name_6">surname:Washington;given-names:P</infon><infon key="name_7">surname:De;given-names:T</infon><infon key="name_8">surname:Feinstein;given-names:C</infon><infon key="name_9">surname:Winograd;given-names:T</infon><infon key="pub-id_doi">10.1016/j.jaac.2017.09.296</infon><infon key="section_type">REF</infon><infon key="source">J Am Acad Child Psy</infon><infon key="type">ref</infon><infon key="volume">56</infon><infon key="year">2017</infon><offset>39124</offset><text>5.13 Design and efficacy of a wearable device for social affective learning in children with autism</text></passage><passage><infon key="fpage">32</infon><infon key="name_0">surname:Daniels;given-names:J</infon><infon key="name_1">surname:Schwartz;given-names:JN</infon><infon key="name_2">surname:Voss;given-names:C</infon><infon key="name_3">surname:Haber;given-names:N</infon><infon key="name_4">surname:Fazel;given-names:A</infon><infon key="name_5">surname:Kline;given-names:A</infon><infon key="name_6">surname:Washington;given-names:P</infon><infon key="name_7">surname:Feinstein;given-names:C</infon><infon key="name_8">surname:Winograd;given-names:T</infon><infon key="name_9">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1038/s41746-018-0035-3</infon><infon key="pub-id_medline">31304314</infon><infon key="pub-id_pmid">31304314</infon><infon key="section_type">REF</infon><infon key="source">NPJ Digit Med</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2018</infon><offset>39224</offset><text>Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism</text></passage><passage><infon key="fpage">129</infon><infon key="issue">1</infon><infon key="lpage">40</infon><infon key="name_0">surname:Daniels;given-names:J</infon><infon key="name_1">surname:Haber;given-names:N</infon><infon key="name_10">surname:Feinstein;given-names:C</infon><infon key="name_11">surname:Wall;given-names:D</infon><infon key="name_2">surname:Voss;given-names:C</infon><infon key="name_3">surname:Schwartz;given-names:J</infon><infon key="name_4">surname:Tamura;given-names:S</infon><infon key="name_5">surname:Fazel;given-names:A</infon><infon key="name_6">surname:Kline;given-names:A</infon><infon key="name_7">surname:Washington;given-names:P</infon><infon key="name_8">surname:Phillips;given-names:J</infon><infon key="name_9">surname:Winograd;given-names:T</infon><infon key="pub-id_doi">10.1055/s-0038-1626727</infon><infon key="pub-id_medline">29466819</infon><infon key="pub-id_pmid">29466819</infon><infon key="section_type">REF</infon><infon key="source">Appl Clin Inform</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2018</infon><offset>39349</offset><text>Feasibility testing of a wearable behavioral aid for social learning in children with autism</text></passage><passage><infon key="fpage">446</infon><infon key="issue">5</infon><infon key="lpage">54</infon><infon key="name_0">surname:Voss;given-names:C</infon><infon key="name_1">surname:Schwartz;given-names:J</infon><infon key="name_10">surname:Feinstein;given-names:C</infon><infon key="name_11">surname:Winograd;given-names:T</infon><infon key="name_12">surname:Wall;given-names:DP</infon><infon key="name_2">surname:Daniels;given-names:J</infon><infon key="name_3">surname:Kline;given-names:A</infon><infon key="name_4">surname:Haber;given-names:N</infon><infon key="name_5">surname:Washington;given-names:P</infon><infon key="name_6">surname:Tariq;given-names:Q</infon><infon key="name_7">surname:Robinson;given-names:TN</infon><infon key="name_8">surname:Desai;given-names:M</infon><infon key="name_9">surname:Phillips;given-names:JM</infon><infon key="pub-id_doi">10.1001/jamapediatrics.2019.0285</infon><infon key="pub-id_medline">30907929</infon><infon key="pub-id_pmid">30907929</infon><infon key="section_type">REF</infon><infon key="source">JAMA Pediatr</infon><infon key="type">ref</infon><infon key="volume">173</infon><infon key="year">2019</infon><offset>39442</offset><text>Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial</text></passage><passage><infon key="fpage">1218</infon><infon key="lpage">26</infon><infon key="name_0">surname:Voss;given-names:C</infon><infon key="name_1">surname:Washington;given-names:P</infon><infon key="name_10">surname:Wall;given-names:D</infon><infon key="name_2">surname:Haber;given-names:N</infon><infon key="name_3">surname:Kline;given-names:A</infon><infon key="name_4">surname:Daniels;given-names:J</infon><infon key="name_5">surname:Fazel;given-names:A</infon><infon key="name_6">surname:De;given-names:T</infon><infon key="name_7">surname:McCarthy;given-names:B</infon><infon key="name_8">surname:Feinstein;given-names:C</infon><infon key="name_9">surname:Winograd;given-names:T</infon><infon key="pub-id_doi">10.1145/2968219.2968310</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>39581</offset><text>Superpower Glass: Delivering Unobtrusive Real-Time Social Cues in Wearable Systems</text></passage><passage><infon key="comment">[Online ahead of print]</infon><infon key="name_0">surname:Voss;given-names:C</infon><infon key="name_1">surname:Haber;given-names:N</infon><infon key="name_2">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1001/jamapediatrics.2019.2969</infon><infon key="pub-id_medline">31498377</infon><infon key="section_type">REF</infon><infon key="source">JAMA Pediatr</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>39664</offset><text>The potential for machine learning-based wearables to improve socialization in teenagers and adults with autism spectrum disorder-reply</text></passage><passage><infon key="fpage">35</infon><infon key="issue">2</infon><infon key="lpage">8</infon><infon key="name_0">surname:Kline;given-names:A</infon><infon key="name_1">surname:Voss;given-names:C</infon><infon key="name_2">surname:Washington;given-names:P</infon><infon key="name_3">surname:Haber;given-names:N</infon><infon key="name_4">surname:Schwartz;given-names:H</infon><infon key="name_5">surname:Tariq;given-names:Q</infon><infon key="name_6">surname:Winograd;given-names:T</infon><infon key="name_7">surname:Feinstein;given-names:C</infon><infon key="name_8">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1145/3372300.3372308</infon><infon key="section_type">REF</infon><infon key="source">GetMobile</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2019</infon><offset>39800</offset><text>Superpower glass</text></passage><passage><infon key="comment">Face: An AI Service That Analyzes Faces in Imageshttps://azure.microsoft.com/en-us/services/cognitive-services/face/</infon><infon key="section_type">REF</infon><infon key="source">Microsoft Azure</infon><infon key="type">ref</infon><offset>39817</offset></passage><passage><infon key="comment">Amazon Rekognitionhttps://aws.amazon.com/rekognition/</infon><infon key="section_type">REF</infon><infon key="source">Amazon Web Services (AWS) - Cloud Computing Services</infon><infon key="type">ref</infon><offset>39818</offset></passage><passage><infon key="comment">Vision AIhttps://cloud.google.com/vision/</infon><infon key="section_type">REF</infon><infon key="source">Google Cloud: Cloud Computing Services</infon><infon key="type">ref</infon><offset>39819</offset></passage><passage><infon key="comment">Detection API &amp; Recognition APIhttps://www.sighthound.com/products/cloud</infon><infon key="section_type">REF</infon><infon key="source">Sighthound</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39820</offset></passage><passage><infon key="comment">Cohn-Kanade AU-Coded Facial Expression Databasehttps://www.ri.cmu.edu/project/cohn-kanade-au-coded-facial-expression-database/</infon><infon key="name_0">surname:Cohn;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">The Robotics Institute Carnegie Mellon University</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>39821</offset></passage><passage><infon key="fpage">32</infon><infon key="issue">1</infon><infon key="lpage">41</infon><infon key="name_0">surname:Sneddon;given-names:I</infon><infon key="name_1">surname:McRorie;given-names:M</infon><infon key="name_2">surname:McKeown;given-names:G</infon><infon key="name_3">surname:Hanratty;given-names:J</infon><infon key="pub-id_doi">10.1109/t-affc.2011.26</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Affective Comput</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2012</infon><offset>39822</offset><text>The Belfast induced natural emotion database</text></passage><passage><infon key="fpage">43</infon><infon key="issue">1</infon><infon key="lpage">66</infon><infon key="name_0">surname:Kalantarian;given-names:H</infon><infon key="name_1">surname:Washington;given-names:P</infon><infon key="name_2">surname:Schwartz;given-names:J</infon><infon key="name_3">surname:Daniels;given-names:J</infon><infon key="name_4">surname:Haber;given-names:N</infon><infon key="name_5">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1007/s41666-018-0034-9</infon><infon key="section_type">REF</infon><infon key="source">J Healthc Inform Res</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2018</infon><offset>39867</offset><text>Guess what?</text></passage><passage><infon key="fpage">350</infon><infon key="lpage">2</infon><infon key="name_0">surname:Kalantarian;given-names:H</infon><infon key="name_1">surname:Washington;given-names:P</infon><infon key="name_2">surname:Schwartz;given-names:J</infon><infon key="name_3">surname:Daniels;given-names:J</infon><infon key="name_4">surname:Haber;given-names:N</infon><infon key="name_5">surname:Wall;given-names:D</infon><infon key="pub-id_doi">10.1109/ichi.2018.00052</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 IEEE International Conference on Healthcare Informatics</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39879</offset><text>A Gamified Mobile System for Crowdsourcing Video for Autism Research</text></passage><passage><infon key="fpage">77</infon><infon key="lpage">86</infon><infon key="name_0">surname:Kalantarian;given-names:H</infon><infon key="name_1">surname:Jedoui;given-names:K</infon><infon key="name_2">surname:Washington;given-names:P</infon><infon key="name_3">surname:Tariq;given-names:Q</infon><infon key="name_4">surname:Dunlap;given-names:K</infon><infon key="name_5">surname:Schwartz;given-names:J</infon><infon key="name_6">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1016/j.artmed.2019.06.004</infon><infon key="pub-id_medline">31521254</infon><infon key="pub-id_pmid">31521254</infon><infon key="section_type">REF</infon><infon key="source">Artif Intell Med</infon><infon key="type">ref</infon><infon key="volume">98</infon><infon key="year">2019</infon><offset>39948</offset><text>Labeling images with facial emotion and the potential for pediatric healthcare</text></passage><passage><infon key="comment">epub ahead of print</infon><infon key="fpage">1</infon><infon key="name_0">surname:Kalantarian;given-names:H</infon><infon key="name_1">surname:Jedoui;given-names:K</infon><infon key="name_2">surname:Washington;given-names:P</infon><infon key="name_3">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1109/tg.2018.2877325</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Games</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>40027</offset><text>A mobile game for automatic emotion-labeling of images</text></passage><passage><infon key="comment">Guess What?https://play.google.com/store/apps/details?id=walllab.guesswhat</infon><infon key="section_type">REF</infon><infon key="source">Google Play</infon><infon key="type">ref</infon><offset>40082</offset></passage><passage><infon key="comment">Guess What? (Wall Lab)https://apps.apple.com/us/app/guess-what-wall-lab/id1426891832</infon><infon key="section_type">REF</infon><infon key="source">Apple Store</infon><infon key="type">ref</infon><offset>40083</offset></passage><passage><infon key="fpage">3575</infon><infon key="issue">1535</infon><infon key="lpage">84</infon><infon key="name_0">surname:Picard;given-names:RW</infon><infon key="pub-id_doi">10.1098/rstb.2009.0143</infon><infon key="pub-id_medline">19884152</infon><infon key="pub-id_pmid">19884152</infon><infon key="section_type">REF</infon><infon key="source">Philos Trans R Soc Lond B Biol Sci</infon><infon key="type">ref</infon><infon key="volume">364</infon><infon key="year">2009</infon><offset>40084</offset><text>Future affective technology for autism and emotion communication</text></passage><passage><infon key="fpage">228</infon><infon key="lpage">48</infon><infon key="name_0">surname:el Kaliouby;given-names:R</infon><infon key="name_1">surname:Picard;given-names:R</infon><infon key="name_2">surname:Baron-Cohen;given-names:S</infon><infon key="pub-id_doi">10.1196/annals.1382.016</infon><infon key="pub-id_medline">17312261</infon><infon key="pub-id_pmid">17312261</infon><infon key="section_type">REF</infon><infon key="source">Ann N Y Acad Sci</infon><infon key="type">ref</infon><infon key="volume">1093</infon><infon key="year">2006</infon><offset>40149</offset><text>Affective computing and autism</text></passage><passage><infon key="comment">The Development of the SenseWear Armband, A Revolutionary Energy Assessment Device to Assess Physical Activity and Lifestylehttps://www.semanticscholar.org/paper/The-Development-of-the-SenseWear-%C2%AE-armband-%2C-a-to-Andre-Pelletier/e9e115cb6f381a706687982906d45ed28d40bbac</infon><infon key="name_0">surname:Andre;given-names:D</infon><infon key="name_1">surname:Pelletier;given-names:R</infon><infon key="name_10">surname:Boehmke;given-names:SK</infon><infon key="name_11">surname:Stivoric;given-names:J</infon><infon key="name_12">surname:Teller;given-names:A</infon><infon key="name_2">surname:Farringdon;given-names:J</infon><infon key="name_3">surname:Er;given-names:S</infon><infon key="name_4">surname:Talbott;given-names:WA</infon><infon key="name_5">surname:Phillips;given-names:PP</infon><infon key="name_6">surname:Vyas;given-names:N</infon><infon key="name_7">surname:Trimble;given-names:JL</infon><infon key="name_8">surname:Wolf;given-names:D</infon><infon key="name_9">surname:Vishnubhatla;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">Semantic Scholar</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>40180</offset></passage><passage><infon key="fpage">e1002705</infon><infon key="issue">11</infon><infon key="name_0">surname:Tariq;given-names:Q</infon><infon key="name_1">surname:Daniels;given-names:J</infon><infon key="name_2">surname:Schwartz;given-names:JN</infon><infon key="name_3">surname:Washington;given-names:P</infon><infon key="name_4">surname:Kalantarian;given-names:H</infon><infon key="name_5">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1371/journal.pmed.1002705</infon><infon key="pub-id_medline">30481180</infon><infon key="pub-id_pmid">30481180</infon><infon key="section_type">REF</infon><infon key="source">PLoS Med</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2018</infon><offset>40181</offset><text>Mobile detection of autism through machine learning on home video: A development and prospective validation study</text></passage><passage><infon key="fpage">e13822</infon><infon key="issue">4</infon><infon key="name_0">surname:Tariq;given-names:Q</infon><infon key="name_1">surname:Fleming;given-names:SL</infon><infon key="name_2">surname:Schwartz;given-names:JN</infon><infon key="name_3">surname:Dunlap;given-names:K</infon><infon key="name_4">surname:Corbin;given-names:C</infon><infon key="name_5">surname:Washington;given-names:P</infon><infon key="name_6">surname:Kalantarian;given-names:H</infon><infon key="name_7">surname:Khan;given-names:NZ</infon><infon key="name_8">surname:Darmstadt;given-names:GL</infon><infon key="name_9">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.2196/13822</infon><infon key="pub-id_medline">31017583</infon><infon key="pub-id_pmid">31017583</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>40295</offset><text>Detecting developmental delay and autism through machine learning models using home videos of Bangladeshi children: Development and validation study</text></passage><passage><infon key="fpage">e13668</infon><infon key="issue">5</infon><infon key="name_0">surname:Washington;given-names:P</infon><infon key="name_1">surname:Kalantarian;given-names:H</infon><infon key="name_10">surname:Paskov;given-names:K</infon><infon key="name_11">surname:Voss;given-names:C</infon><infon key="name_12">surname:Haber;given-names:N</infon><infon key="name_13">surname:Wall;given-names:DP</infon><infon key="name_2">surname:Tariq;given-names:Q</infon><infon key="name_3">surname:Schwartz;given-names:J</infon><infon key="name_4">surname:Dunlap;given-names:K</infon><infon key="name_5">surname:Chrisman;given-names:B</infon><infon key="name_6">surname:Varma;given-names:M</infon><infon key="name_7">surname:Ning;given-names:M</infon><infon key="name_8">surname:Kline;given-names:A</infon><infon key="name_9">surname:Stockham;given-names:N</infon><infon key="pub-id_doi">10.2196/13668</infon><infon key="pub-id_medline">31124463</infon><infon key="pub-id_pmid">31124463</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>40444</offset><text>Validity of online screening for autism: crowdsourcing study comparing paid and unpaid diagnostic tasks</text></passage><passage><infon key="fpage">707</infon><infon key="lpage">18</infon><infon key="name_0">surname:Washington;given-names:P</infon><infon key="name_1">surname:Paskov;given-names:KM</infon><infon key="name_10">surname:Dunlap;given-names:K</infon><infon key="name_11">surname:Schwartz;given-names:J</infon><infon key="name_12">surname:Haber;given-names:N</infon><infon key="name_13">surname:Wall;given-names:DP</infon><infon key="name_2">surname:Kalantarian;given-names:H</infon><infon key="name_3">surname:Stockham;given-names:N</infon><infon key="name_4">surname:Voss;given-names:C</infon><infon key="name_5">surname:Kline;given-names:A</infon><infon key="name_6">surname:Patnaik;given-names:R</infon><infon key="name_7">surname:Chrisman;given-names:B</infon><infon key="name_8">surname:Varma;given-names:M</infon><infon key="name_9">surname:Tariq;given-names:Q</infon><infon key="pub-id_medline">31797640</infon><infon key="pub-id_pmid">31797640</infon><infon key="section_type">REF</infon><infon key="source">Pac Symp Biocomput</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2020</infon><offset>40548</offset><text>Feature selection and dimension reduction of social autism data</text></passage><passage><infon key="fpage">1000</infon><infon key="issue">8</infon><infon key="lpage">7</infon><infon key="name_0">surname:Abbas;given-names:H</infon><infon key="name_1">surname:Garberson;given-names:F</infon><infon key="name_2">surname:Glover;given-names:E</infon><infon key="name_3">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1093/jamia/ocy039</infon><infon key="pub-id_medline">29741630</infon><infon key="pub-id_pmid">29741630</infon><infon key="section_type">REF</infon><infon key="source">J Am Med Inform Assoc</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2018</infon><offset>40612</offset><text>Machine learning approach for early detection of autism by combining questionnaire and home video screening</text></passage><passage><infon key="fpage">65</infon><infon key="name_0">surname:Levy;given-names:S</infon><infon key="name_1">surname:Duda;given-names:M</infon><infon key="name_2">surname:Haber;given-names:N</infon><infon key="name_3">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1186/s13229-017-0180-6</infon><infon key="pub-id_medline">29270283</infon><infon key="pub-id_pmid">29270283</infon><infon key="section_type">REF</infon><infon key="source">Mol Autism</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>40720</offset><text>Sparsifying machine learning models identify stable subsets of predictive features for behavioral detection of autism</text></passage><passage><infon key="fpage">e93533</infon><infon key="issue">4</infon><infon key="name_0">surname:Fusaro;given-names:VA</infon><infon key="name_1">surname:Daniels;given-names:J</infon><infon key="name_2">surname:Duda;given-names:M</infon><infon key="name_3">surname:DeLuca;given-names:TF</infon><infon key="name_4">surname:D'Angelo;given-names:O</infon><infon key="name_5">surname:Tamburello;given-names:J</infon><infon key="name_6">surname:Maniscalco;given-names:J</infon><infon key="name_7">surname:Wall;given-names:DP</infon><infon key="pub-id_doi">10.1371/journal.pone.0093533</infon><infon key="pub-id_medline">24740236</infon><infon key="pub-id_pmid">24740236</infon><infon key="section_type">REF</infon><infon key="source">PLoS One</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>40838</offset><text>The potential of accelerating early detection of autism through content analysis of YouTube videos</text></passage><passage><infon key="fpage">e1</infon><infon key="issue">1</infon><infon key="name_0">surname:Sahin;given-names:NT</infon><infon key="name_1">surname:Keshav;given-names:NU</infon><infon key="name_2">surname:Salisbury;given-names:JP</infon><infon key="name_3">surname:Vahabzadeh;given-names:A</infon><infon key="pub-id_doi">10.2196/humanfactors.8785</infon><infon key="pub-id_medline">29301738</infon><infon key="pub-id_pmid">29301738</infon><infon key="section_type">REF</infon><infon key="source">JMIR Hum Factors</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2018</infon><offset>40937</offset><text>Second version of Google Glass as a wearable socio-affective aid: positive school desirability, high usability, and theoretical framework in a sample of children with autism</text></passage><passage><infon key="fpage">e140</infon><infon key="issue">9</infon><infon key="name_0">surname:Keshav;given-names:NU</infon><infon key="name_1">surname:Salisbury;given-names:JP</infon><infon key="name_2">surname:Vahabzadeh;given-names:A</infon><infon key="name_3">surname:Sahin;given-names:NT</infon><infon key="pub-id_doi">10.2196/mhealth.8534</infon><infon key="pub-id_medline">28935618</infon><infon key="pub-id_pmid">28935618</infon><infon key="section_type">REF</infon><infon key="source">JMIR Mhealth Uhealth</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>41111</offset><text>Social communication coaching smartglasses: well tolerated in a diverse sample of children and adults with autism</text></passage><passage><infon key="fpage">77</infon><infon key="lpage">8</infon><infon key="name_0">surname:Hernandez;given-names:J</infon><infon key="name_1">surname:Picard;given-names:W</infon><infon key="pub-id_doi">10.1145/2658779.2658784</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the adjunct publication of the 27th annual ACM symposium on User interface software and technology</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>41225</offset><text>SenseGlass: Using Google Glass to Sense Daily Emotions</text></passage><passage><infon key="fpage">262</infon><infon key="lpage">3</infon><infon key="name_0">surname:Scheirer;given-names:J</infon><infon key="name_1">surname:Fernandez;given-names:R</infon><infon key="name_2">surname:Picard;given-names:R</infon><infon key="pub-id_doi">10.1145/632716.632878</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Extended Abstracts on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>41280</offset><text>Expression Glasses: A Wearable Device for Facial Expression Recognition</text></passage><passage><infon key="fpage">1257</infon><infon key="lpage">63</infon><infon key="name_0">surname:Katsutoshi;given-names:M</infon><infon key="name_1">surname:Kunze;given-names:K</infon><infon key="name_2">surname:Billinghurst;given-names:M</infon><infon key="pub-id_doi">10.1145/2851581.2892370</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>41352</offset><text>Empathy Glasses</text></passage><passage><infon key="fpage">38</infon><infon key="issue">1</infon><infon key="lpage">46</infon><infon key="name_0">surname:Escobedo;given-names:L</infon><infon key="name_1">surname:Tentori;given-names:M</infon><infon key="name_2">surname:Quintana;given-names:E</infon><infon key="name_3">surname:Favela;given-names:J</infon><infon key="name_4">surname:Garcia-Rosas;given-names:D</infon><infon key="pub-id_doi">10.1109/mprv.2014.19</infon><infon key="section_type">REF</infon><infon key="source">IEEE Pervasive Comput</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2014</infon><offset>41368</offset><text>Using augmented reality to help children with autism stay focused</text></passage><passage><infon key="fpage">275</infon><infon key="lpage">94</infon><infon key="name_0">surname:Scassellati;given-names:B</infon><infon key="name_1">surname:Admoni;given-names:H</infon><infon key="name_2">surname:Matarić;given-names:M</infon><infon key="pub-id_doi">10.1146/annurev-bioeng-071811-150036</infon><infon key="pub-id_medline">22577778</infon><infon key="pub-id_pmid">22577778</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Biomed Eng</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2012</infon><offset>41434</offset><text>Robots for use in autism research</text></passage><passage><infon key="fpage">201</infon><infon key="lpage">10</infon><infon key="name_0">surname:Feil-Seifer;given-names:D</infon><infon key="name_1">surname:Matarić;given-names:MJ</infon><infon key="name_2">surname:Khatib;given-names:O</infon><infon key="name_3">surname:Kumar;given-names:V</infon><infon key="name_4">surname:Pappas;given-names:GJ</infon><infon key="section_type">REF</infon><infon key="source">Experimental Robotics: The Eleventh International Symposium</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>41468</offset><text>Toward socially assistive robotics for augmenting interventions for children with autism spectrum disorders</text></passage><passage><infon key="fpage">105</infon><infon key="issue">2</infon><infon key="lpage">20</infon><infon key="name_0">surname:Robins;given-names:B</infon><infon key="name_1">surname:Dautenhahn;given-names:K</infon><infon key="name_2">surname:Boekhorst;given-names:RT</infon><infon key="name_3">surname:Billard;given-names:A</infon><infon key="pub-id_doi">10.1007/s10209-005-0116-3</infon><infon key="section_type">REF</infon><infon key="source">Univ Access Inf Soc</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2005</infon><offset>41576</offset><text>Robotic assistants in therapy and education of children with autism: can a small humanoid robot help encourage social interaction skills?</text></passage><passage><infon key="fpage">2589</infon><infon key="lpage">98</infon><infon key="name_0">surname:Escobedo;given-names:L</infon><infon key="name_1">surname:Nguyen;given-names:DH</infon><infon key="name_2">surname:Boyd;given-names:LA</infon><infon key="name_3">surname:Hirano;given-names:S</infon><infon key="name_4">surname:Rangel;given-names:A</infon><infon key="name_5">surname:Garcia-Rosas;given-names:D</infon><infon key="name_6">surname:Tentori;given-names:ME</infon><infon key="name_7">surname:Hayes;given-names:GR</infon><infon key="pub-id_doi">10.1145/2207676.2208649</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>41714</offset><text>MOSOCO: A Mobile Assistive Tool to Support Children With Autism Practicing Social Skills in Real-Life Situations</text></passage><passage><infon key="fpage">518</infon><infon key="lpage">28</infon><infon key="name_0">surname:Palestra;given-names:G</infon><infon key="name_1">surname:Pettiniccho;given-names:A</infon><infon key="name_2">surname:Del;given-names:CM</infon><infon key="name_3">surname:Carcagni;given-names:P</infon><infon key="pub-id_doi">10.1007/978-3-319-23234-8_48</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Image Analysis and Processing</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>41827</offset><text>Improved Performance in Facial Expression Recognition Using 32 Geometric Features</text></passage><passage><infon key="issue">11</infon><infon key="name_0">surname:Leo;given-names:M</infon><infon key="name_1">surname:Carcagnì;given-names:P</infon><infon key="name_10">surname:Lecciso;given-names:F</infon><infon key="name_2">surname:Distante;given-names:C</infon><infon key="name_3">surname:Spagnolo;given-names:P</infon><infon key="name_4">surname:Mazzeo;given-names:P</infon><infon key="name_5">surname:Rosato;given-names:A</infon><infon key="name_6">surname:Petrocchi;given-names:S</infon><infon key="name_7">surname:Pellegrino;given-names:C</infon><infon key="name_8">surname:Levante;given-names:A</infon><infon key="name_9">surname:de Lumè;given-names:F</infon><infon key="pub-id_doi">10.3390/s18113993</infon><infon key="pub-id_medline">30453518</infon><infon key="section_type">REF</infon><infon key="source">Sensors (Basel)</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>41909</offset><text>Computational assessment of facial expression production in ASD children</text></passage><passage><infon key="fpage">2423</infon><infon key="lpage">8</infon><infon key="name_0">surname:Park;given-names:JH</infon><infon key="name_1">surname:Abirached;given-names:B</infon><infon key="name_2">surname:Zhang;given-names:Y</infon><infon key="pub-id_doi">10.1145/2212776.2223813</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Extended Abstracts on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>41982</offset><text>A Framework for Designing Assistive Technologies for Teaching Children With ASDs Emotions</text></passage><passage><infon key="comment">[Online ahead of print]</infon><infon key="name_0">surname:Washington;given-names:P</infon><infon key="name_1">surname:Park;given-names:N</infon><infon key="name_10">surname:Chrisman;given-names:B</infon><infon key="name_11">surname:Stockham;given-names:N</infon><infon key="name_12">surname:Paskov;given-names:K</infon><infon key="name_13">surname:Haber;given-names:N</infon><infon key="name_14">surname:Wall;given-names:DP</infon><infon key="name_2">surname:Srivastava;given-names:P</infon><infon key="name_3">surname:Voss;given-names:C</infon><infon key="name_4">surname:Kline;given-names:A</infon><infon key="name_5">surname:Varma;given-names:M</infon><infon key="name_6">surname:Tariq;given-names:Q</infon><infon key="name_7">surname:Kalantarian;given-names:H</infon><infon key="name_8">surname:Schwartz;given-names:J</infon><infon key="name_9">surname:Patnaik;given-names:R</infon><infon key="pub-id_doi">10.1016/j.bpsc.2019.11.015</infon><infon key="pub-id_medline">32085921</infon><infon key="section_type">REF</infon><infon key="source">Biol Psychiatry Cogn Neurosci Neuroimaging</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>42072</offset><text>Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="lpage">23</infon><infon key="name_0">surname:Christensen;given-names:DL</infon><infon key="name_1">surname:Baio;given-names:J</infon><infon key="name_10">surname:Lee;given-names:LC</infon><infon key="name_11">surname:Pettygrove;given-names:S</infon><infon key="name_12">surname:Robinson;given-names:C</infon><infon key="name_13">surname:Schulz;given-names:E</infon><infon key="name_14">surname:Wells;given-names:C</infon><infon key="name_15">surname:Wingate;given-names:MS</infon><infon key="name_16">surname:Zahorodny;given-names:W</infon><infon key="name_17">surname:Yeargin-Allsopp;given-names:M</infon><infon key="name_2">surname:Braun;given-names:KN</infon><infon key="name_3">surname:Bilder;given-names:D</infon><infon key="name_4">surname:Charles;given-names:J</infon><infon key="name_5">surname:Constantino;given-names:JN</infon><infon key="name_6">surname:Daniels;given-names:J</infon><infon key="name_7">surname:Durkin;given-names:MS</infon><infon key="name_8">surname:Fitzgerald;given-names:RT</infon><infon key="name_9">surname:Kurzius-Spencer;given-names:M</infon><infon key="pub-id_doi">10.15585/mmwr.ss6503a1</infon><infon key="pub-id_medline">27031587</infon><infon key="section_type">REF</infon><infon key="source">MMWR Surveill Summ</infon><infon key="type">ref</infon><infon key="volume">65</infon><infon key="year">2016</infon><offset>42212</offset><text>Prevalence and characteristics of autism spectrum disorder among children aged 8 years--autism and developmental disabilities monitoring network, 11 sites, United States, 2012</text></passage><passage><infon key="fpage">712</infon><infon key="issue">4</infon><infon key="lpage">7</infon><infon key="name_0">surname:Ekman;given-names:P</infon><infon key="name_1">surname:Friesen;given-names:WV</infon><infon key="name_2">surname:O'Sullivan;given-names:M</infon><infon key="name_3">surname:Chan;given-names:A</infon><infon key="name_4">surname:Diacoyanni-Tarlatzis;given-names:I</infon><infon key="name_5">surname:Heider;given-names:K</infon><infon key="name_6">surname:Krause;given-names:R</infon><infon key="name_7">surname:LeCompte;given-names:WA</infon><infon key="name_8">surname:Pitcairn;given-names:T</infon><infon key="name_9">surname:Ricci-Bitti;given-names:PE</infon><infon key="pub-id_doi">10.1037//0022-3514.53.4.712</infon><infon key="pub-id_medline">3681648</infon><infon key="pub-id_pmid">3681648</infon><infon key="section_type">REF</infon><infon key="source">J Pers Soc Psychol</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">1987</infon><offset>42388</offset><text>Universals and cultural differences in the judgments of facial expressions of emotion</text></passage><passage><infon key="fpage">63</infon><infon key="issue">2</infon><infon key="lpage">72</infon><infon key="name_0">surname:Sinzig;given-names:J</infon><infon key="name_1">surname:Morsch;given-names:D</infon><infon key="name_2">surname:Lehmkuhl;given-names:G</infon><infon key="pub-id_doi">10.1007/s00787-007-0637-9</infon><infon key="pub-id_medline">17896119</infon><infon key="pub-id_pmid">17896119</infon><infon key="section_type">REF</infon><infon key="source">Eur Child Adolesc Psychiatry</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2008</infon><offset>42474</offset><text>Do hyperactivity, impulsivity and inattention have an impact on the ability of facial affect recognition in children with autism and ADHD?</text></passage></document></collection>
