<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220303</date><key>pmc.key</key><document><id>8883156</id><infon key="license">CC BY</infon><passage><infon key="alt-title">Sawayama et al.</infon><infon key="article-id_doi">10.1167/jov.22.2.17</infon><infon key="article-id_pmc">8883156</infon><infon key="article-id_pmid">35195670</infon><infon key="article-id_publisher-id">JOV-07201-2019</infon><infon key="elocation-id">17</infon><infon key="issue">2</infon><infon key="kwd">material perception image and observer database visual psychophysics computer graphics crowdsourcing</infon><infon key="license">This work is licensed under a Creative Commons Attribution 4.0 International License.</infon><infon key="name_0">surname:Sawayama;given-names:Masataka</infon><infon key="name_1">surname:Dobashi;given-names:Yoshinori</infon><infon key="name_2">surname:Okabe;given-names:Makoto</infon><infon key="name_3">surname:Hosokawa;given-names:Kenchi</infon><infon key="name_4">surname:Koumura;given-names:Takuya</infon><infon key="name_5">surname:Saarela;given-names:Toni P.</infon><infon key="name_6">surname:Olkkonen;given-names:Maria</infon><infon key="name_7">surname:Nishida;given-names:Shin'ya</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">22</infon><infon key="year">2022</infon><offset>0</offset><text>Visual discrimination of optical material properties: A large-scale study</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>74</offset><text>Complex visual processing involved in perceiving the object materials can be better elucidated by taking a variety of research approaches. Sharing stimulus and response data is an effective strategy to make the results of different studies directly comparable and can assist researchers with different backgrounds to jump into the field. Here, we constructed a database containing several sets of material images annotated with visual discrimination performance. We created the material images using physically based computer graphics techniques and conducted psychophysical experiments with them in both laboratory and crowdsourcing settings. The observer's task was to discriminate materials on one of six dimensions (gloss contrast, gloss distinctness of image, translucent vs. opaque, metal vs. plastic, metal vs. glass, and glossy vs. painted). The illumination consistency and object geometry were also varied. We used a nonverbal procedure (an oddity task) applicable for diverse use cases, such as cross-cultural, cross-species, clinical, or developmental studies. Results showed that the material discrimination depended on the illuminations and geometries and that the ability to discriminate the spatial consistency of specular highlights in glossiness perception showed larger individual differences than in other tasks. In addition, analysis of visual features showed that the parameters of higher order color texture statistics can partially, but not completely, explain task performance. The results obtained through crowdsourcing were highly correlated with those obtained in the laboratory, suggesting that our database can be used even when the experimental conditions are not strictly controlled in the laboratory. Several projects using our dataset are underway.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1857</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1870</offset><text>Humans can visually recognize a variety of material properties of the objects they encounter daily. Although material properties, such as glossiness and wetness, substantially contribute to recognition, the contributions of value-based decision making, motor control, and computational and neural mechanisms underlying material perception had been overlooked until relatively recently—for a long time, vision science mainly used simple artificial stimuli to elucidate the underlying brain mechanisms. In the last two decades, however, along with advancements in computer graphics and machine vision, material perception has become a major topic in vision science.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2536</offset><text>Visual material perception can be considered to be an estimation of material-related properties from an object image. For example, the perception of gloss and matte entails a visual computation of the specular and diffuse reflections of the surface, respectively. However, psychophysical studies have shown that human gloss perception does not have robust constancy against changes in surface geometry and illumination (e.g.,), the other two main factors of image formation. Such estimation errors have provided useful information as to what kind of image cues humans use to estimate gloss. A significant number of psychophysical studies have been carried out not only on gloss but also on other optical material properties (e.g., translucency, transparency, wetness) (a,) and mechanical material properties (e.g., viscosity, elasticity). Neurophysiological and neuroimaging studies have revealed various neural mechanisms underlying material perception. Some recent studies have also focused on developmental, environmental, and clinical factors of material processing. For example, showed in their monkey functional magnetic resonance imaging study that the visuohaptic experience of material objects alters the visual cortical representation. In addition, large individual differences in the perception of colors and materials depicted in one photo (#TheDress) have attracted much interest and provoked intense discussions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3963</offset><text>A promising strategy for a more global understanding of material perception is to promote multidisciplinary studies comparing behavioral and physiological responses of humans and animals obtained under a variety of developmental, environmental, cultural, and clinical conditions. There are two problems, however. One lies in the high degree of freedom in selecting experimental stimulus parameters and task procedures. Because the appearance of a material depends not only on reflectance parameters but also on geometry and illumination, all of which are high dimensional, the use of different stimuli (and different tasks) in different studies could impose serious limitations on direct data comparisons. The other problem is the technical expertise necessary for rendering realistic images, which could discourage researchers unfamiliar with graphics from beginning material perception studies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4860</offset><text>Aiming at removing these obstacles, we attempted to build a database that can be shared among multidisciplinary material studies. We rendered several sets of material images. The images in each set were changed in one of material dimensions in addition to illumination and viewing conditions. We then measured the behavioral performance for those image sets using a large number of “standard” observers. We used a simple task that can be used in a variety of human, animal, and computational studies. By using our database, one would be able to efficiently begin a new study, shortening the time for stimulus preparation, as well as time for control data collection with standard human observers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5561</offset><text>Specifically, we selected six dimensions of material property (Figure 1). These dimensions have been extensively studied in the past material perception studies. Most of them can be unambiguously manipulated by changing the corresponding rendering parameters. Although we attempted to cover a wide range of optical material topics, we do not believe that we have assembled an exclusive list of critical material properties that vision science should challenge. Our intention is not to build the standard database for all material recognition research, but to establish one primitive test set that promotes further examination of the previous findings on material recognition in more diverse research contexts (see Discussion).</text></passage><passage><infon key="file">jovi-22-2-17-f001.jpg</infon><infon key="id">fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>6289</offset><text>Schematic overview of six tasks recorded in the database.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6347</offset><text>Three of these dimensions are related to gloss (Figure 1, tasks 1, 2, and 6), the most widely investigated material attribute. We controlled the contrast gloss and gloss distinctness of image (DOI) as in previous studies. For example, found neurons in the inferior temporal cortex of monkeys that selectively and parametrically respond to gloss changes in these two dimensions. We also controlled the spatial consistency of specular highlights, which is another stimulus manipulation of gloss perception (Figure 1, task 6). By breaking the spatial consistency, some highlights look like albedo changes by white paint. In addition to gloss perception, translucency perception has also been widely investigated. We adopted the task of discriminating opaque from translucent objects by controlling the thickness of the translucent media (Figure 1, task 3). Furthermore, we adopted the task of plastic-yellow/gold discrimination (task 4, MP) and glass/silver discrimination (task 5, MG).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7334</offset><text>We used an oddity task (Figure 3) to evaluate the capability of discriminating each material dimension. We chose this task because it requires neither complex verbal instruction nor verbal responses by the observer. Therefore, it can be applied to a wide variety of observers including infants, animals, and machine vision algorithms, and their task performances can be directly compared. Indeed, several research projects using our dataset are underway (see the Discussion section).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7819</offset><text>To control the task difficulty, we varied the value of the parameter of each material dimension. In addition, we manipulated the stimulus in two ways that affected the task difficulty. First, we set three illumination conditions. One set of stimuli included images of different poses taken in identical illumination environments (Figure 2a, illumination condition 1); the second set contained stimuli of identical poses taken in slightly different illumination environments (Figure 2a, illumination condition 2); and the third set contained identical poses taken in largely different illumination environments (Figure 2a, illumination condition 3). Then, we used the five different object geometries for each task (Figure 2b).</text></passage><passage><infon key="file">jovi-22-2-17-f002.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>8550</offset><text>(a) Illumination conditions. Object images were rendered with six global illumination environments and were presented to observers under three illumination conditions. Under illumination condition 1, a stimulus display consisted of four objects (same shape, different poses) rendered with the same illumination environment. Under illumination condition 2, a stimulus display consisted of three objects (same shape, same pose) rendered with slightly different (in terms of their pixel histograms) light probes. Under illumination condition 3, a stimulus display consisted of three objects (same shape, same pose) rendered with largely different illumination environments. (b) Geometrical conditions. We used five different object shapes for each material task under each illumination condition. The stimulus condition is also summarized in Table 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9400</offset><text>We wished to collect data from a large number of observers. A laboratory experiment affords control over the stimulus presentation environment but is unsuited to collecting a large amount of data from numerous participants. In contrast, one can collect a lot of data through crowdsourcing, at the expense of reliable stimulus control. To overcome this trade-off, we conducted identical psychophysical experiments both in the laboratory and through crowdsourcing. This enabled us to evaluate individual difference distributions along with the effects of environmental factors on task performance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9996</offset><text>In sum, we made a large set of image stimuli for evaluations of visual discrimination performance on six material dimensions (gloss contrast, gloss DOI, translucency–opaque, plastic–gold, glass–silver, and glossy–painted) and measured a large number of adult human observers performing oddity tasks in the laboratory and through crowdsourcing. The tasks had three illumination conditions and five object geometries. Although the original motivation of this project was to make a standard stimulus–response dataset of material recognition for promotion of multidisciplinary studies, it also has its own scientific value as, to the best of our knowledge, it is the first systematic comparison of the effects of illumination condition and object geometry, as well as of individual variations across a variety of material dimensions. Our data include several novel findings, as reported below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>10896</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10904</offset><text>We evaluated the observers’ performance of six material recognition tasks. We selected tasks that had been used in previous material studies: (1) gloss contrast discrimination (GC); (2) gloss DOI discrimination (GD); (3) opaque versus translucent (OT); (4) metallic gold versus plastic yellow (MP); (5) metallic silver versus glass (MG); and (6) glossy versus painted (GP). For each task, we used five geometry models. We used six global illuminations for tasks 1 to 5 and one for task 6. We conducted behavioral experiments using an oddity task, which can be used even with human babies, animals, and brain-injured participants, because it does not entail complex verbal instructions. In the experiment, the observers were asked to select the stimulus that represented an oddity among three or four object stimuli. They were not given any feedback about whether or not their responses were correct. We controlled the task difficulty by changing the illumination and material parameters. To test the generality of the resultant database, we conducted identical experiments in the laboratory and through crowdsourcing. Our dataset is available at https://github.com/mswym/material_dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12095</offset><text>Image generation for making standard image database</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12147</offset><text>We utilized the physically based rendering software called Mitsuba to make images of objects consisting of different materials, and we controlled six different material dimensions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>12328</offset><text>Task 1 (GC) and task 2 (GD) conditions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12367</offset><text>To control the material property of the gloss discrimination tasks, we used the perceptual light reflection model proposed by. They constructed a model based on the results of psychophysical experiments using stimuli rendered by the Ward reflection model and rewrote the Ward model parameters in perceptual terms. The model of Pellacini et al. has two parameters, d and c, which roughly correspond to the DOI gloss and the contrast gloss, respectively, of. The difficulty of our two gloss discrimination tasks was controlled by separately modulating these two parameters.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12940</offset><text>The parameter space of the Ward reflection model can be described as follows: where ρ(θi, ϕi, θo, ϕo) is the surface reflection model, and θi, ϕi and θo, ϕo are the incoming and outgoing directions, respectively. The model has three parameters; ρd is the diffuse reflectance of a surface, ρs is the energy of its specular component, and α is the spread of the specular lobe. As noted earlier, defined two perceptual dimensions, c and d, on the basis of the Ward model parameters, such that d corresponds to DOI gloss and is calculated from α, and c corresponds to perceptual glossiness contrast and is calculated from ρs and ρd using the following formula: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13640</offset><text>Although more physically feasible bidirectional reflectance distribution function (BRDF) models than the Ward model have been proposed for gloss simulation, we based ours on the Ward model because it has been used in many previous psychophysics and neuroscience studies.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13911</offset><text>For the task of gloss discrimination in the contrast dimension, the specular reflectance (ρs) was varied in a range from 0.00 to 0.12 in steps of 0.02 while keeping the diffuse reflectance (ρd) constant (0.416), resulting in the contrast parameters 0, 0.018, 0.035, 0.052, 0.067, 0.082, and 0.097. The DOI (d) was the fixed value (0.94) (Figure 4, task 1). As c approaches 0, the object appears to have a matte surface. The specular reflectance (ρs) of the non-target stimulus in the task was 0.06.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14416</offset><text>For the task of gloss discrimination in the DOI dimension, the parameter d was varied from 0.88 to 1.00 in 0.02 steps while keeping ρs constant (0.06) (Figure 4, task 2). As d approaches 1.00, the highlights of the object appear sharper. The DOI parameter (d) of the non-target stimuli was 0.94.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>14714</offset><text>Task 3 (OT) conditions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14737</offset><text>To make translucent materials, we used the function of homogeneous participating medium implemented in the Mitsuba renderer. In this function, a flexible homogeneous participating medium is embedded in each object model. The intensity of the light that travels in the medium is decreased by scattering and absorption and is increased by nearby scattering. The parameters of the absorption and scattering coefficients of the medium describe how the light is decreased. We used the parameters of the “whole milk” measured by. The parameter of the phase function describes the directional scattering properties of the medium. We used an isotropic phase function. To control the task difficulty, we modulated the scale parameter of the scattering and absorption coefficients. The parameter describes the density of the medium. The smaller the scale parameter is, the more translucent the medium becomes. The scale parameter was varied as follows: 0.0039, 0.0156, 0.0625, 0.25, and 1.00 (Figure 5, task 3). The scale parameter of the non-target stimulus in the task was 1.00. In addition, the surface of the object was modeled as a smooth dielectric material to produce strong specular highlights, as in previous studies. That is, non-target objects were always opaque, and the degree of transparency of the target object was changed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16072</offset><text>Task 4 (MP) conditions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16095</offset><text>To morph the material between gold and plastic yellow, we utilized a linear combination of gold and plastic BRDFs, implemented in the Mitsuba renderer. By changing the weight of the combination, the appearance of a material (e.g., gold) can be modulated toward that of the other material (e.g., plastic yellow). In this task, the weight was varied in a range from 0.00 to 0.80 in steps of 0.20 (Figure 5, task 4). The parameter of the non-target stimulus was 0, at which the material appeared to be pure gold.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16606</offset><text>Task 5 (MG) conditions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16629</offset><text>Similar to task 4, we utilized a linear combination of dielectric glass and silver materials, also implemented in the Mitsuba renderer. The weight of the combination was varied from 0.00 to 0.80. The parameter of the non-target stimulus was 0, at which the material appeared to be pure silver (Figure 5, task 5).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16943</offset><text>As noted above, for tasks 3, 4, and 5 in which the parameters of the target stimulus were varied between two material states (i.e., opaque vs. transparent, metallic vs. plastic, and metallic vs. glass), we placed the non-target objects at one end (i.e., one of two material states). If we placed the non-target stimuli in the middle of the stimulus variable, as in tasks 1 and 2, and when the difference between the target and non-target stimuli was small, the display contained only ambiguous material objects. In such cases, the observers might not pay attention to the material dimension relevant to the task. By placing the non-target at one extreme value, we could make the stimulus display always contain the object images in a specific material state, helping participants focus on the task relevant material dimension.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>17770</offset><text>Task 6 (GP) conditions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17793</offset><text>The skewed intensity distribution due to specular highlights of an object image can be a diagnostic cue for gloss perception. However, when the specular highlights are inconsistent in terms of their position and/or orientation with respect to the diffuse shading component, they look more like white blobs produced by surface reflectance changes even if the intensity distribution is kept constant. For our last task of glossy objects versus matte objects with white paint, we rendered the glossy objects on the basis of the model of. The parameter c was set to 0.067, and the parameter d ranged from 0.88 to 1.00 in steps of 0.04 (Figure 6, lower). Considering material naturalness, these objects may not be typically encountered in the real world, but this task is theoretically important because it will provide insights into the underlying visual computation of material recognition.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18682</offset><text>To make object images with inconsistent highlights (white paints), we rendered each scene twice with different object materials with identical shapes. First, we rendered a glossy object image by setting the diffuse reflectance to 0 (i.e., the image including only specular highlights). The rendered image of specular highlights was a two-dimensional texture for the second rendering. We eliminated the brown table when rendering the first scene. Next, we rendered a diffuse object image (i.e., one without specular reflection with the texture of specular highlights). The object and illumination for the first and second renderings were the same. We mapped the specular image rendered in one object pose to the three-dimensional geometry by spherical mapping and repeating the image. Because the position of the texture mapping was randomly determined, the highlight texture positions were inconsistent with diffuse shadings. We varied the parameter d of the first rendering from 1.00 to 0.88 (Figure 6, lower). After we rendered the inconsistent-highlights image, the color histogram of the image was set to that of a consistent glossy object image by using a standard histogram matching method.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19880</offset><text>We made task 6 only under illumination 1. This is because it was difficult to match the color distributions of the target and non-target stimuli for illuminations 2 and 3, where one stimulus set was rendered under different illuminations. If we matched the color histograms of the objects under these conditions, the colors of the objects could be incongruent with their background colors (i.e., the table and the shadow in this scene). This could produce another cue to find an outlier, thus making these conditions inappropriate for the task purpose.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>20433</offset><text>Geometry</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20442</offset><text>For each material, we rendered the object images by using five different abstract geometries (Figure 2b). These geometries were made from a sphere by modulating each surface normal direction with different kinds of noise using ShapeToolbox. Specifically, Object_1 was made from modulations of low-spatial-frequency noise and crater-like patterns (the source code of this geometry is available on the web, http://saarela.github.io/ShapeToolbox/gallery-moon.html). Object_2 was a bumpy sphere modulated by lowpass bandpass noise. Object_3 was a bumpy sphere modulated by sine-wave noise. Object_4 and Object_5 were bumpy spheres modulated by Perlin noise. These objects were also rendered using ShapeToolbox.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21150</offset><text>Five samples were too small to systematically vary shape parameters. Instead, we handcrafted sphere-based abstract shapes in such a way expected to maximize the shape diversity. It is known that, even when rendering with the same reflectance function (BRDF), objects with smooth, low-frequency surface modulations and those with spiky, high-frequency surface modulations could have very different material appearance. We therefore created five geometries with a variety of low and high spatial frequency surface modulations to see human material perception under widely different geometry conditions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>21751</offset><text>Illumination and pose</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21773</offset><text>We used six high-dynamic-range light-probe images as illuminations for rendering. These images were obtained from Bernhard Vogl's light probe database (http://dativ.at/lightprobes/). To vary the task difficulty, we used three illumination conditions (illumination conditions 1, 2, and 3) (Figure 2a). Under illumination condition 1, the observers selected one oddity from four images in a task. We rendered the images by using an identical light probe: “Overcast Day/Building Site (Metro Vienna).” We prepared five poses for each task of illumination condition 1 by rotating each object in 36° steps; four of them were randomly selected in each task.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22430</offset><text>Under illumination condition 2, the observers selected one oddity from three images in a task. We created the images by using slightly different (in terms of their pixel histograms) light probes, including “Overcast Day/Building Site (Metro Vienna),” “Overcast Day at Techgate Donaucity,” and “Metro Station (Vienna Metro).” The task procedure of illumination condition 3 was the same as that of illumination condition 2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22864</offset><text>For illumination condition 3, we created the three images by using light probes that were rather different from each other: “Inside Tunnel Machine,” “Tungsten Light in the Evening (Metro Building Site Vienna),” and “Building Site Interior (Metro Vienna).” We computed the pixel histogram similarity for each illumination pair and used it as the distance for the multidimensional scaling analysis (MDS). We extracted three largely different light probes in the MDS space and used them for illumination condition 3. We also selected three similar light probes in the space and used them for illumination condition 2. The pose of each object in illumination conditions 2 and 3 was not changed. The stimulus condition is summarized in Table 1.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>23618</offset><text>Summary of stimulus conditions.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Illumination&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 1 (GC)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 2 (GD)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 3 (OT)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 4 (MP)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 5 (MG)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Task 6 (GP)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (1), pose (5)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination (3), pose (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Object (5), illumination, (3), pose (1)&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>23650</offset><text>Illumination	Task 1 (GC)	Task 2 (GD)	Task 3 (OT)	Task 4 (MP)	Task 5 (MG)	Task 6 (GP)	 	1	Object (5), illumination (1), pose (5)	Object (5), illumination (1), pose (5)	Object (5), illumination (1), pose (5)	Object (5), illumination (1), pose (5)	Object (5), illumination (1), pose (5)	Object (5), illumination (1), pose (5)	 	2	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	—	 	3	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination (3), pose (1)	Object (5), illumination, (3), pose (1)	—	 	</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>24384</offset><text>Digits in parentheses indicate the number of each condition.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24445</offset><text>Rendering</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24455</offset><text>To render the images, we used the integrator of the photon mapping method for tasks 1, 2, 4, 5, and 6 and used the integrator of the simple volumetric path tracer implemented in the Mitsuba renderer for task 3 (OT). The calculation was conducted using single-float precision. Each rendered image was converted into sRGB format with a gamma of 2.2 and saved as an 8-bit PNG image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24835</offset><text>Behavioral experiments</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24858</offset><text>Laboratory experiment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24880</offset><text>Twenty paid volunteers participated in the laboratory experiment. Before starting the experiment, we confirmed that all of them had normal color vision by having them take the Farnsworth Munsell 100 Hue Test and that all had normal or corrected-to-normal vision by having them take a simple visual acuity test. The participants were naïve to the purpose and methods of the experiment. The experiment was approved by the ethical committees at NTT Communication Science Laboratories.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25363</offset><text>The generated stimuli were presented on a calibrated 30-inch color monitor (ColorEdge CG303W; EIZO, Hakusan, Ishikawa, Japan) controlled with a Quadro 600 video card (NVIDIA Corporation, Santa Clara, CA). Each participant viewed the stimuli in a dark room at a viewing distance of 86 cm, where a single pixel subtended 1 arcmin. Each object image of 512 × 512 pixels was presented at a size of 8.5° × 8.5°.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25774</offset><text>In each trial, four (illumination 1) or three (illuminations 2 and 3) object images chosen for each task were presented on the monitor (Figure 3456). Measurements of different illumination conditions were conducted in different blocks. Under illumination condition 1, four different object images in different orientations were presented. Under illumination conditions 2 and 3, the three different object images had different illuminations. The order of illumination conditions 1, 2, and 3 was counterbalanced across observers. The observers were asked to report which of the object images looked odd by pushing one of the keys. The stimuli were presented until the observer made a response. The task instructions were simply to find the odd one with no further explanation about how it was different from the rest. The observers were not given any feedback about whether or not their response was correct. All made 10 judgments for each task of illumination condition 1. Seventeen observers made 10 judgments for each task of illumination condition 2, and three made only seven judgments due to the experiment's time limitation. Seventeen observers made 10 judgments for each task of illumination condition 3, and three made seven judgments due to the experiment's time limitation.</text></passage><passage><infon key="file">jovi-22-2-17-f003.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27058</offset><text>Example of a four-object oddity task (illumination condition 1) used for collecting standard observer data. The observers were asked to select which image was the odd one out in the four images. We did not tell the observer that the experiment was on material recognition. We conducted experiments both in the laboratory and through crowdsourcing.</text></passage><passage><infon key="file">jovi-22-2-17-f004.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27406</offset><text>Material examples of tasks 1 (GC) and 2 (GD). For task 1 (GC), the specular reflectance of the odd target stimulus was varied from 0.00 to 0.12. The non-target stimuli that were presented as the context objects in each task had specular reflectance of 0.06. For task 2 (GD), the DOI parameter of the target specular reflection was varied from 1.00 to 0.88, and that of the non-target stimuli was 0.94.</text></passage><passage><infon key="file">jovi-22-2-17-f005.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27808</offset><text>Material examples of tasks 3 (OT), 4 (MP), and 5 (MG). For task 3 (OT), the scale of the volume media that consisted of milk was varied from 1.0 to 0.0039. For task 4 (MP) and 5 (MG), the blending ratio of the two materials was varied from 0.0 to 0.8. The non-target stimuli in the tasks were as shown in the legend.</text></passage><passage><infon key="file">jovi-22-2-17-f006.jpg</infon><infon key="id">fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28125</offset><text>Material examples of task 6. The DOI of the specular reflection was varied from 1.00 to 0.88. This parameter was the same for the non-target painted objects and the target glossy object in each stimulus display.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>28337</offset><text>Crowdsourcing experiment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28362</offset><text>In the web experiment, 416, 411, and 405 paid volunteers participated in the tasks of illumination conditions 1, 2, and 3, respectively. We recruited these observers through a Japanese commercial crowdsourcing service. All who participated under illumination condition 3 also participated under illumination conditions 1 and 2. Moreover, all who participated in illumination condition 2 had also participated under illumination condition 1. The experiment was approved by the ethical committees at NTT Communication Science Laboratories.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28900</offset><text>All observers used the web browsers of their own personal computers or tablets to participate in the experiment. We asked them to watch the screen from a distance of about 60 cm. Each object image was shown on the screen at a size of 512 × 512 pixels. We did not strictly control the visual angle of the image participants observed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29234</offset><text>The procedure was similar to that of the laboratory experiment. In each trial, four or three object images that had been chosen depending on the task were presented on the screen, as shown in Figure 3. The measurement was conducted under illumination condition 1 first, followed by one under illumination condition 2 and one under illumination condition 3. The observers were asked to report which of the object images looked odd by clicking one of the images. Each participant made one judgment for each condition. The other steps of the procedure were the same as those in the laboratory experiment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>29837</offset><text>Data analysis</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29851</offset><text>For each oddity task, we computed the proportion that each participant got correct. The chance level of the correct proportion was 0.25 for illumination condition 1 and 0.33 for illumination conditions 2 and 3. We computed the sensitivity d' from each correct proportion by using a numerical simulation to estimate the sensitivity of the oddity task. We used the Palamedes data analysis library for the simulation. To avoid values of infinity, we converted the one probability according to the total trial number in the simulation; that is, we corrected the one value to 1 – (1/2N), where N is the total trial number. For the laboratory experiment, we computed the sensitivity (d′) of each observer and averaged it across observers. For the crowdsourcing experiment, because each observer engaged in each task one time, we computed the proportion correct for each task from all of the observers’ responses and used that value to compute d′.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>30800</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30808</offset><text>In this section, we describe the results of our benchmark data acquisition. First, we evaluate the environment dependency of our experiment—the performance difference between the online and laboratory experiments. Then, we describe the illumination and geometry effect on each task. After discussing each task, we show how intermediate visual features contribute to task performance. In the end, we analyze the individual difference in each task.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>31257</offset><text>Environment dependence</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31280</offset><text>For cross-cultural, cross-species, brain-dysfunction, and developmental studies, stimulus presentation on a monitor cannot always be strictly controlled because of apparatus or ethical limitations. Therefore, a performance validation of each task across different apparatuses is critical to decide which tasks the users of our database should select in their experimental environment. Figure 7a shows the results of the correlation analysis between the laboratory and crowdsourcing experiments. The coefficient of determination (R2) of the linear regression between the sensitivity (d′) in the laboratory experiment and that of the crowdsourcing experiment is 0.79, indicating a high linear correlation. However, the slope of the regression is less than 1. This shows that the sensitivity of the crowdsourcing experiment was worse than that of the laboratory experiment, with many repetitions in general. These findings suggest that the present tasks maintain relative performance across different experimental environments.</text></passage><passage><infon key="file">jovi-22-2-17-f007.jpg</infon><infon key="id">fig7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32309</offset><text>Results of laboratory and crowdsourcing experiments. The sensitivity d′ in each task in the crowdsourcing experiment is plotted as a function of that in the laboratory experiment. (a) Results of all tasks. Each plot indicates a task with an object, an illumination, and a difficulty. The red line indicates the linear regression between the crowdsourcing and laboratory results. The coefficient of determination (R2) of the regression and the equation are shown in the legend. The results show that the tasks were generally robust across experimental conditions. (b) Results of individual tasks. Different panels indicate tasks involving different materials. Each plot in a panel indicates a task with an object, illumination, and difficulty. The red line indicates the linear regression between the laboratory and crowdsourcing results. The coefficient of determination (R2) of the regression and the equation are shown in the legend. The accuracy of task 2 (GD) in the crowdsourcing experiment was generally lower than that in the laboratory experiment. The correlation of task 6 (GP) between the laboratory and crowdsourcing experiments was the worst.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33466</offset><text> Figure 7b shows the results for each task of the laboratory and crowdsourcing experiments in more detail. The coefficients of determination (R2) in tasks 1 to 6 were 0.60, 0.40, 0.86, 0.60, 0.62, and 0.39, respectively. The coefficient of task 6 (GP) was the worst, followed by task 2 (GD). As in the latter section, task 6 (GP) also showed large individual differences; thus, the correlation between the laboratory and crowdsourcing experiments was decreased. The slope of the linear regression on task 2 (GD) was 0.44, and the proportion correct in the crowdsourcing experiment for task 2 was generally lower than that in the laboratory for task 2. In the laboratory experiment, we used a 30-inch liquid-crystal display monitor, and the stimulus size of each image was presented at a size of 8.5° × 8.5°, which we expected to be larger than when participants on the web observed the image on a tablet or PC. Task 2 (GD) is related to the DOI of the specular reflection; thus, the spatial resolution might have affected the accuracy of the observers’ responses, although the relative difficulty for task 2 (GD) even in the crowdsourcing experiment was similar to that in the laboratory experiment. These findings suggest that the absolute accuracy of task 2 (GD) depends largely upon the experimental environment.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>34788</offset><text>Illumination and geometry</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34814</offset><text> Figures 8 to 13 show the performance of each task in the laboratory experiment. Different panels depict results obtained for different objects. Different symbols in each panel depict different illumination conditions. The results of the crowdsourcing experiment are shown in Appendix A. For tasks 1 to 5 (Figures 8 to 12), we parametrically changed the material parameters (e.g., the contrast dimensions for task 1, GC). Results show that the discrimination accuracy increased as the target material parameters deviated from the non-target one. This trend is most readily observed for illumination 1 on each task condition. In contrast, the accuracy did not change much with the material parameters for some conditions. This trend can be observed for illuminations 2 and 3 of task 1 (GC) and objects 4 and 5 of task 2 (GD). For task 6 (GP), the relation of target and non-target stimuli differed from that of the other tasks. In this task, the non-target stimulus was made for each material parameter (i.e., DOI). As shown in Figure 13, this material parameter did not affect the task difficulty.</text></passage><passage><infon key="file">jovi-22-2-17-f008.jpg</infon><infon key="id">fig8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35915</offset><text>Results of task 1 (GC) in the laboratory experiment. Different panels show different objects. Different symbols in each panel depict different illumination conditions. The vertical red line in each panel indicates the parameter of the non-target stimulus. Error bars indicate ±1 SEM across observers.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36217</offset><text>By comprehensively assessing material recognition performance across different stimulus conditions, we found novel properties that have been overlooked in the previous literature, one of which pertains to the geometrical dependence of material recognition. When object images changed in the gloss DOI discrimination task (task 2, GD) (Figure 9), the observers could detect the material difference better for smooth objects (objects 2 and 3) than for rugged objects (objects 4 and 5). In contrast, when the object images changed in the gloss contrast discrimination task (task 1, GC) (Figure 8), little geometrical dependence was found. We also found little geometrical dependence when observers detected highlight-shading consistency (task 6, GP) (Figure 13). Although geometrical dependencies of glossiness perception have been reported before, they were mainly about the effects of shape on apparent gloss characteristics, not on gloss discrimination. Furthermore, our results also show a geometrical dependence of translucency perception (task 3, OT) (Figure 10). Similar to the dependence on the DOI dimension, the sensitivity changed between the smooth objects (objects 2 and 3) and rugged objects (objects 4 and 5), but in the opposite way. Specifically, the translucent difference was more easily detected for the rugged objects than for the smooth objects (Figure 10).</text></passage><passage><infon key="file">jovi-22-2-17-f009.jpg</infon><infon key="id">fig9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37599</offset><text>Results of task 2 (GD) in the laboratory experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f010.jpg</infon><infon key="id">fig10</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37652</offset><text>Results of task 3 (OT) in the laboratory experiment.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37705</offset><text>We also found an illumination dependence in material recognition. We used three illumination conditions, wherein the illumination environments used in a task were identical (illumination 1), similar to each other (illumination 2), or largely different from each other (illumination 3). The results showed that task accuracy decreased as the difference in light probes across the images increased from illumination 1 to illuminations 2 and 3 (Figures 89101112–13). This finding not only confirms the large effect of illumination on gloss perception reported before but also demonstrates similarly strong effects of illumination on other material discrimination tasks (OT, MP, and MG). The observers' data is stored in the repository with the image dataset (Appendix B).</text></passage><passage><infon key="file">jovi-22-2-17-f011.jpg</infon><infon key="id">fig11</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38477</offset><text>Results of task 4 (MP) in the laboratory experiment. Data for one of the observers for object 1 and illumination 2 are missing due to a mistake in the stimulus presentation.</text></passage><passage><infon key="file">jovi-22-2-17-f012.jpg</infon><infon key="id">fig12</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38651</offset><text>Results of task 5 (MG) in the laboratory experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f013.jpg</infon><infon key="id">fig13</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38704</offset><text>Results of task 6 (GP) in the laboratory experiment.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>38757</offset><text>Intermediate visual feature analysis</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38794</offset><text>One may raise the concern that our observers might have made oddity judgments based on differences in low-level superficial image properties such as the mean color of the object. We did not explicitly ask the observers to select one object image in terms of the material appearance. This procedure could lead observers to take a simple strategy unrelated to material judgment. A related question is that, if not such simple properties, are there any intermediate image features in hierarchical visual processing that can explain the observers’ responses? Recent studies have shown that the intermediate processing in the ventral visual stream of humans and monkeys encodes the higher-order image features as computed in texture synthesis algorithms or deep convolutional neural networks. It has been suggested that the processing in the visual ventral stream also mediates material recognition for static objects. We asked how such intermediate features possibly processed in material computation could explain the observers’ responses.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39835</offset><text>More specifically, we analyzed how various image feature differences on each task could explain the observers’ task performance. Each task (i.e., a material dimension with an object under an illumination condition) included a set of material objects with different combinations of poses (illumination condition 1) or illuminations (illumination conditions 2 and 3). These combinations were used as repetition for the behavioral experiment. In the analysis, we chose all combinations for each task and calculated the mean feature distance. We calculated this distance metric using various image features (e.g., pixel statistics, texture statistics) as described below in detail. If the distance metric of each image feature is correlated with human performance, then the feature can be diagnostic for human judgments.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40654</offset><text>We linearly regressed the discrimination sensitivity d' for each task using the distance metric calculated from various image features. Specifically, we used the texture parameters originally proposed in the literature of texture synthesis by. They suggested that natural textures can be synthesized by the probabilistic summary statistics derived from the pixel histogram and the subband distribution, including higher-order statistics such as the correlations across the subband filter outputs. More recently, many studies have shown that the intermediate visual processing in the ventral stream, such as V2 or V4, encodes these texture parameters. Following the previous studies, we reduced the original texture parameters by removing redundant features because a large number of parameters can make the fitting unreliable. Specifically, we conducted the same reduction as, except that (1) we included the mean, SD, and kurtosis of the marginal statistics, as well as the skewness, and (2) we calculated these statistics not only for grayscale images (CIE L* image) but also for color images (CIE a* and CIE b* images). We defined the white XYZ value averaging the diffuse white sphere rendered under each illumination condition and used it to calculate the CIE L*a*b* of each image. We extracted the center 128 × 128 pixels of each image and calculated the texture parameters using the texture synthesis algorithm by with four scales and four orientations. We reduced these original texture parameters of each CIE L*, a*, or b* image to 32 parameters following. More details are described in the supplementary tables S1 and S2 of. In total, we used 96 parameters for the regression analysis.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42351</offset><text>We conducted five regressions with different types of parameters to explore the contribution of different statistics. Specifically, we used (1) pixel color means, (2) pixel color statistics, (3) Portilla–Simoncelli (PS) grayscale texture statistics, (4) PS grayscale and pixel color statistics, and (5) PS color statistics. The pixel color means and the pixel color statistics were the marginal statistics in the PS texture statistics. The pixel color means indicated the averaged pixel values of each L*a*b* channel. The pixel color statistics indicated the mean, SD, skewness, and kurtosis of each color channel. The numbers of these parameters were 3 and 12, respectively. For the two conditions, we used a linear regression without regularization to fit the discrimination sensitivity (blue and red in Figure 14). For the three PS texture statistics conditions (yellow, purple, and green in Figure 14), we used the compressed PS statistics as described above. Because the number of parameters for these conditions was large (32, 48, and 96, respectively), we used L1-penalized linear least-squares regression (i.e., lasso) to avoid overfitting. We controlled the hyperparameters so that the number of independent variables was 18, where the regression of the PS grayscale statistics condition showed the minimum mean-squared error (MSE).</text></passage><passage><infon key="file">jovi-22-2-17-f014.jpg</infon><infon key="id">fig14</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>43699</offset><text>Results of the linear regressions using different parameters. We regressed the human discrimination performance on pixel color means (three parameters, blue), pixel color statistics (12 parameters, red), PS grayscale texture statistics (regularized 18 parameters, yellow), PS grayscale statistics and pixel color statistics (regularized 18 parameters, purple), or PS color statistics (regularized 18 parameters, purple). (a) Results of the MSE for each regression. (b) Results of the MSE for each regression. These results are shown using a violin plot. (c) Results of the MSE for each task. The error bars indicate the bootstrap 95% confidence intervals.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44355</offset><text>We divided all tasks into training and test datasets with a ratio of four to one, respectively, and conducted the above five regressions. The task ratio was kept constant across the training and test datasets. For the training dataset on the lasso regressions, we regressed the discrimination sensitivity using 5-fold-cross validation. Figure 14 shows the MSE and the determinant coefficient for the test datasets. We resampled the training and test datasets 10,000 times and have depicted the distribution using a violin plot. The predictions based on the color mean statistics did not match the observers’ discrimination sensitivity at all (Figs. 14a, 14b). These results suggest that the observers did not simply rely on the mean differences to perform the oddity tasks. The MSE and the determinant coefficient for the marginal statistics condition were more improved when we added the higher-order statistics (marginal statistics condition, PS grayscale statistics condition, and PS color statistics condition). Because the regularization parameter was controlled under the PS color and grayscale statistics conditions, these results cannot be ascribed to the number of independent variables. It is noteworthy that, even when all of the PS color statistics were used, the prediction was not sufficient to explain the observers’ discrimination performance. This finding suggests that human material judgments also rely on higher-order features the PS statistics do not cover. One possible future direction is to use the intermediate activation of the deep neural networks. To support this direction, we include in our database the activation data of VGG-19, a feedforward convolutional neural network, for our image dataset and the analysis about how the dataset is represented in each layer (see Appendix C). In short, our dataset images were clustered in higher layers of the pretrained network according to object differences, and the material differences were represented in each object cluster.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>46365</offset><text>Individual differences</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46388</offset><text>Next, we evaluated the individual differences of each task in the Japanese adult population. Figure 15 shows the histogram of the response accuracy for each observer in the crowdsourcing and laboratory experiments. For the crowdsourcing experiment, the number of observers for illumination conditions 1, 2, and 3 was 416, 411, and 405, respectively. For the laboratory experiment, the number of observers was 20. For each condition, the probability of a correct response was calculated by averaging the responses of each observer across objects and task difficulties. The standard deviations of tasks 1 to 6 under illumination condition 1 were 0.14, 0.11, 0.12, 0.12, 0.12, and 0.23, indicating a particularly large individual difference for task 6 (GP). The standard deviation under illumination conditions 2 and 3 ranged from 0.09 to 0.18. It should also be noted that most of the conditions showed unimodal distributions, whereas task 6 (GP) showed a nearly uniform distribution. This finding suggests that individual differences in the discrimination ability of the spatial consistency of specular highlights are larger than those for other material properties, including glossiness contrast and DOI (GC and GD).</text></passage><passage><infon key="file">jovi-22-2-17-f015.jpg</infon><infon key="id">fig15</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47607</offset><text>Histogram of response accuracy for each observer in the crowdsourcing (blue) and lab (red) experiments. Different panels indicate different material tasks and illumination conditions. For each condition, the probability of a correct response was calculated by averaging the responses of each observer across objects and task difficulties. The histograms of crowdsourcing and lab experiments are overlaid in each panel. The mean and standard deviation of each distribution are shown in each panel.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>48104</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48115</offset><text>The present study aimed to construct a database of material images annotated with the results of human discrimination tasks. We created material images that varied in six different material dimensions on the basis of the previous material-recognition studies. Our dataset includes various objects and illuminations so that users can comprehensively investigate the effects of these physical causes on material recognition. The results of psychophysical experiments showed that most of the task difficulty could be appropriately controlled by manipulating the material parameters. Furthermore, analysis of visual feature showed that the parameters of higher-order color texture statistics (Figure 14, PS color statistics) can partially, but not completely, explain task performance. One crucial point of our dataset is that we used a nonverbal procedure to collect the observers’ data. Because this procedure is widely used in babies, brain-injured participants, and animals, the current behavioral data can be a benchmark for more diverse research fields.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49174</offset><text>Because we comprehensively investigated the material recognition using a structured dataset, our dataset itself revealed novel findings about material recognition. For instance, the present results indicate that the performance of the tasks in the crowdsourcing experiment was strongly correlated with that in the laboratory experiment. This suggests that the dataset has enough tolerance to conduct new experiments involving a variety of observers and experimental conditions. Another is that geometry dependency on material recognition emerges similarly in different material attributes such as gloss DOI or translucency (Figure 10). Specifically, the translucency discrimination sensitivity was high when the object had rugged surfaces (e.g., objects 1, 4, and 5). Some studies have shown that physically prominent features of translucent objects appear around sharp corners on the surface (Fleming &amp; Bülthoff, 2005). One possibility is that the diagnostic features for translucent perception lie in the edge/corner of a translucent object and our rugged objects included much information to judge translucency. More recently, investigated the effect of geometry on translucency perception. In their experiments, they changed the smoothness of the object edges. In agreement with our findings, the edge modulation was critical to the translucency perception. Specifically, the object with the smooth edge was perceived as more translucent than the sharp one.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>50638</offset><text>Another finding is that the ability to discriminate the spatial consistency of specular highlights in glossiness perception has large individual differences, although other glossiness discrimination tasks do not show such large differences. Some studies suggest that image statistics are diagnostic for glossiness perception. However, when specular highlights of an object image are inconsistent in terms of their position and/or orientation with respect to the diffuse shading component, they look more like white blobs produced by surface reflectance changes. This is why the highlight-inconsistency effect is considered to be a counterexample to the image statistics explanation. The large individual differences suggest that the discrimination of the spatial consistency of specular highlights may be mediated by a different, and possibly more complicated, mechanism than that responsible for glossiness contrast/DOI discrimination. In agreement with this notion, showed that highlight inconsistency is discriminated by image gradient features different from those used in the human material computation. This suggests that the glossiness computation is mediated by multiple stages, such that one step is to discriminate different materials on a surface for extracting a region-of-interest (ROI) and another is to compute the degree of glossiness in the ROI as shown in.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>52013</offset><text>One may have a concern that the intermediate objects in tasks 4 and 5 are physically not feasible because they are a mixture of two physically distinct materials. However, our stimuli do not look so unrealistic. The dielectric/metal materials are distinct material categories when considering an object with a uniform single material, but many daily objects surrounding us are a mixture of various materials, and we often see a plastic object coated by a metallic material. We can regard our intermediate materials as an approximation of such coated materials. In addition, continuously connecting distinct categories is common in various research fields, such as speech recognition (e.g.,) or face recognition (e.g.,), especially to elucidate what stimulus image features are involved in the processing. Considering the literature, we think our intermediate approach is reasonable.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>52896</offset><text>Although our database includes diverse material dimensions, they are still not enough to cover the full range of natural materials. One example is cloth. Cloth materials are ubiquitous in everyday environments. A reason we did not include this class of materials is that it has been shown that the cloth perception strongly relies on dynamic information. Because of the limited experimental time, our database currently focuses on static images. This is why other materials related to dynamic information (reviewed by) related to the perception of liquidness, viscosity, and stiffness (Paulun et al., 2017), among others, were not used in the current investigation. In addition, the perception of wetness and the fineness of surface microstructures were not investigated because of the difficulty of continuously controlling physical material parameters by using identical geometries of other tasks. Because we only used five geometries, material perceptions derived from object mechanical properties were also not investigated. A crucial point is that we can share our source code to reproduce images. We hope to remove obstacles to constructing a new dataset and contribute to future work on material recognition. Sharing the datasets with the source code should allow researchers to easily conduct new studies within this literature. For example, we measured the discrimination sensitivities in our experiments from one side of the materials in tasks 3, 4, and 5 (i.e., opaque, gold, and silver). The sensitivities from the other side (i.e., transparent, plastic, and glass) could be slightly different from the current results. Researchers can easily render new images of different material parameters in the same scene condition and conduct a new investigation.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>54664</offset><text>Our datasets also highlight the difficulty of choosing appropriate parameters that cover the full range of the material sensitivity. We chose the stimulus parameters based on the preliminary experiments. We tried to choose the parameters so that we can measure the sensitivity of each task in the full range, from the level of chance to maximum accuracy. However, we found large individual differences in some tasks (e.g., task 6), and they resulted in the partial measurement of the narrow sensitivity range. This unpredictability is one of the difficulties of producing the large size of the dataset. The current findings should contribute to future attempts to create material image datasets.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>55360</offset><text>Our dataset focuses on expanding the previous findings regarding material recognition into more diverse research fields. From the view of a global standard dataset, our dataset has several limitations as described above. However, it did contribute to this expansion purpose. Specifically, several research groups of behavioral science, computer science, and neuroscience have ongoing projects utilizing our dataset, and some findings have already been reported at conferences and journals. used our dataset to explore the role of the monkey inferior temporal cortex on material perception by using electrocorticography recordings. investigated the role of working memory on material processing using our dataset. explored how mid-level features in deep convolutional neural networks can explain human behavioral data.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>56178</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>56189</offset><text>We constructed image and observer database for material recognition experiments. We collected observation data about material discrimination in tasks that had a nonverbal procedure for six material dimensions and several task difficulties. The results of psychophysical experiments in laboratory and crowdsourcing environments showed that the performance of the tasks in the crowdsourcing experiment was strongly correlated with the performance of the tasks in the laboratory experiment. In addition, by using the above comprehensive data, we obtained novel findings on the perception of translucence and glossiness. Not only can the database be used as benchmark data for neuroscience and psychophysics studies on the material recognition capability of healthy adult humans, but it can also be used in cross-cultural, cross-species, brain-dysfunction, and developmental studies of humans and animals.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>57091</offset><text>References</text></passage><passage><infon key="fpage">21866</infon><infon key="pub-id_pmid">26915492</infon><infon key="section_type">REF</infon><infon key="source">Scientific Reports,</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>57102</offset><text>Touch influences perceived gloss</text></passage><passage><infon key="fpage">pp. 1–12</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of SPIE 4299: Human Vision And Electronic Imaging VI</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>57135</offset><text>On seeing stuff: The perception of materials by humans and machines</text></passage><passage><infon key="fpage">1</infon><infon key="issue">11</infon><infon key="lpage">17</infon><infon key="pub-id_doi">10.1167/jov.9.11.10</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2009</infon><offset>57203</offset><text>Image statistics do not explain the perception of gloss and lightness</text></passage><passage><infon key="fpage">65</infon><infon key="lpage">74</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>57273</offset><text>A microfacet-based BRDF generator</text></passage><passage><infon key="fpage">407</infon><infon key="issue">4</infon><infon key="lpage">410</infon><infon key="section_type">REF</infon><infon key="source">Attention, Perception, &amp; Psychophysics,</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">1981</infon><offset>57307</offset><text>Highlights and the perception of glossiness</text></passage><passage><infon key="fpage">19</infon><infon key="lpage">23</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM Symposium on Applied Perception</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>57351</offset><text>Perceptual constancy of mechanical properties of fabrics under variation of external force</text></passage><passage><infon key="fpage">1</infon><infon key="issue">5</infon><infon key="lpage">20</infon><infon key="pub-id_doi">10.1167/18.5.12</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>57442</offset><text>Estimating mechanical properties of cloth from videos using dense motion trajectories: Human psychophysics and machine learning</text></passage><passage><infon key="fpage">1</infon><infon key="issue">5</infon><infon key="lpage">18</infon><infon key="pub-id_doi">10.1167/19.5.18</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2019</infon><offset>57570</offset><text>Manipulating patterns of dynamic deformation elicits the impression of cloth with varying stiffness</text></passage><passage><infon key="fpage">R551</infon><infon key="issue">13</infon><infon key="lpage">R554</infon><infon key="pub-id_pmid">26126278</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2015</infon><offset>57670</offset><text>Colour vision: understanding #TheDress</text></passage><passage><infon key="fpage">1</infon><infon key="issue">11</infon><infon key="lpage">15</infon><infon key="pub-id_doi">10.1167/18.11.18</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>57709</offset><text>Beyond scattering and absorption: perceptual unmixing of translucent liquids</text></passage><passage><infon key="fpage">379</infon><infon key="issue">4</infon><infon key="lpage">385</infon><infon key="pub-id_pmid">1603651</infon><infon key="section_type">REF</infon><infon key="source">Perception &amp; Psychophysics,</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">1992</infon><offset>57786</offset><text>A table of d′ for M-alternative odd-man-out forced-choice procedures</text></passage><passage><infon key="fpage">2010</infon><infon key="issue">23</infon><infon key="lpage">2016</infon><infon key="pub-id_pmid">22119529</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2011</infon><offset>57857</offset><text>Visual motion and the perception of surface material</text></passage><passage><infon key="fpage">365</infon><infon key="lpage">388</infon><infon key="section_type">REF</infon><infon key="source">Annual Review of Vision Science,</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2017</infon><offset>57910</offset><text>Material perception</text></passage><passage><infon key="fpage">346</infon><infon key="lpage">382</infon><infon key="section_type">REF</infon><infon key="source">ACM Transactions on Applied Perception,</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2005</infon><offset>57930</offset><text>Low-level image cues in the perception of translucent materials</text></passage><passage><infon key="fpage">1</infon><infon key="issue">13</infon><infon key="lpage">23</infon><infon key="pub-id_doi">10.1167/jov.21.13.3</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2003</infon><offset>57994</offset><text>Real-world illumination and the perception of surface reflectance properties</text></passage><passage><infon key="fpage">1195</infon><infon key="issue">9</infon><infon key="lpage">1201</infon><infon key="pub-id_pmid">21841776</infon><infon key="section_type">REF</infon><infon key="source">Nature Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2011</infon><offset>58071</offset><text>Metamers of the ventral stream.</text></passage><passage><infon key="fpage">974</infon><infon key="issue">7</infon><infon key="lpage">981</infon><infon key="pub-id_pmid">23685719</infon><infon key="section_type">REF</infon><infon key="source">Nature Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2013</infon><offset>58103</offset><text>A functional and perceptual signature of the second visual area in primates</text></passage><passage><infon key="fpage">R543</infon><infon key="issue">13</infon><infon key="lpage">R544</infon><infon key="pub-id_pmid">25981790</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2015</infon><offset>58179</offset><text>The many colours of ‘the dress.</text></passage><passage><infon key="fpage">1</infon><infon key="issue">8</infon><infon key="lpage">41</infon><infon key="pub-id_doi">10.1167/jov.21.8.4</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2021</infon><offset>58213</offset><text>Translucency perception: A review</text></passage><passage><infon key="fpage">1</infon><infon key="issue">5</infon><infon key="lpage">19</infon><infon key="section_type">REF</infon><infon key="source">ACM Transactions on Graphics,</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2013</infon><offset>58247</offset><text>Understanding the role of phase function in translucent appearance</text></passage><passage><infon key="fpage">928</infon><infon key="issue">7</infon><infon key="lpage">934</infon><infon key="pub-id_pmid">26996504</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2016</infon><offset>58314</offset><text>Crossmodal association of visual and haptic material properties of objects in the monkey ventral visual cortex</text></passage><passage><infon key="fpage">1493</infon><infon key="issue">5</infon><infon key="lpage">1500</infon><infon key="section_type">REF</infon><infon key="source">The Journal of the Acoustical Society of America,</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">1978</infon><offset>58425</offset><text>Perceptual effects of spectral modifications on musical timbres</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Methods of determining gloss</infon><infon key="type">ref</infon><infon key="year">1937</infon><offset>58489</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>58490</offset><text>Mitsuba: Physically Based Renderer</text></passage><passage><infon key="fpage">511</infon><infon key="lpage">518</infon><infon key="section_type">REF</infon><infon key="source">SIGGRAPH ’01: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>58525</offset><text>A practical model for subsurface light transport</text></passage><passage><infon key="fpage">E4620</infon><infon key="issue">33</infon><infon key="lpage">E4627</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences, USA,</infon><infon key="type">ref</infon><infon key="volume">112</infon><infon key="year">2015</infon><offset>58574</offset><text>Perceptual transparency from image deformation</text></passage><passage><infon key="fpage">125</infon><infon key="lpage">138</infon><infon key="pub-id_pmid">25102388</infon><infon key="section_type">REF</infon><infon key="source">Vision Research,</infon><infon key="type">ref</infon><infon key="volume">109</infon><infon key="year">2015</infon><offset>58621</offset><text>Seeing liquids from visual motion</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Neuroscience 2019</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>58655</offset><text>Spatial and time-frequency representations of glossy material properties in the monkey inferior temporal cortex</text></passage><passage><infon key="fpage">1244</infon><infon key="issue">9</infon><infon key="lpage">1246</infon><infon key="pub-id_pmid">22402337</infon><infon key="section_type">REF</infon><infon key="source">Cortex,</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2012</infon><offset>58767</offset><text>Glossiness perception can be mediated independently of cortical processing of colour or texture</text></passage><passage><infon key="fpage">2041669516671566</infon><infon key="issue">5</infon><infon key="pub-id_pmid">27733897</infon><infon key="section_type">REF</infon><infon key="source">i-Perception,</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2016</infon><offset>58863</offset><text>Turning the world upside down to understand perceived transparency</text></passage><passage><infon key="fpage">1</infon><infon key="issue">9</infon><infon key="lpage">19</infon><infon key="pub-id_doi">10.1167/11.9.4</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2011</infon><offset>58930</offset><text>The perception of gloss depends on highlight congruence with surface shading</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Psychophysics: A practical introduction</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>59007</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Psychophysics: A practical introduction</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>59008</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">28th Annual Conference of Japanese Neural Network Society (JNNS2018)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>59009</offset><text>Explaining behavioral data of visual material discrimination with a neural network for natural image recognition</text></passage><passage><infon key="fpage">1</infon><infon key="issue">2</infon><infon key="lpage">23</infon><infon key="pub-id_doi">10.1167/jov.22.2.6</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2022</infon><offset>59122</offset><text>Crystal or jelly? Effect of color on the perception of translucent materials with photographs of real-world objects</text></passage><passage><infon key="fpage">185</infon><infon key="issue">1</infon><infon key="pub-id_pmid">4034817</infon><infon key="section_type">REF</infon><infon key="source">Psychological Bulletin,</infon><infon key="type">ref</infon><infon key="volume">98</infon><infon key="year">1985</infon><offset>59238</offset><text>Detection theory analysis of group data: estimating sensitivity from average hit and false-alarm rates</text></passage><passage><infon key="fpage">1</infon><infon key="issue">9</infon><infon key="lpage">12</infon><infon key="pub-id_doi">10.1167/11.9.16</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2011</infon><offset>59341</offset><text>The role of brightness and orientation congruence in the perception of surface gloss</text></passage><passage><infon key="fpage">1909</infon><infon key="issue">20</infon><infon key="lpage">1913</infon><infon key="pub-id_pmid">22959347</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2012</infon><offset>59426</offset><text>The perception and misperception of specular surface reflectance</text></passage><passage><infon key="fpage">17</infon><infon key="pub-id_pmid">28367117</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in Neural Circuits,</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2017</infon><offset>59491</offset><text>Representation of glossy material surface in ventral superior temporal sulcal area of common marmosets</text></passage><passage><infon key="fpage">1</infon><infon key="issue">9</infon><infon key="lpage">11</infon><infon key="pub-id_doi">10.1167/10.9.6</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2010</infon><offset>59594</offset><text>Highlight-shading relationship as a cue for the perception of translucent and transparent materials</text></passage><passage><infon key="fpage">30</infon><infon key="issue">1</infon><infon key="lpage">39</infon><infon key="pub-id_pmid">22138530</infon><infon key="section_type">REF</infon><infon key="source">Vision Research,</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2012</infon><offset>59694</offset><text>Variability in constancy of the perceived surface reflectance across different illumination statistics</text></passage><passage><infon key="fpage">206</infon><infon key="issue">7141</infon><infon key="lpage">209</infon><infon key="pub-id_pmid">17443193</infon><infon key="section_type">REF</infon><infon key="source">Nature,</infon><infon key="type">ref</infon><infon key="volume">447</infon><infon key="year">2007</infon><offset>59797</offset><text>Image statistics and the perception of surface qualities</text></passage><passage><infon key="fpage">407</infon><infon key="issue">6</infon><infon key="lpage">428</infon><infon key="pub-id_pmid">24349699</infon><infon key="section_type">REF</infon><infon key="source">i-Perception,</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2013</infon><offset>59854</offset><text>Image regions contributing to perceptual translucency: A psychophysical reverse-correlation study</text></passage><passage><infon key="fpage">94</infon><infon key="lpage">99</infon><infon key="section_type">REF</infon><infon key="source">Current Opinion in Behavioral Sciences,</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2019</infon><offset>59952</offset><text>Image statistics for material perception</text></passage><passage><infon key="fpage">501</infon><infon key="lpage">523</infon><infon key="section_type">REF</infon><infon key="source">Annual Review of Vision Science,</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2018</infon><offset>59993</offset><text>Motion perception: From detection to interpretation</text></passage><passage><infon key="fpage">2951</infon><infon key="issue">12</infon><infon key="lpage">2965</infon><infon key="section_type">REF</infon><infon key="source">Journal of the Optical Society of America A: Optics and Image Science, and Vision,</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">1998</infon><offset>60045</offset><text>Use of image-based information in judgments of surface-reflectance properties</text></passage><passage><infon key="fpage">10780</infon><infon key="issue">31</infon><infon key="lpage">10793</infon><infon key="pub-id_pmid">22855825</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2012</infon><offset>60123</offset><text>Neural selectivity and representation of gloss in the monkey inferior temporal cortex</text></passage><passage><infon key="fpage">11143</infon><infon key="issue">33</infon><infon key="lpage">11151</infon><infon key="pub-id_pmid">25122910</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2014</infon><offset>60209</offset><text>Perceptual gloss parameters are encoded by population responses in the monkey inferior temporal cortex</text></passage><passage><infon key="fpage">277</infon><infon key="lpage">290</infon><infon key="pub-id_pmid">29673784</infon><infon key="section_type">REF</infon><infon key="source">Cortex,</infon><infon key="type">ref</infon><infon key="volume">103</infon><infon key="year">2018</infon><offset>60312</offset><text>Visual texture agnosia in dementia with Lewy bodies and Alzheimer's disease</text></passage><passage><infon key="fpage">1</infon><infon key="issue">8</infon><infon key="lpage">19</infon><infon key="pub-id_doi">10.1167/11.8.4</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2011</infon><offset>60388</offset><text>Categorical properties of the color term “GOLD</text></passage><passage><infon key="fpage">4867</infon><infon key="issue">10</infon><infon key="lpage">4880</infon><infon key="pub-id_pmid">27655929</infon><infon key="section_type">REF</infon><infon key="source">Cerebral Cortex,</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2017</infon><offset>60437</offset><text>Gradual development of visual texture-selective properties between macaque areas V2 and V4</text></passage><passage><infon key="fpage">E351</infon><infon key="issue">4</infon><infon key="lpage">E360</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences,</infon><infon key="type">ref</infon><infon key="volume">112</infon><infon key="year">2015</infon><offset>60528</offset><text>Image statistics underlying natural texture selectivity of neurons in macaque V4</text></passage><passage><infon key="fpage">1</infon><infon key="issue">9</infon><infon key="lpage">19</infon><infon key="pub-id_doi">10.1167/10.9.5</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2010</infon><offset>60609</offset><text>Perceived glossiness and lightness under real-world illumination</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">22</infon><infon key="pub-id_doi">10.1167/17.1.20</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>60674</offset><text>Shape, motion, and optical cues to stiffness of elastic objects</text></passage><passage><infon key="fpage">55</infon><infon key="lpage">64</infon><infon key="section_type">REF</infon><infon key="source">SIGGRAPH ’00: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>60738</offset><text>Toward a psychophysically-based light reflection model for image synthesis</text></passage><passage><infon key="fpage">49</infon><infon key="issue">1</infon><infon key="lpage">70</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision,</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2000</infon><offset>60813</offset><text>A parametric texture model based on joint statistics of complex wavelet coefficients</text></passage><passage><infon key="fpage">1250</infon><infon key="pub-id_pmid">30083122</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in Psychology,</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2018</infon><offset>60898</offset><text>Applying the model-comparison approach to test specific research hypotheses in psychophysical research using the Palamedes Toolbox</text></passage><passage><infon key="fpage">211</infon><infon key="issue">3</infon><infon key="lpage">252</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision,</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">2015</infon><offset>61029</offset><text>Imagenet large scale visual recognition challenge</text></passage><passage><infon key="fpage">229</infon><infon key="issue">10</infon><infon key="pub-id_doi">10.1167/18.10.229</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>61079</offset><text>ShapeToolbox: Creating 3D models for vision research</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>61132</offset><text>ShapeToolbox</text></passage><passage><infon key="fpage">1</infon><infon key="issue">5</infon><infon key="lpage">24</infon><infon key="pub-id_doi">10.1167/17.5.7</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017a</infon><offset>61145</offset><text>Visual wetness perception based on image color statistics</text></passage><passage><infon key="fpage">e1006061</infon><infon key="issue">4</infon><infon key="pub-id_pmid">29702644</infon><infon key="section_type">REF</infon><infon key="source">PLoS Computational Biology,</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2018</infon><offset>61203</offset><text>Material and shape perception based on two types of intensity gradient information</text></passage><passage><infon key="section_type">REF</infon><infon key="source">International Conference on Learning Representations (ICLR)</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>61286</offset><text>Very deep convolutional networks for large-scale image recognition</text></passage><passage><infon key="fpage">1</infon><infon key="issue">4</infon><infon key="lpage">18</infon><infon key="pub-id_doi">10.1167/17.4.8</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017b</infon><offset>61353</offset><text>Human perception of subresolution fineness of dense textures based on image intensity statistics</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">32</infon><infon key="pub-id_doi">10.1167/18.1.14</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>61450</offset><text>Shatter and splatter: The contribution of mechanical and optical properties to the perception of soft and hard breaking materials</text></passage><passage><infon key="fpage">2019</infon><infon key="lpage">12</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>61580</offset><text>Material category of visual objects computed from specular image structure</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="lpage">17</infon><infon key="pub-id_doi">10.1167/17.3.18</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>61655</offset><text>Inferring the stiffness of unfamiliar objects from optical, shape, and motion cues</text></passage><passage><infon key="fpage">1402</infon><infon key="issue">10</infon><infon key="lpage">1417</infon><infon key="section_type">REF</infon><infon key="source">Nature Human Behaviour,</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2021</infon><offset>61738</offset><text>Unsupervised learning predicts human perception and misperception of gloss</text></passage><passage><infon key="fpage">149</infon><infon key="lpage">157</infon><infon key="pub-id_pmid">25490434</infon><infon key="section_type">REF</infon><infon key="source">Vision Research,</infon><infon key="type">ref</infon><infon key="volume">109</infon><infon key="year">2015</infon><offset>61813</offset><text>fMRI evidence for areas that process surface gloss in the human visual cortex</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>61891</offset><text>Distinguishing mirror from glass: A ‘big data’ approach to material perception</text></passage><passage><infon key="fpage">1</infon><infon key="issue">10</infon><infon key="lpage">16</infon><infon key="pub-id_doi">10.1167/jov.20.10.10</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>61974</offset><text>Material constancy in perception and working memory</text></passage><passage><infon key="fpage">841</infon><infon key="issue">9</infon><infon key="lpage">842</infon><infon key="pub-id_pmid">12195428</infon><infon key="section_type">REF</infon><infon key="source">Nature Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2002</infon><offset>62026</offset><text>Mike or me? Self-recognition in a split-brain patient</text></passage><passage><infon key="fpage">452</infon><infon key="issue">3</infon><infon key="lpage">458</infon><infon key="pub-id_pmid">29395924</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2018</infon><offset>62080</offset><text>Visual features in the perception of liquids</text></passage><passage><infon key="fpage">1</infon><infon key="issue">15</infon><infon key="lpage">20</infon><infon key="pub-id_doi">10.1167/16.15.12</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>62125</offset><text>Influence of optical material properties on the perception of liquids</text></passage><passage><infon key="fpage">e1008018</infon><infon key="issue">8</infon><infon key="pub-id_pmid">32813688</infon><infon key="section_type">REF</infon><infon key="source">PLoS Computational Biology,</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2020</infon><offset>62195</offset><text>Visual perception of liquids: Insights from deep neural networks</text></passage><passage><infon key="issue">11</infon><infon key="section_type">REF</infon><infon key="source">Journal of Machine Learning Research,</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2008</infon><offset>62260</offset><text>Visualizing data using t-SNE</text></passage><passage><infon key="fpage">267</infon><infon key="issue">3</infon><infon key="lpage">276</infon><infon key="section_type">REF</infon><infon key="source">ACM Transactions on Graphics,</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2007</infon><offset>62289</offset><text>The influence of shape on the perception of material reflectance</text></passage><passage><infon key="fpage">195</infon><infon key="lpage">206</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th Eurographics Conference on Rendering Techniques</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>62354</offset><text>Microfacet models for refraction through rough surfaces</text></passage><passage><infon key="fpage">265</infon><infon key="lpage">272</infon><infon key="section_type">REF</infon><infon key="source">ACM SIGGRAPH Computer Graphics,</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">1992</infon><offset>62410</offset><text>Measuring and modeling anisotropic reflection</text></passage><passage><infon key="fpage">1–22,</infon><infon key="issue">3</infon><infon key="pub-id_doi">10.1167/14.10.1316</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2014</infon><offset>62456</offset><text>Looking against the light: how perception of translucency depends on lighting direction</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3</infon><infon key="lpage">15</infon><infon key="pub-id_doi">10.1167/16.3.34</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>62544</offset><text>Can you see what you feel? Color and folding properties affect visual–tactile material discrimination of fabrics</text></passage><passage><infon key="fpage">1</infon><infon key="issue">7</infon><infon key="lpage">17</infon><infon key="pub-id_doi">10.1167/jov.20.7.10</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>62659</offset><text>Effect of geometric sharpness on translucent material perception</text></passage><passage><infon key="fpage">3209</infon><infon key="issue">24</infon><infon key="lpage">3212</infon><infon key="pub-id_pmid">26671667</infon><infon key="section_type">REF</infon><infon key="source">Current Biology,</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2015</infon><offset>62724</offset><text>Pre-constancy vision in infants</text></passage><passage><infon key="fpage">356</infon><infon key="issue">3</infon><infon key="lpage">365</infon><infon key="pub-id_pmid">26906502</infon><infon key="section_type">REF</infon><infon key="source">Nature Neuroscience,</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2016</infon><offset>62756</offset><text>Using goal-driven deep learning models to understand sensory cortex</text></passage><passage><infon key="fpage">1</infon><infon key="issue">4</infon><infon key="lpage">22</infon><infon key="pub-id_doi">10.1167/19.4.11</infon><infon key="section_type">REF</infon><infon key="source">Journal of Vision,</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2019</infon><offset>62824</offset><text>A systematic approach to testing and predicting light-material interactions</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">title</infon><offset>62900</offset><text>Appendix A. Crowdsourcing experiment</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>62937</offset><text>The results of the crowdsourcing experiment are shown in Figures A1 to A6. The same experiments were also conducted in the laboratory environment, and their results are shown in Figures 8 to 13 in the text.</text></passage><passage><infon key="file">jovi-22-2-17-f016.jpg</infon><infon key="id">figA1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63150</offset><text>Results of task 1 (GC) in the crowdsourcing experiment. Different panels show different objects. Different symbols in each panel depict different illumination conditions. The vertical red line in each panel indicates the parameter of the non-target stimulus. Error bars indicate the 95% bootstrap confidence intervals.</text></passage><passage><infon key="file">jovi-22-2-17-f017.jpg</infon><infon key="id">figA2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63469</offset><text>Results of task 2 (GD) in the crowdsourcing experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f018.jpg</infon><infon key="id">figA3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63525</offset><text>Results of task 3 (OT) in the crowdsourcing experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f019.jpg</infon><infon key="id">figA4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63581</offset><text>Results of task 4 (MP) in the crowdsourcing experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f020.jpg</infon><infon key="id">figA5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63637</offset><text>Results of task 5 (MG) in the crowdsourcing experiment.</text></passage><passage><infon key="file">jovi-22-2-17-f021.jpg</infon><infon key="id">figA6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>63693</offset><text>Results of task 6 (GP) in the crowdsourcing experiment.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">title</infon><offset>63749</offset><text>Appendix B. Data records</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>63774</offset><text>The database is available at https://github.com/mswym/material_dataset. Figure B1 shows the data structure. The standard data are divided into three folders according to the illumination conditions. Each illumination condition folder contains folders of the material tasks (tasks 1 to 6). Each material task folder includes experimental task folders. Each experimental task folder corresponds to one task in the behavioral experiments. The name of each folder indicates the illumination condition, object, material task, and task level. For example, the name “Il1_obj1_Task1_06_12” indicates illumination condition 1 (Il1), object 1 (obj1), task 1 (Task1), contrast of 0.06 for the non-target stimulus, and contrast of 0.12 for the comparison stimulus.</text></passage><passage><infon key="file">jovi-22-2-17-f022.jpg</infon><infon key="id">figB1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>64533</offset><text>Data structure in the database. Solid rectangles indicate a folder; dashed ones indicate a file.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>64630</offset><text>Each task folder contains the two folders named “1” and “0”. The images in folder “0” indicate the non-target stimuli, and the images in folder “1” are the target stimuli. Under illumination condition 1, three images are randomly selected from folder “0”, and one correct image is selected from folder “1.” Five images with different poses are stored in each “1” or “0” folder for illumination condition 1, whereas three images with different illuminations are stored for illumination conditions 2 and 3. The images in the database are in PNG format and have a size of 512 × 512 pixels. In addition, standard observer data are placed on the top layer in the database in a CSV file. The file contains observer data including the probability of the correct response and the sensitivity d′ for each task in the crowdsourcing and laboratory experiments.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">title</infon><offset>65515</offset><text>Appendix C. Convolutional neural networks</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>65557</offset><text>We analyzed how our datasets are represented in convolutional neural networks (CNNs). We extracted the visual features from each intermediate layer of a CNN. We used VGGNet16, pretrained for the object recognition task using ImageNet 2012, and computed the activation of 30 convolution layers and three fully connected layers of the model. To reduce the number of dimensions, we spatially averaged the activation of each channel. Thus, we obtained the multidimensional activation vector for each layer with the dimension number of the channels.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>66102</offset><text> Figures C1 to C4 show the t-SNE embedding of each layer. Figure C1 shows the results of the first convolution layer (conv 1_1), the last convolution layer (conv 5_3), and the third fully connected layer. Each plot indicates each material image. Different panels in each column indicate different labelings based on task, object, and illumination, as shown in the legends. Figure C2 shows the embeddings of all of the layers, which are colored by different tasks. Figures C3 and C4 show the same embeddings as Figure C2, except colored according to different objects and illuminations, respectively.</text></passage><passage><infon key="file">jovi-22-2-17-f023.jpg</infon><infon key="id">figC1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>66712</offset><text>Embedding spaces of intermediate features of a deep neural network trained for object recognition. The top, center, and bottom rows show the same embedding spaces with different color symbols as indicated in the legend. The left, middle, and right columns are the results of the first convolution layer (conv1_1), the final convolution layer (conv5_3), and the third fully connected layer (fc 3), respectively.</text></passage><passage><infon key="file">jovi-22-2-17-f024.jpg</infon><infon key="id">figC2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>67123</offset><text>Embedding spaces of intermediate features of a deep neural network trained for object recognition. Results of all of the 16 layers are shown with coloring for different tasks.</text></passage><passage><infon key="file">jovi-22-2-17-f025.jpg</infon><infon key="id">figC3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>67299</offset><text>Embedding spaces of intermediate features of a deep neural network trained for object recognition. Results of all of the 16 layers are shown with coloring for different objects.</text></passage><passage><infon key="file">jovi-22-2-17-f026.jpg</infon><infon key="id">figC4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>67477</offset><text>Embedding spaces of intermediate features of a deep neural network trained for object recognition. Results of all of the 16 layers are shown with coloring for different objects.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>67655</offset><text>The embedding of the first convolution layer (conv 1_1) showed the clusters according to task differences, especially the MG, MP, and OT clusters. In contrast, this embedding did not show any object-based clusters. Earlier layers are generally sensitive to lower image features. Different tasks have different colors in our datasets, except that tasks GC, GD, and GP share similar green colors. In addition, some clusters of illumination condition 3 emerged in the first layer embedding. The pixel color distribution of illumination condition 3 is also largely different from the others. These results suggest that the first layer codes such lower image features.</text></passage><passage><infon key="section_type">APPENDIX</infon><infon key="type">paragraph</infon><offset>68319</offset><text>The embeddings of the last convolution layer and the third fully connected layer show the clusters according to object differences. Different tasks and illuminations are separately distributed within each object cluster. Although the embedding is clustered according to object differences, it does not show the separation between objects 2 and 3. This finding is consistent with human discrimination performance. The results of behavioral experiments indicate that the task accuracies of objects 2 and 3 were similar to each other and different from other object conditions, especially for tasks GD and OT.</text></passage></document></collection>
