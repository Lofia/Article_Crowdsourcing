<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201215</date><key>pmc.key</key><document><id>7367527</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.2196/15547</infon><infon key="article-id_pmc">7367527</infon><infon key="article-id_pmid">32602842</infon><infon key="article-id_publisher-id">v22i6e15547</infon><infon key="elocation-id">e15547</infon><infon key="issue">6</infon><infon key="kwd">mHealth crowdsensing tinnitus machine learning mobile operating system differences ecological momentary assessment mobile phone</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.</infon><infon key="name_0">surname:Eysenbach;given-names:Gunther</infon><infon key="name_1">surname:González;given-names:Alejandro</infon><infon key="name_10">surname:Breitmayer;given-names:Marius</infon><infon key="name_11">surname:Probst;given-names:Thomas</infon><infon key="name_2">surname:Messner;given-names:Eva-Maria</infon><infon key="name_3">surname:Zolnoori;given-names:Maryam</infon><infon key="name_4">surname:Pryss;given-names:Rüdiger</infon><infon key="name_5">surname:Schlee;given-names:Winfried</infon><infon key="name_6">surname:Hoppenstedt;given-names:Burkhard</infon><infon key="name_7">surname:Reichert;given-names:Manfred</infon><infon key="name_8">surname:Spiliopoulou;given-names:Myra</infon><infon key="name_9">surname:Langguth;given-names:Berthold</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">22</infon><infon key="year">2020</infon><offset>0</offset><text>Applying Machine Learning to Daily-Life Data From the TrackYourTinnitus Mobile Health Crowdsensing Platform to Predict the Mobile Operating System Used With High Accuracy: Longitudinal Observational Study</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>205</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>216</offset><text>Tinnitus is often described as the phantom perception of a sound and is experienced by 5.1% to 42.7% of the population worldwide, at least once during their lifetime. The symptoms often reduce the patient’s quality of life. The TrackYourTinnitus (TYT) mobile health (mHealth) crowdsensing platform was developed for two operating systems (OS)—Android and iOS—to help patients demystify the daily moment-to-moment variations of their tinnitus symptoms. In all platforms developed for more than one OS, it is important to investigate whether the crowdsensed data predicts the OS that was used in order to understand the degree to which the OS is a confounder that is necessary to consider.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>910</offset><text>Objective</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>920</offset><text>In this study, we explored whether the mobile OS—Android and iOS—used during user assessments can be predicted by the dynamic daily-life TYT data.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1071</offset><text>Methods</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1079</offset><text>TYT mainly applies the paradigms ecological momentary assessment (EMA) and mobile crowdsensing to collect dynamic EMA (EMA-D) daily-life data. The dynamic daily-life TYT data that were analyzed included eight questions as part of the EMA-D questionnaire. In this study, 518 TYT users were analyzed, who each completed at least 11 EMA-D questionnaires. Out of these, 221 were iOS users and 297 were Android users. The iOS users completed, in total, 14,708 EMA-D questionnaires; the number of EMA-D questionnaires completed by the Android users was randomly reduced to the same number to properly address the research question of the study. Machine learning methods—a feedforward neural network, a decision tree, a random forest classifier, and a support vector machine—were applied to address the research question.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1898</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1906</offset><text>Machine learning was able to predict the mobile OS used with an accuracy up to 78.94% based on the provided EMA-D questionnaires on the assessment level. In this context, the daily measurements regarding how users concentrate on the actual activity were particularly suitable for the prediction of the mobile OS used.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>2224</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>2236</offset><text>In the work at hand, two particular aspects have been revealed. First, machine learning can contribute to EMA-D data in the medical context. Second, based on the EMA-D data of TYT, we found that the accuracy in predicting the mobile OS used has several implications. Particularly, in clinical studies using mobile devices, the OS should be assessed as a covariate, as it might be a confounder.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2630</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>2643</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2654</offset><text>Mobile health (mHealth) uses smart mobile devices to address various questions in the context of neuroscience, psychology, and medicine. New paradigms, such as ecological momentary assessment (EMA), mobile crowdsourcing, and mobile crowdsensing, as well as mHealth apps, in general, have enabled data collection procedures that surpass many existing methods in gathering valuable medical data by several orders of magnitude. Among others, by using smart mobile devices, data can be gathered in everyday life, on a cost-effective basis, and by adding contextual information sources, such as Twitter or Facebook. As many medical phenomena pose daily variations, mHealth technology is predestined to be utilized in this context. Along these trends, many insights have been presented by researchers that show that smart mobile devices can help to establish new data sources in many scenarios.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3543</offset><text>In these data collection scenarios, which are built on the usage of mobile devices and their sensors, one dimension has been less considered so far. It refers to the question of whether the operating system (OS) of the mobile technology being used (eg, iOS or Android) constitutes a valuable information source or confounder for medical data analyses. Or, as another example, is it possible to derive insights if a patient changes the OS during a study when using mHealth apps? As Android and iOS dominate the mobile OS market —with a market share of 99.32% in May 2020 (72.52% Android and 26.80% iOS)—any insights gained based on differences from users regarding these OS types could provide a representative picture for the OS market. Following this, data that were gathered with the TrackYourTinnitus (TYT) mHealth crowdsensing platform for tinnitus patients over 5 years of age are analyzed in this paper. TYT is an mHealth crowdsensing platform that offers iOS and Android apps that can empower patients to learn more about their tinnitus symptoms over time. Tinnitus is the phantom perception of a sound and it is experienced by 5.1% to 42.7% of the population worldwide at least once during their lifetime. The symptoms often reduce the patient’s quality of life. As tinnitus constitutes a chronic condition for which currently no cure or general treatment exists, patients suffering from it crave for new treatment procedures or at least new medical insights. With the idea of EMA, also known as ambulatory assessment or experience sampling, and mobile crowdsensing techniques in mind, TYT was developed by an interdisciplinary team of medical experts, psychologists, and computer scientists.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5250</offset><text>The development of TYT was motivated by the clinical experience that among many tinnitus patients, tinnitus loudness and tinnitus annoyance vary over time and that patients’ experiences differ in the pattern of these fluctuations. Therefore, the variations are considered to provide new valuable insights in the pathophysiological mechanisms of this chronic condition. To learn more about these fluctuations, TYT applies EMA and mobile crowdsensing to capture them. In EMA, the variable in question (eg, a symptom) is assessed repeatedly in daily life. In mobile crowdsensing, only mobile devices are used for the data collection procedure, while their sensors are used to capture, for example, the GPS position or the external sound level. In contrast, in mobile crowdsourcing, tasks are proposed by a crowdsourcer to a group of individuals, who voluntarily undertake tasks. The undertaking of the task always entails mutual benefit. The user will receive the satisfaction of a given type of need, while the crowdsourcer will obtain and utilize to their advantage what the user has brought to the venture. In contrast to mobile crowdsourcing, mobile crowdsensing relies solely on mobile technology and integrates sensors to collect data. Two recent works that discuss mobile crowdsensing in the context of health care can be found in Kraft et al and Pryss. In TYT, the users fill in a registration questionnaire (ie, static data) and can provide repeated assessments in daily life (ie, dynamic data) afterward.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6764</offset><text>Objectives</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6775</offset><text>Compared to the existing studies on TYT, this work investigates repeatedly provided EMA datasets from TYT users (ie, dynamic data) and their relation to the mobile OS used. While this study analyzes this dynamic data, a previous study focused on differences between Android and iOS users in the static data given at registration. Contrary to the Android versus iOS comparison of the SmokeFree28 (SF28) smoking cessation app, in our study we found no differences in gender, but we did find differences in age for TYT users. However, in Pryss et al, we found differences that might be of interest for medical purposes. More specifically, we revealed that Android users reported a significantly longer tinnitus duration than did iOS users, cross-sectionally. Future longitudinal research is necessary to address the question of whether users with longer tinnitus duration prefer Android to iOS or whether users of Android tend to develop longer tinnitus durations than iOS users. In another recent work, we investigated differences in Android and iOS users of the TrackYourHearing (TYH) mHealth crowdsensing platform. This platform aims to measure fluctuations in hearing of users with hearing loss. In the TYH study, we found no differences in gender or age, but significant differences were revealed in three questions of the dynamic data that were repeatedly provided. This shows that the dynamic data in combination with the OS are worth being investigated more deeply.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8246</offset><text>As another current trend, the application of machine learning techniques in different fields is promising. In the medical field, there is a remarkable discrepancy between huge expectations in the potential of machine learning on one side and the current application of this technique on the other. Importantly, there is an increasing consensus about its potential in the context of mobile technology. However, the application of machine learning to a large group of users of an mHealth crowdsensing platform that gathers EMA datasets is still rare. As we already found relevant differences between Android and iOS pertaining to the TYT users’ static characteristics at registration, this work investigates the following research question: Is it possible to predict the mobile OS used based on dynamic TYT data with high accuracy using machine learning methods? More specifically, is it possible to predict the mobile OS used based on the repeatedly given daily data provided by the TYT users with high accuracy using machine learning methods? To the best of our knowledge, thus far, no other work has considered this research question in the given context.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>9405</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9413</offset><text>Overview</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9422</offset><text>TYT was developed to track the individual tinnitus perception of users in their daily lives. In this context, the procedure shown in Figure 1 is applied to all TYT users. In general, TYT pursues three major goals.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9636</offset><text>First, dynamic EMA (EMA-D) data shall be collected during the continuous mobile crowdsensing procedure (see Figure 1, box #4). Importantly, a crowdsensing user shall not foresee the times he or she is asked to provide the data (see Figure 1, box #3). This is ensured by asking the crowdsensing users for data in various daily-life situations by the use of smartphone notifications. When a user clicks on such a notification, the tinnitus-tracking questionnaire is presented to a user, consisting of eight EMA-D questions. Table 1 lists the eight questions of the EMA-D questionnaire.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10220</offset><text>Note that the questionnaire appears visually on both mobile OS types in the same way. For more information on the questionnaire shown in Table 1 and how it appears on the mobile devices, see Pryss et al.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10424</offset><text>Second, the collected data shall enable innovative data analyses, such as juxtaposing the prospectively assessed EMA-D and retrospectively assessed static EMA (EMA-S) at registration (see Figure 1, box #2). Third, gathered data shall be used to provide feedback to the mobile crowdsensing users.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10720</offset><text>When initially designing the user procedure of TYT, we had not yet considered comparing users based on the mobile OS they used. The initial intention to collect information about the mobile OS used (see Figure 1, box #1) when filling out a questionnaire had been to quickly identify technical issues that could emerge with the large variety of mobile OS versions and mobile devices used. However, it turned out that the information can be also used for innovative analyses. For interested readers, more technical information of the platform can be found in Pryss et al.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11290</offset><text>A further note is provided to distinguish between static and dynamic data in the procedure shown in Figure 1. Usually, existing works distinguish between questionnaire, sensor, and behavioral data when utilizing mHealth crowdsensing approaches. However, our distinction between static trait (ie, EMA-S) or dynamic state EMA data (ie, EMA-D) is done less frequently by other works. This is remarkable, as the distinction between trait (ie, static) and state (ie, dynamic) variables is fundamental in clinical and psychological research. As an example, trait data are expected to have a closer association with genetic information as compared to state data, which depend more strongly on environmental factors.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11999</offset><text>The experimental protocols were approved by the Ethics Committee of the University Clinic of Regensburg, Germany. All methods were carried out in accordance with the relevant guidelines and regulations. The users of the app were informed that their gathered data will be used for scientific analyses; informed consent was given.</text></passage><passage><infon key="file">jmir_v22i6e15547_fig1.jpg</infon><infon key="id">figure1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12328</offset><text>TrackYourTinnitus (TYT) mobile crowdsensing collection procedure. EMA: ecological momentary assessment.</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>12432</offset><text>Questions from the dynamic ecological momentary assessment (EMA-D) questionnaire.</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;80&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;720&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;200&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Question (Q)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Answer type&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Did you perceive the tinnitus right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes or no&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;How loud is the tinnitus right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Slider&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;How stressful is the tinnitus right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Slider&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;How is your mood right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Manikins&lt;sup&gt;b&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;How is your arousal right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Manikins&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Do you feel stressed right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Slider&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q7&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;How much did you concentrate on the things you are doing right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Slider&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Do you feel irritable right now?&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes or no&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>12514</offset><text>Number	Question (Q)	Answer type	 	Q1	Did you perceive the tinnitus right now?	Yes or no	 	Q2	How loud is the tinnitus right now?	Slidera	 	Q3	How stressful is the tinnitus right now?	Slider	 	Q4	How is your mood right now?	Manikinsb	 	Q5	How is your arousal right now?	Manikins	 	Q6	Do you feel stressed right now?	Slider	 	Q7	How much did you concentrate on the things you are doing right now?	Slider	 	Q8	Do you feel irritable right now?	Yes or no	 	</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>12967</offset><text>aEach slider has a different range; the slider for Q2, for example, ranges from not audible to maximal loudness.</text></passage><passage><infon key="file">table1.xml</infon><infon key="id">table1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>13080</offset><text>bWe made use of the Self-Assessment Manikin (SAM) scales, which are a pictorial rating system to obtain self-assessments of experienced emotions on the dimensions affective valence, dominance, and arousal.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13286</offset><text>Data Source</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13298</offset><text>The TYT platform includes a website, uses a relational database, and includes an iOS and Android app. The latter are implemented as native apps. Users can register with the platform by using the website or the mobile apps. After that, three registration questionnaires must be completed—EMA-S questionnaires, which can be filled out using the website or the mobile apps—before users can provide the EMA-D data repeatedly in daily life—this is denoted as the EMA-D questionnaire, which can only be filled out using the mobile apps. After completing the registration questionnaires, users must decide whether they want to use the default notification schema for the EMA-D questionnaire. The default setting means users would receive random notifications up to eight times per day. This setting can be changed by a user in many ways. The user can reduce the notification number to a minimum value of three or a maximum value of 12 notifications per day. In addition, a user can select specific days of the week when no notifications shall appear. Finally, a user can switch to the fixed notification mode, in which he or she specifies exact notification points. Note that in this analysis, it is not distinguished which mode has been selected by a user. Finally, if the user clicks on a notification, the EMA-D questionnaire appears. A detailed description can be found in Pryss et al. Finally, note that users can fill out the EMA-D questionnaire in a user-initiated manner as well (ie, without getting a notification to fill out a questionnaire).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14850</offset><text>Another feature is offered to the TYT users. They can obtain their results of all answered EMA-D questionnaires through the apps or the website. For this purpose, two options are provided: first, they can visualize the results via the website or the mobile apps; or second, they can download a CSV (comma-separated values) file, only via the website, for further personal evaluations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15235</offset><text>Participants</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15248</offset><text>The analysis was conducted in March 2020. At this time, the TYT platform had 4835 registered users. From them, 2584 users completed the EMA-D questionnaire at least once and, in total, 75,278 EMA-D questionnaires were available. To get an impression of how TYT is used worldwide, the country distribution was determined; it shows the number of completed EMA-D questionnaires (ie, all eight items filled in) from 2065 users from the 12 countries with the most completed EMA-D questionnaires out of the 2584 users who completed the questionnaires. This resulted in 67,789 EMA-D questionnaires from 2065 users. The worldwide distribution is shown in Table 2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15904</offset><text>The OS distribution of the 2584 users who completed the EMA-D questionnaire at least once is as follows: 40.02% (1034/2584) of the data were provided by iOS users, while 59.98% (1550/2584) were provided by Android users. The OS distribution of all completed EMA-D questionnaires in TYT is as follows: 32.00% (24,089/75,278) of the data were provided by iOS users, while 68.00% (51,189/75,278) were provided by Android users.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16329</offset><text>The data preparation steps for the machine learning analysis, including use of a scikit-learn function to compare the same number of EMA-D questionnaires from Android and iOS users, can be seen in Figure 2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16536</offset><text>For the final study sample of 297 Android users and 221 iOS users, Table 3 shows statistical comparisons between the Android and iOS users in terms of gender, age, and numbers of completed EMA-D questionnaires (chi-square test and t tests for independent samples, two-sided). Age was set to missing if users provided invalid entries.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16870</offset><text>Finally, Figure 3 shows the histogram for the number of completed EMA-D questionnaires for the 518 investigated TYT users (see Figure 2).</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>17008</offset><text>Country distribution of TrackYourTinnitus (TYT) users (n=2065) in ascending order.</text></passage><passage><infon key="file">table2.xml</infon><infon key="id">table2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;120&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;280&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;600&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Country&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Completed dynamic ecological momentary assessment (EMA-D) questionnaires (n=67,789), n (%)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Australia&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;535 (0.79)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Belgium&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;819 (1.21)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Italy&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1026 (1.51)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Russia&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1076 (1.59)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Austria&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1110 (1.64)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Norway&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1159 (1.71)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Canada&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2113 (3.12)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Great Britain&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3202 (4.72)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Switzerland&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5229 (7.71)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Netherlands&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6917 (10.20)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;United States&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9117 (13.45)&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Germany&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;35,486 (52.35)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>17091</offset><text>Number	Country	Completed dynamic ecological momentary assessment (EMA-D) questionnaires (n=67,789), n (%)	 	1	Australia	535 (0.79)	 	2	Belgium	819 (1.21)	 	3	Italy	1026 (1.51)	 	4	Russia	1076 (1.59)	 	5	Austria	1110 (1.64)	 	6	Norway	1159 (1.71)	 	7	Canada	2113 (3.12)	 	8	Great Britain	3202 (4.72)	 	9	Switzerland	5229 (7.71)	 	10	Netherlands	6917 (10.20)	 	11	United States	9117 (13.45)	 	12	Germany	35,486 (52.35)	 	</text></passage><passage><infon key="file">jmir_v22i6e15547_fig2.jpg</infon><infon key="id">figure2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>17511</offset><text>Data preparation steps for the machine learning analysis. aInformation about the scikit-learn function can be found on the scikit-learn website. EMA-D: dynamic ecological momentary assessment; Q: question; TYT: TrackYourTinnitus.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>17741</offset><text>Comparisons between iOS and Android users regarding gender, age, and number of completed dynamic ecological momentary assessment (EMA-D) questionnaires.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;320&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;130&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;140&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;150&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;190&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;70&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Variable&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Android&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;iOS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Chi-square (&lt;italic&gt;df&lt;/italic&gt;)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Two-tailed &lt;italic&gt;t&lt;/italic&gt; test (&lt;italic&gt;df&lt;/italic&gt;)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;P&lt;/italic&gt; value&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gender&lt;sup&gt;a&lt;/sup&gt; (male), n (%)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;221 (74.4)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;147 (66.5)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.2 (1)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;sup&gt;b&lt;/sup&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;.27&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Age&lt;sup&gt;c&lt;/sup&gt; (years), mean (SD)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.76 (12.29)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.57 (13.09)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;–0.71 (497)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;.48&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of completed EMA-D questionnaires&lt;sup&gt;d&lt;/sup&gt; (ie, all eight questions completed), mean (SD)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.52 (80.32)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.55 (125.92)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;–1.87 (516)&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;.06&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>17894</offset><text>Variable	Android	iOS	Chi-square (df)	Two-tailed t test (df)	P value	 	Gendera (male), n (%)	221 (74.4)	147 (66.5)	1.2 (1)	N/Ab	.27	 	Agec (years), mean (SD)	49.76 (12.29)	50.57 (13.09)	N/A	–0.71 (497)	.48	 	Number of completed EMA-D questionnairesd (ie, all eight questions completed), mean (SD)	49.52 (80.32)	66.55 (125.92)	N/A	–1.87 (516)	.06	 	</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>18246</offset><text>aSample sizes for gender are n=297 for Android and n=221 for iOS.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>18312</offset><text>bN/A: not applicable.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>18334</offset><text>cSample sizes for age are n=295 for Android and n=204 for iOS.</text></passage><passage><infon key="file">table3.xml</infon><infon key="id">table3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>18397</offset><text>dSample sizes for number of completed EMA-D questionnaires are n=297 for Android and n=221 for iOS.</text></passage><passage><infon key="file">jmir_v22i6e15547_fig3.jpg</infon><infon key="id">figure3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18497</offset><text>Frequencies of completed questionnaires of the investigated TrackYourTinnitus (TYT) users (n=518). EMA-D: dynamic ecological momentary assessment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18644</offset><text>Machine Learning Analysis</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18670</offset><text>We applied machine learning approaches with the goal to predict the OS—Android or iOS—of a provided assessment of the EMA-D data. For this purpose, four machine learning approaches were applied to the dataset: a feedforward neural network (FNN), a decision tree (DT), a support vector machine (SVM), and a random forest classifier (RFC). All approaches were chosen because they are appropriate for high-dimensional datasets, which is the case for the given EMA-D questionnaires of the TYT users. This is supported by similar works.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19206</offset><text>Concerning the dataset in general, it is noteworthy that the machine learning approaches were applied on the assessment level of the EMA-D questionnaires. This means that assessments from one user can be in both the training and the validation datasets. Performing a separation on assessment level has advantages and disadvantages. As the main disadvantage, it can be argued that if a participant is in both datasets, then there might be a bias. On the other hand, if users of the training phase are separated from the validation phase users, then it must be ensured that the user characteristics between the training and validation phases generate no bias. In an EMA-driven approach, where daily assessments on a random and voluntary basis are the main goal, it is difficult to be able to evaluate a large group of users with similar assessment characteristics. However, in future work, it will be a further goal to also separate the dataset on the user level in a reasonable manner. That individual users play an important role in health care studies is emphasized by the emerging paradigm of N-of-1 studies.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20317</offset><text>Concerning the validation procedure, the following has been accomplished: in a first step, the validation was based on a 10-fold cross-validation approach (ie, for the SVM, the DT, and the RFC). Here, the entire dataset was distributed into 10 equal parts. Nine of these parts were used for the training phase, while the remaining one part was used for the testing phase. The whole procedure was repeated 10 times and the average values were then calculated over all 10 runs. To foster our results, another validation was performed for the SVM, the DT, and the FNN. We conducted a leave-one-out approach on the user level, for all of the 518 included users, combined with a majority vote for the EMA-D questionnaires from the user that was left out, to see whether the prediction differs if the EMA-D questionnaires from the user who was left out are excluded. In conclusion, there was no obvious difference observed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21235</offset><text>For all analyses, the following technical environment was used: a laptop with an i7 core (2.60 GHz); MATLAB, version R2017a (MathWorks); the Statistics and Machine Learning Toolbox (MathWorks); and scikit-learn, open source machine learning library, for Python (Python Software Foundation). For all applied methods, we used the default parameters of the technical environment. In MATLAB, the FNN, the SVM, and the DT were calculated, while in Python scikit-learn, the RFC was calculated.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>21723</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21731</offset><text>Overview</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21740</offset><text>The four applied machine learning approaches showed different results for the research question. In general, the prediction accuracies were unexpectedly high.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>21899</offset><text>Feedforward Neural Network</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21926</offset><text>The FNN was the worst-performing candidate. Here, for 72.67% of the EMA-D questionnaires, the mobile OS could be correctly predicted. In the MATLAB toolbox that was used, the essential parameter for the calculation was feedforwardnet(10).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>22165</offset><text>Decision Tree</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22179</offset><text>The DT, in turn, performed as the third-best candidate. It was applied with a 10-fold cross-validation and it predicted the correct mobile OS for 76.36% of the EMA-D questionnaires. Importantly, the resulting DT has a depth of 379, showing that the prediction can be categorized into a high-dimensional calculation. In the MATLAB toolbox that was used, the essential parameter for the calculation was fitctree(X,Y,'CrossVal','on').</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>22611</offset><text>Random Forest Classifier</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22636</offset><text>The RFC performed as the best candidate; the mobile OS could be predicted correctly for 78.94% of the EMA-D questionnaires. In the Python scikit-learn method that was used, the essential parameter for the calculation was as follows: RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt', random_state=42). In addition to the prediction results, Table 4 shows the importance of the eight EMA-D questions for the overall prediction result of 78.94%; here, we used the model.feature_importances_ feature of Python scikit-learn. Importantly, question 7 and then question 2 are the most important questions for the prediction result of 78.94%.</text></passage><passage><infon key="file">table4.xml</infon><infon key="id">table4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>23298</offset><text>Importance of the eight dynamic ecological momentary assessment (EMA-D) questions for the random forest classifier prediction. Question 1 (Q1): Did you perceive the tinnitus right now? (yes or no); Q2: How loud is the tinnitus right now? (slider); Q3: How stressful is the tinnitus right now? (slider); Q4: How is your mood right now? (manikins); Q5: How is your arousal right now? (manikins); Q6: Do you feel stressed right now? (slider); Q7: How much did you concentrate on the things you are doing right now? (slider); and Q8: Do you feel irritable right now? (yes or no).</text></passage><passage><infon key="file">table4.xml</infon><infon key="id">table4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;180&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;90&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;90&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;90&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Question Number&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q7&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Percentage of Importance&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.03043&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.03985&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.08728&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0913&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.17246&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.17425&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.19247&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.21194&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>23874</offset><text>Question Number	Q1	Q8	Q5	Q4	Q3	Q6	Q2	Q7	 	Percentage of Importance	0.03043	0.03985	0.08728	0.0913	0.17246	0.17425	0.19247	0.21194	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24007</offset><text>Support Vector Machine</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24030</offset><text>The SVM performed as the second-best candidate. Overall, using all eight questions, the mobile OS could be predicted correctly for 78.65% of the EMA-D questionnaires. For the SVM, detailed results for single questions and question combinations are discussed in more detail. This will show that all eight questions are needed to get a prediction result with an accuracy that shows that the OS might be a confounder that should be further considered. The same detailed discussion could be accomplished for the other approaches, such as the RFC. We opted for the SVM for a more detailed discussion and to compare the results to other approaches to see if they deviate significantly from each other. More specifically, prediction results for combinations of two questions as well as single questions are shown in Table 5. Seven results will be further discussed. The discussion will show that the accuracies vary among the eight EMA-D questions on one hand. One the other hand, it will show that despite the observed variances, the overall achieved accuracy is high for different questions and their combinations.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25140</offset><text>First, we consider question 7—How much did you concentrate on the things you are doing right now? (slider)—and question 8—Do you feel irritable right now? (yes or no). They performed as the two best single questions for the prediction. Each of them has an accuracy of 58.80%. This result is only partly confirmed by the RFC. For the RFC, question 7 is also very important, but question 8 is less important for the RFC.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25565</offset><text>Second, question 5—How is your arousal right now? (manikins)—performed with the third-best result for a single question; here, an accuracy of 57.14% was attained. This question is like question 8, in that it is less important in the case of the RFC.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25819</offset><text>Third, the combination of question 7—How much did you concentrate on the things you are doing right now? (slider)—and question 8—Do you feel irritable right now? (yes or no)—performed as the best candidate for two-question combinations; in this case, an accuracy of 63.95% was achieved. This result is again only partly supported by the RFC (ie, for the RFC, question 8 was less important; see Table 4).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26231</offset><text>Fourth, the worst result was achieved when only using question 4—How is your mood right now? (manikins)—as the predictor. For question 4, an accuracy of 54.07% was achieved. Again, this deviates from the result of the RFC, where question 1 was the worst candidate.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26500</offset><text>Fifth, when solely combining yes or no questions (ie, question 1—Did you perceive the tinnitus right now?—and question 8—Do you feel irritable right now?), the mobile OS could be predicted correctly for 63.37% of the user assessments. This result also shows that without slider questions, a meaningful accuracy can be achieved.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26834</offset><text>Sixth, when looking at question-question combinations that include only sliders as answer types, the highest accuracies were achieved by the combination of question 2—How loud is the tinnitus right now? (slider)—and question 7—How much did you concentrate on the things you are doing right now? (slider). Here, an accuracy of 59.86% was achieved. This, in turn, is supported by the result of the RFC.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27241</offset><text>Seventh, it is remarkable that the overall prediction result with all eight questions is considerably higher than with single questions or combinations of two questions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27411</offset><text>Finally, Table 6 represents the confusion table for the SVM calculations. Note that the values are for all eight EMA-D questions of the considered 14,708 Android questionnaires as well as 14,708 iOS EMA-D questionnaires.</text></passage><passage><infon key="file">table5.xml</infon><infon key="id">table5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27632</offset><text>Prediction accuracies of the support vector machine (SVM) based on the eight dynamic ecological momentary assessment (EMA-D) questions and their combinations.</text></passage><passage><infon key="file">table5.xml</infon><infon key="id">table5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;170&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;100&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;100&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;100&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;100&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;100&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;110&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Question&lt;sup&gt;a&lt;/sup&gt; (Q)&lt;/td&gt;&lt;td colspan=&quot;8&quot; rowspan=&quot;1&quot;&gt;Accuracy for each question (Q) combination, %&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q7&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q8&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.69&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.18&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.90&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.53&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.80&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.61&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.37&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.89&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.28&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;54.07&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q5&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.55&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.08&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.31&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.10&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.14&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.59&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.27&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.35&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.69&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.83&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.28&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q7&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.24&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.86&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.40&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.19&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.38&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.33&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.80&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;—&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Q8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.37&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.32&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.57&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.18&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.57&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.67&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.95&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.80&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27791</offset><text>Questiona (Q)	Accuracy for each question (Q) combination, %	 		Q1	Q2	Q3	Q4	Q5	Q6	Q7	Q8	 	Q1	55.69	—	—	—	—	—	—	—	 	Q2	59.18	55.90	—	—	—	—	—	—	 	Q3	58.53	56.80	56.61	—	—	—	—	—	 	Q4	56.37	57.89	58.28	54.07	—	—	—	—	 	Q5	59.55	61.08	61.31	60.10	57.14	—	—	—	 	Q6	58.59	57.27	56.35	58.69	62.83	56.28	—	—	 	Q7	61.24	59.86	59.40	60.19	62.38	59.33	58.80	—	 	Q8	63.37	62.32	63.57	60.18	61.57	62.67	63.95	58.80	 	</text></passage><passage><infon key="file">table5.xml</infon><infon key="id">table5</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>28249</offset><text>aQ1: Did you perceive the tinnitus right now? (yes or no); Q2: How loud is the tinnitus right now? (slider); Q3: How stressful is the tinnitus right now? (slider); Q4: How is your mood right now? (manikins); Q5: How is your arousal right now? (manikins); Q6: Do you feel stressed right now? (slider); Q7: How much did you concentrate on the things you are doing right now? (slider); and Q8: Do you feel irritable right now? (yes or no).</text></passage><passage><infon key="file">table6.xml</infon><infon key="id">table6</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>28686</offset><text>Confusion table for the support vector machine (SVM) calculations over all eight dynamic ecological momentary assessment (EMA-D) questions.</text></passage><passage><infon key="file">table6.xml</infon><infon key="id">table6</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; width=&quot;1000&quot; cellpadding=&quot;5&quot; cellspacing=&quot;0&quot; border=&quot;1&quot;&gt;&lt;col width=&quot;190&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;190&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;210&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;200&quot; span=&quot;1&quot;/&gt;&lt;col width=&quot;210&quot; span=&quot;1&quot;/&gt;&lt;thead&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Predicted class&lt;/td&gt;&lt;td colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;Actual class&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;iOS&lt;/td&gt;&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Android&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;break/&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True positives, n&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False negatives, n&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False positives, n&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True negatives, n&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;iOS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13,002&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;sup&gt;a&lt;/sup&gt;&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1967&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr valign=&quot;top&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Android&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1706&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N/A&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12,741&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>28826</offset><text>Predicted class	Actual class	 		iOS	Android	 		True positives, n	False negatives, n	False positives, n	True negatives, n	 	iOS	13,002	N/Aa	1967	N/A	 	Android	N/A	1706	N/A	12,741	 	</text></passage><passage><infon key="file">table6.xml</infon><infon key="id">table6</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>29007</offset><text>aN/A: not applicable.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>29029</offset><text>Importance of Questions</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29053</offset><text>In general, the question emerges as to why some of the eight EMA-D questions are better suited than others to correctly predict the mobile OS. One possible explanation refers to the answering behavior of the users of the two mobile OS types. To illustrate this, Figures 4 and 5 show, as examples, histograms of question 3—How stressful is the tinnitus right now? (slider)—and question 5—How is your arousal right now? (manikins). It is obvious that Android and iOS users answer differently.</text></passage><passage><infon key="file">jmir_v22i6e15547_fig4.jpg</infon><infon key="id">figure4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29550</offset><text>Answers to question 3—How stressful is the tinnitus right now? (slider)—and the difference vector of Android and iOS users.</text></passage><passage><infon key="file">jmir_v22i6e15547_fig5.jpg</infon><infon key="id">figure5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29678</offset><text>Answers to question 5 ”How is your arousal right now? (manikins)” and the difference vector of Android and iOS users.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29800</offset><text>Question 3 and question 5 have been chosen as examples, as they differ in their importance between the machine learning approaches: question 3 is the fourth-most important in SVM versus the third-most important in RFC, while question 5 is the third-most important in SVM versus the sixth-most important in RFC. In addition, other questions are more suitable for the overall prediction. Although they differ and other questions are better, they still show striking differences between assessments from Android and iOS users. To support this result, further consider Figures 6 and 7; they each show data for 100 users in total, distributed among Android and iOS. The data were randomly selected out of the entire dataset. This subset was chosen for the sake of clarity; if all data points were shown, less could be visually observed. In Figure 6, for question 2—How loud is the tinnitus right now? (slider)—in combination with question 3—How stressful is the tinnitus right now? (slider)—shown on the left-hand side of the figure, or question 6—Do you feel stressed right now? (slider)—shown on the right-hand side of the figure, the blue dots show the answers from the Android users, while the red dots show answers from the iOS users. It is striking that Android and iOS users answer differently. Furthermore, in Figure 7, for question 4—How is your mood right now? (manikins)—in combination with question 3—How stressful is the tinnitus right now? (slider)—shown on the left-hand side of the figure, or question 5—How is your arousal right now? (manikins)—shown on the right-hand side of the figure, the same can be observed. Importantly, Figures 6 and 7 are not representative of the entire dataset, but it is nevertheless notable that Android and iOS users answer differently. Further note that in Figures 6 and 7, we do not illustrate the achieved predictions. Instead, the attained loss is shown (ie, 1-loss denotes the achieved accuracy). Furthermore, these combinations have been selected as they also show clear differences between Android and iOS assessments, although other questions have higher prediction accuracies.</text></passage><passage><infon key="file">jmir_v22i6e15547_fig6.jpg</infon><infon key="id">figure6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>31953</offset><text>Support vector machine (SVM) results for question 2 (Q2), combined with question 3 (Q3) and question 6 (Q6), from 100 data entries. Q2: How loud is the tinnitus right now? (slider); Q3: How stressful is the tinnitus right now? (slider); and Q6: Do you feel stressed right now? (slider).</text></passage><passage><infon key="file">jmir_v22i6e15547_fig7.jpg</infon><infon key="id">figure7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32240</offset><text>Support vector machine (SVM) results for question 4 (Q4), combined with question 3 (Q3) and question 5 (Q5), from 100 data entries. Q3: How stressful is the tinnitus right now? (slider); Q4: How is your mood right now? (manikins); and Q5: How is your arousal right now? (manikins).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>32522</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>32533</offset><text>Principal Findings</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32552</offset><text>This study evaluated whether it is possible to predict the mobile OS used by respondents for a provided EMA-D questionnaire based on the eight daily tinnitus questions included in the questionnaire, which was administered via TYT. Although the applied machine learning approaches showed different prediction results, in general, the achieved accuracies indicate that the mobile OS is a confounder that must be further considered. This confirms the investigated research question. We are able to predict the mobile OS used with high accuracy based on the dynamic daily assessment data. Compared to Pryss et al, the users’ ages were no longer different between Android and iOS users, which might be explained by the selection of the sample for this study: only users with more than 10 completed EMA-D questionnaires were selected. In addition to our prior works, this study shows that the mobile OS not only reveals insights into the tinnitus characteristics of the users, but it is possible to predict the mobile OS based on the provided daily TYT data. On top of this, widely used machine learning approaches with commonly used frameworks and without parameter tuning are able to predict the mobile OS with high accuracy. Note that the RFC achieved the highest prediction result of 78.94%, with default parameter settings using Python scikit-learn. In this context, question 7—How much did you concentrate on the things you are doing right now? (slider)—of the EMA-D questionnaire, which measures the concentration level of a TYT user at the moment, has especially revealed a high accuracy for the RFC prediction. In summary, four important results were found. First, the research question can be answered positively. We are able to predict the mobile OS used for a given EMA-D questionnaire with high accuracy using machine learning methods. Second, the prediction is possible with well-known machine learning methods and frameworks without parameter tuning. Third, machine learning indicates promising results on the EMA-D from TYT users. Therefore, this result should be exploited for further analyses. Fourth, when using mobile devices to collect clinically relevant data, the mobile OS used might be a confounder. Therefore, this information should be collected for each measurement and could be a relevant covariate in data analyses.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>34898</offset><text>Strengths and Limitations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34924</offset><text>In general, as a strength of this work, it could be shown that the technical peculiarities of different mobile OS types must be considered for the collection of clinically relevant data. As another positive aspect of this work, it could be shown that the types of answers for the questions do not necessarily indicate that a particular answer type, such as a slider, is used a priori with a bias. Otherwise, sliders or any other answer type would be more important than others. In general, we aimed at technically implementing TYT in a way that made sure the questionnaires looked identical on Android and iOS devices as well as having no default setting. Despite this way of implementing the questionnaires visually, a potential bias cannot be excluded. Therefore, further investigations are required. For example, the sliders on Android and iOS have different numbers of decimal places. On Android, only 2 decimal places are stored, while on iOS, more than 2 decimal places are stored. For the investigation in this paper, the scales of all sliders were harmonized, but such differences must also be further investigated.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36048</offset><text>For the aspect of whether EMA-D can be used to predict not only the TYT assessments but the TYT users in general, we are conducting another study, in which we investigate whether we are able to predict the mobile OS used on the user level instead of on the assessment level. However, such investigation requires many more considerations. For example, how can we ensure that the training dataset users have similar characteristics as the users for which we apply the trained classifier? Note that such an investigation requires efforts regarding the frameworks used and their provided features.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>36642</offset><text>Conclusions</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36654</offset><text>This work has shown opportunities on one hand and limitations on the other. A particular strength of this study is that TYT has a unique dataset, which is able to comprehensively compare Android and iOS OS types in a medical context. However, the different results between different machine learning approaches showed that it is difficult to predict which questions and answer types are, in general, appropriate for predictions. If a new platform shall be realized and one goal of the platform constitutes using machine learning methods for a prediction, this analysis has not revealed general guidelines that can be followed. Thus, these results can only be seen as a particular outcome for TYT. In addition, when gathering additional contextual information from the TYT users, such as geospatial data, new investigations become possible. In a recent work, for example, we investigated geospatial data of mobile crowdsensing users and whether their movement behavior could be a predictor for their current stress situation. As this work also revealed promising results, in the next version of TYT, GPS data can be gathered while filling out the EMA-D questionnaire, if a user allows this measurement.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37856</offset><text>In future work, we will further address the following three aspects. First, more studies must confirm the results of this work. Second, the results of TYT must be compared to other similar EMA datasets in order to confirm the results between different scenarios. Third, we need to conduct this study again based on the user level instead of on the assessment level.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38222</offset><text>However, if future work can confirm the presented results, then the combination of EMA, mobile crowdsensing, and machine learning seems to be a worthwhile research endeavor. Nevertheless, we are far from using the results of this work in clinical practice. On the other hand, together with already-revealed medical insights on TYT, the results of this work show that new opportunities are possible in the broader EMA and mobile crowdsensing contexts. In particular, EMA data that were gathered by mobile devices, as well as the crowdsensing paradigm, seem to be promising targets for the application of machine learning algorithms.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>38854</offset><text>Authors' Contributions: RP substantially contributed to the TYT platform, study design, data analysis, and data interpretation and wrote as well as revised the manuscript. WS substantially contributed to the TYT platform and data interpretation and revised the manuscript. BH substantially contributed to the TYT platform, study design, data analysis, and data interpretation and drafted as well as revised the manuscript. MR substantially contributed to the TYT platform and revised the manuscript. MS substantially contributed to data interpretation and revised the manuscript. BL substantially contributed to data interpretation and revised the manuscript. MB substantially contributed to the data analysis and data interpretation and revised the manuscript. TP substantially contributed to the TYT platform, study design, data analysis, and data interpretation and wrote as well as revised the manuscript.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>39764</offset><text>Conflicts of Interest: None declared.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>39802</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39816</offset><text>CSV</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39820</offset><text>comma-separated values</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39843</offset><text>DT</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39846</offset><text>decision tree</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39860</offset><text>EMA</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39864</offset><text>ecological momentary assessment</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39896</offset><text>EMA-D</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39902</offset><text>dynamic ecological momentary assessment</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39942</offset><text>EMA-S</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39948</offset><text>static ecological momentary assessment</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39987</offset><text>FNN</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>39991</offset><text>feedforward neural network</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40018</offset><text>mHealth</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40026</offset><text>mobile health</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40040</offset><text>OS</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40043</offset><text>operating system</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40060</offset><text>RFC</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40064</offset><text>random forest classifier</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40089</offset><text>SF28</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40094</offset><text>SmokeFree28</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40106</offset><text>SVM</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40110</offset><text>support vector machine</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40133</offset><text>TYH</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40137</offset><text>TrackYourHearing</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40154</offset><text>TYT</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>40158</offset><text>TrackYourTinnitus</text></passage><passage><infon key="fpage">164</infon><infon key="name_0">surname:Kraft;given-names:R</infon><infon key="name_1">surname:Schlee;given-names:W</infon><infon key="name_2">surname:Stach;given-names:M</infon><infon key="name_3">surname:Reichert;given-names:M</infon><infon key="name_4">surname:Langguth;given-names:B</infon><infon key="name_5">surname:Baumeister;given-names:H</infon><infon key="name_6">surname:Probst;given-names:T</infon><infon key="name_7">surname:Hannemann;given-names:R</infon><infon key="name_8">surname:Pryss;given-names:R</infon><infon key="pub-id_doi">10.3389/fnins.2020.00164</infon><infon key="pub-id_medline">32184708</infon><infon key="pub-id_pmid">32184708</infon><infon key="section_type">REF</infon><infon key="source">Front Neurosci</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2020</infon><offset>40176</offset><text>Combining mobile crowdsensing and ecological momentary assessments in the healthcare domain</text></passage><passage><infon key="fpage">4616417</infon><infon key="name_0">surname:Baksa;given-names:D</infon><infon key="name_1">surname:Gecse;given-names:K</infon><infon key="name_2">surname:Kumar;given-names:S</infon><infon key="name_3">surname:Toth;given-names:Z</infon><infon key="name_4">surname:Gal;given-names:Z</infon><infon key="name_5">surname:Gonda;given-names:X</infon><infon key="name_6">surname:Juhasz;given-names:G</infon><infon key="pub-id_doi">10.1155/2019/4616417</infon><infon key="pub-id_medline">31534960</infon><infon key="pub-id_pmid">31534960</infon><infon key="section_type">REF</infon><infon key="source">Biomed Res Int</infon><infon key="type">ref</infon><infon key="volume">2019</infon><infon key="year">2019</infon><offset>40268</offset><text>Circadian variation of migraine attack onset: A review of clinical studies</text></passage><passage><infon key="fpage">20</infon><infon key="issue">5</infon><infon key="name_0">surname:Menictas;given-names:M</infon><infon key="name_1">surname:Rabbi;given-names:M</infon><infon key="name_2">surname:Klasnja;given-names:P</infon><infon key="name_3">surname:Murphy;given-names:S</infon><infon key="pub-id_doi">10.1042/BIO04105020</infon><infon key="section_type">REF</infon><infon key="source">Biochem (Lond)</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2019</infon><offset>40343</offset><text>Artificial intelligence decision-making in mobile health</text></passage><passage><infon key="comment">Mobile operating system market share worldwide: May 2019 - May 2020https://gs.statcounter.com/os-market-share/mobile/worldwide</infon><infon key="section_type">REF</infon><infon key="source">Statcounter Global Stats</infon><infon key="type">ref</infon><offset>40400</offset></passage><passage><infon key="fpage">1635</infon><infon key="issue">8</infon><infon key="lpage">1643</infon><infon key="name_0">surname:Langguth;given-names:B</infon><infon key="pub-id_doi">10.1185/03007995.2011.595781</infon><infon key="pub-id_medline">21699365</infon><infon key="pub-id_pmid">21699365</infon><infon key="section_type">REF</infon><infon key="source">Curr Med Res Opin</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2011</infon><offset>40401</offset><text>A review of tinnitus symptoms beyond 'ringing in the ears': A call to action</text></passage><passage><infon key="fpage">294</infon><infon key="name_0">surname:Schlee;given-names:W</infon><infon key="name_1">surname:Pryss;given-names:RC</infon><infon key="name_2">surname:Probst;given-names:T</infon><infon key="name_3">surname:Schobel;given-names:J</infon><infon key="name_4">surname:Bachmeier;given-names:A</infon><infon key="name_5">surname:Reichert;given-names:M</infon><infon key="name_6">surname:Langguth;given-names:B</infon><infon key="pub-id_doi">10.3389/fnagi.2016.00294</infon><infon key="pub-id_medline">28018210</infon><infon key="pub-id_pmid">28018210</infon><infon key="section_type">REF</infon><infon key="source">Front Aging Neurosci</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2016</infon><offset>40478</offset><text>Measuring the moment-to-moment variability of tinnitus: The TrackYourTinnitus smart phone app</text></passage><passage><infon key="fpage">151</infon><infon key="lpage">176</infon><infon key="name_0">surname:Trull;given-names:TJ</infon><infon key="name_1">surname:Ebner-Priemer;given-names:U</infon><infon key="pub-id_doi">10.1146/annurev-clinpsy-050212-185510</infon><infon key="pub-id_medline">23157450</infon><infon key="pub-id_pmid">23157450</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Clin Psychol</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2013</infon><offset>40572</offset><text>Ambulatory assessment</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">31</infon><infon key="name_0">surname:Guo;given-names:B</infon><infon key="name_1">surname:Wang;given-names:Z</infon><infon key="name_2">surname:Yu;given-names:Z</infon><infon key="name_3">surname:Wang;given-names:Y</infon><infon key="name_4">surname:Yen;given-names:N</infon><infon key="name_5">surname:Huang;given-names:R</infon><infon key="name_6">surname:Zhou;given-names:X</infon><infon key="pub-id_doi">10.1145/2794400</infon><infon key="section_type">REF</infon><infon key="source">ACM Comput Surv</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2015</infon><offset>40594</offset><text>Mobile crowd sensing and computing: The review of an emerging human-powered sensing paradigm</text></passage><passage><infon key="fpage">189</infon><infon key="issue">2</infon><infon key="lpage">200</infon><infon key="name_0">surname:Estellés-Arolas;given-names:E</infon><infon key="name_1">surname:González-Ladrón-de-Guevara;given-names:F</infon><infon key="pub-id_doi">10.1177/0165551512437638</infon><infon key="section_type">REF</infon><infon key="source">J Inf Sci</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2012</infon><offset>40687</offset><text>Towards an integrated crowdsourcing definition</text></passage><passage><infon key="fpage">221</infon><infon key="lpage">234</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Baumeister;given-names:H</infon><infon key="name_2">surname:Montag;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Digital Phenotyping and Mobile Sensing: New Developments in Psychoinformatics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>40734</offset><text>Mobile crowdsensing in healthcare scenarios: Taxonomy, conceptual pillars, smart mobile crowdsensing services</text></passage><passage><infon key="fpage">327</infon><infon key="issue">4</infon><infon key="lpage">338</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Probst;given-names:T</infon><infon key="name_2">surname:Schlee;given-names:W</infon><infon key="name_3">surname:Schobel;given-names:J</infon><infon key="name_4">surname:Langguth;given-names:B</infon><infon key="name_5">surname:Neff;given-names:P</infon><infon key="name_6">surname:Spiliopoulou;given-names:M</infon><infon key="name_7">surname:Reichert;given-names:M</infon><infon key="pub-id_doi">10.1007/s41060-018-0111-4</infon><infon key="section_type">REF</infon><infon key="source">Int J Data Sci Anal</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2018</infon><offset>40844</offset><text>Prospective crowdsensing versus retrospective ratings of tinnitus variability and tinnitus–stress associations based on the TrackYourTinnitus mobile platform</text></passage><passage><infon key="fpage">411</infon><infon key="lpage">416</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Reichert;given-names:M</infon><infon key="name_2">surname:Schlee;given-names:W</infon><infon key="name_3">surname:Spiliopoulou;given-names:M</infon><infon key="name_4">surname:Langguth;given-names:B</infon><infon key="name_5">surname:Probst;given-names:T</infon><infon key="pub-id_doi">10.1109/CBMS.2018.00078</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st International Symposium on Computer-Based Medical Systems</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>41004</offset><text>Differences between Android and iOS users of the TrackYourTinnitus mobile crowdsensing mHealth platform</text></passage><passage><infon key="fpage">166</infon><infon key="issue">2</infon><infon key="lpage">171</infon><infon key="name_0">surname:Ubhi;given-names:HK</infon><infon key="name_1">surname:Kotz;given-names:D</infon><infon key="name_2">surname:Michie;given-names:S</infon><infon key="name_3">surname:van Schayck;given-names:OCP</infon><infon key="name_4">surname:West;given-names:R</infon><infon key="pub-id_doi">10.1007/s13142-016-0455-z</infon><infon key="pub-id_medline">28168609</infon><infon key="pub-id_pmid">28168609</infon><infon key="section_type">REF</infon><infon key="source">Transl Behav Med</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2017</infon><offset>41108</offset><text>A comparison of the characteristics of iOS and Android users of a smoking cessation app</text></passage><passage><infon key="fpage">3951</infon><infon key="lpage">3955</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Schlee;given-names:W</infon><infon key="name_10">surname:Hoppenstedt;given-names:B</infon><infon key="name_11">surname:Spiliopoulou;given-names:M</infon><infon key="name_12">surname:Langguth;given-names:B</infon><infon key="name_13">surname:Probst;given-names:T</infon><infon key="name_2">surname:Reichert;given-names:M</infon><infon key="name_3">surname:Kurthen;given-names:I</infon><infon key="name_4">surname:Giroud;given-names:N</infon><infon key="name_5">surname:Jagoda;given-names:L</infon><infon key="name_6">surname:Neuschwander;given-names:P</infon><infon key="name_7">surname:Meyer;given-names:M</infon><infon key="name_8">surname:Neff;given-names:P</infon><infon key="name_9">surname:Schobel;given-names:J</infon><infon key="pub-id_doi">10.1109/embc.2019.8857854</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>41196</offset><text>Ecological momentary assessment-based differences between Android and iOS users of the TrackYourHearing mHealth crowdsensing platform</text></passage><passage><infon key="fpage">23</infon><infon key="lpage">47</infon><infon key="name_0">surname:Mohr;given-names:DC</infon><infon key="name_1">surname:Zhang;given-names:M</infon><infon key="name_2">surname:Schueller;given-names:SM</infon><infon key="pub-id_doi">10.1146/annurev-clinpsy-032816-044949</infon><infon key="pub-id_medline">28375728</infon><infon key="pub-id_pmid">28375728</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Clin Psychol</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2017</infon><offset>41330</offset><text>Personal sensing: Understanding mental health using ubiquitous sensors and machine learning</text></passage><passage><infon key="fpage">212</infon><infon key="issue">2</infon><infon key="lpage">216</infon><infon key="name_0">surname:Boyer;given-names:EW</infon><infon key="name_1">surname:Smelson;given-names:D</infon><infon key="name_2">surname:Fletcher;given-names:R</infon><infon key="name_3">surname:Ziedonis;given-names:D</infon><infon key="name_4">surname:Picard;given-names:RW</infon><infon key="pub-id_doi">10.1007/s13181-010-0080-z</infon><infon key="pub-id_medline">20623215</infon><infon key="pub-id_pmid">20623215</infon><infon key="section_type">REF</infon><infon key="source">J Med Toxicol</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2010</infon><offset>41422</offset><text>Wireless technologies, ubiquitous computing and mobile health: Application to drug abuse treatment and compliance with HIV therapies</text></passage><passage><infon key="fpage">233</infon><infon key="issue">1</infon><infon key="name_0">surname:Colombo;given-names:D</infon><infon key="name_1">surname:Palacios;given-names:AG</infon><infon key="name_2">surname:Alvarez;given-names:JF</infon><infon key="name_3">surname:Patané;given-names:A</infon><infon key="name_4">surname:Semonella;given-names:M</infon><infon key="name_5">surname:Cipresso;given-names:P</infon><infon key="name_6">surname:Kwiatkowska;given-names:M</infon><infon key="name_7">surname:Riva;given-names:G</infon><infon key="name_8">surname:Botella;given-names:C</infon><infon key="pub-id_doi">10.1186/s13643-018-0899-y</infon><infon key="pub-id_medline">30545415</infon><infon key="pub-id_pmid">30545415</infon><infon key="section_type">REF</infon><infon key="source">Syst Rev</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2018</infon><offset>41555</offset><text>Current state and future directions of technology-based ecological momentary assessments and interventions for major depressive disorder: Protocol for a systematic review</text></passage><passage><infon key="fpage">51</infon><infon key="issue">7</infon><infon key="name_0">surname:Torous;given-names:J</infon><infon key="name_1">surname:Larsen;given-names:ME</infon><infon key="name_2">surname:Depp;given-names:C</infon><infon key="name_3">surname:Cosco;given-names:TD</infon><infon key="name_4">surname:Barnett;given-names:I</infon><infon key="name_5">surname:Nock;given-names:MK</infon><infon key="name_6">surname:Firth;given-names:J</infon><infon key="pub-id_doi">10.1007/s11920-018-0914-y</infon><infon key="pub-id_medline">29956120</infon><infon key="pub-id_pmid">29956120</infon><infon key="section_type">REF</infon><infon key="source">Curr Psychiatry Rep</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2018</infon><offset>41726</offset><text>Smartphones, sensors, and machine learning to advance real-time prediction and interventions for suicide prevention: A review of current progress and next steps</text></passage><passage><infon key="fpage">91</infon><infon key="lpage">118</infon><infon key="name_0">surname:Dwyer;given-names:DB</infon><infon key="name_1">surname:Falkai;given-names:P</infon><infon key="name_2">surname:Koutsouleris;given-names:N</infon><infon key="pub-id_doi">10.1146/annurev-clinpsy-032816-045037</infon><infon key="pub-id_medline">29401044</infon><infon key="pub-id_pmid">29401044</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Clin Psychol</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2018</infon><offset>41887</offset><text>Machine learning approaches for clinical psychology and psychiatry</text></passage><passage><infon key="fpage">e10131</infon><infon key="issue">7</infon><infon key="name_0">surname:Boonstra;given-names:TW</infon><infon key="name_1">surname:Nicholas;given-names:J</infon><infon key="name_2">surname:Wong;given-names:QJ</infon><infon key="name_3">surname:Shaw;given-names:F</infon><infon key="name_4">surname:Townsend;given-names:S</infon><infon key="name_5">surname:Christensen;given-names:H</infon><infon key="pub-id_doi">10.2196/10131</infon><infon key="pub-id_medline">30061092</infon><infon key="pub-id_pmid">30061092</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2018</infon><offset>41954</offset><text>Using mobile phone sensor technology for mental health research: Integrated analysis to identify hidden challenges and potential solutions</text></passage><passage><infon key="fpage">e190</infon><infon key="issue">12</infon><infon key="name_0">surname:Jake-Schoffman;given-names:DE</infon><infon key="name_1">surname:Silfee;given-names:VJ</infon><infon key="name_10">surname:Pagoto;given-names:SL</infon><infon key="name_2">surname:Waring;given-names:ME</infon><infon key="name_3">surname:Boudreaux;given-names:ED</infon><infon key="name_4">surname:Sadasivam;given-names:RS</infon><infon key="name_5">surname:Mullen;given-names:SP</infon><infon key="name_6">surname:Carey;given-names:JL</infon><infon key="name_7">surname:Hayes;given-names:RB</infon><infon key="name_8">surname:Ding;given-names:EY</infon><infon key="name_9">surname:Bennett;given-names:GG</infon><infon key="pub-id_doi">10.2196/mhealth.8758</infon><infon key="pub-id_medline">29254914</infon><infon key="pub-id_pmid">29254914</infon><infon key="section_type">REF</infon><infon key="source">JMIR Mhealth Uhealth</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>42093</offset><text>Methods for evaluating the content, usability, and efficacy of commercial mobile health apps</text></passage><passage><infon key="fpage">97</infon><infon key="issue">1</infon><infon key="lpage">98</infon><infon key="name_0">surname:Torous;given-names:J</infon><infon key="name_1">surname:Andersson;given-names:G</infon><infon key="name_10">surname:Mohr;given-names:DC</infon><infon key="name_11">surname:Pratap;given-names:A</infon><infon key="name_12">surname:Roux;given-names:S</infon><infon key="name_13">surname:Sherrill;given-names:J</infon><infon key="name_14">surname:Arean;given-names:PA</infon><infon key="name_2">surname:Bertagnoli;given-names:A</infon><infon key="name_3">surname:Christensen;given-names:H</infon><infon key="name_4">surname:Cuijpers;given-names:P</infon><infon key="name_5">surname:Firth;given-names:J</infon><infon key="name_6">surname:Haim;given-names:A</infon><infon key="name_7">surname:Hsin;given-names:H</infon><infon key="name_8">surname:Hollis;given-names:C</infon><infon key="name_9">surname:Lewis;given-names:S</infon><infon key="pub-id_doi">10.1002/wps.20592</infon><infon key="pub-id_medline">30600619</infon><infon key="pub-id_pmid">30600619</infon><infon key="section_type">REF</infon><infon key="source">World Psychiatry</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2019</infon><offset>42186</offset><text>Towards a consensus around standards for smartphone apps and digital mental health</text></passage><passage><infon key="comment">StratCare trial: Pragmatic randomised controlled trial of a stratified care model for depression and anxietyhttp://www.isrctn.com/ISRCTN11106183</infon><infon key="name_0">surname:Delgadillo;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">ISRCTN Registry</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>42270</offset></passage><passage><infon key="fpage">232</infon><infon key="lpage">238</infon><infon key="name_0">surname:Boukhechba;given-names:M</infon><infon key="name_1">surname:Cai;given-names:L</infon><infon key="name_2">surname:Chow;given-names:P</infon><infon key="name_3">surname:Fua;given-names:K</infon><infon key="name_4">surname:Gerber;given-names:M</infon><infon key="name_5">surname:Teachman;given-names:B</infon><infon key="name_6">surname:Barnes;given-names:L</infon><infon key="pub-id_doi">10.1145/3240925.3240967</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 12th EAI International Conference on Pervasive Computing Technologies for Healthcare</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>42271</offset><text>Contextual analysis to understand compliance with smartphone-based ecological momentary assessment</text></passage><passage><infon key="fpage">112</infon><infon key="lpage">117</infon><infon key="name_0">surname:Muniandi;given-names:LP</infon><infon key="name_1">surname:Schlee;given-names:W</infon><infon key="name_2">surname:Pryss;given-names:R</infon><infon key="name_3">surname:Reichert;given-names:M</infon><infon key="name_4">surname:Schobel;given-names:J</infon><infon key="name_5">surname:Kraft;given-names:R</infon><infon key="name_6">surname:Spiliopoulou;given-names:M</infon><infon key="pub-id_doi">10.1109/cbms.2018.00027</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st International Symposium on Computer-Based Medical Systems</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>42370</offset><text>Finding tinnitus patients with similar evolution of their ecological momentary assessments</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">15</infon><infon key="name_0">surname:Unnikrishnan;given-names:V</infon><infon key="name_1">surname:Beyer;given-names:C</infon><infon key="name_2">surname:Matuszyk;given-names:P</infon><infon key="name_3">surname:Niemann;given-names:U</infon><infon key="name_4">surname:Pryss;given-names:R</infon><infon key="name_5">surname:Schlee;given-names:W</infon><infon key="name_6">surname:Ntoutsi;given-names:E</infon><infon key="name_7">surname:Spiliopoulou;given-names:M</infon><infon key="pub-id_doi">10.1007/s41060-019-00177-1</infon><infon key="section_type">REF</infon><infon key="source">Int J Data Sci Anal</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2019</infon><offset>42461</offset><text>Entity-level stream classification: Exploiting entity similarity to label the future observations referring to an entity</text></passage><passage><infon key="fpage">350</infon><infon key="lpage">355</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:John;given-names:D</infon><infon key="name_10">surname:Langguth;given-names:B</infon><infon key="name_11">surname:Probst;given-names:T</infon><infon key="name_2">surname:Reichert;given-names:M</infon><infon key="name_3">surname:Hoppenstedt;given-names:B</infon><infon key="name_4">surname:Schmid;given-names:L</infon><infon key="name_5">surname:Schlee;given-names:W</infon><infon key="name_6">surname:Spiliopoulou;given-names:M</infon><infon key="name_7">surname:Schobel;given-names:J</infon><infon key="name_8">surname:Kraft;given-names:R</infon><infon key="name_9">surname:Schickler;given-names:M</infon><infon key="pub-id_doi">10.1109/iri.2019.00061</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 20th International Conference on Information Reuse and Integration for Data Science</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>42582</offset><text>Machine learning findings on geospatial data of users from the TrackYourStress mHealth crowdsensing platform</text></passage><passage><infon key="fpage">352</infon><infon key="lpage">359</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Reichert;given-names:M</infon><infon key="name_2">surname:Langguth;given-names:B</infon><infon key="name_3">surname:Schlee;given-names:W</infon><infon key="pub-id_doi">10.1109/mobserv.2015.55</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 3rd International Conference on Mobile Services</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>42691</offset><text>Mobile crowd sensing services for tinnitus assessment, therapy, and research</text></passage><passage><infon key="fpage">22</infon><infon key="lpage">29</infon><infon key="name_0">surname:Pryss;given-names:R</infon><infon key="name_1">surname:Schlee;given-names:W</infon><infon key="name_2">surname:Langguth;given-names:B</infon><infon key="name_3">surname:Reichert;given-names:M</infon><infon key="pub-id_doi">10.1109/aims.2017.12</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on AI &amp; Mobile Services (AIMS)</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>42768</offset><text>Mobile crowdsensing services for tinnitus assessment and patient feedback</text></passage><passage><infon key="fpage">415</infon><infon key="lpage">426</infon><infon key="name_0">surname:Xiong;given-names:H</infon><infon key="name_1">surname:Huang;given-names:Y</infon><infon key="name_2">surname:Barnes;given-names:L</infon><infon key="name_3">surname:Gerber;given-names:M</infon><infon key="pub-id_doi">10.1145/2971648.2971711</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>42842</offset><text>Sensus: A cross-platform, general-purpose system for mobile crowdsensing in human-subject studies</text></passage><passage><infon key="fpage">1139</infon><infon key="lpage">1143</infon><infon key="name_0">surname:Chow;given-names:P</infon><infon key="name_1">surname:Bonelli;given-names:W</infon><infon key="name_2">surname:Huang;given-names:Y</infon><infon key="name_3">surname:Fua;given-names:K</infon><infon key="name_4">surname:Teachman;given-names:BA</infon><infon key="name_5">surname:Barnes;given-names:LE</infon><infon key="pub-id_doi">10.1145/2968219.2968300</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>42940</offset><text>DEMONS: An integrated framework for examining associations between physiology and self-reported affect tied to depressive symptoms</text></passage><passage><infon key="fpage">2277</infon><infon key="issue">6</infon><infon key="lpage">2289</infon><infon key="name_0">surname:Beierle;given-names:F</infon><infon key="name_1">surname:Tran;given-names:VT</infon><infon key="name_2">surname:Allemand;given-names:M</infon><infon key="name_3">surname:Neff;given-names:P</infon><infon key="name_4">surname:Schlee;given-names:W</infon><infon key="name_5">surname:Probst;given-names:T</infon><infon key="name_6">surname:Zimmermann;given-names:J</infon><infon key="name_7">surname:Pryss;given-names:R</infon><infon key="pub-id_doi">10.1007/s12652-019-01355-6</infon><infon key="section_type">REF</infon><infon key="source">J Ambient Intell Humaniz Comput</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2019</infon><offset>43071</offset><text>What data are smartphone users willing to share with researchers?</text></passage><passage><infon key="comment">sklearn.utils.random.sample_without_replacementhttps://scikit-learn.org/stable/modules/generated/sklearn.utils.random.sample_without_replacement.html</infon><infon key="section_type">REF</infon><infon key="source">scikit-learn</infon><infon key="type">ref</infon><offset>43137</offset></passage><passage><infon key="fpage">76</infon><infon key="lpage">83</infon><infon key="name_0">surname:Santhanam;given-names:T</infon><infon key="name_1">surname:Padmavathi;given-names:M</infon><infon key="pub-id_doi">10.1016/j.procs.2015.03.185</infon><infon key="section_type">REF</infon><infon key="source">Procedia Comput Sci</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>43138</offset><text>Application of k-means and genetic algorithms for dimension reduction by integrating SVM for diabetes diagnosis</text></passage><passage><infon key="fpage">e55</infon><infon key="issue">3</infon><infon key="name_0">surname:Burns;given-names:MN</infon><infon key="name_1">surname:Begale;given-names:M</infon><infon key="name_2">surname:Duffecy;given-names:J</infon><infon key="name_3">surname:Gergle;given-names:D</infon><infon key="name_4">surname:Karr;given-names:CJ</infon><infon key="name_5">surname:Giangrande;given-names:E</infon><infon key="name_6">surname:Mohr;given-names:DC</infon><infon key="pub-id_doi">10.2196/jmir.1838</infon><infon key="pub-id_medline">21840837</infon><infon key="pub-id_pmid">21840837</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2011</infon><offset>43250</offset><text>Harnessing context sensing to develop a mobile intervention for depression</text></passage><passage><infon key="fpage">e12641</infon><infon key="issue">4</infon><infon key="name_0">surname:Percha;given-names:B</infon><infon key="name_1">surname:Baskerville;given-names:EB</infon><infon key="name_2">surname:Johnson;given-names:M</infon><infon key="name_3">surname:Dudley;given-names:JT</infon><infon key="name_4">surname:Zimmerman;given-names:N</infon><infon key="pub-id_doi">10.2196/12641</infon><infon key="pub-id_medline">30932871</infon><infon key="pub-id_pmid">30932871</infon><infon key="section_type">REF</infon><infon key="source">J Med Internet Res</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>43325</offset><text>Designing robust N-of-1 studies for precision medicine: Simulation study and design recommendations</text></passage><passage><infon key="fpage">20382</infon><infon key="name_0">surname:Probst;given-names:T</infon><infon key="name_1">surname:Pryss;given-names:R</infon><infon key="name_2">surname:Langguth;given-names:B</infon><infon key="name_3">surname:Schlee;given-names:W</infon><infon key="pub-id_doi">10.1038/srep20382</infon><infon key="pub-id_medline">26853815</infon><infon key="pub-id_pmid">26853815</infon><infon key="section_type">REF</infon><infon key="source">Sci Rep</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>43425</offset><text>Emotional states as mediators between tinnitus loudness and tinnitus distress in daily life: Results from the &quot;TrackYourTinnitus&quot; application</text></passage><passage><infon key="fpage">209</infon><infon key="lpage">220</infon><infon key="name_0">surname:Schlee;given-names:W</infon><infon key="name_1">surname:Kraft;given-names:R</infon><infon key="name_2">surname:Schobel;given-names:J</infon><infon key="name_3">surname:Langguth;given-names:B</infon><infon key="name_4">surname:Probst;given-names:T</infon><infon key="name_5">surname:Neff;given-names:P</infon><infon key="name_6">surname:Reichert;given-names:M</infon><infon key="name_7">surname:Pryss;given-names:R</infon><infon key="name_8">surname:Baumeister;given-names:H</infon><infon key="name_9">surname:Montag;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Digital Phenotyping and Mobile Sensing: New Developments in Psychoinformatics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>43567</offset><text>Momentary assessment of tinnitus—How smart mobile applications advance our understanding of tinnitus</text></passage><passage><infon key="fpage">31166</infon><infon key="name_0">surname:Probst;given-names:T</infon><infon key="name_1">surname:Pryss;given-names:R</infon><infon key="name_2">surname:Langguth;given-names:B</infon><infon key="name_3">surname:Schlee;given-names:W</infon><infon key="pub-id_doi">10.1038/srep31166</infon><infon key="pub-id_medline">27488227</infon><infon key="pub-id_pmid">27488227</infon><infon key="section_type">REF</infon><infon key="source">Sci Rep</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>43670</offset><text>Emotion dynamics and tinnitus: Daily life data from the &quot;TrackYourTinnitus&quot; application</text></passage><passage><infon key="fpage">253</infon><infon key="name_0">surname:Probst;given-names:T</infon><infon key="name_1">surname:Pryss;given-names:RC</infon><infon key="name_2">surname:Langguth;given-names:B</infon><infon key="name_3">surname:Rauschecker;given-names:JP</infon><infon key="name_4">surname:Schobel;given-names:J</infon><infon key="name_5">surname:Reichert;given-names:M</infon><infon key="name_6">surname:Spiliopoulou;given-names:M</infon><infon key="name_7">surname:Schlee;given-names:W</infon><infon key="name_8">surname:Zimmermann;given-names:J</infon><infon key="pub-id_doi">10.3389/fnagi.2017.00253</infon><infon key="pub-id_medline">28824415</infon><infon key="pub-id_pmid">28824415</infon><infon key="section_type">REF</infon><infon key="source">Front Aging Neurosci</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2017</infon><offset>43758</offset><text>Does tinnitus depend on time-of-day? An ecological momentary assessment study with the &quot;TrackYourTinnitus&quot; application</text></passage></document></collection>
