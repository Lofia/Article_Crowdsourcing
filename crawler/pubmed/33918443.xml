<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210503</date><key>pmc.key</key><document><id>8038307</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/s21072477</infon><infon key="article-id_pmc">8038307</infon><infon key="article-id_pmid">33918443</infon><infon key="article-id_publisher-id">sensors-21-02477</infon><infon key="elocation-id">2477</infon><infon key="issue">7</infon><infon key="kwd">HD map crowdsourcing update semantic segmentation visual SLAM autonomous driving</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Zhang;given-names:Pan</infon><infon key="name_1">surname:Zhang;given-names:Mingming</infon><infon key="name_2">surname:Liu;given-names:Jingnan</infon><infon key="name_3">surname:Nurmi;given-names:Jari</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">21</infon><infon key="year">2021</infon><offset>0</offset><text>Real-Time HD Map Change Detection for Crowdsourcing Update Based on Mid-to-High-End Sensors</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>92</offset><text>Continuous maintenance and real-time update of high-definition (HD) maps is a big challenge. With the development of autonomous driving, more and more vehicles are equipped with a variety of advanced sensors and a powerful computing platform. Based on mid-to-high-end sensors including an industry camera, a high-end Global Navigation Satellite System (GNSS)/Inertial Measurement Unit (IMU), and an onboard computing platform, a real-time HD map change detection method for crowdsourcing update is proposed in this paper. First, a mature commercial integrated navigation product is directly used to achieve a self-positioning accuracy of 20 cm on average. Second, an improved network based on BiSeNet is utilized for real-time semantic segmentation. It achieves the result of 83.9% IOU (Intersection over Union) on Nvidia Pegasus at 31 FPS. Third, a visual Simultaneous Localization and Mapping (SLAM) associated with pixel type information is performed to obtain the semantic point cloud data of features such as lane dividers, road markings, and other static objects. Finally, the semantic point cloud data is vectorized after denoising and clustering, and the results are matched with a pre-constructed HD map to confirm map elements that have not changed and generate new elements when appearing. The experiment conducted in Beijing shows that the method proposed is effective for crowdsourcing update of HD maps.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1510</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1526</offset><text>Autonomous vehicles use various sensors to achieve different levels of autonomy (L1-L5, e.g., see), such as cameras, Global Navigation Satellite System (GNSS), Radio Detection and Ranging (RADAR), Light Detection and Ranging (LIDAR). However, these sensors have a limited perception range, and they are very vulnerable to bad weather. To overcome the limitations, the pre-built digital map can be utilized to improve perception and robustness. Many autonomous vehicle prototypes rely on precise 3D maps, which are also called high-definition (HD) maps. An HD map is a precise map with rich lane-level information for autonomous driving. It can provide prior information robustly about the static environment in a range of more than 200 m ahead or around corners. The features in the map can be fused with the recognition results from camera/LIDAR to realize high accuracy localization of the vehicle.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2427</offset><text>Compared with the car navigation map, an HD map greatly improves the accuracy to a few centimeters level. It also has richer and more detailed content, such as lane boundaries, lane centerlines, road markings on the ground, and guardrails on both sides of the road. Lane boundaries have many attributes in HD maps, such as type, color, and width. Therefore, HD maps reflect a more realistic and detailed real world, containing a lot of subtle changes. For example, the lane boundary is re-brushed, and arrows are added to the road surface.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2967</offset><text>At present, the production of HD maps requires professional data collection, that is professional surveying and mapping using the Mobile Mapping System (MMS). Then, HD maps are constructed from the road images and 3D point cloud data. The entire data collection and production takes a long time. In addition, the professional survey fleet is very expensive to set up. All of these make it difficult to update the HD map in real-time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3401</offset><text>Therefore, more and more researchers focus on crowdsourcing update of HD maps. In the future, driverless cars will be no different from professional survey cars as they are equipped with similar sensors. So, when they are driving, they will also be collecting data. The collected data of every car can be aggregated and then used to update HD maps. This is the concept of crowdsourcing updates of HD maps. Once the map update is completed in the cloud, the update package can be passed back to the vehicles. Then real-time HD map updates and services can be realized.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3969</offset><text>Ref uses a single front-facing camera, a consumer-grade GNSS/IMU, a Qualcomm Snapdragon 820A SoC in the vehicle, and a backend mapping server to realize the crowdsourcing update of traffic signs and lane boundaries. Every single-journey perception data and triangulation outputs are shipped over a commercial LTE link to the backend mapping server. Thus, the amount of data transmitted by the network is very large. proposed a generation method of new feature layers in the accuracy level of the HD map using the existing HD map and crowd-sourced information without additional costs. The generated new feature layer is uploaded to the map cloud by the mobile network. The amount of data transmitted is small, but the computing power requirements on the terminal are high. In addition, it focuses on the new feature types that have not appeared on the HD map.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4829</offset><text>First, what kind of sensors are there in the car, and what is the accuracy range of these sensors? For example, is the LIDAR included, and what is the accuracy of GNSS?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4998</offset><text>Second, whether the full amount of sensing raw data is uploaded to the cloud, only some keyframe data is uploaded, or just the recognized results are uploaded.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5158</offset><text>Third, is there an HD map on the end? If yes, whether the difference data between the HD map and real-time environment perception needs to be uploaded.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5310</offset><text>Therefore, there are many different strategies based on different sensors and processing methods for the crowdsourcing update solution. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5447</offset><text>There are no very definite answers to these questions. However, there is no doubt that a reasonable sensor configuration and the corresponding processing method are very important for large-scale crowdsourcing update in order to ensure efficiency and reduce the amount of transmission data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5738</offset><text>Currently, most social vehicles are not equipped with advanced sensors, only cameras for dashcam and low-precision positioning system for navigation. There is no strong perceptual computing power to process the original image. However, in recent years, mass-produced cars with automatic driving above L2 level, such as Audi A8 and BMW iNext, are not only equipped with cameras, millimeter-wave radar, and other sensors, but also equipped with a high-precision positioning system, HD maps, as well as chips or domain controllers with powerful real-time sensing computing power. Therefore, crowdsourcing map updating based on mid-high-end sensors is becoming more feasible and will be the trend in the future. In this paper, based on these mid-high-end sensors, vectorized real-time perception data is generated after real-time semantic segmentation, SLAM and other key technical processing. Then through the matching between the vectorized data and the pre-constructed HD map, map elements that have not changed are confirmed and new elements that are not on the map but in the real world are generated. This also means that real-time HD map change detection is realized for crowdsourcing update.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>6934</offset><text>2. The Architecture</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6954</offset><text>This paper proposes a real-time HD map change detection method in the vehicle terminal based on an industry camera, a high-end GNSS/IMU, and a high-performance onboard computing platform. Semantic SLAM (Simultaneous Localization and Mapping) technology is mainly used to obtain semantic point cloud data of lanes and other features based on an improved BiSeNet network. Then, the semantic point data is vectorized and matched with the HD map to detect differences. The proposed architecture is shown in Figure 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7467</offset><text>Localization. Localization is one of the major subsystems of autonomous vehicles. Currently, the main sensors used for localization on vehicles are GNSS, IMU, cameras, and Controller Area Network Bus (CAN Bus) information such as wheel odometer. GNSS and IMU are commonly used to form an inertial navigation system. In this paper, NovAtel SPAN-IGM-A1, a commercial integrated navigation product, is directly used to simulate the high-end positioning system on the vehicle. When using RTK (real-time kinematic) mode, it can provide a self -positioning accuracy of 20 cm on average. In this paper, we do not address this module.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8094</offset><text>Image semantic segmentation. Semantic segmentation amounts to assign semantic labels to each pixel. With the development of deep learning, some networks could achieve good performance in semantic segmentation. BiSeNet is one of the best real-time semantic segmentation networks in recent years. Thus, it is chosen to recognize the images collected by a single front-facing camera. In fact, this paper optimizes the BiSeNet network to improve recognition performance. NVIDIA DRIVE AGX Pegasus™ is utilized to simulate the onboard high-performance computing platform. Static features such as lane boundaries, road markings, traffic signs, and moving objects such as vehicles can be recognized with a relative high IOU (Intersection over Union). IOU is a commonly used measure for determining how accurate a proposed image segmentation is, compared to a ground-truth segmentation. Section 3 covers the image semantic segmentation in detail.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9034</offset><text>Semantic visual SLAM. SLAM aims to self-localize a robot and estimate a model of its environment from sensory information. The framework of the visual SLAM system is quite mature, which is generally composed of several essential parts such as front-end, back-end, and loop closure detection. Some advanced SLAM algorithms have already attained satisfactory performance, such as feature-based ORB-SLAM2, direct method LSD-SLAM, and semi-direct visual odometer method (SVO). Vision SLAM can produce geometric maps composed of points or edges but without any associated meaning or semantic content. As every pixel in the image has a known type after semantic segmentation, the usual SLAM maps can be enriched by associating the geometric estimation with object information. This process is called semantic SLAM. In addition, the adaptability of visual SLAM to the dynamic scene is generally poor because of the limitations of the sparse image features. The constructed map often contains moving objects. Due to the influence of moving objects, there will be residual shadows of moving objects on the map. The semantic segmentation results can be used to remove these moving objects in the dynamic scenes to improve the quality of SLAM maps. Through the implementation of semantic visual SLAM technology, we can get semantic point cloud data with the spatial location of features such as lane boundaries, road markings. The details of semantic visual SLAM are described in Section 4.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10514</offset><text>Vectorization and HD map matching. The semantic point cloud data is vectorized after denoising and clustering. The KD-tree and RANSAC algorithms are used. Then these vectorization results are matched with the local HD map to detect changes. The details of vectorization and HD map matching are given in Section 5.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10828</offset><text>According to the architecture, the system comprises several modules as follows. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>10909</offset><text>3. Semantic Segmentation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10934</offset><text>NVIDIA DRIVE AGX Pegasus™ is used to simulate the vehicle-mounted high-performance computing platform. It achieves a 320 TOPS (Tera Operations Per Second) of supercomputer. Its next generation product will increase the computing power several times. Therefore, it is foreseeable that the on-board computing power will continue to increase. This also means that more and more processing can be done on the end, such as images semantic segmentation, object detection, and so on.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11413</offset><text>BiSeNet is a real-time semantic segmentation network proposed by Megvii Technology on ECCV2018. Using Res18 as a base model, the fast version of BiSeNet achieved the result of 74.8% Mean IOU (mIOU) on the CitiScapes verification dataset at 65.5 FPS. Our application scenarios require far less real-time performance of semantic segmentation than 60 fps, so we can improve the network to reduce the real-time performance, but increase the segmentation accuracy.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11873</offset><text>The original network uses the method of upsampling 8 times, 8 times, and 16 times in the last three output layers to directly restore the original size. It is modified to restore the image size by 4 times, 4 times, and 8 times through deconvolution. Finally, the original image size is restored by 2 times upsampling directly.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12200</offset><text>The classic attention idea is used in the original network, that is, average global pooling is utilized to obtain a sizeable receptive field. After our optimization, local attention and multi-scale attention are used to further improve the segmentation performance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12466</offset><text>The original BiSeNet consists of two parts: Spatial Path (SP) and Context Path (CP). These two components are used to confront with the loss of spatial information and shrinkage of the receptive field. Spatial Path is designed to retain the spatial information of the original image. Context Path utilizes the lightweight model and global average pooling to quickly obtain a large receptive field. We have made targeted optimizations to these two parts. The improved network architecture is shown in Figure 2. The details of the improvement are as follows. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13024</offset><text>The modifications are shown in Figure 2, such as the abbreviation “deconv” for deconvolution. The key point is that we use deconvolution for restoring the original size of the image. The realization of deconvolution needs to obtain parameters through learning, so as to achieve higher accuracy. The original BiSeNet uses interpolation for upsampling directly. The advantage is that it is fast and does not require to obtain parameters. The disadvantage is that the accuracy is lower than deconvolution.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13531</offset><text>Regarding the loss function, the original BiSeNet uses Softmax loss and we adopt focal-loss. Softmax loss is essentially a kind of cross-entropy loss function. Focal-loss adds weight on the basis of cross-entropy and solves the problem of sample imbalance. So focal-loss performs better in accuracy.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13831</offset><text>The first step of focal-loss is the cross-entropy loss function for binary classification, defined as:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13934</offset><text>In the above,  specifies the ground-truth class and  indicates the model’s estimated probability for the class with label . For simplicity, we define :</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14088</offset><text>Thus, </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14095</offset><text>Then in order to address class imbalance, a weighting factor  is introduced:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14172</offset><text>For simplicity, we define  like we defined . </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14218</offset><text>Then a modulating factor  is added to the cross-entropy loss for reducing the loss contribution from easily classified samples: , which is a tunable focusing parameter. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14388</offset><text>Thus, the focal loss is defined as:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14424</offset><text>The data set for model training and verification comes from the image data collected by Kuandeng Technology. The original image resolution is 2048 × 2448, and there are more than 80 types of labeling categories. The data set has a total of 11,830 images, including 10,597 images in the training set and 1240 images in the verification set. The recognition results of the main categories are shown in the Table 1. The average IOU reaches 83.90%. While the average IOU of the original BiSeNet is 76.8%, which is very close to its results on the CitiScapes dataset. Thus, after our optimization, the accuracy has been improved. The IOU of objects on the road such as vehicles reaches 96.99%. The recognition performance of the stop line is relatively poor. It can be seen from the confusion matrix that the stop line is mainly misidentified as a road surface. In terms of inference speed, the improved BiSeNet’s inference speed on Pegasus is about 31 fps, which is lower than the original BiSeNet but already meets our real-time requirements. An example of image semantic segmentation by the improved BiSeNet network is shown in Figure 3.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>15563</offset><text>4. Semantic SLAM</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15580</offset><text>After semantic segmentation, each pixel in the image is labeled with semantic tags. Then the dynamic vehicles and other outliers in the image can be filtered in the process of tracking in the dynamic environment. The meaning of the static features can be associated with points after visual SLAM mapping.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15885</offset><text>The front-end of SLAM is also called visual odometer (VO), which tracks the camera’s position and pose through the geometric relationship between multiple views. Because semi-direct VO (SVO) can combine the success-factors of feature-based methods (tracking many features) with the accuracy and speed of direct methods, it is adopted as the front end of our Visual SLAM system. As a mature and open-source method, the detailed implementation of SVO can be found in.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16353</offset><text>With the continuous increase of images and the continuous operation of the SLAM system, the observation error of each frame will accumulate to the next. Thus, the measurement error will continue to accumulate. Therefore, it is very important for the SLAM system to optimize trajectory on the back-end. BA (bundle adjustment) based on graph optimization is commonly used for global optimization. The open-source graph optimization library g2o (general graph optimization) that contains the implementation of BA is utilized in this process.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16892</offset><text>When we know the camera pose from the motion estimation, the depth at a single pixel can be estimated from multiple observations by means of a recursive Bayesian depth filter. From the pixel with highest correlation in the epipolar line, the depth measurement is triangulated to update the depth filter. For forward motions, it is beneficial to update the depth filters with the previous frame. While in the older version of SVO, the depth filter is only updated with newer frames, which works well for down-looking cameras in micro aerial vehicle applications. In fact, whether it is SVO or feature-based SLAM, such as OBR-SLAM2, triangulation is the most basic depth estimation method. Through the triangulation method, all the keyframe points can be transformed into a unified coordinate system through the corresponding perspective transformation matrix, so as to generate the point cloud map.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>17790</offset><text>5. Vectorization and Matching with the HD Map</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17836</offset><text>The vectorization of semantic point cloud data consists of three steps as shown in Figure 4. The first step is denoising. As the pixel accuracy of semantic segmentation is not 100%, the noise generated needs to be filtered out. After SLAM processing, the semantic point cloud is attached to the spatial position. Thus, the Euclidean distance measure can be used for KD-Tree construction to find n points near one point and then discard the points with long distance. The second is clustering. The Euclidean cluster extraction algorithm is utilized to cluster the denoised point cloud data for every object. Some examples of clustering are shown in Figure 5. We can see that the performance is acceptable. Finally, vectorization is implemented. The minimum bounding box is calculated for the surface element as its geometry. An RANSAC algorithm is used for curve fitting for line elements. These algorithms are not the focus of this article, so they are not described in detail. In the process of implementation, an open-source Point Cloud Library (PCL) is used directly, which provides a lot of general point-cloud-related algorithms and efficient data structures.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19001</offset><text>After denoising, clustering, and vectorization, the vectorized results are generated. For lane dividers, line-to-line matching is needed. The matching degree is normally equivalent to the similarity calculated through distance and angle. For polygon elements like arrows, the matching degree is evaluated by the proportion of the area covered. The matching between the vectorized polygon and the HD map is illustrated by an experiment conducted on the Fifth Ring Road in Beijing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19481</offset><text>As shown in Figure 6 and Figure 7, the green point group is the semantic point cloud data after semantic visual SLAM. The green rectangle represents the bounding box of an arrow in the pre-constructed HD map. When the car is moving, the semantic point cloud continues to grow. The vectorized results extracted from the semantic point cloud can be used to either confirm the existence of map features or generate new ones when new map features appear. In order to do the experiment, some arrows were deleted from the pre-constructed HD map.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20021</offset><text>In Figure 6, when the vehicle ran through a lane, the semantic point cloud is used to confirm the existence of arrows on the ground. It is indicated by a change of color from green to blue as shown in the middle and lower parts of Figure 6.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20262</offset><text>In Figure 7, the semantic point cloud is used to add the missing arrows, which is indicated by the new addition of red color as shown in the lower part of Figure 7.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20427</offset><text>Obviously, there is a difference in precision between the extracted features from semantic point cloud data and the map features. In order to find the corresponding map features, the matching degree between these two should be calculated. As shown in Figure 8, the green rectangle represents the bounding box of a polygon map element (e.g., an arrow), and the red represents the extracted element. The yellow part of their overlap is the area that is really matched.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20894</offset><text>The definition and calculation formula of the matching degree is as follows. That is, the matching degree is equal to the area of the overlapping divided by the area of the map element bounding box. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21094</offset><text>The matching degree can reflect the accuracy of the semantic point cloud data. The statistical distribution of the matching degree is shown in Figure 9. The X axis represents the matching degree. The matching degrees from 0 to 1 are divided into intervals according to a fixed range 0.05. The Y axis represents the counts in each matching degree interval. As we can see, no matching degree is between 0 and 10%. Actually, the minimum matching degree is 14.6% if matching is successful. This can indicate that features that are not matched should not be included in the map data. In other words, the case shown in Figure 10 is unlikely to happen. By the way, the length of arrows is 6 m. This conclusion is crucial to the application of crowdsourcing data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21850</offset><text>Regarding the data volume of the semantic point cloud, the average data volume per second is 1126 KB and the average data volume per kilometer is 56,704 KB. After vectorization and matching with the HD map, the difference data is identified, which has a much smaller amount of data for transmission. The amount of the difference data depends on the changes between the old HD map and the real-time reality. The more changes, the greater the amount of the different data. In our experiment, the average amount of the different data is 126 KB per kilometer, which is much less than the amount of the whole data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22460</offset><text>In terms of accuracy and the amount of transmitted data, the results show that the sensors and methods proposed in this paper are effective for crowdsourcing update.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>22626</offset><text>6. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>22641</offset><text>With the unlimited range of environment perception and other advantages, the HD map is widely considered to be an essential part of autonomous driving. However, due to the complex and expensive data collection and production of HD maps, continuous maintenance and real-time updates have become a huge challenge. With the development and maturity of autonomous driving technology, more and more vehicles are equipped with a variety of advanced sensors and a powerful computing platform. Therefore, the crowdsourcing update of HD maps based on mid-to-high-end sensors is becoming more feasible.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>23234</offset><text>In this context, this article uses a mature commercial GNSS/IMU integrated navigation device, an industrial camera, and NVIDIA Pegasus with GPU (Graphics Processing Unit) to launch the research. The main method is to perform real-time semantic segmentation of images based on the improved BiSeNet network and then fuse the results with visual SLAM to obtain semantic point cloud data. After denoising, clustering, and vectorization, the vectorized results are extracted from the semantic point cloud data and then matched with a pre-constructed HD map. The map elements that have not changed can be confirmed and the elements that have changes can be detected. In the experiment, the existence of arrows in the HD map is confirmed and new arrows are generated. In summary, real-time HD map change detection is realized and validated, which also demonstrates the feasibility and significant value of crowdsourcing update for HD maps.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>24167</offset><text>Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>24293</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>24314</offset><text>Conceptualization, P.Z. and J.L.; methodology, P.Z.; software, P.Z. and M.Z.; validation, P.Z. and M.Z.; formal analysis, P.Z.; investigation, P.Z. and M.Z.; resources, P.Z.; data curation, P.Z. and M.Z.; writing—original draft preparation, P.Z.; writing—review and editing, J.L.; visualization, P.Z. and M.Z.; supervision, J.L.; project administration, P.Z.; funding acquisition, J.L. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>24781</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>24789</offset><text>This research was funded by the Strategy Research Project on “Beidou+5G” Technology and Industry Development in 2020 under Chinese Engineering Technology Development Strategy Hubei Research Institute, grant number HB2020B13.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>25018</offset><text>Institutional Review Board Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>25055</offset><text>Not applicable.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>25071</offset><text>Informed Consent Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>25098</offset><text>Not applicable.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>25114</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>25142</offset><text>Publicly available datasets were analyzed in this study. This data can be found here:  (accessed on 1 April 2021).</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>25257</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>25279</offset><text>The authors declare no conflict of interest.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>25324</offset><text>References</text></passage><passage><infon key="comment">Available online: https://saemobilus.sae.org/content/J3016_201806</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>25335</offset><text>SAE Standard J3016_201806: Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles</text></passage><passage><infon key="fpage">163</infon><infon key="lpage">168</infon><infon key="name_0">surname:Levinson;given-names:J.</infon><infon key="name_1">surname:Askeland;given-names:J.</infon><infon key="name_2">surname:Becker;given-names:J.</infon><infon key="name_3">surname:Dolson;given-names:J.</infon><infon key="name_4">surname:Held;given-names:D.</infon><infon key="name_5">surname:Kammel;given-names:S.</infon><infon key="name_6">surname:Kolter;given-names:J.Z.</infon><infon key="name_7">surname:Langer;given-names:D.</infon><infon key="name_8">surname:Pink;given-names:O.</infon><infon key="name_9">surname:Pratt;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 IEEE Intelligent Vehicles Symposium (IV)</infon><infon key="type">ref</infon><offset>25462</offset><text>Towards fully autonomous driving: Systems and algorithms</text></passage><passage><infon key="fpage">8</infon><infon key="lpage">20</infon><infon key="name_0">surname:Ziegler;given-names:J.</infon><infon key="name_1">surname:Bender;given-names:P.</infon><infon key="name_2">surname:Schreiber;given-names:M.</infon><infon key="name_3">surname:Lategahn;given-names:H.</infon><infon key="name_4">surname:Strauss;given-names:T.</infon><infon key="name_5">surname:Stiller;given-names:C.</infon><infon key="name_6">surname:Dang;given-names:T.</infon><infon key="name_7">surname:Franke;given-names:U.</infon><infon key="name_8">surname:Appenrodt;given-names:N.</infon><infon key="name_9">surname:Keller;given-names:C.G.</infon><infon key="pub-id_doi">10.1109/MITS.2014.2306552</infon><infon key="section_type">REF</infon><infon key="source">IEEE Intell. Transp. Syst. Mag.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2014</infon><offset>25519</offset><text>Making Bertha Drive—An Autonomous Journey on a Historic Route</text></passage><passage><infon key="fpage">449</infon><infon key="lpage">454</infon><infon key="name_0">surname:Schreiber;given-names:M.</infon><infon key="name_1">surname:Knoppel;given-names:C.</infon><infon key="name_2">surname:Franke;given-names:U.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2013 IEEE Intelligent Vehicles Symposium (IV)</infon><infon key="type">ref</infon><offset>25583</offset><text>LaneLoc: Lane marking based localization using highly accurate maps</text></passage><passage><infon key="fpage">28</infon><infon key="lpage">33</infon><infon key="name_0">surname:Strijbosch;given-names:W.</infon><infon key="pub-id_doi">10.1007/s38311-018-0166-9</infon><infon key="section_type">REF</infon><infon key="source">ATZ Worldw.</infon><infon key="type">ref</infon><infon key="volume">120</infon><infon key="year">2018</infon><offset>25651</offset><text>Safe Autonomous Driving with High-definition Maps</text></passage><passage><infon key="fpage">19</infon><infon key="lpage">29</infon><infon key="name_0">surname:Joshi;given-names:A.</infon><infon key="name_1">surname:James;given-names:M.R.</infon><infon key="pub-id_doi">10.1109/MITS.2014.2364081</infon><infon key="section_type">REF</infon><infon key="source">IEEE Intell. Transp. Syst. Mag.</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2015</infon><offset>25701</offset><text>Generation of Accurate Lane-Level Maps from Coarse Prior Maps and Lidar</text></passage><passage><infon key="fpage">1035</infon><infon key="lpage">1042</infon><infon key="name_0">surname:Chen;given-names:A.</infon><infon key="name_1">surname:Ramanandan;given-names:A.</infon><infon key="name_2">surname:Farrell;given-names:J.A.</infon><infon key="pub-id_doi">10.1109/plans.2010.5507331</infon><infon key="section_type">REF</infon><infon key="source">IEEE/ION Position Locat. Navig. Symp.</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>25773</offset><text>High-precision lane-level road map building for vehicle navigation</text></passage><passage><infon key="fpage">634</infon><infon key="lpage">641</infon><infon key="name_0">surname:Dabeer;given-names:O.</infon><infon key="name_1">surname:Ding;given-names:W.</infon><infon key="name_2">surname:Gowaiker;given-names:R.</infon><infon key="name_3">surname:Grzechnik;given-names:S.K.</infon><infon key="name_4">surname:Lakshman;given-names:M.J.</infon><infon key="name_5">surname:Lee;given-names:S.</infon><infon key="name_6">surname:Reitmayr;given-names:G.</infon><infon key="name_7">surname:Sharma;given-names:A.</infon><infon key="name_8">surname:Somasundaram;given-names:K.</infon><infon key="name_9">surname:Sukhavasi;given-names:R.T.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</infon><infon key="type">ref</infon><offset>25840</offset><text>An end-to-end system for crowdsourced 3D maps for autonomous vehicles: The mapping component</text></passage><passage><infon key="elocation-id">4172</infon><infon key="name_0">surname:Kim;given-names:C.</infon><infon key="name_1">surname:Cho;given-names:S.</infon><infon key="name_2">surname:Sunwoo;given-names:M.</infon><infon key="name_3">surname:Jo;given-names:K.</infon><infon key="pub-id_doi">10.3390/s18124172</infon><infon key="pub-id_pmid">30487399</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>25933</offset><text>Crowd-Sourced Mapping of New Feature Layer for High-Definition Map</text></passage><passage><infon key="fpage">334</infon><infon key="lpage">349</infon><infon key="name_0">surname:Yu;given-names:C.</infon><infon key="name_1">surname:Wang;given-names:J.</infon><infon key="name_2">surname:Peng;given-names:C.</infon><infon key="name_3">surname:Gao;given-names:C.</infon><infon key="name_4">surname:Yu;given-names:G.</infon><infon key="name_5">surname:Sang;given-names:N.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Formal Concept Analysis</infon><infon key="type">ref</infon><offset>26000</offset><text>BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation</text></passage><passage><infon key="comment">Available online: https://novatel.com/support/span-gnss-inertial-navigation-systems/span-combined-systems/span-igm-a1</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26076</offset><text>SPAN-IGM-A1</text></passage><passage><infon key="fpage">2481</infon><infon key="lpage">2495</infon><infon key="name_0">surname:Badrinarayanan;given-names:V.</infon><infon key="name_1">surname:Kendall;given-names:A.</infon><infon key="name_2">surname:Cipolla;given-names:R.</infon><infon key="pub-id_doi">10.1109/TPAMI.2016.2644615</infon><infon key="pub-id_pmid">28060704</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Pattern Anal. Mach. Intell.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2017</infon><offset>26088</offset><text>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</text></passage><passage><infon key="fpage">99</infon><infon key="lpage">110</infon><infon key="name_0">surname:Durrant-Whyte;given-names:H.</infon><infon key="name_1">surname:Bailey;given-names:T.</infon><infon key="pub-id_doi">10.1109/MRA.2006.1638022</infon><infon key="section_type">REF</infon><infon key="source">Robot. Autom. Mag.</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2006</infon><offset>26169</offset><text>Simultaneous localisation and mapping (SLAM): Part I the essential algorithms</text></passage><passage><infon key="fpage">1168</infon><infon key="lpage">1174</infon><infon key="name_0">surname:Yu;given-names:C.</infon><infon key="name_1">surname:Liu;given-names:Z.</infon><infon key="name_2">surname:Liu;given-names:X.-J.</infon><infon key="name_3">surname:Xie;given-names:F.</infon><infon key="name_4">surname:Yang;given-names:Y.</infon><infon key="name_5">surname:Wei;given-names:Q.</infon><infon key="name_6">surname:Fei;given-names:Q.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</infon><infon key="type">ref</infon><offset>26247</offset><text>DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments</text></passage><passage><infon key="fpage">1255</infon><infon key="lpage">1262</infon><infon key="name_0">surname:Mur-Artal;given-names:R.</infon><infon key="name_1">surname:Tardos;given-names:J.D.</infon><infon key="pub-id_doi">10.1109/TRO.2017.2705103</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Robot.</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2017</infon><offset>26308</offset><text>ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</text></passage><passage><infon key="fpage">834</infon><infon key="lpage">849</infon><infon key="name_0">surname:Engel;given-names:J.</infon><infon key="name_1">surname:Schops;given-names:T.</infon><infon key="name_2">surname:Cremers;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Computer Vision</infon><infon key="type">ref</infon><offset>26387</offset><text>LSD-SLAM: Large-scaledirect monocular SLAM</text></passage><passage><infon key="fpage">249</infon><infon key="lpage">265</infon><infon key="name_0">surname:Forster;given-names:C.</infon><infon key="name_1">surname:Zhang;given-names:Z.</infon><infon key="name_2">surname:Gassner;given-names:M.</infon><infon key="name_3">surname:Werlberger;given-names:M.</infon><infon key="name_4">surname:Scaramuzza;given-names:D.</infon><infon key="pub-id_doi">10.1109/TRO.2016.2623335</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Robot.</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2016</infon><offset>26430</offset><text>SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems</text></passage><passage><infon key="fpage">1277</infon><infon key="lpage">1284</infon><infon key="name_0">surname:Civera;given-names:J.</infon><infon key="name_1">surname:Gálvez-López;given-names:D.</infon><infon key="name_2">surname:Riazuelo;given-names:L.</infon><infon key="name_3">surname:Tardós;given-names:J.D.</infon><infon key="name_4">surname:Montiel;given-names:J.M.M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</infon><infon key="type">ref</infon><offset>26500</offset><text>Towards semantic SLAM using a monocular camera</text></passage><passage><infon key="fpage">4628</infon><infon key="lpage">4635</infon><infon key="name_0">surname:McCormac;given-names:J.</infon><infon key="name_1">surname:Handa;given-names:A.</infon><infon key="name_2">surname:Davison;given-names:A.</infon><infon key="name_3">surname:Leutenegger;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA)</infon><infon key="type">ref</infon><offset>26547</offset><text>SemanticFusion: Dense 3D semantic mapping with convolutional neural networks</text></passage><passage><infon key="comment">Available online: https://nvidianews.nvidia.com/news/nvidia-introduces-drive-agx-orin-advanced-software-defined-platform-for-autonomous-machines</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26624</offset><text>NVIDIA Introduces DRIVE AGX Orin—Advanced, Software-Defined Platform for Autonomous Machines</text></passage><passage><infon key="name_0">surname:Lin;given-names:T.-Y.</infon><infon key="name_1">surname:Goyal;given-names:P.</infon><infon key="name_2">surname:Girshick;given-names:R.</infon><infon key="name_3">surname:He;given-names:K.</infon><infon key="name_4">surname:Dollar;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</infon><infon key="type">ref</infon><offset>26719</offset><text>Focal Loss for Dense Object Detection</text></passage><passage><infon key="fpage">19</infon><infon key="lpage">67</infon><infon key="name_0">surname:De Boer;given-names:P.-T.</infon><infon key="name_1">surname:Kroese;given-names:D.P.</infon><infon key="name_2">surname:Mannor;given-names:S.</infon><infon key="name_3">surname:Rubinstein;given-names:R.Y.</infon><infon key="pub-id_doi">10.1007/s10479-005-5724-z</infon><infon key="section_type">REF</infon><infon key="source">Ann. Oper. Res.</infon><infon key="type">ref</infon><infon key="volume">134</infon><infon key="year">2005</infon><offset>26757</offset><text>A Tutorial on the Cross-Entropy Method</text></passage><passage><infon key="fpage">298</infon><infon key="lpage">372</infon><infon key="name_0">surname:Triggs;given-names:B.</infon><infon key="name_1">surname:Mclauchlan;given-names:P.F.</infon><infon key="name_2">surname:Hartley;given-names:R.I.</infon><infon key="name_3">surname:FitzGibbon;given-names:A.W.</infon><infon key="section_type">REF</infon><infon key="source">International Workshop on Vision Algorithms</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>26796</offset><text>Bundle Adjustment—A Modern Synthesis</text></passage><passage><infon key="fpage">1066</infon><infon key="lpage">1077</infon><infon key="name_0">surname:Konolige;given-names:K.</infon><infon key="name_1">surname:Agrawal;given-names:M.</infon><infon key="pub-id_doi">10.1109/TRO.2008.2004832</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Robot.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2008</infon><offset>26835</offset><text>FrameSLAM: From Bundle Adjustment to Real-Time Visual Mapping</text></passage><passage><infon key="fpage">15</infon><infon key="lpage">22</infon><infon key="name_0">surname:Forster;given-names:C.</infon><infon key="name_1">surname:Pizzoli;given-names:M.</infon><infon key="name_2">surname:Scaramuzza;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2014 IEEE International Conference on Robotics and Automation (ICRA)</infon><infon key="type">ref</infon><offset>26897</offset><text>SVO: Fast semi-direct monocular visual odometry</text></passage><passage><infon key="name_0">surname:Greenspan;given-names:M.</infon><infon key="name_1">surname:Yurick;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Fourth International Conference on 3-D Digital Imaging and Modeling</infon><infon key="type">ref</infon><offset>26945</offset><text>Approximate K-D tree search for efficient ICP</text></passage><passage><infon key="fpage">1472</infon><infon key="lpage">1482</infon><infon key="name_0">surname:Chum;given-names:O.</infon><infon key="name_1">surname:Matas;given-names:J.</infon><infon key="pub-id_doi">10.1109/TPAMI.2007.70787</infon><infon key="pub-id_pmid">18566499</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Pattern Anal. Mach. Intell.</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2008</infon><offset>26991</offset><text>Optimal Randomized RANSAC</text></passage><passage><infon key="file">sensors-21-02477-g001.jpg</infon><infon key="id">sensors-21-02477-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27017</offset><text>The architecture.</text></passage><passage><infon key="file">sensors-21-02477-g002.jpg</infon><infon key="id">sensors-21-02477-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27035</offset><text>The improved network based on BiSeNet.</text></passage><passage><infon key="file">sensors-21-02477-g003.jpg</infon><infon key="id">sensors-21-02477-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27074</offset><text>An example of image semantic segmentation.</text></passage><passage><infon key="file">sensors-21-02477-g004.jpg</infon><infon key="id">sensors-21-02477-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27117</offset><text>Vectorization of semantic point cloud data (taking arrows as an example).</text></passage><passage><infon key="file">sensors-21-02477-g005.jpg</infon><infon key="id">sensors-21-02477-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27191</offset><text>Examples of clustering of semantic point cloud.</text></passage><passage><infon key="file">sensors-21-02477-g006.jpg</infon><infon key="id">sensors-21-02477-f006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27239</offset><text>Confirmation of existence for map elements.</text></passage><passage><infon key="file">sensors-21-02477-g007.jpg</infon><infon key="id">sensors-21-02477-f007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27283</offset><text>Generation of new map features from semantic point cloud.</text></passage><passage><infon key="file">sensors-21-02477-g008.jpg</infon><infon key="id">sensors-21-02477-f008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27341</offset><text>The case that bounding boxes of the map element and extracted feature cover each other.</text></passage><passage><infon key="file">sensors-21-02477-g009.jpg</infon><infon key="id">sensors-21-02477-f009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27429</offset><text>Statistical results of matching degree.</text></passage><passage><infon key="file">sensors-21-02477-g010.jpg</infon><infon key="id">sensors-21-02477-f010</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27469</offset><text>The case that bounding boxes of the map element and extracted feature don’t cover each other.</text></passage><passage><infon key="file">sensors-21-02477-t001.xml</infon><infon key="id">sensors-21-02477-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27565</offset><text>Performance of improved BiSeNet network on Kuandeng verification dataset. IOU: Intersection over Union.</text></passage><passage><infon key="file">sensors-21-02477-t001.xml</infon><infon key="id">sensors-21-02477-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Type&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pixel Accuracy&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IOU&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Traffic sign&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.40%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.29%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.98%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pole&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.15%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.05%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.31%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Vehicles and other objects on the road&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.97%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.97%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.99%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Lane divider-white&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.42%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.81%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.69%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Lane-divider-yellow&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.32%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.57%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.76%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Speed bump&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.17%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.42%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.02%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Road surface&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.41%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.89%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.31%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crosswalk&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.60%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.79%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.93%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gore&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.90%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.11%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.57%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Text and symbol on the road&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.16%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.40%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.76%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Curb&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.45%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.11%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.96%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Others (Sky\Trees)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.61%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.37%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.99%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Left road boundary&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.99%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.94%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.36%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Right road boundary&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.26%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.14%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.63%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Stop line&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.52%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.82%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52.00%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dedicated lane dividers&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.79%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.55%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.07%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Average&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.20%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.20%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.90%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27669</offset><text>ID	Type	Recall	Pixel Accuracy	IOU	 	1	Traffic sign	94.40%	97.29%	91.98%	 	2	Pole	82.15%	90.05%	75.31%	 	3	Vehicles and other objects on the road	97.97%	98.97%	96.99%	 	4	Lane divider-white	92.42%	96.81%	89.69%	 	5	Lane-divider-yellow	77.32%	84.57%	67.76%	 	6	Speed bump	88.17%	93.42%	83.02%	 	7	Road surface	99.41%	98.89%	98.31%	 	8	Crosswalk	86.60%	97.79%	84.93%	 	9	Gore	94.90%	94.11%	89.57%	 	10	Text and symbol on the road	90.16%	93.40%	84.76%	 	11	Curb	86.45%	90.11%	78.96%	 	12	Others (Sky\Trees)	99.61%	99.37%	98.99%	 	13	Left road boundary	82.99%	91.94%	77.36%	 	14	Right road boundary	90.26%	93.14%	84.63%	 	15	Stop line	53.52%	94.82%	52.00%	 	16	Dedicated lane dividers	94.79%	92.55%	88.07%	 	Average	88.20%	94.20%	83.90%	 	</text></passage></document></collection>
