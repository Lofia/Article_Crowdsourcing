<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210219</date><key>pmc.key</key><document><id>7865129</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1073/pnas.2011362118</infon><infon key="article-id_pmc">7865129</infon><infon key="article-id_pmid">33495346</infon><infon key="article-id_publisher-id">202011362</infon><infon key="elocation-id">e2011362118</infon><infon key="issue">5</infon><infon key="kwd">machine learning competition laboratory earthquakes earthquake prediction physics of faulting</infon><infon key="license">This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).</infon><infon key="name_0">surname:Johnson;given-names:Paul A.</infon><infon key="name_1">surname:Rouet-Leduc;given-names:Bertrand</infon><infon key="name_10">surname:Levinson;given-names:Corey J.</infon><infon key="name_11">surname:Pfeiffer;given-names:Pascal</infon><infon key="name_12">surname:Puk;given-names:Kin Ming</infon><infon key="name_13">surname:Reade;given-names:Walter</infon><infon key="name_2">surname:Pyrak-Nolte;given-names:Laura J.</infon><infon key="name_3">surname:Beroza;given-names:Gregory C.</infon><infon key="name_4">surname:Marone;given-names:Chris J.</infon><infon key="name_5">surname:Hulbert;given-names:Claudia</infon><infon key="name_6">surname:Howard;given-names:Addison</infon><infon key="name_7">surname:Singer;given-names:Philipp</infon><infon key="name_8">surname:Gordeev;given-names:Dmitry</infon><infon key="name_9">surname:Karaflos;given-names:Dimosthenis</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">118</infon><infon key="year">2021</infon><offset>0</offset><text>Laboratory earthquake forecasting: A machine learning competition</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>66</offset><text>Earthquake prediction, the long-sought holy grail of earthquake science, continues to confound Earth scientists. Could we make advances by crowdsourcing, drawing from the vast knowledge and creativity of the machine learning (ML) community? We used Google’s ML competition platform, Kaggle, to engage the worldwide ML community with a competition to develop and improve data analysis approaches on a forecasting problem that uses laboratory earthquake data. The competitors were tasked with predicting the time remaining before the next earthquake of successive laboratory quake events, based on only a small portion of the laboratory seismic data. The more than 4,500 participating teams created and shared more than 400 computer programs in openly accessible notebooks. Complementing the now well-known features of seismic data that map to fault criticality in the laboratory, the winning teams employed unexpected strategies based on rescaling failure times as a fraction of the seismic cycle and comparing input distribution of training and testing data. In addition to yielding scientific insights into fault processes in the laboratory and their relation with the evolution of the statistical properties of the associated seismic data, the competition serves as a pedagogical tool for teaching ML in geophysics. The approach may provide a model for other competitions in geosciences or other domains of study to help engage the ML community on problems of significance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1544</offset><text>Earthquake prediction, which requires determination of the time, location, and size of an event before it begins, has had a long and controversial history. Tremendous effort has been expended in pursuing it, with occasional glimmers of hope but ultimately, disappointing results, leading many to conclude that short-term earthquake prediction is at best infeasible and perhaps impossible. In Charles Richter‘s own words, “only fools, charlatans and liars predict earthquakes.”</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2027</offset><text>With machine learning (ML), the earthquake science community has a new suite of tools to apply to this long-standing problem; however, applying ML to the prediction problem raises multiple thorny issues, including how to properly validate performance on rare events, what to do with models that seem to have strong predictive value but may not generalize, and how to handle the output of opaque ML methods. Despite these challenges, recent work has shown that progress on some aspects of the prediction problem is possible. For example, ML has revealed that the time remaining before an earthquake in the laboratory and particular types of tectonic earthquakes known as slow slip events can be anticipated from statistical characteristics extracted from seismic data. In the following, we present an overview of the laboratory earthquake prediction problem. We then describe the methods of the ML competition, its outcomes, and resulting insights into fault physics.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>2994</offset><text>Perspective: Earthquake Prediction and Forecasting</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>3045</offset><text>Earthquake prediction, the “when, where, and magnitude” of an upcoming event, relies on the identification of distinctive precursors that might precede a large earthquake. It is well established that earthquakes may be preceded by foreshocks and followed by aftershocks —known as the “foreshock–mainshock–aftershock” sequence. Foreshocks occur during earthquake nucleation as the fault prepares for rupture and are thought to represent failure of small frictional patches at or near where the fault will ultimately rupture in a mainshock. Numerous researchers have studied precursors in the laboratory, in simulations, and in the Earth. While seismic precursors are often observed in laboratory studies and in simulation, they are not reliably observed in the Earth, with a few notable exceptions where slow earthquakes have been observed prior to large subduction earthquakes.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>3937</offset><text>Indeed, in the early 1990s, the International Association of Seismology and Physics of the Earth’s Interior solicited information for a list of significant precursors. After intensive review of the scientific literature, the International Commission on Earthquake Forecasting for Civil Protection concluded in 2011 that there was “considerable room for methodological improvements”, an understatement to be sure. The report noted that many instances of reported precursors are contradictory and unsuitable for rigorous statistical evaluation. Published results are biased toward positive outcomes, and therefore, the rate of false negatives, where an earthquake exhibits no precursory signal, is unclear. The rate of false positives for proposed precursory phenomena is also rarely quantified.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>4737</offset><text>Moreover, there exists a broad and contentious discussion regarding the nature of fault rupture—whether earthquakes are predictable or not. If faults slip in an entirely deterministic manner, prediction well in advance of an earthquake may be possible; if they slip in a stochastic manner, forecasting immediately preceding failure may be possible but not long before.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>5108</offset><text>In summary, we are a long way from earthquake prediction and forecasting, yet recent work focused on laboratory quakes offers a glimmer of hope.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>5253</offset><text>Recent Applications of ML in Earthquake Science</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>5301</offset><text>ML applications in geoscience have expanded rapidly over the last two decades. The confluence of new ML algorithms, fast and inexpensive graphical processing units and tensor processing units, and the availability of massive, often continuous datasets has driven this revolution in data-driven analysis. This rapid expansion has seen application of existing and new ML tools to a suite of geoscientific problems that span seismic wave detection and phase identification and location, geological formation identification, earthquake early warning, volcano monitoring, denoising Interferometric Synthetic Aperture Radar (InSAR), tomographic imaging, reservoir characterization, and more. Of particular note is that, over the past 5 y, considerable effort has been devoted to using these approaches to characterize fault physics and forecast fault failure.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>6155</offset><text>A Brief Sketch of ML</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>6176</offset><text>There are two broad categories of ML—&quot;supervised” and “unsupervised” learning (some would argue that “reinforcement” learning is a third category).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>6336</offset><text>Supervised learning describes a class of problems that involve teaching a model how to approximate a hypothetical function that maps input data to output data based on a number of input–output examples. After it is trained, the model is used to make predictions on test input data that the model has not seen before and with no knowledge of the target for the test data. This problem can be formulated either as a classification or as a regression. Regression is a familiar supervised learning approach that involves the prediction of a continuous label. Classification is a supervised learning problem that involves prediction of a class (a discrete target). Both classification and regression problems may have one or more input data variables of any dimension that may be any data type, such as numerical, time series, or image.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>7170</offset><text>Unsupervised learning describes a class of problems that use an ML model to describe or extract relationships in data. Unsupervised learning operates upon only the input data without outputs or target. In the following, we describe advances toward earthquake prediction through the lens of supervised learning.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>7481</offset><text>ML Advances Earthquake Prediction in the Laboratory</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>7533</offset><text>Earthquake forecasting with ML had its first verifiable success in the laboratory by analyzing the characteristics of a continuous seismic signal broadcast from a fault shear zone. The data were obtained from an experiment conducted in a double-direct shear geometry using a biaxial shear apparatus (Fig. 1).</text></passage><passage><infon key="file">pnas.2011362118fig01.jpg</infon><infon key="id">fig01</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>7842</offset><text>Experimental configuration and data collected. The biaxial experiment shear configuration and data collected. (Upper Left) The experiment is composed of three steel blocks with fault gouge contained within two shear zones. The fault gouge is composed of glass beads with dimensions on the order of 120 μm and a layer thickness of 4 mm for each fault. The configuration is held in place by a fixed horizontal (normal) load of 5 MPa. The central block is driven downward at a constant velocity. Acoustic emission is recorded by a lead zirconate titanate (PZT) piezoelectric ceramic transducer. (Upper Right) Measured shear stress as a function of experimental run time. There is a run-in displacement during which the shear stress increases until the gouge material begins to stick–slip quasiperiodically. This occurs for roughly half the experiment, followed by the central block failing intermittently and then sliding stably. Lower Left and Lower Right show expanded views of the acoustic emission signal and shear stress signal, respectively, for the shaded region in the shear stress signal, where the laboratory quakes are relatively periodic. arb., arbitrary. Reprinted with permission from ref..</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>9047</offset><text>In the experiment, two faults containing granular fault gouge were sheared simultaneously with a prescribed shear velocity and constant normal load. Mechanical data measured on the apparatus included the shearing block velocity, the applied load, the gouge-layer thickness, the shear stress, and friction. In addition, continuous records of fault zone seismic wave radiation were recorded with piezoceramics embedded in side blocks of the shear assembly. The laboratory faults fail in quasirepetitive cycles of stick and slip that mimic to some degree the seismic cycle of loading and failure on tectonic faults (Fig. 1).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>9669</offset><text>The approach uses a decision tree-based ML model, known as a random forest algorithm. The study relies exclusively on a snapshot of the continuous seismic signals recorded during shear (Fig. 2) to predict failure time (independent decision tree models were developed to predict the instantaneous fault shear stress, shear displacement, and gouge thickness). The problem, posed as a regression, uses the continuous recorded seismic data as input and the fault time to failure as the target (and respectively, the shear stress, displacement, and gouge thickness using independent ML models). The time to failure, used as ground truth, is calculated from the shear stress signal measured on the device. During training, the input and target data are used to construct the ML model. During testing, only the seismic data are seen by the model—the recorded shear stress is taken as ground truth and was not seen by the model during testing.</text></passage><passage><infon key="file">pnas.2011362118fig02.jpg</infon><infon key="id">fig02</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>10607</offset><text>Random forest (RF) approach for predicting time remaining before failure. Failure times are determined by the large stress drops associated with the laboratory earthquakes, as seen in Fig. 1, Lower Right. The RF prediction (blue line) is shown on the testing data (data not previously seen by the ML algorithm) with 90 CIs (blue shaded region). The predictions agree very well with the actual remaining times before failure (red curve). The testing data are entirely independent of the training data and were not used to construct the model. Inset shows an expanded view of three slip cycles, illustrating that the trained model does well on aperiodic slip events (data are from laboratory experiment no. p2394 at Penn State). Reprinted with permission from ref..</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>11371</offset><text>The model predicts the timing of laboratory earthquakes with high fidelity (Fig. 2). By applying a related decision tree ML model known as gradient-boosted trees, Hulbert et al. can predict failure times for slow slip as well [a slow slip event is a “slow earthquake” that occurs in the laboratory and in Earth and is a member of the spectrum of slip behaviors that range from fast (earthquakes) to very slow ]. Hulbert et al. are also able to predict slow earthquake magnitude using a separate ML model, albeit with less accuracy than for the earthquake timing. The ML model identifies why prediction was possible—the continuous seismic signal power evolves in a predictable manner throughout the stress cycle. This characteristic is used by the ML model to learn instantaneous and future characteristics of the fault system (e.g., see Fig. 4C). Other studies were also conducted with unsupervised approaches.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>12288</offset><text>Following analysis of the continuous wave seismic signals, a high-fidelity catalog of seismic signals was assembled from the large number of minute seismic precursor events occurring in the laboratory. The catalog feature characteristics are used to forecast future fault slip by applying a gradient-boosting model. The target is the time to failure, obtained from the measured shear stress on the sample. The study also shows that, as the earthquake catalog recording fidelity is decreased, predicting performance progressively decreases.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>12828</offset><text>ML Advances Slow Earthquake Prediction in Earth</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>12876</offset><text>The laboratory fault can be viewed as a single frictional patch and represents a relatively homogeneous system that is designed to facilitate the understanding of the underlying processes. A fault in the Earth, on the other hand, is composed of a very large number of frictional patches that behave as an ensemble when fault slip occurs and does so in the context of complex and heterogeneous Earth materials. These differences raise the question of how readily one can generalize from the laboratory to the Earth. The prediction approach devised from the laboratory data is scaled to Earth in the Cascadia subduction zone, where the Juan de Fuca tectonic plate subducts beneath the North American plate. Parts of the Cascadia subduction zone exhibit slip events with a duration on the order of weeks, occurring approximately every 14 mo, manifest by the North American plate slipping southwesterly over the Juan de Fuca plate (Fig. 3).</text></passage><passage><infon key="file">pnas.2011362118fig03.jpg</infon><infon key="id">fig03</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>13813</offset><text>Subduction in Cascadia. Cross-sectional view of the Cascadia subduction zone in the region of Vancouver Island. Arrows indicate the sense of motion of the subducting Jan de Fuca plate beneath the North American plate. Adapted from ref., which is licensed under CC BY 4.0.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>14085</offset><text>In the laboratory, slip characteristics are measured directly on the device. In Earth, slip takes place on the subduction interface at depth, all of the while emitting seismic signals. The fault displacement is measured indirectly at Earth’s surface using data from global positioning system stations. Estimates of the time remaining before the next slow slip event on the testing dataset are shown in Fig. 4 and are from ref.. This plot shows the ML slip timing estimates (in blue) and the time remaining before the next slow slip event (ground truth; dashed red lines). This ground truth is a countdown to the next slip event, similar to that developed for the laboratory data.</text></passage><passage><infon key="file">pnas.2011362118fig04.jpg</infon><infon key="id">fig04</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>14767</offset><text>Slow slip time to failure estimates, seismic features identified by the ML model, and comparison with laboratory experiments. (A) Testing set ML estimates of time to failure (blue) and measured time to failure of slow earthquakes in Cascadia, in the region of Vancouver Island. CC, correlation coefficient. (B) The most important statistical feature of the seismic data is related to seismic signal energy (black curve). The seismic feature shows systematic patterns evolving through the slip cycle in relation to the timing of the succeeding slow earthquake in Cascadia (left axis). (The feature IQ60-40 range is the interquantile obtained by subtracting the 60th percentile from the 40th percentile.) (C) For comparison, the most important statistical feature found in laboratory slow slip experiments is acoustic power, which is proportional to signal energy (right-hand vertical axis). The similarity of the two measures, one in Earth and the other in the laboratory, suggests that the slip processes at both scales are related. Adapted from ref., which is licensed under CC BY 4.0.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>15854</offset><text>Why Scientific Competitions?</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>15883</offset><text>Scientific challenge competitions were common in the 1800s in France to advance the understanding of light and how light interacts with materials (e.g., ref.). More recently, challenges such as the “XPRIZE” (https://www.xprize.org/) have been used since 1994 to promote the development of technology and methods related to spaceflight, learning, and oil cleanup. Competitions enable identification of the current state of the art, drive innovation, and attract engineers and scientists from vastly different disciplines, and potentially different scientific subcultures within disciplines, to develop novel solutions to proposed problems.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16526</offset><text>Since 2010, Kaggle (which was acquired by Google in 2017) has provided a platform for the ML world community that hosts competitions to propose and develop data analysis solutions for a wide range of problems. One notable example is the competition sponsored by NASA, the European Space Agency, and the Royal Astronomical Society to detect dark matter through gravitational lensing (https://www.kaggle.com/c/mdm). Surprisingly, this competition was won by a glaciologist who used techniques that had been developed for detecting edges in glacier fronts from satellite images.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>17102</offset><text>A major idea driving the Kaggle philosophy is to facilitate the linking of data science and domain expertise for the effective application of ML approaches to challenging scientific problems.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>17294</offset><text>The retrospective analysis of the laboratory and Cascadia datasets demonstrates that information on the state of a fault is imprinted in continuously generated signals. This raises the question of what other information may be contained in a signal that could have predictive value for time to failure in the Earth. With that in mind, we created a competition with the goal of attracting worldwide attention and to generate new ideas for addressing the challenge of earthquake prediction. Our aim was to learn about novel ML approaches that may help with earthquake prediction and also to attract ML-centered talent to Earth-related problems. In the next section, we describe the details and outcomes of the competition.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>18015</offset><text>The Kaggle Competition</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>18038</offset><text>The competition posed the question: can ML extract informative signatures from a small portion of continuous seismic data to predict the timing of upcoming laboratory earthquakes? The data were collected with the same biaxial device described in Fig. 1 but for conditions near the frictional stability transition where laboratory quakes exhibit complex metastable slip behavior. In particular, the laboratory earthquake data exhibited less periodic failures in contrast to the experiments described previously. Aperiodic failures are more difficult to predict, especially during the initial stages of the earthquake cycle. Predictions typically improve as failure approaches, as was shown in Fig. 2.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>18738</offset><text>The training data comprised a single continuous time segment of the recorded seismic data exhibiting multiple laboratory earthquakes (Fig. 5). The test data consisted of individual small segments of a different portion of the same experimental data. Thus, the predictions from the test data could not be assumed by contestants to follow the same regular pattern seen in the training data, making the prediction challenging. There was no overlap between the training and testing sets.</text></passage><passage><infon key="file">pnas.2011362118fig05.jpg</infon><infon key="id">fig05</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19222</offset><text>Competition training data. The black curve shows the seismic signal recorded on a piezoceramic transducer located on the biaxial apparatus side block (apparatus is shown in Fig. 1). Each burst in amplitude corresponds to a laboratory earthquake. The red curve shows the time to failure derived from the earthquakes and the measured shear stress on the experimental apparatus (as in Fig. 1). Competitors were tasked with predicting the time before the next laboratory quake only based on a small snapshot of seismic data.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>19743</offset><text>The Kaggle Competition Outcome</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>19774</offset><text>The Kaggle competition was announced at a special workshop that we organized on ML in geoscience held at the December 2018 Neural Information Processing Systems Conference in Montreal and the following week at a special session on ML in Geoscience at the 2018 American Geophysical Union Fall Meeting in Washington, DC. The competition was officially launched on 10 January 2019 and ended on 3 June 2019. The competition attracted 4,521 teams with over 5,450 competitors, the largest number of researchers working simultaneously on the same geophysical dataset ever, to our knowledge (Fig. 6). Over 59,890 entries were submitted to compete for monetary prizes (a total of US $50,000) awarded to the top five teams.</text></passage><passage><infon key="file">pnas.2011362118fig06.jpg</infon><infon key="id">fig06</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>20488</offset><text>Evolution of MAE scores. The number of teams (light blue squares) and the value of the MAE of the daily first place team on the validation set (black circles) over the period of the competition as determined on the public validation set until early June and finally determined on the private test set for the final ranking (hence the jump in MAE for the final evaluation). The gray dots represent MAE values for all other submissions on each day. The public (red circles; validation set) and private (green squares; testing set) MAEs for the winning team are shown for the winning team's submissions.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>21089</offset><text>Each day, competitors/teams were allowed to submit a maximum of two solutions/methods to test their ML approach. Competitors made predictions on the entire test set each time, but only the resulting score from a validation subset (13% of the testing data) was revealed to the teams during the competition. Results of the validation set were posted on a “public leaderboard” for all to see. The model prediction scores from the remaining approximately 87% of the test data were kept hidden from the participants (“private leaderboard”), and at the conclusion of the competition, the scores from this portion of the data were used to obtain the final standings (which could be, and were, different from the public leaderboard scores) (see Figs. 6 and 9).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>21850</offset><text>For each submission, the Kaggle platform provided a score based on the mean absolute error (MAE; a measure of distance between the ML prediction and the testing data) and used the MAE on the validation set to rank the competitors. Six days into the competition, the MAE of the daily first place team was 1.396 and dropped to 1.080 on the day prior to closure of the competition (Fig. 6 has a summary). When the ML entries were run on the private leaderboard data, the MAE increased to 2.2650, with the result that some of the daily top teams dropped in the rankings (Fig. 7), a telltale sign of overfitting. The large gap between training and testing performance can be explained in part by the nonstationarity of the data (some physical properties of the experiment slowly evolved over time) that heavily penalized overfitting (more on that in the next section).</text></passage><passage><infon key="file">pnas.2011362118fig07.jpg</infon><infon key="id">fig07</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>22714</offset><text>Comparison of the change in rank for the top five competitors on the last day of submission with that of the top five winners. Tables provide the rank, MAE, and total number of submissions for the top five competitors on the last day and for the winners.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>22969</offset><text>The evident camaraderie among the participants was surprising and gratifying—approaches were posted and discussed daily on the discussion board. Participants aided one another to improve their respective results. Indeed, this kind of collaborative work is often the norm in ML research, where codes are shared on, for example, GitHub and can readily be built upon by “forking,” in contrast to research in the natural sciences, where methods are described in detail in publications but often not shared directly.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>23487</offset><text>The winning team, named “The Zoo,” comprised eight members from the United States and Europe, with expertise in mathematics, computer science, operations research, and electrical engineering. The members of Team Zoo have diverse backgrounds such as energy, credit risk, hotels, insurance, and Artificial Intelligence (AI). Many of the team members had not previously worked together. One member had experience working with signals from electroencephalogram research, while others had previous Kaggle experience (e.g., Walmart Recruiting: Trip Type Classification, Springleaf Marketing Response, Allstate Claims Severity) or worked professionally in ML. The diverse backgrounds of Team Zoo underscore the diversity of participants and demonstrate that the goal of assembling new perspectives and divergent experience and disciplines to the field of earthquake prediction was achieved through the competition. Team Zoo submitted a total of 238 entries and climbed 355 places to reach first place (Fig. 7). Interestingly, Team Zoo never achieved a daily first place ranking on the public leaderboard during the competition (Fig. 6). The winning teams that ranked second to fifth had very similar results, with differences in MAE of about 0.001 (Fig. 7). Team Zoo, however, had a large performance advantage of 30 times this small gap over other winning teams. Here, we will briefly describe the approach taken by the various teams.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>24920</offset><text>Team Zoo generated hundreds of features based on the original signals, denoised signals, and spectra. The four most important features used in their approach were 1) number of peaks in the denoised signal, 2) 20 percentile on the standard deviation (SD) of a moving window of size 50 points, 3) 4th Mel Frequency Cepstral Coefficient (MFCC) mean, and 4) 18th MFCC mean. MFCC is often used in speech processing for nonstationary signals. The diversity of these measures provides an example of how the competition resulted in new approaches. Their final approach included a blend of Light Gradient Boosting Machine (LGBM) and a neural network model fitted on threefold shuffled cross-validation (Fig. 8). Using this method, the team found that using “time-since-failure” as an additional loss improved model training. Most importantly, before feature calculation, their approach entailed adding noise to the data and training their models on a subset of the training data that have similar feature distribution compared with the test data (based on Kolmogorov–Smirnov tests). In doing so, Team Zoo effectively used the test (private) data as an additional validation set. Further, noise was added to allow for features that rely on median values and to allow for the removal of the median instead of the mean from each sample for better generalization. (Median removal is generally more robust to outliers than mean removal.) A full description of their approach can be found at https://www.kaggle.com/c/LANL-earthquake-Prediction/discussion/94390.</text></passage><passage><infon key="file">pnas.2011362118fig08.jpg</infon><infon key="id">fig08</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26473</offset><text>Winning model of the competition, by Team Zoo, on the test set. Red indicates time remaining before the next laboratory earthquake, as the experimental time progresses. Blue indicates predictions of Team Zoo’s winning model (an ensemble model of gradient-boosted trees and neural networks) based on small snapshots of seismic data (https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb has additional details).</text></passage><passage><infon key="file">pnas.2011362118fig09.jpg</infon><infon key="id">fig09</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26909</offset><text>Distribution of MAE for all of the teams. Model performance of all of the competing teams on the two test sets (public and private). The performance dropped on the private set, a telltale of overfitting.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>27113</offset><text>With a PhD in physics and currently working on the power spectrum of galaxy distributions, “JunKoda” (team of one) placed second by using CatBoost (an implementation of gradient-boosting trees) and 32 features. JunKoda found that the most important feature was the SD in the signal amplitude after the large-amplitude peaks were removed and that it was also important to avoid using means, as the data were not stationary. Similarly to Team Zoo’s approach of restricting training data to a distribution similar to test input data, JunKoda modified the training set based on figures from previously published work.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>27733</offset><text>Team “Character Ranking,” with backgrounds in mathematics and law and with work experience in information security, cartoon publishing, and mobile games, placed third. This team found that using LGBM with many features performed slightly better than a recurrent neural network (RNN) approach with six features and simple gated recurrent unit. The two team members tried, independently, 1,900 and 2,400 features but found that by using only 2 features of the RNN, they could achieve a score of 2.3273 on the private leaderboard, showing that simpler models generalize better. In the end, they used a blend of both of their LGBM methods and the RNN.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>28385</offset><text>Team “Reza,” composed of an electrical engineer, came in fourth with an LGBM model that included hyperparameter tuning and the selection of features based on the Kolmogorov–Smirnov test between the training and test datasets and again only used earthquake events that were similar to the test set, similar to Team Zoo’s approach. Selected features included moving SD/variance, moving skew/kurtosis, moving moments 5 and 6, autocorrelation, threshold detection, and peak detection. Team Reza found that most selected features were highly correlated with the target (time to failure). Team Reza developed 63 different LGBM algorithms over 63 different combinations of earthquakes in the training set. All models were trained with the same set of features but different sets of earthquakes. The final result was a simple average of the predictions from the 63 models.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>29258</offset><text>The fifth place team “GloryorDeath” from Seattle, WA used features from standard signal processing libraries, including Butterworth filters and wavelet decomposition. Removal of the largest-amplitude peaks from the signals was used. An arbitrary scaling factor of 1.05 was one of their hyperparameters. The features were used in a simple feed-forward neural net in the deep learning library pytorch. One key innovation to their solution that dramatically improved their results was using a scaling of the time remaining before failure to indicate the state of the system as opposed to an absolute time to failure. In other words, predicting how far along in the seismic cycle the laboratory fault system is turned out to be easier than predicting the specific time remaining before the next quake. Additional details can be found at https://www.kaggle.com/c/LANL-earthquake-Prediction/discussion/94484.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>30165</offset><text>What Did We Learn from the Kaggle Competition?</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30212</offset><text>Previous work on seismic data from Earth suggests that the underlying physics may scale from a laboratory fault to large fault systems in Earth. If this is indeed the case, improvements in our ability to predict earthquakes in the laboratory could lead to significant progress in time-dependent earthquake hazard characterization. The ultimate goal of the earthquake prediction challenge was to identify promising ML approaches for seismic data analysis that may enable improved estimates of fault failure in the Earth. In the following, we will discuss shortcomings of the competition but also key innovations that improved laboratory quake predictions and may be transposed to Earth studies.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30906</offset><text>The approaches employed by the winning teams included several innovations considerably different from our initial work on laboratory quake prediction. Team Zoo added synthetic noise to the input seismic data before feature computing and model training, thus making their models more robust to noise and more likely to generalize.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31236</offset><text>Team Zoo, JunKoda, and GloryorDeath only considered features that exhibited similar distributions between the training and testing data, thereby ensuring that nonstationary features could not be used in the learning phase and again, improving model generalization. We note that employing the distribution of the testing set input is a form of data snooping that effectively made the test set actually a validation set. However, the idea of employing only features with distributions that do not evolve over time is insightful and could be used for scientific purposes by comparing feature distribution between portions of training data, for example.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31886</offset><text>Perhaps most interestingly from a physical standpoint, the fifth team, Team Reza, changed the target to be predicted and endeavored to predict the seismic cycle fraction remaining instead of time remaining before failure. Because they did not employ the approach of comparing input distribution between training and testing sets as done by the first, second, and fourth teams, the performance impact from the prediction of normalized time to failure (seismic cycle fraction) was significant.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32378</offset><text>As in any level of statistics, more data are in general better and can improve model performance. Thus, had the competitors been given more training data, in principle scores may have improved. At the same time, there is an element of nonstationarity in the experiment because the fault gouge layer thins as the experiment progresses, and therefore, even an extremely large dataset would not lead to a perfect prediction. In addition, Kaggle keeps the public/private test set split in such a way as to not reward overfitting. No matter how large the dataset is, if a model iterates enough times on that dataset, it will not translate well into “the real world,” so the competition structure was designed to prevent that opportunity.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33115</offset><text>It is worth noting that the ML metric should be carefully considered. In Earth, it will be important to accurately predict the next quake as it approaches, but MAE treats each time step equally with respect to the absolute error making this challenging.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33369</offset><text>Individuals participate on the Kaggle platform for many reasons; the most common are the ability to participate in interesting and challenging projects in many different domains, the ability to learn and practice ML and data science skills, the ability to interact with others who are seeking the same, and of course, cash prizes. The astounding intellectual diversity the Kaggle platform attracted for this competition, with team representations from cartoon publishers, insurance agents, and hotel managers, is especially notable. In fact, none of the competition winners came from geophysics. Teams exhibit collective interaction, evidenced by the step changes in the MAE through time (Fig. 6), likely precipitated by communication through the discussion board and shared code.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34150</offset><text>The competition contributed to an accelerating increase in ML applications in the geosciences, has become an introductory problem for the geoscience community to learn different ML approaches, and is used for ML classes in geoscience departments. Students and researchers have used the top five approaches to compare the nuances of competing ML methods, as well as to try to adapt and improve the approaches for other applications.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>34582</offset><text>The authors declare no competing interest.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>34625</offset><text>This article is a PNAS Direct Submission.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>34667</offset><text>Data Availability.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>34686</offset><text>The competition dataset and binary data have been deposited in Kaggle (https://www.kaggle.com/c/LANL-Earthquake-Prediction/data).</text></passage><passage><infon key="fpage">9276</infon><infon key="lpage">9282</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2017</infon><offset>34816</offset><text>Machine learning predicts laboratory earthquakes</text></passage><passage><infon key="fpage">69</infon><infon key="lpage">74</infon><infon key="section_type">REF</infon><infon key="source">Nat. Geosci.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2019</infon><offset>34865</offset><text>Similarity of fast and slow earthquakes illuminated by machine learning</text></passage><passage><infon key="fpage">908</infon><infon key="lpage">1752</infon><infon key="section_type">REF</infon><infon key="source">Nat. Geosci.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2019</infon><offset>34937</offset><text>Continuous chatter of the Cascadia subduction zone revealed by machine learning</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The Mechanics of Earthquakes and Faulting</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>35017</offset></passage><passage><infon key="fpage">1853</infon><infon key="lpage">1876</infon><infon key="section_type">REF</infon><infon key="source">Pure Appl. Geophys.</infon><infon key="type">ref</infon><infon key="volume">161</infon><infon key="year">2004</infon><offset>35018</offset><text>Review of the physical basis of laboratory-derived relations for brittle failure and their implications for earthquake occurrence and earthquake nucleation</text></passage><passage><infon key="fpage">1005</infon><infon key="lpage">1009</infon><infon key="pub-id_pmid">15329715</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">430</infon><infon key="year">2004</infon><offset>35174</offset><text>Detachment fronts and the onset of dynamic friction</text></passage><passage><infon key="fpage">5627</infon><infon key="lpage">5631</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2013</infon><offset>35226</offset><text>Acoustic emission and microslip precursors to stick-slip failure in sheared granular material</text></passage><passage><infon key="fpage">L12302</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2011</infon><offset>35320</offset><text>Micromechanics of asperity rupture during laboratory stick slip experiments</text></passage><passage><infon key="fpage">5467</infon><infon key="lpage">5475</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2014</infon><offset>35396</offset><text>Seismic precursors to the shear failure of rock discontinuities</text></passage><passage><infon key="fpage">8569</infon><infon key="lpage">8594</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">121</infon><infon key="year">2016</infon><offset>35460</offset><text>On the evolution of elastic properties during laboratory stick-slip experiments spanning the transition from slow slip to dynamic rupture</text></passage><passage><infon key="fpage">9516</infon><infon key="lpage">9522</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2018</infon><offset>35598</offset><text>Emergent wave conversion as a precursor to shear crack initiation</text></passage><passage><infon key="fpage">e2020GL086986</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2020</infon><offset>35664</offset><text>Preseismic fault creep and elastic wave amplitude precursors scale with lab earthquake magnitude for the continuum of tectonic failure modes</text></passage><passage><infon key="fpage">e2019GL086615</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2020</infon><offset>35805</offset><text>Predicting imminence of analog megathrust earthquakes with machine learning: Implications for monitoring subduction zones</text></passage><passage><infon key="fpage">1363</infon><infon key="lpage">1377</infon><infon key="section_type">REF</infon><infon key="source">Bull. Seismol. Soc. Am.</infon><infon key="type">ref</infon><infon key="volume">67</infon><infon key="year">1977</infon><offset>35927</offset><text>Numerical simulation of earthquake sequences</text></passage><passage><infon key="fpage">B11414</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">113</infon><infon key="year">2008</infon><offset>35972</offset><text>Episodic slow slip events and rate-and-state friction</text></passage><passage><infon key="fpage">B11312</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">110</infon><infon key="year">2005</infon><offset>36026</offset><text>Earthquake nucleation on (aging) rate and state faults</text></passage><passage><infon key="fpage">851</infon><infon key="lpage">855</infon><infon key="pub-id_pmid">17792179</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">268</infon><infon key="year">1995</infon><offset>36081</offset><text>Seismic evidence for an earthquake nucleation phase</text></passage><passage><infon key="fpage">425</infon><infon key="lpage">450</infon><infon key="section_type">REF</infon><infon key="source">Geophys. J. Int.</infon><infon key="type">ref</infon><infon key="volume">131</infon><infon key="year">1997</infon><offset>36133</offset><text>Earthquake prediction: A critical review</text></passage><passage><infon key="fpage">877</infon><infon key="lpage">880</infon><infon key="pub-id_pmid">21330536</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">331</infon><infon key="year">2011</infon><offset>36174</offset><text>Extended nucleation of the 1999 Mw7.6 Izmit earthquake</text></passage><passage><infon key="fpage">299</infon><infon key="lpage">302</infon><infon key="section_type">REF</infon><infon key="source">Nat. Geosci.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2013</infon><offset>36229</offset><text>The long precursory phase of most large interplate earthquakes</text></passage><passage><infon key="fpage">583097</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Geophys.</infon><infon key="type">ref</infon><infon key="volume">2012</infon><infon key="year">2012</infon><offset>36292</offset><text>Precursor-like anomalies prior to the 2008 Wenchuan earthquake: A critical-but-constructive review</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="section_type">REF</infon><infon key="source">Geod. Geodyn.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>36391</offset><text>Studies on earthquake precursors in China: A review for recent 50 years</text></passage><passage><infon key="fpage">8772</infon><infon key="lpage">8781</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2019</infon><offset>36463</offset><text>Pervasive foreshock activity across southern California</text></passage><passage><infon key="fpage">1569</infon><infon key="lpage">1582</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">124</infon><infon key="year">2019</infon><offset>36519</offset><text>Foreshocks and mainshock nucleation of the 1999 mw 7.1 Hector Mine, California, earthquake</text></passage><passage><infon key="fpage">2330</infon><infon key="lpage">2342</infon><infon key="section_type">REF</infon><infon key="source">Seismol. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">91</infon><infon key="year">2020</infon><offset>36610</offset><text>Neural network applications in earthquake prediction (1994–2019): Meta-analytic insight on their limitations</text></passage><passage><infon key="fpage">924</infon><infon key="lpage">928</infon><infon key="section_type">REF</infon><infon key="source">Geology</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2020</infon><offset>36721</offset><text>Seismic velocity precursors to the 2016 Mw 6.5 Norcia (Italy) earthquake</text></passage><passage><infon key="fpage">705</infon><infon key="lpage">708</infon><infon key="pub-id_pmid">22267578</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">335</infon><infon key="year">2012</infon><offset>36794</offset><text>Propagation of slow slip leading up to the 2011 Mw 9.0 Tohoku-oki earthquake</text></passage><passage><infon key="fpage">5420</infon><infon key="lpage">5427</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2014</infon><offset>36871</offset><text>Multiple slow-slip events during a foreshock sequence of the 2014 Iquique, Chile Mw 8.1 earthquake</text></passage><passage><infon key="fpage">1165</infon><infon key="lpage">1169</infon><infon key="pub-id_pmid">25061132</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">345</infon><infon key="year">2014</infon><offset>36970</offset><text>Intense foreshocks and a slow slip event preceded the 2014 Iquique Mw 8.1 earthquake</text></passage><passage><infon key="fpage">829</infon><infon key="lpage">834</infon><infon key="section_type">REF</infon><infon key="source">Nat. Geosci.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2016</infon><offset>37055</offset><text>Triggering of the 2014 Mw7.3 Papanoa earthquake by a slow slip event in Guerrero, Mexico</text></passage><passage><infon key="fpage">eaat8472</infon><infon key="pub-id_pmid">30402540</infon><infon key="section_type">REF</infon><infon key="source">Sci. Adv.</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2018</infon><offset>37144</offset><text>Do slow slip events trigger large and great megathrust earthquakes?</text></passage><passage><infon key="fpage">4</infon><infon key="section_type">REF</infon><infon key="source">Ann. Geophys.</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2011</infon><offset>37212</offset><text>Operational earthquake forecasting: State of knowledge and guidelines for utilization</text></passage><passage><infon key="fpage">eaau0323</infon><infon key="pub-id_pmid">30898903</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">363</infon><infon key="year">2019</infon><offset>37298</offset><text>Machine learning for data-driven discovery in solid Earth geoscience</text></passage><passage><infon key="fpage">3</infon><infon key="lpage">14</infon><infon key="section_type">REF</infon><infon key="source">Seismol. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">90</infon><infon key="year">2019</infon><offset>37367</offset><text>Machine learning in seismology: Turning data into insights</text></passage><passage><infon key="fpage">57</infon><infon key="lpage">107</infon><infon key="section_type">REF</infon><infon key="source">Advances in Geophysics</infon><infon key="type">ref</infon><infon key="volume">vol. 61</infon><infon key="year">2020</infon><offset>37426</offset><text>Chapter two - machine learning and fault rupture: A review</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">55</infon><infon key="section_type">REF</infon><infon key="source">Advances in Geophysics</infon><infon key="type">ref</infon><infon key="volume">vol. 61</infon><infon key="year">2020</infon><offset>37485</offset><text>Chapter one - 70 years of machine learning in geoscience in review</text></passage><passage><infon key="fpage">e1501057</infon><infon key="pub-id_pmid">26665176</infon><infon key="section_type">REF</infon><infon key="source">Sci. Adv.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2015</infon><offset>37552</offset><text>Earthquake detection through computationally efficient similarity search</text></passage><passage><infon key="fpage">eaau0323</infon><infon key="pub-id_pmid">30898903</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">363</infon><infon key="year">2019</infon><offset>37625</offset><text>Machine learning for data-driven discovery in solid {Earth} geoscience</text></passage><passage><infon key="fpage">2894</infon><infon key="lpage">2901</infon><infon key="section_type">REF</infon><infon key="source">Bull. Seismol. Soc. Am.</infon><infon key="type">ref</infon><infon key="volume">108</infon><infon key="year">2018</infon><offset>37696</offset><text>Generalized seismic phase detection with deep learning</text></passage><passage><infon key="comment">arXiv:1803.03211 (8 March 2018)</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>37751</offset><text>PhaseNet: A deep-neural-network-based seismic arrival time picking method</text></passage><passage><infon key="fpage">856</infon><infon key="lpage">869</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">124</infon><infon key="year">2019</infon><offset>37825</offset><text>PhaseLink: A deep learning approach to seismic phase association</text></passage><passage><infon key="comment">arXiv: 1709.07943 (17 September 2017)</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>37890</offset><text>Cascaded contextual region-based convolutional neural network for event detection from time series signals: A seismic application</text></passage><passage><infon key="comment">arXiv:1802.02241 (17 January 2018)</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>38020</offset><text>Seismic-net: A deep densely connected neural network to detect seismic events</text></passage><passage><infon key="fpage">22</infon><infon key="lpage">29</infon><infon key="section_type">REF</infon><infon key="source">Comput. Geosci.</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2018</infon><offset>38098</offset><text>Reducing process delays for real-time earthquake parameter estimation—an application of KD tree to large databases for Earthquake Early Warning</text></passage><passage><infon key="fpage">10267</infon><infon key="pub-id_pmid">31311942</infon><infon key="section_type">REF</infon><infon key="source">Sci. Rep.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2019</infon><offset>38244</offset><text>Cred: A deep residual network of convolutional and recurrent units for earthquake signal detection</text></passage><passage><infon key="fpage">T163</infon><infon key="lpage">T171</infon><infon key="section_type">REF</infon><infon key="source">Interpretation</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>38343</offset><text>Constraining self-organizing map facies analysis with stratigraphy: An approach to increase the credibility in automatic seismic facies classification</text></passage><passage><infon key="fpage">22</infon><infon key="lpage">33</infon><infon key="section_type">REF</infon><infon key="source">Comput. Geosci.</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">2014</infon><offset>38494</offset><text>Geological mapping using remote sensing data: A comparison of five machine learning algorithms, their response to variations in the spatial distribution of training data and the use of explicit spatial information</text></passage><passage><infon key="fpage">517</infon><infon key="lpage">529</infon><infon key="section_type">REF</infon><infon key="source">Seismol Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">90</infon><infon key="year">2019</infon><offset>38708</offset><text>An investigation of rapid earthquake characterization using single-station waveforms and a convolutional neural network</text></passage><passage><infon key="fpage">1528</infon><infon key="section_type">REF</infon><infon key="source">Rem. Sens.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2019</infon><offset>38828</offset><text>Toward global volcano monitoring using multisensor sentinel missions and artificial intelligence: The MOUNTS monitoring system</text></passage><passage><infon key="fpage">111179</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens. Environ.</infon><infon key="type">ref</infon><infon key="volume">230</infon><infon key="year">2019</infon><offset>38955</offset><text>A deep learning approach to detecting volcano deformation from satellite imagery using synthetic datasets</text></passage><passage><infon key="fpage">e2019GL085523</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2020</infon><offset>39061</offset><text>Machine learning reveals the seismic signature of eruptive behavior at Piton de la Fournaise volcano</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2020</infon><offset>39162</offset><text>Probing slow earthquakes with deep learning</text></passage><passage><infon key="fpage">11850</infon><infon key="lpage">11858</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2019</infon><offset>39206</offset><text>The application of convolutional neural networks to detect slow, sustained deformation in InSAR time series</text></passage><passage><infon key="fpage">1572</infon><infon key="lpage">1581</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Computational Science, ICCS 2011</infon><infon key="type">ref</infon><infon key="volume">vol. 4</infon><infon key="year">2011</infon><offset>39314</offset><text>Classification of seismic windows using artificial neural networks</text></passage><passage><infon key="fpage">58</infon><infon key="section_type">REF</infon><infon key="source">Lead. Edge</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2018</infon><offset>39381</offset><text>Deep-learning tomography</text></passage><passage><infon key="fpage">499</infon><infon key="lpage">511</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Comput. Imaging</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2018</infon><offset>39406</offset><text>Travel time tomography with adaptive dictionaries</text></passage><passage><infon key="fpage">2096</infon><infon key="lpage">2100</infon><infon key="section_type">REF</infon><infon key="source">Inversionet: Accurate and Efficient Seismic-Waveform Inversion with Convolutional Neural Networks</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39456</offset></passage><passage><infon key="fpage">033304</infon><infon key="pub-id_pmid">29776097</infon><infon key="section_type">REF</infon><infon key="source">Phys. Rev. E</infon><infon key="type">ref</infon><infon key="volume">97</infon><infon key="year">2018</infon><offset>39457</offset><text>Modeling flow and transport in fracture networks using graphs</text></passage><passage><infon key="fpage">695</infon><infon key="lpage">710</infon><infon key="section_type">REF</infon><infon key="source">Comput. Geosci.</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2018</infon><offset>39519</offset><text>Machine learning for graph-based representations of three-dimensional discrete fracture networks</text></passage><passage><infon key="comment">“3D seismic attribute analysis and machine learning for reservoir characterization in Taranaki Basin, New Zealand,” PhD thesis, Missouri Institute of Mining and Technology, Rolla, Missouri (2018)</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>39616</offset></passage><passage><infon key="fpage">4139</infon><infon key="pub-id_pmid">32811833</infon><infon key="section_type">REF</infon><infon key="source">Nat. Commun.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2020</infon><offset>39617</offset><text>An exponential build-up in seismic energy suggests a months-long nucleation of slow slip in Cascadia</text></passage><passage><infon key="fpage">1303</infon><infon key="lpage">1311</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2019</infon><offset>39718</offset><text>Machine learning can predict the timing and size of analog earthquakes</text></passage><passage><infon key="fpage">13.269</infon><infon key="lpage">13.276</infon><infon key="section_type">REF</infon><infon key="source">Geophys. Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2018</infon><offset>39789</offset><text>Earthquake catalog-based machine learning identification of laboratory fault states and the effects of magnitude of completeness</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Artificial Intelligence: A Modern Approach</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>39918</offset></passage><passage><infon key="fpage">4196</infon><infon key="lpage">4202</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res.</infon><infon key="type">ref</infon><infon key="volume">89</infon><infon key="year">1984</infon><offset>39919</offset><text>Effect of humidity on time- and velocity-dependent friction in rocks</text></passage><passage><infon key="fpage">643</infon><infon key="lpage">696</infon><infon key="section_type">REF</infon><infon key="source">Annu. Rev. Earth Planet Sci.</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">1998</infon><offset>39988</offset><text>Laboratory-derived friction laws and their application to seismic faulting</text></passage><passage><infon key="fpage">B10207</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">2010</infon><offset>40063</offset><text>Frictional strength and strain weakening in simulated fault gouge: Competition between geometrical weakening and chemical strengthening</text></passage><passage><infon key="fpage">695</infon><infon key="pub-id_pmid">27597879</infon><infon key="section_type">REF</infon><infon key="source">Nat. Geosci.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2016</infon><offset>40199</offset><text>Precursory changes in seismic velocity for the spectrum of earthquake failure modes</text></passage><passage><infon key="fpage">1229</infon><infon key="lpage">1232</infon><infon key="pub-id_pmid">23950495</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">341</infon><infon key="year">2013</infon><offset>40283</offset><text>Slow earthquakes, preseismic velocity changes, and the origin of slow frictional stick-slip</text></passage><passage><infon key="fpage">407</infon><infon key="lpage">413</infon><infon key="section_type">REF</infon><infon key="source">Earth Planet Sci. Lett.</infon><infon key="type">ref</infon><infon key="volume">482</infon><infon key="year">2018</infon><offset>40375</offset><text>Evolution of b-value during the seismic cycle: Insights from laboratory experiments on simulated faults</text></passage><passage><infon key="fpage">5</infon><infon key="lpage">32</infon><infon key="section_type">REF</infon><infon key="source">Mach. Learn.</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2001</infon><offset>40479</offset><text>Random forests</text></passage><passage><infon key="fpage">337</infon><infon key="lpage">407</infon><infon key="section_type">REF</infon><infon key="source">Ann. Stat.</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2000</infon><offset>40494</offset><text>Others, Additive logistic regression: A statistical view of boosting (with discussion and a rejoinder by the authors)</text></passage><passage><infon key="fpage">1525</infon><infon key="lpage">1528</infon><infon key="pub-id_pmid">11313500</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">292</infon><infon key="year">2001</infon><offset>40612</offset><text>A silent slip event on the deeper Cascadia subduction interface</text></passage><passage><infon key="fpage">1679</infon><infon key="lpage">1681</infon><infon key="pub-id_pmid">12040191</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">296</infon><infon key="year">2002</infon><offset>40676</offset><text>Nonvolcanic deep tremor associated with subduction in southwest Japan</text></passage><passage><infon key="fpage">271</infon><infon key="lpage">296</infon><infon key="section_type">REF</infon><infon key="source">Annu. Rev. Earth Planet Sci.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2011</infon><offset>40746</offset><text>Slow earthquakes and nonvolcanic tremor</text></passage><passage><infon key="fpage">253</infon><infon key="lpage">257</infon><infon key="pub-id_pmid">27418504</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">353</infon><infon key="year">2016</infon><offset>40786</offset><text>Connecting slow earthquakes to huge earthquakes</text></passage><passage><infon key="fpage">1088</infon><infon key="lpage">1098</infon><infon key="section_type">REF</infon><infon key="source">Seismol Res. Lett.</infon><infon key="type">ref</infon><infon key="volume">90</infon><infon key="year">2019</infon><offset>40834</offset><text>Characterizing acoustic signals and searching for precursors during the laboratory seismic cycle using unsupervised machine learning</text></passage><passage><infon key="comment">arXiv:1912.06087 (12 December 2019)</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>40967</offset><text>Unsupervised classification of acoustic emissions from catalogs and fault time-to-failure prediction</text></passage><passage><infon key="fpage">1942</infon><infon key="lpage">1943</infon><infon key="pub-id_pmid">12738870</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">300</infon><infon key="year">2003</infon><offset>41068</offset><text>Episodic tremor and slip on the Cascadia subduction zone: The chatter of silent slip</text></passage><passage><infon key="fpage">4899</infon><infon key="lpage">4919</infon><infon key="section_type">REF</infon><infon key="source">G-cubed</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2016</infon><offset>41153</offset><text>Variations in slow slip moment rate associated with rapid tremor reversals in Cascadia</text></passage><passage><infon key="fpage">250</infon><infon key="lpage">304</infon><infon key="section_type">REF</infon><infon key="source">Electronic Properties of Crystalline Solids</infon><infon key="type">ref</infon><infon key="year">1974</infon><offset>41240</offset><text>Chapter 8 - scattering processes</text></passage><passage><infon key="fpage">7931</infon><infon key="lpage">7949</infon><infon key="section_type">REF</infon><infon key="source">J. Geophys. Res. Solid Earth</infon><infon key="type">ref</infon><infon key="volume">123</infon><infon key="year">2018</infon><offset>41273</offset><text>Frictional mechanics of slow earthquakes</text></passage></document></collection>
