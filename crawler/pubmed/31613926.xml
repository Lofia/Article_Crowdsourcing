<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201213</date><key>pmc.key</key><document><id>6793944</id><infon key="license">CC0</infon><passage><infon key="alt-title">THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</infon><infon key="article-id_doi">10.1371/journal.pone.0223792</infon><infon key="article-id_pmc">6793944</infon><infon key="article-id_pmid">31613926</infon><infon key="article-id_publisher-id">PONE-D-19-15866</infon><infon key="elocation-id">e0223792</infon><infon key="issue">10</infon><infon key="license">This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.</infon><infon key="name_0">surname:Hebart;given-names:Martin N.</infon><infon key="name_1">surname:Dickter;given-names:Adam H.</infon><infon key="name_2">surname:Kidder;given-names:Alexis</infon><infon key="name_3">surname:Kwok;given-names:Wan Y.</infon><infon key="name_4">surname:Corriveau;given-names:Anna</infon><infon key="name_5">surname:Van Wicklin;given-names:Caitlin</infon><infon key="name_6">surname:Baker;given-names:Chris I.</infon><infon key="name_7">surname:Soto;given-names:Fabian A.</infon><infon key="name_8">surname:Hebart;given-names:Martin N.</infon><infon key="notes">The object concept and image database as well as all data and results are made publicly available through the Open Science foundation at http://doi.org/10.17605/osf.io/jum2f.</infon><infon key="section_type">TITLE</infon><infon key="title">Data Availability</infon><infon key="type">front</infon><infon key="volume">14</infon><infon key="year">2019</infon><offset>0</offset><text>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>92</offset><text>In recent years, the use of a large number of object concepts and naturalistic object images has been growing strongly in cognitive neuroscience research. Classical databases of object concepts are based mostly on a manually curated set of concepts. Further, databases of naturalistic object images typically consist of single images of objects cropped from their background, or a large number of naturalistic images of varying quality, requiring elaborate manual image curation. Here we provide a set of 1,854 diverse object concepts sampled systematically from concrete picturable and nameable nouns in the American English language. Using these object concepts, we conducted a large-scale web image search to compile a database of 26,107 high-quality naturalistic images of those objects, with 12 or more object images per concept and all images cropped to square size. Using crowdsourcing, we provide higher-level category membership for the 27 most common categories and validate them by relating them to representations in a semantic embedding derived from large text corpora. Finally, by feeding images through a deep convolutional neural network, we demonstrate that they exhibit high selectivity for different object concepts, while at the same time preserving variability of different object images within each concept. Together, the THINGS database provides a rich resource of object concepts and object images and offers a tool for both systematic and large-scale naturalistic research in the fields of psychology, neuroscience, and computer science.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1655</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1668</offset><text>Two central goals in cognitive neuroscience are to elucidate how we recognize the objects in the world around us and how we are able to form categories based on our percepts and our semantic knowledge. Reaching these goals requires us to overcome two challenges. First, we need to understand how the visual system identifies the relevant object features from the almost infinite number of possible object appearances. Second, we need to understand how observers use this information about the object and integrate it with their object knowledge to uniquely identify its semantic content while distinguishing it from the thousands of other concepts to which it could belong. Computer science, in particular computer vision, is grappling with similar challenges for artificial visual systems. Crucially, for any study involving object concepts or the visual presentation of object images, both the selection of concepts and the images depicting those concepts can strongly influence the results of a study and the conclusions that are drawn.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>2708</offset><text>The need for large-scale systematic sampling of object concepts and naturalistic object images</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2803</offset><text>In recent years, in the fields of psychology, neuroscience and computer science there has been a growing interest in utilizing a wide range of object concepts and naturalistic images. This interest arises primarily from the goals of (1) achieving an ecologically valid understanding of visual and semantic cognition, and (2) building generalizable computational models of object recognition (e.g.) and semantic knowledge (e.g.). Thus, the availability of a wide range of systematically sampled object concepts and naturalistic object images promises not only to benefit large-scale experimental and computational approaches to the study of visual and semantic cognition; it also offers classical small-scale experimental approaches the possibility of selecting a more representative set of concepts and object images for testing specific hypotheses regarding cognition, behavior, and neural representations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3711</offset><text>Influential databases of object categories and concepts cover only a selective subset of the objects found in the everyday environment. In contrast, the lexical database WordNet offers a highly-systematic taxonomy of a vast range of words and their meaning, but its great detail makes it challenging to select a representative set of object concepts. Understandably, for many experiments the manual selection of individual objects or object categories is still common practice.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4189</offset><text>For object images, there are numerous databases from research in psychology and neuroscience, which most commonly contain line drawings of objects or photographs of objects cropped from their natural background (for a review, see). However, naturalistic object context has been shown to play an important role in object recognition, and cropped object images may overemphasize the role of shape in neural representations of objects. In addition, such databases usually offer only one example per object concept, while the use of multiple object examples is desirable for measuring generalizable representations. In the machine learning community, several large-scale object image databases have been developed and are commonly used, with up to thousands of image examples per concept. However, most of the images contained in these databases are too small to be of practical use in psychology and neuroscience experiments, and images vary strongly in aspect ratio and quality and often still lack naturalistic backgrounds, making it challenging to use them. More recent databases may offer higher quality, but are composed mostly of images containing multiple different objects at the same time or naturalistic scenes such as cities, beaches, or forests. In contrast to real-world scenes, many researchers are interested in using image databases focused on images of individual objects.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>5576</offset><text>Aim of the object concept and object image database</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5628</offset><text>With this present work, we aim at providing researchers in the fields of psychology, neuroscience, and computer science with (1) a large-scale systematic sampling of object concepts, (2) a list of object categories derived from those concepts, and (3) a large set of high-quality color images of objects with naturalistic background. This database, together with similarity matrices based on semantic embeddings and activations in a deep convolutional neural network, will be made freely available for academic purposes in the final journal version of the paper. Until that time, a download link is available upon direct contact with the authors.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>6275</offset><text>Materials and methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>6297</offset><text>Data availability</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6315</offset><text>The object concept and image database as well as all data and results are made publicly available through the Open Science foundation at http://doi.org/10.17605/osf.io/jum2f.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>6490</offset><text>Participants</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6503</offset><text>We recruited a total of 1,395 workers from the online crowdsourcing platform Amazon Mechanical Turk for different tasks involved in the creation of this database, including object image naming and object categorization (see below). In the object naming task, workers were compensated $0.10 for 10 responses (in practice ~$7.50/h), and in the object categorization task $0.05 for 5 responses (in practice ~$5.00/h; all estimates based on the mode of the completion time). This research was approved by the NIH Office of Human Subjects Research Protections (OHSRP), and workers were compensated financially for their time.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>7124</offset><text>Identification of picturable and nameable object concepts</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7182</offset><text>The selection and identification of the 1,854 object concepts used for this image dataset encompassed a three-step procedure that is described in more detail below. First, from an existing word database, we gathered a list of nouns that represent concrete, picturable object concepts. Second, we carried out word-sense disambiguation by assigning each noun one or several unique WordNet identifiers (“synsets”) that represent the meaning of this noun, thus allowing us to eliminate synonyms or identify nouns with multiple meanings. Third, we identified the subset of synsets that matches their use in everyday language, by selecting representative images for all synsets and testing how consistently they were named by human subjects. Note that the final list is not intended to be a complete and definite set of all picturable and nameable object concepts in the English language (see Discussion). However, the steps below constitute a systematic approach towards their selection and a detailed description of the different decisions involved. In addition, note that the actual number of selected nouns at each of these steps may deviate slightly from the numbers reported below, as in some cases during each selection step we identified items that were mistakenly kept but should have been excluded at earlier steps.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>8506</offset><text>Step 1: List of concrete picturable nouns in American English</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8568</offset><text>To compile a list of concrete picturable nouns, we used a list of American English lemma that contains concreteness ratings for ~40,000 words and two-word expressions (e.g. “ice cream”). For simplicity, we will collectively refer to these words or two-word expressions as words. In this list of American English lemma, concreteness ratings ranged from 1 to 5 and reflected the level through which the word could be experienced through one of the five basic senses (5: concrete, 1: abstract). The concreteness rating of each word is based on the average of 25 to 30 unique behavioral responses.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9166</offset><text>To select candidate lemma from this list, we applied two selection criteria. First, we restricted our selection to words that were tagged as nouns. We used part-of-speech tags provided with the concreteness ratings and extended them using the British Lexicon Project, which offers multiple tags per word (e.g. “cook” both as noun and as verb). Second, we restricted our selection of words to those with a concreteness rating ≥ 4. This choice was based on a preliminary manual screening of the list, since we only rarely identified candidate words with a lower concreteness rating. Based on these criteria, we identified 8,671 nouns.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9805</offset><text>Following this initial selection, we manually screened the list based on a number of exclusion criteria (a list of the excluded lemma is provided with this database). The exclusion criteria are: (1) not a noun despite part-of-speech tag, (2) plural form when singular form with the identical meaning is found in the list (e.g. exclude “cats” when “cat” is present, but keep “glasses” when “glass” is present), (3) clear synonym (e.g. “motorcar” vs. “car”), (4) nouns that were clearly not nameable or too general (e.g. “equipment”), (5) navigable places (e.g. “garden”) including integral parts of buildings (e.g. “steeple”), (6) nouns referring to persons in certain roles (e.g. “doctor,” “pilot,” “audience”), with certain ethnic or cultural origin (e.g. “Indian”), or with certain color of skin (e.g. “albino”), (7) fictitious or extinct beings for which no real-world photograph can exist (e.g. “dragon,” “werewolf,” “dinosaur”), (8) relationship statuses (e.g. “father,” “granddaughter”) (9) body parts that are either internal organs or muscles (e.g. “liver”) or where the noun describes these as being parts of other body parts (e.g. “fingernail,” “forearm”), (10) bodily fluids (e.g. “urine”), (11) body parts of animals that do not serve as tools for humans (e.g. exclude “beak” but include “feather”), (12) non-visual but sensory nouns (e.g. “click,” “music”), (13) action nouns (e.g. “smack”), (14) times of day (“night”), (15) units and geometric figures (e.g. “quart,” “hexagon”), (16) fluids or light that cannot be ascribed to an object (e.g. “floodwater,” “moonlight”) or specific drinks that cannot be identified without a label (e.g. “whisky,” “gin”), (17) celestial bodies (e.g. “moon”), (18) specific drugs, pharmaceuticals and chemical compounds, (19) parts of objects that are difficult to describe in isolation (e.g. “seam,” “rim”), (20) fabrics and unspecific surface materials (e.g. “tweed,” “teakwood”), (20) brand names unless objects of other brands share this name (e.g. exclude “iPad” but include “jeep”) and (21) nouns denoting or implying sexual content or nudity. In addition to these exclusion criteria, post-hoc we chose to exclude all nouns that require text for identification of the object (e.g. “lexicon”), or that refer to objects that necessarily contain text (e.g. “newspaper”), since for an image database the identification should not rely on text or be biased by the written text. Finally, we excluded a small set of nouns that were not in WordNet (see below) and that we judged as very unusual (e.g. “wishbone”). This selection left us with a list of 3,397 words.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>12595</offset><text>Step 2: Word sense disambiguation through assignment to WordNet synsets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12667</offset><text>Each of the nouns identified in the previous step may carry more than one meaning, making it ambiguous as to which concept the noun represents (e.g. “bat” as an animal or “bat” as a sports item). In addition, the list may contain synonyms, i.e. different words with the same meaning that could be merged (e.g. “couch” and “sofa”). To identify all unique picturable meanings from this list of words, we used WordNet to carry out word-sense disambiguation, i.e. the assignment of words to unique senses. In WordNet, a meaning or sense is referred to as a synset, which comes with a unique identifier (synset ID), a list of synonyms, and a definition. Note that while the coverage of word meanings in WordNet is extensive, some meanings are not covered, and others are represented by multiple synsets that could in principle be merged.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13516</offset><text>To identify unique synsets, we first compiled all candidate synsets for each word in our list. For words with only one synset, the assignment between word and synset did not require any disambiguation. For those words, we removed synsets whose definition did not represent a picturable object and that had mistakenly not been eliminated in the previous step. For all remaining words, we created a graphical user interface (GUI) to present each word alongside all of its synsets (Fig 1). In the GUI, the reference word appeared on top, alongside all candidate synsets with different synonyms, definitions and, when available, three matching images extracted from ImageNet. For every word, two independent raters selected the meaning(s) that best matched the word. Two raters carried out this assignment for one half of the words (even numbers), and another two raters for the other half (odd numbers). The mean inter-rater agreement was 82.53%, and disagreements were resolved by a third rater. Based on these ratings, we merged synonymous words by picking the synonym used most frequently, using the word frequency provided in the Corpus of Contemporary American English. In addition, we assigned words with multiple meanings separate identifiers (e.g. “bat1” and “bat2”). This process left us with a total of 3,228 object concepts based on the meaning of different words.</text></passage><passage><infon key="file">pone.0223792.g001.jpg</infon><infon key="id">pone.0223792.g001</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>14897</offset><text>Concept and image selection procedure and graphical user interfaces (GUIs) used for concept and image selection.</text></passage><passage><infon key="file">pone.0223792.g001.jpg</infon><infon key="id">pone.0223792.g001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15010</offset><text>(A) Procedure for selection of 1,854 concepts in the THINGS database. (B) GUI used for word-sense disambiguation based on WordNet senses (synsets) and example images from ImageNet. (C) Procedure for selection of 26,107 object images in the THINGS database. Original images replaced due to the journal’s copyright policy. (D) GUI used for initial manual selection of candidate images with sufficient quality. Original images replaced due to the journal’s copyright policy.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>15486</offset><text>Step 3: Object naming task to identify picturable object concepts</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15552</offset><text>Using the list of concepts from the previous step, we identified representative images depicting those concepts and conducted an object naming task. The aim of this task was to identify the set of objects for which participants use the intended object concept and separate them from those for which they use a different one. The reasoning behind this approach is twofold. First, including object concepts according to how they are named improves their comparability and prevents ambiguity with respect to object knowledge. Second, for a database of object images, an observer should be able to both recognize an object and identify the object concept. There are several reasons why there might be disagreement between the concept and the name an observer uses for the object in the image. First, some of these words are quite specific and reflect a subordinate category level (e.g. “blue jay”) which may not correspond to the description commonly used by human observers (e.g. “bird”). Indeed, this procedure has been used previously, and it was demonstrated that naming results commonly reflect the basic level in a naming task (Rosch et al., 1976). However, we did not restrict the selection of concepts to the basic level, but specifically chose concepts based on their everyday use in language. Second, some of the concepts represent object parts (e.g. “hat ring,” “lip”) which can only be shown in the context of an object when zooming in, but which may lead observers to nevertheless focus on the object (e.g. “hat,” “mouth”). Finally, naming might be highly inconsistent or incorrect, indicating that observers have difficulties identifying the object concept or disagree which concept to use.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17277</offset><text>We tested object naming by selecting a representative object image with natural background for each of the 3,228 object concepts and presenting them to workers on Amazon Mechanical Turk (n = 445, mean number of responses per worker: 86.29). Each image was shown to 10 workers who were asked to label the object or “thing” that was most prominent in the image by typing it into a box below the image. An additional set of 10 responses was collected for ambiguous cases. The answers were corrected for spelling errors and plural forms, and different synonyms of a synset were labeled as reflecting the same concept. Our inclusion criteria were rather liberal, i.e. we opted to rather include a concept than to falsely exclude it. All responses are made publicly available with the database, including links to the representative images. Exclusion criteria for object concepts were: (1) if the expected label was provided only once, (2) if the expected label was provided only twice while a different label was consistently provided at least five times, (3) if the expected label was provided only three times while a different label was consistently provided at least six times. Criteria for collecting an additional 10 responses were: (1) If the expected label was provided twice while a different label was consistently provided less than five times, or (2) if there were an equal number of responses with the expected label and a different label. In addition, we manually inspected all concepts marked for exclusion, and when there was any doubt that the image was not representative enough, we selected a new image and retested the concept. After collecting the additional responses, all concepts that were still not clearly included or excluded were left in. Based on this approach, an additional 1,374 concepts were excluded. Of those, 706 were named at a higher taxonomic level, 630 were named inconsistently, and 38 referred to a whole object when the object concept reflected a part. This left us with a final number of 1,854 object concepts.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19331</offset><text>Identification of higher-level object categories</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19380</offset><text>Having identified the final set of 1,854 object concepts, we sought to identify a set of higher-level object categories. To this end, we followed two strategies. In line with the approach of identifying the concepts based on their use in everyday language when naming concrete objects, we chose a similar “bottom-up” strategy of having workers on Amazon Mechanical Turk identify the categories each object belongs to. However, this approach likely results in errors based on incorrect beliefs (e.g. that a peanut is a nut rather than a legume) and may lead to incomplete categories. Hence, we complemented this strategy with a “top-down” approach, identifying members of the dominant categories based on the taxonomy of word senses inherent in WordNet.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>20141</offset><text>Bottom-up determination of object categories</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20186</offset><text>For the bottom-up strategy, we conducted a two-step procedure on Amazon Mechanical Turk, first asking workers to propose candidate categories for those objects and second asking a separate group of workers to select the most appropriate category from those candidates. In more detail, in the first step, for each concept and representative image we asked 20 workers to provide the category this object belongs to (n = 427, mean number of responses per worker: 43.41). The instructions included clear examples using abstract concepts and categories, in order not to set a strong baseline for what results are expected for concrete objects (e.g. “blue” is a “color,” “Susan” is a “name” or “female name”). We excluded responses that simply repeated the original concept, but kept all other responses, including arbitrary categories (e.g. “sports” rather than “sports item” or “sports equipment”). In addition, we corrected all responses for spelling errors, and converted plural form to singular form (unless the original concept was a plural noun, such as “glasses”).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21289</offset><text>In the second step, to identify representative categories from those resulting in the first round, we asked another 20 workers per concept (total n = 523) to provide a category for the object. This time, however, in addition to the word representing the concept and the representative image, we showed workers the answers that had been provided by workers in the previous step, in random order. Workers were instructed that some of the responses were incorrect, that they could combine previous responses, and that they could use their own category if they deemed all responses inappropriate. Responses of one worker were removed who carried out an unusually large number of responses with unusually fast response times (mean number of responses per worker: 67.22, number of responses of excluded worker: 1,990). All remaining responses in this second step were again corrected for spelling errors, and plural form was converted to singular form. In addition, we unified responses that were synonymous (e.g. “beverage” and “drink”) and removed qualitative statements (e.g. “large,” “small,” “hot,” “cold,” etc.), colors, and place of origin (e.g. “Asian,” “French”). Responses were automatically assigned to a higher-level category if (a) there were 11 or more consistent responses or (b) there were at least 5 consistent responses while less than half that number were a consistent alternative. All assignments were later checked manually for accuracy, and higher-level categories were assigned to all concepts based on a summary of the provided categories or the use of synonymous categories. Finally, from the list of all categories, we identified categories that were used synonymously or overlapped strongly (e.g. “kitchen tool” vs. “kitchen utensil”) and merged them.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>23103</offset><text>Top-down determination of object categories</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23147</offset><text>For the top-down strategy, we used the taxonomy in WordNet to identify all concepts that were subordinate to the 27 high-level categories identified in the previous step. To this end, we first identified the synsets of the high-level categories. Of those categories, 23 were found in WordNet, and we used the category “decoration” in place of “home décor,” due to the strong overlap between the members of both categories, making it a total of 24 categories. For each of the 1,854 object concepts, we recursively ascended each branch of the WordNet tree until we reached the top synset. We then assigned the object concept to any of the 24 categories that was crossed while ascending through WordNet. Note that the category “car part” does not contain any subordinate entries in WordNet, i.e. in the Results section, the number of categories is described as 23.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24022</offset><text>Selection of images for object image database</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24068</offset><text>For the final object image database, the goal was to identify at least 12 images for each of the 1,854 object concepts, i.e. a minimum of 22,248 images. Image selection and postprocessing (e.g. cropping) were conducted by the authors. Note that below, the identification of candidate images and postprocessing are described as sequential steps. However, the process of selecting object images went through multiple cycles until a sufficient number of suitable images had been identified for all concepts.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24573</offset><text>Image selection criteria</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24598</offset><text>The goal for this object image database was to select high-quality, naturalistic images of objects belonging to the 1,854 object concepts. Each image was a photograph of one or several examples of a given object concept that was cropped to square size. Since the selection criteria (detailed below) required us to exclude the majority of candidate images, for some concepts it was difficult to find examples of suitable quality. For those concepts, we decided to slightly loosen some of these selection criteria. In the following, when we write of “exclusion,” this refers to strict exclusion criteria for images, while the term “avoiding” refers to exclusion criteria that were less strict, depending on how difficult it was to find object examples.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25357</offset><text>The objects were generally chosen to be the central component of the image, and while additional objects of other concepts were allowed to be present in an image, they were not allowed to dominate the image. For example, the presence of human body parts was permitted in images of clothing; however, we avoided images showing human faces due to their strong salience (with the exception of concepts defined by human faces, e.g. “face,” “man,” “woman”). Since each image was cropped to a square in order to standardize the image size, we focused on identifying images that would still show the majority of an object after cropping.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26000</offset><text>The images were photographs with a minimum size of 480 × 480 pixels, but most were 600 × 600 pixels or larger (see Fig 3B). We selected images of objects with naturalistic background, i.e. we avoided images with uniform background and excluded images for which the natural background had been removed or which had been modified in a non-naturalistic and recognizable fashion. We avoided images in which the object was blurry or for which lighting was over- or underexposed, and we excluded images with non-naturalistic colors (including grayscale), strong color filters, watermarks, borders, or other text added to the images. Finally, we specifically avoided text that naturally appeared within the images (e.g. on a book), especially when the text allowed the identification of the object concept. However, for some object concepts, this was difficult or impossible to avoid (e.g. “police car”).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>26904</offset><text>Identification of candidate images</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26939</offset><text>Due to the large number of object concepts and images as well as the strict selection criteria, we pursued several strategies for identifying suitable images. First, we automatically selected 30 candidate images per concept from each of the image search engines Google Images, Bing Images and the photography website Flickr. Then, we manually selected candidate images, using a custom-made graphical user interface (GUI) written in MATLAB (Mathworks, Natick). Since this strategy did not yield a sufficient number of high-quality object images for most object concepts, we decided to directly identify and download candidate images through manual web searches, with a focus on images from Google Images, Bing Images, and the online-auctioning platform eBay. We accelerated the process by opening a set of candidate images in the browser and using a bulk image downloader that allowed us to select images with sufficient size (https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf). Search terms generally focused on the word reflecting the object concept, in some cases using a translation into Spanish, German, Russian, Chinese, or French. In addition, we identified a set of candidate images of sufficient size from ImageNet, which had been prescreened and selected for the object image database “ecoset”, followed by manual selection with the image selection GUI. Finally, several images taken by the authors were added to the database.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>28430</offset><text>Image cropping, removal of images with border, and manual quality check</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28502</offset><text>Following the manual selection of suitable candidate images, we cropped objects to square size, using a separate custom-made GUI (written in MATLAB) to accelerate the process (Fig 1). This GUI allowed us to select relevant image parts for sufficiently large images, crop images to square size, and exclude bad candidate images. Further, the GUI was written to prevent the selection of image parts that were smaller than 480 × 480 pixels. Following this image cropping, we identified and corrected images that still contained a small uniform border by searching for uniform pixel intensities at the edge of the image. Finally, we manually screened all cropped images a second time to identify and remove images with low quality or those matching other exclusion criteria.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>29274</offset><text>Semi-automatic identification of highly similar or duplicate images</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29342</offset><text>A small number of candidate images within a concept were duplicates, photographs of the same object from a slightly different perspective, or images with different object examples but identical background. To identify highly-similar images or duplicates, we passed all images through the deep convolutional neural network VGG-16, which had been pretrained on 1,000 ImageNet concepts as implemented in the toolbox MatConvNet (http://www.vlfeat.org/matconvnet/pretrained/). We reasoned that highly similar images would produce similar activation vectors at different levels of generalization. For the identification of those images using VGG-16, we focused on the five pooling layers and two fully-connected layers, i.e. a total of seven layers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30086</offset><text>In the following, we describe the process for the first layer, which we repeated separately for all seven layers. For all object images, we computed the activations for the current layer and vectorized them. Subsequently, within each of the 1,854 object concepts, we calculated the Pearson correlation coefficients between all pairs of vectors. For example, for an object concept containing 12 object images, this step yielded 66 correlation coefficients. Finally, we used MATLAB to display the object pairs subsequently, starting with the pair with the largest correlation coefficient and subsequently moving down correlation coefficient size. We manually checked the first 1,500 pairs and removed images that were duplicates, were the same object taken from different angles, or had an identical background. Screening the first 1,500 pairs proved to be effective, and not a single image was removed within the last 100 of those image pairs. We repeated this process for all seven layers. Since different network layers produced similar duplicate candidates, we excluded pairs that had been screen previously, but still screened an additional 1,500 pairs manually per layer.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>31262</offset><text>Assignment of image order and image name in final database</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31321</offset><text>As the final step, we converted all images to jpeg-format, determined the final order of object images, and standardized their filenames. As described above, the goal for the final database was to have at least 12 images per concept. For the reason of simplicity, we deemed the first 12 images of each concept to be the most relevant ones. The first image of each concept was the reference image used in the object naming task, unless it was not of sufficient quality and had been excluded (number of excluded reference images: 191). All other images were initially sorted by image size. Then, we shuffled the order of the 11 largest images. For all concepts with more than 12 images, we separately shuffled the order of the remaining images. Since for comparison to computational vision algorithms it is often important to know which images were used for training an algorithm and since ImageNet is commonly used, all images chosen from ImageNet were labeled with the letter n. All reference images were labeled with the letter b, and all other images with the letter s.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>32393</offset><text>Similarity matrices from computational models of semantics and vision</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>32463</offset><text>Semantic embedding based on synset vectors</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32506</offset><text>Semantic embeddings provide a low dimensional vector representation of words based on their co-occurrence statistics in large text corpora that approximate the relationship between word meanings. Recent developments in word embeddings based on shallow neural networks, in particular word2vec, have led to strong improvements in performance. A recent modeling approach combined word2vec with knowledge about different senses based on WordNet synsets, providing separate vectors for different meanings of words. Here, we extracted those synset vectors (1) to create a similarity matrix in order to visualize the semantic distribution of different concepts, and (2) to provide a quantitative basis for the selection of a representative subset of concepts based on their semantic similarity and (3) to offer a resource for researchers who intend to use them alongside the concepts. In short, for all synsets, we extracted 300-dimensional synset vectors from those provided by Pilehvar and Collier (https://pilehvar.github.io/deconf/) that had been trained with word2vec on the Google News Corpus (https://code.google.com/archive/p/word2vec/). For words not represented in WordNet or missing in the synset vector representations, we chose the original word2vec model, but rescaled word vectors to have the same standard deviation as synset vectors since synset vectors were reduced in variance. Finally, we calculated a similarity matrix between words using the Pearson correlation (the common cosine distance and Euclidean distance led to very similar results, all r &gt; 0.96).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>34078</offset><text>Deep convolutional neural network activations for all object images</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34146</offset><text>Deep convolutional neural networks (CNNs) offer a computational model of object recognition and–due to their excellent performance–have become very popular models not only in the field of computer science, but also in psychology and neuroscience (for review, see). We extracted the activations of CNN layers for two purposes. First, we use them to identify the degree of selectivity of each object concept and how it increases from early to late layers. Second, we provide similarity matrices of the images as an additional resource for researchers. To this end, we used CorNet-S, a comparably shallow recurrent neural network architecture inspired by the ventral visual stream in the macaque brain. For each of the images contained in our database, we extracted the activations for all five layers in CorNet-S and converted them to vectors. For each layer, we then created a similarity matrix by computing the Pearson correlation coefficient between all pairs of vectors.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>35123</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>35131</offset><text>1,854 object concepts and 27 core high-level categories</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35187</offset><text>The final list of concepts comprised 1,854 concrete objects, mass items (e.g. “sand,” “coal,” “gravel”) or other “things” (e.g. “footprint,” “fingerprint”), and objects spanned a wide range of different concepts. All object concepts are provided with their WordNet synset IDs, a link to an example image, concreteness ratings word frequency from several corpora, category membership determined bottom-up through ratings and top-down through the WordNet hierarchy, definitions from WordNet, and others (for a full list, see S1 Table).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35745</offset><text>Based on the bottom-up categorization provided by human raters, 926 of these objects were rated as belonging to the 27 most common categories with at least 15 members (Table 1), with very little overlap between those categories (34 objects belonging to two categories). For simplicity, in the following we will refer to these 27 categories as the “core categories”. The distribution of category membership followed a rapidly decaying function, i.e. many of the remaining objects were categorized as belonging to small and very specific categories. While several of the 27 core categories provided by human raters overlap with those in classical category descriptions categories and in WordNet, other categories were unique to the ratings (“part of car,” “office supply,” “clothing accessory,” and “medical equipment”). A similar trend was observed for the top-down categorization provided by WordNet: 960 object concepts belonged to 23 of the 27 categories that were available in WordNet (the 24th category “car part” did not contain any subordinate entries). Of those, 118 objects belonged to multiple categories, which was mostly explained by the overlap introduced by the presence of subcategories (e.g. both insects and birds are animals).</text></passage><passage><infon key="file">pone.0223792.t001.xml</infon><infon key="id">pone.0223792.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>37013</offset><text>High-level object categories based on object concepts in THINGS database.</text></passage><passage><infon key="file">pone.0223792.t001.xml</infon><infon key="id">pone.0223792.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Category&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Synset&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frequency (Mturk, bottom-up)&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frequency (WordNet, top-down)&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Overlap&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;food&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;food.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;137&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;151&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;animal&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;animal.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;174&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;clothing&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;clothing.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;tool&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;tool.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;sports equipment&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;sports_equipment.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;vegetable&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;vegetable.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;38&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;vehicle&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;vehicle.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;musical instrument&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;musical_instrument.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;fruit&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;fruit.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;body part&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;body_part.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;dessert&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;dessert.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;29&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;toy&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;toy.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;29&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;container&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;container.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;139&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;part of car&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;car_part.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;weapon&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;weapon.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;bird&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;bird.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;furniture&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;furniture.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;kitchen tool&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;kitchen_utensil.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;office supply&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;n/a&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;clothing accessory&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;n/a&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;kitchen appliance&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;kitchen_appliance.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;plant&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;plant.n.02&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;insect&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;insect.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;home décor&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;decoration.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;medical equipment&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;n/a&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;electronic device&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;electronic_device.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;drink&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;beverage.n.01&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>37087</offset><text>Category	Synset	Frequency (Mturk, bottom-up)	Frequency (WordNet, top-down)	Overlap	 	food	food.n.01	137	151	85	 	animal	animal.n.01	95	174	94	 	clothing	clothing.n.01	60	93	55	 	tool	tool.n.01	51	50	25	 	sports equipment	sports_equipment.n.01	46	22	15	 	vegetable	vegetable.n.01	38	27	23	 	vehicle	vehicle.n.01	37	66	28	 	musical instrument	musical_instrument.n.01	33	32	30	 	fruit	fruit.n.01	32	49	31	 	body part	body_part.n.01	31	31	29	 	dessert	dessert.n.01	29	11	9	 	toy	toy.n.01	29	24	17	 	container	container.n.01	28	139	23	 	part of car	car_part.n.01	28	0	0	 	weapon	weapon.n.01	28	20	17	 	bird	bird.n.01	27	27	26	 	furniture	furniture.n.01	27	32	18	 	kitchen tool	kitchen_utensil.n.01	27	17	7	 	office supply		26	n/a	0	 	clothing accessory		21	n/a	0	 	kitchen appliance	kitchen_appliance.n.01	21	10	8	 	plant	plant.n.02	21	37	17	 	insect	insect.n.01	20	16	16	 	home décor	decoration.n.01	19	30	3	 	medical equipment		18	n/a	0	 	electronic device	electronic_device.n.01	16	5	1	 	drink	beverage.n.01	15	16	12	 	</text></passage><passage><infon key="file">pone.0223792.t001.xml</infon><infon key="id">pone.0223792.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>38106</offset><text>The 27 most common high-level categories as determined by Amazon Mechanical Turk workers providing categories for 1,854 objects and listing any category names more than 15 times. For the same categories, we determined the frequency for WordNet synsets and their subordinate members (hyponyms).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38400</offset><text>Since the dichotomies “animate–inanimate” and “man-made–natural” are frequently used high-level categorical distinctions, we counted the number of nameable objects that belonged to either of those categories. To our surprise, only around 11% of the objects were animate–even when including humans and human body parts–while 89% were inanimate. Similarly, for the dimension of naturalness–even when including processed food–only around 33% of objects were natural, whereas 67% were artificial. This demonstrates that, at least based on everyday object naming, a majority of object concepts indeed refers to artificial and inanimate objects. This result makes sense, given that humans are constantly surrounded by a large variety of inanimate and artificial objects whose distinction for everyday use is relevant. However, animals belong to one of the largest categories, are a rather homogenous class (e.g. most animals have faces) and may be more significant from an evolutionary point of view, which may contribute to their increased salience. Note that the significance of distinguishing between concepts by their linguistic use need not imply that categories with a more diverse set of concepts are more common; rather, it may prove to be useful to distinguish many different types of the same category if they have distinct functional significance.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39773</offset><text>A notable feature of the database is that a substantial proportion of objects (bottom-up categorization: 12.73%, top-down categorization: 12.08%) was categorized as belonging to one or several of the categories of edible items (“food,” “vegetable,” “fruit,” “dessert,” “drink”). Two more categories refer to kitchen items (“kitchen tool,” “kitchen appliance”), demonstrating the general importance of discriminating among food-related items.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>40243</offset><text>Relationship of core high-level object categories with semantic embedding</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40317</offset><text>To determine the degree to which the high-level categories reflect their actual use in language, we visualized the similarity of concepts by running t-distributed stochastic neighborhood embedding (t-SNE, perplexity = 30, initialized with perplexity = 5,) on the semantic embedding (i.e. synset vectors) and displaying the 27 core object categories in different colors (Fig 2A). The resulting visualization indicates that much of the categorization provided by humans is also mirrored in the synset vector similarity, although some structure is missed. To provide a quantitative basis for this result, we measured the selectivity of each concept by comparing the correlation of concepts within each of the 27 core categories to the correlation of those concepts with all other concepts (Fig 2B, all left bars). We repeated the same analysis for the high-level categories derived from WordNet (Fig 2B, all right bars). The results demonstrate a positive selectivity (i.e. correlation difference) for all 27 core categories, both for bottom-up and top-down categories (all p &lt; 0.0054, Bonferroni-corrected over 27 categories, based on 5,000 Monte Carlo samples of categories). Overall, the selectivity of categories was higher for the “bottom-up categories” as compared to the categories defined based on WordNet (Δrbottom-up = 0.23, Δrtop-down = 0.19, p &lt; 0.0002).</text></passage><passage><infon key="file">pone.0223792.g002.jpg</infon><infon key="id">pone.0223792.g002</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>41689</offset><text>Category structure of THINGS object concepts.</text></passage><passage><infon key="file">pone.0223792.g002.jpg</infon><infon key="id">pone.0223792.g002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>41735</offset><text>(A) Visualization of the semantic relationship of the 1,854 object concepts applying t-SNE to the semantic embedding, with the 27 core object categories depicted in different colors and example concepts highlighted. (B) Selectivity of the 27 core object categories, separately for bottom-up categorization based on responses of workers on Amazon Mechanical Turk (left bars, darker shades) and top-down categorization based on category membership in WordNet (right bars, lighter shades). Category selectivity was quantified by the difference in correlation of semantic embedding vectors of concepts within each category as compared to the correlation with concepts outside of the category (all p &lt; 0.05, Bonferroni-corrected). Across all concepts, the selectivity for bottom-up categorization was higher than the selectivity for top-down categorization (p &lt; 0.001).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>42600</offset><text>Object image database</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42622</offset><text>For the object image database, we identified a total of 26,107 images (mean number of images per concept: 14.08). Of those images, 1,165 were selected from ImageNet. Example images for a small set of concepts are shown in Fig 3A. The mean image size was 996 × 996 pixels (&lt; 1.8% of images smaller than 500 pixels). The distribution of number of images per concept and pixel dimensions is shown in Fig 3B.</text></passage><passage><infon key="file">pone.0223792.g003.jpg</infon><infon key="id">pone.0223792.g003</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>43028</offset><text>Examples and statistics of images in THINGS database.</text></passage><passage><infon key="file">pone.0223792.g003.jpg</infon><infon key="id">pone.0223792.g003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>43082</offset><text>(A) Example images in THINGS database for different object concepts. Some of the original images were replaced due to the journal’s copyright policy. The original figure can be found at https://www.biorxiv.org/content/10.1101/545954v2. (B) Histograms illustrating the number of images per concept (ranging from 12 to 35) and the image dimensions (peaking at 800 pixels).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43455</offset><text>To determine the degree to which individual object images are good members of a concept while still exhibiting visual variability, we fed all images through different layers of a deep convolutional neural network CorNet-S, as described in the Methods section. We predicted that at high layers, images within a concept would exhibit a larger similarity–as given by their Pearson correlation–than images between different concepts. In addition, we predicted this difference in similarity to be larger in the top layer (classification layer), but smaller in the bottom layer (V1 layer). Finally, for each of the 26,107 object images we investigated to what degree the activation of one object image could be used to predict other members of the same concept. The percentage of correct guesses is given by how many of the most similar images are populated by the n-1 images of the same concept, i.e. excluding the reference image. We define the top-1 accuracy as the first n-1 image ranks, the top-5 accuracy as 5 times that number, and the median rank as how many guesses are required for each other member of the concept. The results of these analyses are shown in Fig 4. For all layers, the correlation was higher within concept than between concept (classification layer: r = 0.51 within, r = 0.04 between; V1 layer: r = 0.39 within, r = 0.34 between, all p &lt; 0.001, based on 1,000 randomizations) with the correlation difference increasing between the first layer and the classification layer (p &lt; 0.001), demonstrating selectivity within each concept that increases across layers. The pairwise similarity was a good predictor of other members of the same concept for the classification layer (top-1 accuracy: 37.29%, top-5 accuracy: 61.56%, median rank: 29), but much less so for the V1 layer (top-1 accuracy: 1.25%, top-5 accuracy: 3.51%, median rank: 5,530), despite the network being trained only on a subset of concepts (334/1000 concepts overlap with THINGS, 212/1000 are subordinate examples of the concepts used in THINGS). Together, these results demonstrate that the object image database constitutes both a good representation of individual object concepts, while still exhibiting notable variation in low-level image properties.</text></passage><passage><infon key="file">pone.0223792.g004.jpg</infon><infon key="id">pone.0223792.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>45701</offset><text>Selectivity of object images for different concepts.</text></passage><passage><infon key="file">pone.0223792.g004.jpg</infon><infon key="id">pone.0223792.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>45754</offset><text>(A) Similarity of image activation vectors across different layers of the deep convolutional neural network CORnet. The first four layers of CORnet are named after brain regions in the macaque monkey. Similarity was estimated by the mean Pearson correlation of activation patterns in a given layer. “Within concept” refers to the similarity of all pairs of activation vectors of images of the same concept, while similarity “between concept” refers to the similarity of those images and all other images in the database. Higher selectivity is indicated by a larger difference in “within concept” and “between concept” similarities. (B) Predictive accuracy of activation vectors based on their pairwise Pearson similarity. “Top-1 accuracy” refers to the percentage of the most similar images that belong to the same concept as a reference image and is averaged across all accuracies for all images. “Top-5 accuracy” allows five guesses. Note that this procedure is not directly comparable to that used in typical machine learning applications (see main text for details).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>46849</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>46860</offset><text>Here we present a large-scale database of 1,854 diverse object concepts and 26,107 high-quality naturalistic images of those objects. For the object concepts, we identified 27 high-level categories that capture around half of those concepts with little overlap. We validated the categories and the object images by relating them to representations in a semantic embedding and a deep convolutional neural network, suggesting that the categories are meaningfully related to their use in language and that the object images represent largely distinct categories while varying in basic visual properties.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>47461</offset><text>Possible applications of the object concept and object image database</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>47531</offset><text>The purpose of this database is to provide researchers in the fields of psychology, neuroscience, and computer science with a resource they can use to systematically select object concepts or object images for their research. The availability of a large-scale resource has the advantage of providing a more standardized approach for the selection of object concepts and object images. In addition, if adopted more widely, it offers an increased level of comparability between different studies.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48026</offset><text>There are a wide range of potential applications of such a database, and we will briefly discuss only a few. In the field of psychology, studies in object recognition, categorization and semantic memory can use this database to identify a more representative set of concepts and provide naturalistic example object images with them. This would allow researchers to study the large-scale structure and format of mental representations of objects, to reveal the degree to which categorization behavior (e.g. as measured in) generalizes to less commonly-used categories and more natural images, and to determine factors affecting the recall and recognition of concepts and images. In the field of neuroscience, the object image database can form the basis for a more systematic large-scale assessment of object representations based on intensive measurements within a small number of subjects and thus offer the basis for a dataset that is more representative than those commonly used as benchmarks in methods development and object recognition research (e.g.). In the field of computer vision, while the number of object images provided in this dataset is too small for training current deep convolutional neural networks, they can be used to test how wide a range of concepts are spanned by a given computational model. More generally, the concepts identified in this research could form a comprehensive set of labels for object classification at a level that is more comparable to humans (see also).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>49526</offset><text>Comparison to previous work and existing databases</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>49577</offset><text>Selection of object concepts and object categories in previous work</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49645</offset><text>For identifying and selecting object concepts for their studies, researchers have been using a variety of strategies, often focusing on a small set of basic-level objects, specific superordinate categories and examples within those categories (e.g. animals, tools, or vehicles), using objects that vary along assumed representational dimensions (e.g. animacy, man-made and biological objects, manipulability, real-world size), and other criteria (for reviews, see). While these selection criteria are useful for the specific hypotheses at hand, the selected concepts are often not general enough to be of use for other research questions, requiring researchers to repeat the identification and selection process for their own purposes. In addition, the definition of what constitutes a category varies between studies, which affects the comparability of results and the conclusions that are drawn. The database presented in this work constitutes a wide range of concepts that allows their systematic and reproducible selection. For example, if the selection of a wide range of 50 object concepts is desired, researchers could apply cluster analysis to the synset vectors provided with this dataset and select one concept from each cluster.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>50885</offset><text>A wider range and more standardized set of concepts to choose from is provided by category norms or object property norms, which can be useful for improving comparability between studies. Despite their common use, it is important to note that the concepts and categories in those norms were selected mostly based on their use in previous studies. Thus, they may span a rather selective set of objects and higher-level categories. In addition, they contain concepts at a level of description that may not align with how the object is commonly named (e.g. “python” vs. “snake”), and for some categories may not conform to the way humans would naturally group their members (e.g. “four-footed animal”). For those reasons, depending on the goal of the study, a more systematic approach for concept selection and category definition may be desired. In the future, we hope to provide more systematic category norms and object property norms for the large set of object concepts in the THINGS database, which would combine the benefits of previous efforts with those of the present work.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>51978</offset><text>As discussed above (see Methods), a highly systematic compilation of object concepts is provided through WordNet, a lexical database that stores words and their hierarchical relationship according to their meaning as so-called synsets. This format is very valuable for identifying unique meaning of ambiguous words (e.g. “bat” as a nocturnal animal or as a club used in sports), for merging synonymous words (e.g. “sofa” and “couch”), and for identifying more high-level categories in general. Some researchers have used WordNet to circumvent selection biases by randomly sampling from a broader set of concepts. For example, the 1,000 synsets in the ImageNet Challenge have become a standard set of object concepts in computer vision research, and a large part of them were sampled at random from WordNet. However, WordNet also contains a large number of concepts that can only be identified by experts (e.g. “tobacco hornworm,” “trogon”) or that cannot be distinguished easily by just looking at pictures of them (e.g. “black pine” vs. “red pine”). In addition, it is not always clear what level of categorization is the most useful for a given object (e.g. “canine” vs. “dog” vs. “poodle”) and to what degree the WordNet hierarchy translates to the everyday use of concepts and categories (e.g. in WordNet a “hydrant” belongs to the category of “discharge pipes”). The list of object concepts presented in this work addresses this challenge by identifying the level of description that matches their use in object naming, while still providing synsets to tie them to unique word meanings and relate them to the WordNet hierarchy.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>53659</offset><text>Finally, another approach for the selection of object concepts in category selection is to label objects based on their natural appearance in photographs or movies and use these labels in later research. While this approach avoids a “top-down” selection bias, sampling of categories may lead to a “bottom-up” bias, by mirroring the statistics of the concepts found in the source stimulus set, thereby potentially overestimating the significance of frequent concepts and underestimating the significance of rarer ones.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>54185</offset><text>Selection of object images in previous work</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>54229</offset><text>The format of visual presentation of objects is important for the study of visual cognition, including visual object recognition, memory, categorization, and naming. To this end, the use of standardized line drawings of objects has been a dominant approach. However, researchers have started to increasingly rely on the use of naturalistic object stimuli from photographs, in order to more closely match the conditions of real-world perception. Those naturalistic stimuli have been ranging from images of isolated objects cropped from their natural background (e.g.) to object renderings placed on naturalistic scenes (e.g.), object images with a naturalistic background (e.g.), multiple objects in naturalistic scenes (e.g.), and objects appearing in dynamic movies (e.g.).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>55004</offset><text>In recent years, numerous standardized object image databases have been published for psychological and neuroscience research (for review, see), which most commonly consist of naturalistic images of objects cropped from their natural background, such as the Bank of Standardized Stimuli (BOSS). This approach has been and still is very valuable for the study of visual cognition and memory. At the same time, most of these image databases contain only one or very few examples of a given object (e.g. “lamp”) or only a small number of object concepts, but not both many concepts and numerous examples. In addition, the naturalistic context in which objects appear is known to be important to object processing, and there is evidence that the use of cropped images may overemphasize the role of shape features in measured neural representations. For those reasons, depending on the research question, it is important to also consider the use of a wider range of object images embedded in their naturalistic context.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>56023</offset><text>In computer vision, several large-scale object image databases exist that provide up to thousands of examples of individual objects in a naturalistic context. However, the size or quality of a large portion of images in those databases is not sufficient for their widespread use in psychology and neuroscience experiments and requires researchers to carefully and manually select candidate images to be of sufficient quality, which even after selection still involves trade-offs with respect to the size, aspect ratio and naturalness of those images. The THINGS database offers a comprehensive set of high-quality object images that should be of sufficient size for most applications in psychology and neuroscience research. While the number of exemplars in the THINGS database is notably lower than that in computer vision databases, the images can serve as a test set for assessing the generality of existing computer vision algorithms. For example, it is promising that a convolutional neural network trained on only a subset of the concepts in the THINGS database can still yield reasonable classification performance even without retraining and when using a comparably simple method for classification.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>57231</offset><text>Limitations of the THINGS database</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>57266</offset><text>Limitations in the selection of concepts</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>57307</offset><text>Many steps were involved in the definition of the list of 1,854 concepts, requiring choices during each step of the selection process. We laid out the exact choices at each step in great detail and provide the list of excluded words alongside the final list of concepts, allowing researchers to choose different inclusion and exclusion criteria if they wish. By the nature of the task, some of our choices were subjective and can be debated, and others involved a trade-off between efficiency and completeness. For example, for the initial selection based on concreteness we defined a cutoff below which we did not select any concept, likely excluding a small number of picturable and nameable objects. Further, we excluded objects that necessarily depict text. This makes sense given the goals of the present database, but it could arguably have been interesting to include them, given the known special role of text characters in the human brain. However, the list of excluded text words is marked and available as part of the database. As another example limitation, one might argue that the list should have focused exclusively on objects and excluded items defined from mass nouns (e.g. “sand,” “coal”) or other items (e.g. “fingerprint”). We chose to include these items, because they are nameable, concrete and refer to entities beyond texture or surface material. Their exclusion would also have led to the exclusion of drinks, which in this database turned out to be one of the most common categories. However, researchers may choose to remove those items for their own purposes.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>58908</offset><text>While we selected object concepts based on whether they were named consistently, this choice was based mostly on one reference image only and a relatively small number of participants per concept. Additionally, it required choosing another arbitrary cutoff for which concept to include or to exclude. As mentioned above, the goal of this database was not to create a definite set of all nameable object concepts. We chose to be rather inclusive in this step, so that excluded concepts would be those that were named inconsistently by participants. Links to all reference images are available as part of this database. In the future, researchers may choose a similar approach with a wider range of reference images and more behavioral responses to identify a more general set of object concepts.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59703</offset><text>Another limitation of our approach is that ultimately object concepts were selected based on a list of nouns. While this approach is common, it may bias the selection of concepts (and categories) towards representations that have a wider linguistic variability (e.g. different food items), which need not be representative of their mental representation. In theory, there are alternative approaches that circumvent the use of language, for example the ability to discriminate between different randomly-selected objects. However, without the use of language, such approaches would be challenging to carry out in practice. We chose a more pragmatic approach based on WordNet synsets and object naming that goes beyond typical approaches, by using a much wider range of nouns as a basis for concept selection. In the future, it might be possible to determine a larger, more representative set of concepts that humans can still distinguish without reverting to linguistic criteria.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>60682</offset><text>Limitations of the object images</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>60715</offset><text>The object image database comes with limitations, as well. Most of these limitations are related to the difficulty of finding good examples of object images. Even though we intended object images to be of high quality and contain natural background throughout the database, for some concepts it was difficult to impossible to find good examples of images embedded in a natural background, so trade-offs with image quality had to be made. Similarly, the choice not to focus exclusively on images with single examples of objects but allow several instances of the same object in an image is debatable. On the other hand, this choice may in fact reflect a more natural form of object context.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61405</offset><text>Object viewpoint and object background might not vary to a degree sufficient to test viewpoint-invariant and background-invariant object representations. At the same time, image representations in early layers of the deep convolutional neural network turned out to be quite unspecific for individual object concepts, while representations at higher layers were quite specific. This indicates that the level of image variation effectively controlled for much of low-level processing while still providing relatively high degrees of concept specificity.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61957</offset><text>Since almost all object images were originally in jpeg-format, we chose to use the same image format for the image database. However, cropping images to square made it necessary to save images again with jpeg-compression, which may have introduced an additional loss in image quality that affects the frequency spectrum of the images. Thus, for future databases the use of lossless formats (e.g. png-format) may prove beneficial.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>62387</offset><text>Finally, one limitation of this object image database is that it likely contains copyrighted material, which limits their use to academic purposes under fair use regulations. In addition, there are limitations for the publication of example images in scientific journals that may vary between journals. For example, even though the images originally selected in Fig 3 were chosen to be examples in THINGS that come from sources permitting reuse with modification, the policy of this journal required us to choose images from the public domain only. However, it is likely not feasible to create an image database of this scale with images from online sources that are exclusively and demonstrably from the public domain.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>63107</offset><text>Future directions</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>63125</offset><text>Apart from potential improvements for the choice of object concepts and object images, there are a number of avenues for future developments as part of the THINGS database. First and foremost, additional high quality images may be selected, cropped, and added to the database, ideally focusing on non-copyrighted examples such as those provided in the Open Images database. Second, for researchers interested in using representative cropped examples, existing databases may be amended for that purpose (e.g.). Third, in addition to the object categorization provided by participants and through WordNet, the existing database can be amended with expert categorization, potentially further improving the correspondence to semantic embeddings. Fourth, those categories could be used to generate a comprehensive set of typicality ratings of objects concepts. Finally, the concepts can form the basis for the creation of feature norms similar to existing ones or explicit ratings of object dimensions (e.g. real-world size, animacy, manipulability, etc.). Together, we hope that the THINGS database is widely adopted by the scientific communities in psychology, neuroscience, and computer science, thereby broadening the use systematic and large-scale naturalistic research and further advancing the communication between these fields of research.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>64469</offset><text>Supporting information</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>64492</offset><text>References</text></passage><passage><infon key="fpage">520</infon><infon key="issue">12</infon><infon key="lpage">7</infon><infon key="name_0">surname:Oliva;given-names:A</infon><infon key="name_1">surname:Torralba;given-names:A</infon><infon key="pub-id_doi">10.1016/j.tics.2007.09.009</infon><infon key="pub-id_pmid">18024143</infon><infon key="section_type">REF</infon><infon key="source">Trends in cognitive sciences</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2007</infon><offset>64503</offset><text>The role of context in object recognition</text></passage><passage><infon key="name_0">surname:Deng;given-names:J</infon><infon key="name_1">surname:Dong;given-names:W</infon><infon key="name_2">surname:Socher;given-names:R</infon><infon key="name_3">surname:Li;given-names:L-J</infon><infon key="name_4">surname:Li;given-names:K</infon><infon key="name_5">surname:Fei-Fei;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>64545</offset><text>Imagenet: A large-scale hierarchical image database</text></passage><passage><infon key="fpage">389</infon><infon key="issue">3</infon><infon key="lpage">95</infon><infon key="name_0">surname:Einhäuser;given-names:W</infon><infon key="name_1">surname:König;given-names:P</infon><infon key="pub-id_doi">10.1016/j.conb.2010.03.010</infon><infon key="pub-id_pmid">20434327</infon><infon key="section_type">REF</infon><infon key="source">Current opinion in neurobiology</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2010</infon><offset>64597</offset><text>Getting real—sensory processing of natural stimuli</text></passage><passage><infon key="fpage">1643</infon><infon key="issue">12</infon><infon key="name_0">surname:Felsen;given-names:G</infon><infon key="name_1">surname:Dan;given-names:Y</infon><infon key="pub-id_doi">10.1038/nn1608</infon><infon key="pub-id_pmid">16306891</infon><infon key="section_type">REF</infon><infon key="source">Nature neuroscience</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2005</infon><offset>64650</offset><text>A natural approach to studying vision</text></passage><passage><infon key="fpage">963</infon><infon key="issue">1</infon><infon key="name_0">surname:Pereira;given-names:F</infon><infon key="name_1">surname:Lou;given-names:B</infon><infon key="name_2">surname:Pritchett;given-names:B</infon><infon key="name_3">surname:Ritter;given-names:S</infon><infon key="name_4">surname:Gershman;given-names:SJ</infon><infon key="name_5">surname:Kanwisher;given-names:N</infon><infon key="pub-id_doi">10.1038/s41467-018-03068-4</infon><infon key="pub-id_pmid">29511192</infon><infon key="section_type">REF</infon><infon key="source">Nature communications</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2018</infon><offset>64688</offset><text>Toward a universal decoder of linguistic meaning from brain activation</text></passage><passage><infon key="name_0">surname:Krizhevsky;given-names:A</infon><infon key="name_1">surname:Sutskever;given-names:I</infon><infon key="name_2">surname:Hinton;given-names:GE</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>64759</offset><text>editors. Imagenet classification with deep convolutional neural networks</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>64832</offset><text>Mikolov T, Yih W-t, Zweig G, editors. Linguistic regularities in continuous space word representations. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2013.</text></passage><passage><infon key="name_0">surname:Pennington;given-names:J</infon><infon key="name_1">surname:Socher;given-names:R</infon><infon key="name_2">surname:Manning;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>65086</offset><text>Glove: Global vectors for word representation</text></passage><passage><infon key="fpage">1</infon><infon key="issue">3p2</infon><infon key="lpage">46</infon><infon key="name_0">surname:Battig;given-names:WF</infon><infon key="name_1">surname:Montague;given-names:WE</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology</infon><infon key="type">ref</infon><infon key="volume">80</infon><infon key="year">1969</infon><offset>65132</offset><text>Category norms of verbal items in 56 categories A replication and extension of the Connecticut category norms</text></passage><passage><infon key="fpage">289</infon><infon key="issue">3</infon><infon key="lpage">335</infon><infon key="name_0">surname:Van Overschelde;given-names:JP</infon><infon key="name_1">surname:Rawson;given-names:KA</infon><infon key="name_2">surname:Dunlosky;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Journal of Memory and Language</infon><infon key="type">ref</infon><infon key="volume">50</infon><infon key="year">2004</infon><offset>65242</offset><text>Category norms: An updated and expanded version of the Battig and Montague (1969) norms</text></passage><passage><infon key="name_0">surname:Fellbaum;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">WordNet: An electronic lexical database</infon><infon key="type">ref</infon><infon key="year">1998</infon><offset>65330</offset></passage><passage><infon key="fpage">174</infon><infon key="issue">2</infon><infon key="name_0">surname:Snodgrass;given-names:JG</infon><infon key="name_1">surname:Vanderwart;given-names:M</infon><infon key="pub-id_pmid">7373248</infon><infon key="section_type">REF</infon><infon key="source">Journal of experimental psychology: Human learning and memory</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">1980</infon><offset>65331</offset><text>A standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity</text></passage><passage><infon key="fpage">e10773</infon><infon key="issue">5</infon><infon key="name_0">surname:Brodeur;given-names:MB</infon><infon key="name_1">surname:Dionne-Dostie;given-names:E</infon><infon key="name_2">surname:Montreuil;given-names:T</infon><infon key="name_3">surname:Lepage;given-names:M</infon><infon key="pub-id_pmid">20532245</infon><infon key="section_type">REF</infon><infon key="source">PloS one</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2010</infon><offset>65445</offset><text>The Bank of Standardized Stimuli (BOSS), a new set of 480 normative photos of objects to be used as visual stimuli in cognitive research</text></passage><passage><infon key="fpage">e106953</infon><infon key="issue">9</infon><infon key="name_0">surname:Brodeur;given-names:MB</infon><infon key="name_1">surname:Guérard;given-names:K</infon><infon key="name_2">surname:Bouras;given-names:M</infon><infon key="pub-id_doi">10.1371/journal.pone.0106953</infon><infon key="pub-id_pmid">25211489</infon><infon key="section_type">REF</infon><infon key="source">PLoS One</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>65582</offset><text>Bank of standardized stimuli (BOSS) phase II: 930 new normative photos</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:Bracci;given-names:S</infon><infon key="name_1">surname:Daniels;given-names:N</infon><infon key="name_2">surname:de Beeck;given-names:HO</infon><infon key="pub-id_doi">10.1093/cercor/bhw362</infon><infon key="pub-id_pmid">28365777</infon><infon key="section_type">REF</infon><infon key="source">Cerebral Cortex</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>65653</offset><text>Task context overrules object- and category-related representational content in the human parietal cortex</text></passage><passage><infon key="fpage">432</infon><infon key="issue">2</infon><infon key="lpage">44</infon><infon key="name_0">surname:Bracci;given-names:S</infon><infon key="name_1">surname:de Beeck;given-names:HO</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.2314-15.2016</infon><infon key="pub-id_pmid">26758835</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2016</infon><offset>65759</offset><text>Dissociations and associations between shape and category representations in the two visual pathways</text></passage><passage><infon key="fpage">107</infon><infon key="lpage">14</infon><infon key="name_0">surname:Coggan;given-names:DD</infon><infon key="name_1">surname:Liu;given-names:W</infon><infon key="name_2">surname:Baker;given-names:DH</infon><infon key="name_3">surname:Andrews;given-names:TJ</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2016.04.060</infon><infon key="pub-id_pmid">27132543</infon><infon key="section_type">REF</infon><infon key="source">Neuroimage</infon><infon key="type">ref</infon><infon key="volume">135</infon><infon key="year">2016</infon><offset>65860</offset><text>Category-selective patterns of neural response in the ventral visual pathway in the absence of categorical information</text></passage><passage><infon key="fpage">238584</infon><infon key="name_0">surname:Proklova;given-names:D</infon><infon key="name_1">surname:Kaiser;given-names:D</infon><infon key="name_2">surname:Peelen;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>65979</offset><text>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</text></passage><passage><infon key="name_0">surname:Proklova;given-names:D</infon><infon key="name_1">surname:Kaiser;given-names:D</infon><infon key="name_2">surname:Peelen;given-names:MV</infon><infon key="section_type">REF</infon><infon key="source">Journal of cognitive neuroscience</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>66082</offset><text>Disentangling representations of object shape and object category in human visual cortex: The animate–inanimate distinction</text></passage><passage><infon key="name_0">surname:Krizhevsky;given-names:A</infon><infon key="name_1">surname:Hinton;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">Citeseer</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>66208</offset><text>Learning multiple layers of features from tiny images</text></passage><passage><infon key="name_0">surname:Griffin;given-names:G</infon><infon key="name_1">surname:Holub;given-names:A</infon><infon key="name_2">surname:Perona;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Caltech-256 object category dataset</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>66262</offset></passage><passage><infon key="fpage">303</infon><infon key="issue">2</infon><infon key="lpage">38</infon><infon key="name_0">surname:Everingham;given-names:M</infon><infon key="name_1">surname:Van Gool;given-names:L</infon><infon key="name_2">surname:Williams;given-names:CK</infon><infon key="name_3">surname:Winn;given-names:J</infon><infon key="name_4">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">International journal of computer vision</infon><infon key="type">ref</infon><infon key="volume">88</infon><infon key="year">2010</infon><offset>66263</offset><text>The pascal visual object classes (voc) challenge</text></passage><passage><infon key="fpage">180901281</infon><infon key="name_0">surname:Chang;given-names:N</infon><infon key="name_1">surname:Pyles;given-names:JA</infon><infon key="name_2">surname:Gupta;given-names:A</infon><infon key="name_3">surname:Tarr;given-names:MJ</infon><infon key="name_4">surname:Aminoff;given-names:EM</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>66312</offset><text>BOLD5000: A public fMRI dataset of 5000 images</text></passage><passage><infon key="fpage">181100982</infon><infon key="name_0">surname:Kuznetsova;given-names:A</infon><infon key="name_1">surname:Rom;given-names:H</infon><infon key="name_2">surname:Alldrin;given-names:N</infon><infon key="name_3">surname:Uijlings;given-names:J</infon><infon key="name_4">surname:Krasin;given-names:I</infon><infon key="name_5">surname:Pont-Tuset;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>66359</offset><text>The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>66478</offset><text>Lin T-Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, et al., editors. Microsoft coco: Common objects in context. European conference on computer vision; 2014: Springer.</text></passage><passage><infon key="fpage">1452</infon><infon key="issue">6</infon><infon key="lpage">64</infon><infon key="name_0">surname:Zhou;given-names:B</infon><infon key="name_1">surname:Lapedriza;given-names:A</infon><infon key="name_2">surname:Khosla;given-names:A</infon><infon key="name_3">surname:Oliva;given-names:A</infon><infon key="name_4">surname:Torralba;given-names:A</infon><infon key="pub-id_doi">10.1109/TPAMI.2017.2723009</infon><infon key="pub-id_pmid">28692961</infon><infon key="section_type">REF</infon><infon key="source">IEEE transactions on pattern analysis and machine intelligence</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2018</infon><offset>66653</offset><text>Places: A 10 million image database for scene recognition</text></passage><passage><infon key="fpage">904</infon><infon key="issue">3</infon><infon key="lpage">11</infon><infon key="name_0">surname:Brysbaert;given-names:M</infon><infon key="name_1">surname:Warriner;given-names:AB</infon><infon key="name_2">surname:Kuperman;given-names:V</infon><infon key="pub-id_doi">10.3758/s13428-013-0403-5</infon><infon key="pub-id_pmid">24142837</infon><infon key="section_type">REF</infon><infon key="source">Behavior research methods</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2014</infon><offset>66711</offset><text>Concreteness ratings for 40 thousand generally known English word lemmas</text></passage><passage><infon key="fpage">991</infon><infon key="issue">4</infon><infon key="lpage">7</infon><infon key="name_0">surname:Brysbaert;given-names:M</infon><infon key="name_1">surname:New;given-names:B</infon><infon key="name_2">surname:Keuleers;given-names:E</infon><infon key="pub-id_doi">10.3758/s13428-012-0190-4</infon><infon key="pub-id_pmid">22396136</infon><infon key="section_type">REF</infon><infon key="source">Behavior research methods</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2012</infon><offset>66784</offset><text>Adding part-of-speech information to the SUBTLEX-US word frequencies</text></passage><passage><infon key="fpage">287</infon><infon key="issue">1</infon><infon key="lpage">304</infon><infon key="name_0">surname:Keuleers;given-names:E</infon><infon key="name_1">surname:Lacey;given-names:P</infon><infon key="name_2">surname:Rastle;given-names:K</infon><infon key="name_3">surname:Brysbaert;given-names:M</infon><infon key="pub-id_doi">10.3758/s13428-011-0118-4</infon><infon key="pub-id_pmid">21720920</infon><infon key="section_type">REF</infon><infon key="source">Behavior research methods</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2012</infon><offset>66853</offset><text>The British Lexicon Project: Lexical decision data for 28,730 monosyllabic and disyllabic English words</text></passage><passage><infon key="name_0">surname:Davies;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">The corpus of contemporary American English</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>66957</offset></passage><passage><infon key="name_0">surname:Mehrer;given-names:J</infon><infon key="name_1">surname:Kietzmann;given-names:TC</infon><infon key="name_2">surname:Kriegeskorte;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">Poster presented at Conference on Cognitive Computational Neuroscience</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>66958</offset><text>Deep neural networks trained on ecologically relevant categories better explain human IT</text></passage><passage><infon key="fpage">14091556</infon><infon key="name_0">surname:Simonyan;given-names:K</infon><infon key="name_1">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>67047</offset><text>Very deep convolutional networks for large-scale image recognition</text></passage><passage><infon key="fpage">160801961</infon><infon key="name_0">surname:Pilehvar;given-names:MT</infon><infon key="name_1">surname:Collier;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>67114</offset><text>De-conflated semantic representations</text></passage><passage><infon key="fpage">27755</infon><infon key="name_0">surname:Cichy;given-names:RM</infon><infon key="name_1">surname:Khosla;given-names:A</infon><infon key="name_2">surname:Pantazis;given-names:D</infon><infon key="name_3">surname:Torralba;given-names:A</infon><infon key="name_4">surname:Oliva;given-names:A</infon><infon key="pub-id_doi">10.1038/srep27755</infon><infon key="pub-id_pmid">27282108</infon><infon key="section_type">REF</infon><infon key="source">Scientific reports</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>67152</offset><text>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</text></passage><passage><infon key="fpage">e1003915</infon><infon key="issue">11</infon><infon key="name_0">surname:Khaligh-Razavi;given-names:S-M</infon><infon key="name_1">surname:Kriegeskorte;given-names:N</infon><infon key="pub-id_doi">10.1371/journal.pcbi.1003915</infon><infon key="pub-id_pmid">25375136</infon><infon key="section_type">REF</infon><infon key="source">PLoS computational biology</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2014</infon><offset>67295</offset><text>Deep supervised, but not unsupervised, models may explain IT cortical representation</text></passage><passage><infon key="fpage">8619</infon><infon key="issue">23</infon><infon key="lpage">24</infon><infon key="name_0">surname:Yamins;given-names:DL</infon><infon key="name_1">surname:Hong;given-names:H</infon><infon key="name_2">surname:Cadieu;given-names:CF</infon><infon key="name_3">surname:Solomon;given-names:EA</infon><infon key="name_4">surname:Seibert;given-names:D</infon><infon key="name_5">surname:DiCarlo;given-names:JJ</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">111</infon><infon key="year">2014</infon><offset>67380</offset><text>Performance-optimized hierarchical models predict neural responses in higher visual cortex</text></passage><passage><infon key="name_0">surname:Kietzmann;given-names:TC</infon><infon key="name_1">surname:McClure;given-names:P</infon><infon key="name_2">surname:Kriegeskorte;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">Deep Neural Networks in Computational Neuroscience</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>67471</offset></passage><passage><infon key="name_0">surname:Cichy;given-names:RM</infon><infon key="name_1">surname:Kaiser;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Trends in cognitive sciences</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>67472</offset><text>Deep neural networks as scientific models</text></passage><passage><infon key="fpage">408385</infon><infon key="name_0">surname:Kubilius;given-names:J</infon><infon key="name_1">surname:Schrimpf;given-names:M</infon><infon key="name_2">surname:Nayebi;given-names:A</infon><infon key="name_3">surname:Bear;given-names:D</infon><infon key="name_4">surname:Yamins;given-names:DL</infon><infon key="name_5">surname:DiCarlo;given-names:JJ</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>67514</offset><text>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</text></passage><passage><infon key="fpage">2579</infon><infon key="issue">Nov</infon><infon key="lpage">605</infon><infon key="name_0">surname:Lvd;given-names:Maaten</infon><infon key="name_1">surname:Hinton;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">Journal of machine learning research</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2008</infon><offset>67580</offset><text>Visualizing data using t-SNE</text></passage><passage><infon key="fpage">23</infon><infon key="issue">1</infon><infon key="lpage">37</infon><infon key="name_0">surname:Konkle;given-names:T</infon><infon key="name_1">surname:Oliva;given-names:A</infon><infon key="pub-id_pmid">20822298</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: Human Perception &amp; Performance</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2011</infon><offset>67609</offset><text>Canonical visual size for real-world objects</text></passage><passage><infon key="fpage">1726</infon><infon key="name_0">surname:Jozwik;given-names:KM</infon><infon key="name_1">surname:Kriegeskorte;given-names:N</infon><infon key="name_2">surname:Storrs;given-names:KR</infon><infon key="name_3">surname:Mur;given-names:M</infon><infon key="pub-id_doi">10.3389/fpsyg.2017.01726</infon><infon key="pub-id_pmid">29062291</infon><infon key="section_type">REF</infon><infon key="source">Frontiers in psychology</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>67654</offset><text>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</text></passage><passage><infon key="fpage">1901.02915</infon><infon key="name_0">surname:Zheng;given-names:CY</infon><infon key="name_1">surname:Pereira;given-names:F</infon><infon key="name_2">surname:Baker;given-names:CI</infon><infon key="name_3">surname:Hebart;given-names:MN</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>67783</offset><text>Revealing interpretable object representations from human behavior</text></passage><passage><infon key="fpage">95</infon><infon key="issue">1</infon><infon key="name_0">surname:Long;given-names:B</infon><infon key="name_1">surname:Konkle;given-names:T</infon><infon key="name_2">surname:Cohen;given-names:MA</infon><infon key="name_3">surname:Alvarez;given-names:GA</infon><infon key="pub-id_pmid">26709591</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: General</infon><infon key="type">ref</infon><infon key="volume">145</infon><infon key="year">2016</infon><offset>67850</offset><text>Mid-level perceptual features distinguish objects of different real-world sizes</text></passage><passage><infon key="fpage">441</infon><infon key="issue">3</infon><infon key="lpage">50</infon><infon key="name_0">surname:Kiran;given-names:S</infon><infon key="name_1">surname:Thompson;given-names:CK</infon><infon key="pub-id_doi">10.1016/s0093-934x(03)00064-6</infon><infon key="pub-id_pmid">12744956</infon><infon key="section_type">REF</infon><infon key="source">Brain and Language</infon><infon key="type">ref</infon><infon key="volume">85</infon><infon key="year">2003</infon><offset>67930</offset><text>Effect of typicality on online category verification of animate category exemplars in aphasia</text></passage><passage><infon key="fpage">1762</infon><infon key="issue">11</infon><infon key="lpage">76</infon><infon key="name_0">surname:Kirchner;given-names:H</infon><infon key="name_1">surname:Thorpe;given-names:SJ</infon><infon key="pub-id_doi">10.1016/j.visres.2005.10.002</infon><infon key="pub-id_pmid">16289663</infon><infon key="section_type">REF</infon><infon key="source">Vision research</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2006</infon><offset>68024</offset><text>Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited</text></passage><passage><infon key="fpage">12127</infon><infon key="lpage">36</infon><infon key="name_0">surname:Rajalingham;given-names:R</infon><infon key="name_1">surname:Schmidt;given-names:K</infon><infon key="name_2">surname:DiCarlo;given-names:JJ</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.0573-15.2015</infon><infon key="pub-id_pmid">26338324</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">2015</infon><offset>68116</offset><text>Comparison of object recognition behavior in human and monkey</text></passage><passage><infon key="fpage">558</infon><infon key="issue">3</infon><infon key="name_0">surname:Konkle;given-names:T</infon><infon key="name_1">surname:Brady;given-names:TF</infon><infon key="name_2">surname:Alvarez;given-names:GA</infon><infon key="name_3">surname:Oliva;given-names:A</infon><infon key="pub-id_pmid">20677899</infon><infon key="section_type">REF</infon><infon key="source">Journal of Experimental Psychology: General</infon><infon key="type">ref</infon><infon key="volume">139</infon><infon key="year">2010</infon><offset>68178</offset><text>Conceptual distinctiveness supports detailed visual long-term memory for real-world objects</text></passage><passage><infon key="fpage">14325</infon><infon key="issue">38</infon><infon key="lpage">9</infon><infon key="name_0">surname:Brady;given-names:TF</infon><infon key="name_1">surname:Konkle;given-names:T</infon><infon key="name_2">surname:Alvarez;given-names:GA</infon><infon key="name_3">surname:Oliva;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the National Academy of Sciences</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2008</infon><offset>68270</offset><text>Visual long-term memory has a massive storage capacity for object details</text></passage><passage><infon key="fpage">833</infon><infon key="issue">5</infon><infon key="lpage">9</infon><infon key="name_0">surname:Klein;given-names:KA</infon><infon key="name_1">surname:Addis;given-names:KM</infon><infon key="name_2">surname:Kahana;given-names:MJ</infon><infon key="pub-id_pmid">16383171</infon><infon key="section_type">REF</infon><infon key="source">Memory &amp; Cognition</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2005</infon><offset>68344</offset><text>A comparative analysis of serial and free recall</text></passage><passage><infon key="fpage">67</infon><infon key="issue">1</infon><infon key="lpage">88</infon><infon key="name_0">surname:Rotello;given-names:CM</infon><infon key="name_1">surname:Macmillan;given-names:NA</infon><infon key="name_2">surname:Van Tassel;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">Journal of Memory and Language</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2000</infon><offset>68393</offset><text>Recall-to-reject in recognition: Evidence from ROC curves</text></passage><passage><infon key="fpage">1210</infon><infon key="issue">6</infon><infon key="lpage">24</infon><infon key="name_0">surname:Huth;given-names:AG</infon><infon key="name_1">surname:Nishimoto;given-names:S</infon><infon key="name_2">surname:Vu;given-names:AT</infon><infon key="name_3">surname:Gallant;given-names:JL</infon><infon key="pub-id_doi">10.1016/j.neuron.2012.10.014</infon><infon key="pub-id_pmid">23259955</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">76</infon><infon key="year">2012</infon><offset>68451</offset><text>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</text></passage><passage><infon key="fpage">902</infon><infon key="issue">6</infon><infon key="lpage">15</infon><infon key="name_0">surname:Naselaris;given-names:T</infon><infon key="name_1">surname:Prenger;given-names:RJ</infon><infon key="name_2">surname:Kay;given-names:KN</infon><infon key="name_3">surname:Oliver;given-names:M</infon><infon key="name_4">surname:Gallant;given-names:JL</infon><infon key="pub-id_doi">10.1016/j.neuron.2009.09.006</infon><infon key="pub-id_pmid">19778517</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">2009</infon><offset>68576</offset><text>Bayesian reconstruction of natural images from human brain activity</text></passage><passage><infon key="fpage">352</infon><infon key="issue">7185</infon><infon key="lpage">5</infon><infon key="name_0">surname:Kay;given-names:KN</infon><infon key="name_1">surname:Naselaris;given-names:T</infon><infon key="name_2">surname:Prenger;given-names:RJ</infon><infon key="name_3">surname:Gallant;given-names:JL</infon><infon key="pub-id_doi">10.1038/nature06713</infon><infon key="pub-id_pmid">18322462</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">452</infon><infon key="year">2008</infon><offset>68644</offset><text>Identifying natural images from human brain activity</text></passage><passage><infon key="fpage">1126</infon><infon key="issue">6</infon><infon key="lpage">41</infon><infon key="name_0">surname:Kriegeskorte;given-names:N</infon><infon key="name_1">surname:Mur;given-names:M</infon><infon key="name_2">surname:Ruff;given-names:DA</infon><infon key="name_3">surname:Kiani;given-names:R</infon><infon key="name_4">surname:Bodurka;given-names:J</infon><infon key="name_5">surname:Esteky;given-names:H</infon><infon key="pub-id_doi">10.1016/j.neuron.2008.10.043</infon><infon key="pub-id_pmid">19109916</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">60</infon><infon key="year">2008</infon><offset>68697</offset><text>Matching categorical object representations in inferior temporal cortex of man and monkey</text></passage><passage><infon key="fpage">2425</infon><infon key="issue">5539</infon><infon key="lpage">30</infon><infon key="name_0">surname:Haxby;given-names:JV</infon><infon key="name_1">surname:Gobbini;given-names:MI</infon><infon key="name_2">surname:Furey;given-names:ML</infon><infon key="name_3">surname:Ishai;given-names:A</infon><infon key="name_4">surname:Schouten;given-names:JL</infon><infon key="name_5">surname:Pietrini;given-names:P</infon><infon key="pub-id_doi">10.1126/science.1063736</infon><infon key="pub-id_pmid">11577229</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">293</infon><infon key="year">2001</infon><offset>68787</offset><text>Distributed and overlapping representations of faces and objects in ventral temporal cortex</text></passage><passage><infon key="fpage">356</infon><infon key="issue">2</infon><infon key="lpage">70</infon><infon key="name_0">surname:Eger;given-names:E</infon><infon key="name_1">surname:Ashburner;given-names:J</infon><infon key="name_2">surname:Haynes;given-names:J-D</infon><infon key="name_3">surname:Dolan;given-names:RJ</infon><infon key="name_4">surname:Rees;given-names:G</infon><infon key="pub-id_doi">10.1162/jocn.2008.20019</infon><infon key="pub-id_pmid">18275340</infon><infon key="section_type">REF</infon><infon key="source">Journal of cognitive neuroscience</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2008</infon><offset>68879</offset><text>fMRI activity patterns in human LOC carry information about object exemplars within category</text></passage><passage><infon key="fpage">309</infon><infon key="issue">4</infon><infon key="lpage">21</infon><infon key="name_0">surname:Edelman;given-names:S</infon><infon key="name_1">surname:Grill-Spector;given-names:K</infon><infon key="name_2">surname:Kushnir;given-names:T</infon><infon key="name_3">surname:Malach;given-names:R</infon><infon key="section_type">REF</infon><infon key="source">Psychobiology</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">1998</infon><offset>68972</offset><text>Toward direct visualization of the internal shape representation space by fMRI</text></passage><passage><infon key="fpage">8837</infon><infon key="issue">26</infon><infon key="lpage">44</infon><infon key="name_0">surname:Rice;given-names:GE</infon><infon key="name_1">surname:Watson;given-names:DM</infon><infon key="name_2">surname:Hartley;given-names:T</infon><infon key="name_3">surname:Andrews;given-names:TJ</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.5265-13.2014</infon><infon key="pub-id_pmid">24966383</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2014</infon><offset>69051</offset><text>Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway</text></passage><passage><infon key="fpage">1329</infon><infon key="issue">10</infon><infon key="lpage">39</infon><infon key="name_0">surname:Tranel;given-names:D</infon><infon key="name_1">surname:Logan;given-names:CG</infon><infon key="name_2">surname:Frank;given-names:RJ</infon><infon key="section_type">REF</infon><infon key="source">Damasio ARJN.</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">1997</infon><offset>69196</offset><text>Explaining category-related effects in the retrieval of conceptual and lexical knowledge for concrete entities: Operationalization and analysis of factors</text></passage><passage><infon key="fpage">296</infon><infon key="issue">2</infon><infon key="lpage">314</infon><infon key="name_0">surname:Gerlach;given-names:C</infon><infon key="pub-id_doi">10.1162/jocn.2007.19.2.296</infon><infon key="pub-id_pmid">17280518</infon><infon key="section_type">REF</infon><infon key="source">Journal of Cognitive Neuroscience</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2007</infon><offset>69351</offset><text>A review of functional imaging studies on category specificity</text></passage><passage><infon key="fpage">281</infon><infon key="issue">2</infon><infon key="lpage">90</infon><infon key="name_0">surname:Liu;given-names:H</infon><infon key="name_1">surname:Agam;given-names:Y</infon><infon key="name_2">surname:Madsen;given-names:JR</infon><infon key="name_3">surname:Kreiman;given-names:G</infon><infon key="pub-id_doi">10.1016/j.neuron.2009.02.025</infon><infon key="pub-id_pmid">19409272</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2009</infon><offset>69414</offset><text>Timing, timing, timing: Fast decoding of object information from intracranial field potentials in human visual cortex</text></passage><passage><infon key="fpage">863</infon><infon key="issue">5749</infon><infon key="lpage">6</infon><infon key="name_0">surname:Hung;given-names:CP</infon><infon key="name_1">surname:Kreiman;given-names:G</infon><infon key="name_2">surname:Poggio;given-names:T</infon><infon key="name_3">surname:DiCarlo;given-names:JJ</infon><infon key="pub-id_doi">10.1126/science.1117593</infon><infon key="pub-id_pmid">16272124</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">310</infon><infon key="year">2005</infon><offset>69532</offset><text>Fast readout of object identity from macaque inferior temporal cortex</text></passage><passage><infon key="fpage">2608</infon><infon key="issue">8</infon><infon key="lpage">18</infon><infon key="name_0">surname:Connolly;given-names:AC</infon><infon key="name_1">surname:Guntupalli;given-names:JS</infon><infon key="name_2">surname:Gors;given-names:J</infon><infon key="name_3">surname:Hanke;given-names:M</infon><infon key="name_4">surname:Halchenko;given-names:YO</infon><infon key="name_5">surname:Wu;given-names:Y-C</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.5547-11.2012</infon><infon key="pub-id_pmid">22357845</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2012</infon><offset>69602</offset><text>The representation of biological classes in the human brain</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">34</infon><infon key="name_0">surname:Caramazza;given-names:A</infon><infon key="name_1">surname:Shelton;given-names:JR</infon><infon key="pub-id_pmid">9526080</infon><infon key="section_type">REF</infon><infon key="source">Journal of Cognitive Neuroscience</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">1998</infon><offset>69662</offset><text>Domain-specific knowledge systems in the brain: The animate-inanimate distinction</text></passage><passage><infon key="fpage">829</infon><infon key="issue">3</infon><infon key="lpage">53</infon><infon key="name_0">surname:Warrington;given-names:EK</infon><infon key="name_1">surname:Shallice;given-names:T</infon><infon key="pub-id_pmid">6206910</infon><infon key="section_type">REF</infon><infon key="source">Brain</infon><infon key="type">ref</infon><infon key="volume">107</infon><infon key="year">1984</infon><offset>69744</offset><text>Category specific semantic impairments</text></passage><passage><infon key="fpage">25</infon><infon key="lpage">45</infon><infon key="name_0">surname:Martin;given-names:A</infon><infon key="pub-id_doi">10.1146/annurev.psych.57.102904.190143</infon><infon key="pub-id_pmid">16968210</infon><infon key="section_type">REF</infon><infon key="source">Annual Review of Psychology</infon><infon key="type">ref</infon><infon key="volume">58</infon><infon key="year">2007</infon><offset>69783</offset><text>The representation of object concepts in the brain</text></passage><passage><infon key="name_0">surname:Murphy;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">The big book of concepts</infon><infon key="type">ref</infon><infon key="year">2004</infon><offset>69834</offset></passage><passage><infon key="fpage">536</infon><infon key="issue">8</infon><infon key="name_0">surname:Grill-Spector;given-names:K</infon><infon key="name_1">surname:Weiner;given-names:KS</infon><infon key="pub-id_doi">10.1038/nrn3747</infon><infon key="pub-id_pmid">24962370</infon><infon key="section_type">REF</infon><infon key="source">Nature Reviews Neuroscience</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2014</infon><offset>69835</offset><text>The functional architecture of the ventral temporal cortex and its role in categorization</text></passage><passage><infon key="fpage">27</infon><infon key="lpage">51</infon><infon key="name_0">surname:Mahon;given-names:BZ</infon><infon key="name_1">surname:Caramazza;given-names:A</infon><infon key="pub-id_doi">10.1146/annurev.psych.60.110707.163532</infon><infon key="pub-id_pmid">18767921</infon><infon key="section_type">REF</infon><infon key="source">Annual Review of Psychology</infon><infon key="type">ref</infon><infon key="volume">60</infon><infon key="year">2009</infon><offset>69925</offset><text>Concepts and categories: A cognitive neuropsychological perspective</text></passage><passage><infon key="fpage">1119</infon><infon key="issue">4</infon><infon key="lpage">27</infon><infon key="name_0">surname:Devereux;given-names:BJ</infon><infon key="name_1">surname:Tyler;given-names:LK</infon><infon key="name_2">surname:Geertzen;given-names:J</infon><infon key="name_3">surname:Randall;given-names:B</infon><infon key="pub-id_doi">10.3758/s13428-013-0420-4</infon><infon key="pub-id_pmid">24356992</infon><infon key="section_type">REF</infon><infon key="source">Behavior research methods</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2014</infon><offset>69993</offset><text>The Centre for Speech, Language and the Brain (CSLB) concept property norms</text></passage><passage><infon key="fpage">547</infon><infon key="issue">4</infon><infon key="lpage">59</infon><infon key="name_0">surname:McRae;given-names:K</infon><infon key="name_1">surname:Cree;given-names:GS</infon><infon key="name_2">surname:Seidenberg;given-names:MS</infon><infon key="name_3">surname:McNorgan;given-names:C</infon><infon key="pub-id_pmid">16629288</infon><infon key="section_type">REF</infon><infon key="source">Behavior research methods</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2005</infon><offset>70069</offset><text>Semantic feature production norms for a large set of living and nonliving things</text></passage><passage><infon key="fpage">211</infon><infon key="issue">3</infon><infon key="lpage">52</infon><infon key="name_0">surname:Russakovsky;given-names:O</infon><infon key="name_1">surname:Deng;given-names:J</infon><infon key="name_2">surname:Su;given-names:H</infon><infon key="name_3">surname:Krause;given-names:J</infon><infon key="name_4">surname:Satheesh;given-names:S</infon><infon key="name_5">surname:Ma;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">2015</infon><offset>70150</offset><text>Imagenet large scale visual recognition challenge</text></passage><passage><infon key="fpage">157</infon><infon key="issue">1–3</infon><infon key="lpage">73</infon><infon key="name_0">surname:Russell;given-names:BC</infon><infon key="name_1">surname:Torralba;given-names:A</infon><infon key="name_2">surname:Murphy;given-names:KP</infon><infon key="name_3">surname:Freeman;given-names:WT</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Computer Vision</infon><infon key="type">ref</infon><infon key="volume">77</infon><infon key="year">2008</infon><offset>70200</offset><text>LabelMe: a database and web-based tool for image annotation</text></passage><passage><infon key="fpage">4296</infon><infon key="issue">6</infon><infon key="lpage">309</infon><infon key="name_0">surname:Kiani;given-names:R</infon><infon key="name_1">surname:Esteky;given-names:H</infon><infon key="name_2">surname:Mirpour;given-names:K</infon><infon key="name_3">surname:Tanaka;given-names:K</infon><infon key="pub-id_doi">10.1152/jn.00024.2007</infon><infon key="pub-id_pmid">17428910</infon><infon key="section_type">REF</infon><infon key="source">Journal of neurophysiology</infon><infon key="type">ref</infon><infon key="volume">97</infon><infon key="year">2007</infon><offset>70260</offset><text>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</text></passage><passage><infon key="fpage">e1003167</infon><infon key="issue">8</infon><infon key="name_0">surname:Baldassi;given-names:C</infon><infon key="name_1">surname:Alemi-Neissi;given-names:A</infon><infon key="name_2">surname:Pagan;given-names:M</infon><infon key="name_3">surname:DiCarlo;given-names:JJ</infon><infon key="name_4">surname:Zecchina;given-names:R</infon><infon key="name_5">surname:Zoccolan;given-names:D</infon><infon key="pub-id_doi">10.1371/journal.pcbi.1003167</infon><infon key="pub-id_pmid">23950700</infon><infon key="section_type">REF</infon><infon key="source">PLoS computational biology</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2013</infon><offset>70365</offset><text>Shape similarity, better than semantic membership, accounts for the structure of visual object representations in a population of monkey inferotemporal neurons</text></passage><passage><infon key="fpage">12978</infon><infon key="issue">39</infon><infon key="lpage">95</infon><infon key="name_0">surname:Rust;given-names:NC</infon><infon key="name_1">surname:DiCarlo;given-names:JJ</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.0179-10.2010</infon><infon key="pub-id_pmid">20881116</infon><infon key="section_type">REF</infon><infon key="source">Journal of Neuroscience</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2010</infon><offset>70525</offset><text>Selectivity and tolerance (“invariance”) both increase as visual information propagates from cortical area V4 to IT</text></passage><passage><infon key="fpage">520</infon><infon key="issue">6582</infon><infon key="lpage">2</infon><infon key="name_0">surname:Thorpe;given-names:S</infon><infon key="name_1">surname:Fize;given-names:D</infon><infon key="name_2">surname:Marlot;given-names:C</infon><infon key="pub-id_doi">10.1038/381520a0</infon><infon key="pub-id_pmid">8632824</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">381</infon><infon key="year">1996</infon><offset>70645</offset><text>Speed of processing in the human visual system</text></passage><passage><infon key="fpage">94</infon><infon key="issue">7251</infon><infon key="lpage">7</infon><infon key="name_0">surname:Peelen;given-names:MV</infon><infon key="name_1">surname:Fei-Fei;given-names:L</infon><infon key="name_2">surname:Kastner;given-names:S</infon><infon key="pub-id_doi">10.1038/nature08103</infon><infon key="pub-id_pmid">19506558</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">460</infon><infon key="year">2009</infon><offset>70692</offset><text>Neural mechanisms of rapid natural scene categorization in human visual cortex</text></passage><passage><infon key="fpage">766</infon><infon key="issue">4</infon><infon key="name_0">surname:Torralba;given-names:A</infon><infon key="name_1">surname:Oliva;given-names:A</infon><infon key="name_2">surname:Castelhano;given-names:MS</infon><infon key="name_3">surname:Henderson;given-names:JM</infon><infon key="pub-id_doi">10.1037/0033-295X.113.4.766</infon><infon key="pub-id_pmid">17014302</infon><infon key="section_type">REF</infon><infon key="source">Psychological review</infon><infon key="type">ref</infon><infon key="volume">113</infon><infon key="year">2006</infon><offset>70771</offset><text>Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</text></passage><passage><infon key="fpage">291</infon><infon key="issue">2</infon><infon key="lpage">307</infon><infon key="name_0">surname:Cohen;given-names:L</infon><infon key="name_1">surname:Dehaene;given-names:S</infon><infon key="name_2">surname:Naccache;given-names:L</infon><infon key="name_3">surname:Lehéricy;given-names:S</infon><infon key="name_4">surname:Dehaene-Lambertz;given-names:G</infon><infon key="name_5">surname:Hénaff;given-names:M-A</infon><infon key="pub-id_pmid">10648437</infon><infon key="section_type">REF</infon><infon key="source">Brain</infon><infon key="type">ref</infon><infon key="volume">123</infon><infon key="year">2000</infon><offset>70889</offset><text>The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients</text></passage></document></collection>
