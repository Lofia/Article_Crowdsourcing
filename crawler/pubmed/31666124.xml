<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201223</date><key>pmc.key</key><document><id>6822440</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1186/s13104-019-4764-4</infon><infon key="article-id_pmc">6822440</infon><infon key="article-id_pmid">31666124</infon><infon key="article-id_publisher-id">4764</infon><infon key="elocation-id">715</infon><infon key="kwd">Crowdsourcing emotions Empirical study Rating behavior Reliability</infon><infon key="license">Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.</infon><infon key="name_0">surname:Korovina;given-names:Olga</infon><infon key="name_1">surname:Baez;given-names:Marcos</infon><infon key="name_2">surname:Casati;given-names:Fabio</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">12</infon><infon key="year">2019</infon><offset>0</offset><text>Reliability of crowdsourcing as a method for collecting emotions labels on pictures</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>84</offset><text>Objective</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>94</offset><text>In this paper we study if and under what conditions crowdsourcing can be used as a reliable method for collecting high-quality emotion labels on pictures. To this end, we run a set of crowdsourcing experiments on the widely used IAPS dataset, using the Self-Assessment Manikin (SAM) emotion collection instrument, in order to rate pictures on valence, arousal and dominance, and explore the consistency of crowdsourced results across multiple runs (reliability) and the level of agreement with the gold labels (quality). In doing so, we explored the impact of targeting populations of different level of reputation (and cost) and collecting varying numbers of ratings per picture.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>775</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>783</offset><text>The results tell us that crowdsourcing can be a reliable method, reaching excellent levels of reliability and agreement with only 3 ratings per picture for valence and 8 per arousal, with only marginal difference between target populations. Results for dominance were very poor, echoing previous studies on the data collection instrument used. We also observed that specific types of content generate diverging opinions in participants (leading to higher variability or multimodal distributions), which remain consistent across pictures of the same theme. These can inform the data collection and exploitation of crowdsourced emotion datasets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1427</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1440</offset><text>The popularity of digital photography along with the explosion in volume of online social data have greatly motivated and promoted the research on large-scale multimedia analysis and the development of novel concepts for exploiting the expressive nature of images. Doing so requires annotated datasets to guide the analyses and train algorithms. The most widely used datasets in this regard, the International affective picture system (IAPS) dataset and the Geneva affective picture database (GAPED), rely on normative values collected in controlled settings. The need for larger volumes of pictures, however, are motivating researchers to turn to crowdsourcing as an alternative method for collecting emotion labels.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2158</offset><text>Using crowdsourcing as a method for collecting emotion labels comes however with its challenges. Concerns about running subjective and qualitative studies in uncontrolled remote settings as well as known biases associated to the method require the use and design of crowdsourcing tasks to be carefully planned. This means that unlike data collection in controlled environments, in crowdsourcing we have to deal with strategies of quality control, task design, and proper sampling of participants so as to have reliable results. In this regard, crowdsourced datasets (e.g.,) rely on a variety of different techniques (sometimes not fully detailed) in terms of task designs, number of ratings in the dataset generation, and others, leaving researchers with no clear guidelines for how to run such data collection processes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2980</offset><text>In this paper we study if and under what conditions crowdsourcing can be used as a reliable method for collecting high-quality emotion labels on pictures. To this end, we run a set of crowdsourcing experiments on a representative set of the IAPS dataset, using the Self-Assessment Manikin (SAM) emotion collection instrument, in order to rate pictures on valence, arousal and dominance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>3367</offset><text>Main text</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>3377</offset><text>Methods</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>3385</offset><text>Research questions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3404</offset><text>We focus on the following specific research questions:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3459</offset><text>RQ1. How reliable is crowdsourcing as a method for collecting emotion labels about pictures? We particularly study if and under what configuration (e.g., number of collected labels, target population) results can be considered reliable.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3696</offset><text>RQ2. How does crowdsourced labels compare to those collected in lab settings? We focus on the results of the process and compare whether crowdsourced labels collected in uncontrolled settings can approximate the labels collected in controlled lab settings.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>3953</offset><text>Dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3961</offset><text>We base our experiments on the widely used IAPS dataset. This dataset contains 1182 images that have been labeled along the three dimensions of valence, arousal, and dominance to indicate emotional reactions. The tags were collected with both paper-and-pencil and computer-based versions of SAM, using a 9-point rating scale for each dimension. The dataset is organized in diverse sets of 60 pictures, each set assigned and annotated in full by 8–25 persons. IAPS was created in the US, but repetitions of the same emotion labeling method in different cultures have resulted in similar results. For our experiments we took a set of 60 pictures, which constitutes a sizeable and representative sample.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>4664</offset><text>Experimental design</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4684</offset><text>The task design is based on the instructions and emotion collection methods used in the original IAPS study but adapted to the crowdsourcing environment and ethics. In this sense, and unlike the original IAPS, the annotation task was divided in three pages of 20 pictures each, and workers could decide how many pages to label (min 1 page with 20 pictures). This was to reduce worker’s fatigue and give them more control about when to stop, especially considering the sensitive nature of some of the pictures (e.g., sexual, mutilations). We include screenshots of the task design in Additional file 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5288</offset><text>On top of this task design, we explored two specific dimensions:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5353</offset><text>Target population, refers to the trade-off between level of reputation of workers, and the availability and costs of worker contributions. We explored two conditions: (i) specialized and expensive, focusing on contributors from English-speaking countries belonging to F8 top-tier, as a way of approximating the demographics and level of trust of a controlled lab environment, (ii) general and cheaper, focusing on contributors from the rest of the world belonging to F8 s-tier, a much larger pool of workers composed by a cheaper but less reputable population.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5915</offset><text>Number of ratings, refers to the number of ratings to be collected for each picture in order to reach repeatable results. This aspect was simulated based on the dataset of 30–50 crowdsourced ratings per picture and therefore did not impact the task configuration. To simulate the crowdsourcing runs, we selected random groups of workers so as to have 30 runs per each rating size (1–15 ratings per picture) and computed the mean and standard deviation of the pictures in each run. As a result, we had 30 simulated runs per each rating size.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6460</offset><text>We run two tasks featuring the specialized (expensive) and the general (cheaper) settings, which amounted to 2cents and 1cent respectively. Each task was running for a week-long period starting September 2019.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>6670</offset><text>Metrics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6678</offset><text>In addressing the research questions, we rely on two main measures:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6746</offset><text>Consistency. To assess the reliability of crowdsourcing as a method (RQ1), we compute inter-class correlation (two-way random effects, single rater/measurement) between the mean values of multiple crowdsourcing runs. The measure, represented as , focus on the definition of consistency which denotes whether participants’ scores to the same group of pictures are correlated in an additive manner.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7145</offset><text>Agreement. When comparing the resulting crowdsourced labels with the gold labels from IAPS (RQ2), we compute the inter-class correlation,  but focusing on the agreement definition which in this case relates to the absolute agreement between the methods.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>7399</offset><text>Results</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>7407</offset><text>Reliability of crowdsourcing for emotion labelling</text></passage><passage><infon key="file">13104_2019_4764_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>7458</offset><text>a Reliability of crowd ratings per metric and group. b Distribution of standard deviation per emotion dimension score</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7576</offset><text>The experimental results summarized in Fig. 1a show that crowdsourcing can be a reliable method for assessing valence, reaching an excellent level of consistency () with only 3 ratings per picture. Collecting emotion labels for the arousal dimension can also reach similar level of agreement but with at least 8 ratings per picture. Dominance showed a lot of variability between runs, achieving fair consistency levels () only with a large number of ratings. Comparing the target populations, we see not surprisingly that the specialized group performed more reliably than the general population. Interestingly, however, the performance is comparable for valence, slightly lower for arousal (around 1 rating difference) and more noticeable only for dominance. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8338</offset><text>Digging into the variability, we observed that the standard deviation in the ratings tend to be higher around the neutral range of the scale and lower at the extremes, suggesting a lower variability in the most extreme emotions (see Fig. 1b).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>8582</offset><text>Characterizing the distribution of crowd votes</text></passage><passage><infon key="file">13104_2019_4764_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>8629</offset><text>Distribution modality by group and metric</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8671</offset><text>The original IAPS dataset relied on mean and standard deviation to aggregate and report the results. Considering the heterogeneity of the crowd population, we examine the distribution of ratings to assess whether this is a reasonable choice for crowdsourced labels (see Fig. 2). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8952</offset><text>Looking into the modality of crowd ratings per picture (computed using the dip test) we see that for valence around 75% of the pictures featured a unimodal distribution, while for arousal and dominance between 40 and 50%, suggesting that groups perceived pictures more differently for the latter dimensions. Polarizing views were particularly found in arousal. Examples of pictures triggering opposite reactions in terms of arousal are P4609 (couple in a sensual pose) and P9470 (demolished building), which arose calm or excited reactions among participants in both target populations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9539</offset><text>Given the above results, we investigated whether there is consistency to the diverging opinions in subgroups of participants, and if those opinions can be transferred from one type of picture to another. To this end, we took pairs of multimodal pictures on the arousal dimension, transformed them to a binary scale and looked at the number of participants who rated both pictures below (0-0) and above (1-1) the neutral value, and those who switched ratings. The results (included in Additional file 2) tell us that people tend to maintain their opinions across pictures featuring themes that relate to their sensitivity (violence, accidents, mutilations), preferences or affinities (sport, children). Thus, depending on the type of content, people might show more or less diverging opinions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>10332</offset><text>Quality of collected labels</text></passage><passage><infon key="file">13104_2019_4764_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>10360</offset><text>Distribution of crowd votes per picture and group, along with the mean values from the IAPS dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10460</offset><text>In order to compare the quality of crowdsourced emotion labels to those of controlled lab settings, we took the mean ratings from the simulated runs with 8 ratings (same number of ratings in the original dataset) and compared it to those reported in IAPS (see Fig. 3). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10731</offset><text>The results show crowdsourced labels for valence to be of high quality, as the resulting labels feature very high levels of agreement () with IAPS. The few notable deviations can be seen in pictures P4004 (woman posing with her breast), P9434 (breast cancer survivor) where both crowd groups rated more negatively (especially the general population), and P2661 (premature baby taking a bath) where the specialized population rated lower. These deviations can be attributed to different demographic populations and potential cultural perceptions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11277</offset><text>Results for arousal also show high levels of consistency and agreement between the crowdsourced and original labels from IAPS. Breaking down the results by group, we see that while the specialized group shows slightly higher consistency with IAPS (, ) than the general population (, (), he latter features a higher absolute agreement. The reason as observed in Fig. 3 is that the specialized group consistently displayed higher levels of arousal than IAPS (pairwise comparison with bonferroni correction showing a significant difference between the mean values at p = .007), while the general group concentrated its values around the original labels. Crowdsourcing the dominance dimension results in low-quality labels.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12002</offset><text>We also compared the impact of (specialized crowd) workers rating all pictures as opposed to batches of 20. The results were very similar for valence (), significantly dropped for arousal () and improved for dominance () although still moderate. This insight calls for further investigation into the tradeoff between worker variability and potential fatigue in large scale emotion collection.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>12395</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>12406</offset><text>Crowdsourcing can be a reliable method for collecting high-quality emotion labels, for valence and arousal (3–8 ratings) but not for dominance. The results for dominance are in lines with previous empirical studies with IAPS as stimuli dataset and SAM as affect measurement instrument. Bradley and Lang, in their seminal study observed dominance to be the dimension with higher variability in terms of semantic label. They concluded that SAM might be better suited for capturing the individual’s and not the stimulus feelings of control. Given the diversity of crowdsourcing workers, this can be one contributing factor. Another explanation relates to SAM requiring additional instructions to support the pictorial representations and the dominance dimension being more difficult to explain. Thus, an uncontrolled experiment without the researcher to clarify doubts could lead to higher variability.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>13310</offset><text>The type of pictures can lead to diverging opinions, which tend to be consistent and transferable to pictures of similar themes. We noticed that some variability in the ratings can be expected in (i) more neutral pictures—where the differences in the mean tend to be higher than pictures depicting stronger emotions—and (ii) depending on the content of the pictures, especially around themes that are more sensible to demographics and cultural settings. This was particularly true for the arousal and dominance dimensions—although for the latter, for the reasons addressed before. In this regard, validations of the IAPS dataset across countries have also shown different rating scores for arousal for the same set of pictures (e.g.,). This, along with gender differences, stresses the importance of carefully selecting the target crowd.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>14154</offset><text>Limitations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>14166</offset><text>While the results can potentially be generalized to other types of emotion collection instruments, the empirical results are limited to SAM and pictures with the level of subjectivity found in IAPS.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>14365</offset><text>Supplementary information</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>14391</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>14405</offset><text>SAM</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>14409</offset><text>Self-Assessment Manikin</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>14433</offset><text>IAPS</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>14438</offset><text>International Affective Picture System</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>14477</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>14494</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>14613</offset><text>Supplementary information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>14639</offset><text>Supplementary information accompanies this paper at 10.1186/s13104-019-4764-4.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>14718</offset><text>Authors’ contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>14743</offset><text>OK and FC prepared the study design and rationale. MB analyzed the data and prepared the first draft of the paper. All authors read and approved the final manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>14910</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>14918</offset><text>The study was supported by the Russian Science Foundation (Project No. 19-18-00282). The funding body had no role in the design of the study and data collection, analysis and interpretation of data and in writing the manuscript.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>15147</offset><text>Availability of data and materials</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15182</offset><text>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>15308</offset><text>Ethics approval and consent to participate</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15351</offset><text>Not applicable.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>15367</offset><text>Consent for publication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15391</offset><text>Not applicable.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>15407</offset><text>Competing interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>15427</offset><text>The authors declare that they have no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>15486</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>15497</offset><text>Lang PJ. International affective picture system (IAPS): affective ratings of pictures and instruction manual. Technical report. 2005.</text></passage><passage><infon key="fpage">468</infon><infon key="name_0">surname:Dan-Glauser;given-names:ES</infon><infon key="name_1">surname:Scherer;given-names:KR</infon><infon key="pub-id_doi">10.3758/s13428-011-0064-1</infon><infon key="pub-id_pmid">21431997</infon><infon key="section_type">REF</infon><infon key="source">Behav Res Methods</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2011</infon><offset>15631</offset><text>The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>15752</offset><text>Kittur A, Chi EH, Suh B. Crowdsourcing user studies with Mechanical Turk. In: Proceedings of the SIGCHI conference on human factors in computing systems 2008. ACM.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>15916</offset><text>Eickhoff C. Cognitive biases in crowdsourcing. In: Proceedings of the eleventh ACM international conference on web search and data mining 2018. ACM.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>16065</offset><text>You Q, Luo J, Jin H, Yang J. Building a large scale dataset for image emotion recognition: The fine print and the benchmark. In: Thirtieth AAAI conference on artificial intelligence 2016.</text></passage><passage><infon key="fpage">457</infon><infon key="lpage">470</infon><infon key="name_0">surname:Kurdi;given-names:B</infon><infon key="name_1">surname:Lozano;given-names:S</infon><infon key="name_2">surname:Banaji;given-names:MR</infon><infon key="pub-id_doi">10.3758/s13428-016-0715-3</infon><infon key="pub-id_pmid">26907748</infon><infon key="section_type">REF</infon><infon key="source">Behav Res Methods</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2017</infon><offset>16253</offset><text>Introducing the open affective standardized image set (OASIS)</text></passage><passage><infon key="name_0">surname:Lang;given-names:PJ</infon><infon key="section_type">REF</infon><infon key="source">Self-assessment manikin, Gainesville</infon><infon key="type">ref</infon><infon key="year">1980</infon><offset>16315</offset></passage><passage><infon key="fpage">17</infon><infon key="lpage">26</infon><infon key="name_0">surname:Drače;given-names:S</infon><infon key="name_1">surname:Efendić;given-names:E</infon><infon key="name_2">surname:Kusturica;given-names:M</infon><infon key="name_3">surname:Landžo;given-names:L</infon><infon key="pub-id_doi">10.2298/PSI1301017D</infon><infon key="section_type">REF</infon><infon key="source">Psihologija.</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2013</infon><offset>16316</offset><text>Cross-cultural validation of the “International Affective Picture System” (IAPS) on a sample from Bosnia and Herzegovina</text></passage><passage><infon key="fpage">811</infon><infon key="lpage">837</infon><infon key="name_0">surname:Schlagwein;given-names:D</infon><infon key="name_1">surname:Cecez-Kecmanovic;given-names:D</infon><infon key="name_2">surname:Hanckel;given-names:B</infon><infon key="pub-id_doi">10.1111/isj.12227</infon><infon key="section_type">REF</infon><infon key="source">Information Systems Journal.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2019</infon><offset>16441</offset><text>Ethical norms and issues in crowdsourcing practices: a Habermasian analysis</text></passage><passage><infon key="fpage">155</infon><infon key="lpage">163</infon><infon key="name_0">surname:Koo;given-names:TK</infon><infon key="name_1">surname:Li;given-names:MY</infon><infon key="pub-id_doi">10.1016/j.jcm.2016.02.012</infon><infon key="section_type">REF</infon><infon key="source">J Chiropractic Med.</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2016</infon><offset>16517</offset><text>A guideline of selecting and reporting intraclass correlation coefficients for reliability research</text></passage><passage><infon key="fpage">70</infon><infon key="lpage">84</infon><infon key="name_0">surname:Hartigan;given-names:JA</infon><infon key="name_1">surname:Hartigan;given-names:PM</infon><infon key="pub-id_doi">10.1214/aos/1176346577</infon><infon key="section_type">REF</infon><infon key="source">Annals Stat.</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">1985</infon><offset>16617</offset><text>The dip test of unimodality</text></passage><passage><infon key="fpage">49</infon><infon key="lpage">59</infon><infon key="name_0">surname:Bradley;given-names:MM</infon><infon key="name_1">surname:Lang;given-names:PJ</infon><infon key="pub-id_doi">10.1016/0005-7916(94)90063-9</infon><infon key="pub-id_pmid">7962581</infon><infon key="section_type">REF</infon><infon key="source">J Behav Ther Exp Psychiatry</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">1994</infon><offset>16645</offset><text>Measuring emotion: the self-assessment manikin and the semantic differential</text></passage><passage><infon key="fpage">33</infon><infon key="lpage">42</infon><infon key="name_0">surname:Broekens;given-names:J</infon><infon key="pub-id_doi">10.4018/jse.2012010103</infon><infon key="section_type">REF</infon><infon key="source">Int J Synth Emot</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2012</infon><offset>16722</offset><text>In defense of dominance: PAD usage in computational representations of affect</text></passage><passage><infon key="fpage">270</infon><infon key="lpage">275</infon><infon key="name_0">surname:Lasaitis;given-names:C</infon><infon key="name_1">surname:Ribeiro;given-names:RL</infon><infon key="name_2">surname:Bueno;given-names:OF</infon><infon key="pub-id_doi">10.1590/S0047-20852008000400008</infon><infon key="section_type">REF</infon><infon key="source">J Brasileiro Psiquiatria.</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2008</infon><offset>16800</offset><text>Brazilian norms for the International Affective Picture System (IAPS): comparison of the affective ratings for new stimuli between Brazilian and North-American subjects</text></passage><passage><infon key="fpage">521</infon><infon key="lpage">533</infon><infon key="name_0">surname:Dufey;given-names:M</infon><infon key="name_1">surname:Fernández;given-names:A</infon><infon key="name_2">surname:Mayol;given-names:R</infon><infon key="pub-id_doi">10.11144/Javeriana.upsy10-2.asce</infon><infon key="section_type">REF</infon><infon key="source">Universitas Psychologica.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2011</infon><offset>16969</offset><text>Adding support to cross-cultural emotional assessment: validation of the International Affective Picture System in a Chilean sample</text></passage></document></collection>
