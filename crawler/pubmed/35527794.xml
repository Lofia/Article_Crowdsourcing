<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220612</date><key>pmc.key</key><document><id>9075103</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/frai.2022.826499</infon><infon key="article-id_pmc">9075103</infon><infon key="article-id_pmid">35527794</infon><infon key="elocation-id">826499</infon><infon key="kwd">explainability crowdsourcing gamification game with a purpose Explainable AI</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Tocchetti;given-names:Andrea</infon><infon key="name_1">surname:Corti;given-names:Lorenzo</infon><infon key="name_2">surname:Brambilla;given-names:Marco</infon><infon key="name_3">surname:Celino;given-names:Irene</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">5</infon><infon key="year">2022</infon><offset>0</offset><text>EXP-Crowd: A Gamified Crowdsourcing Framework for Explainability</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>65</offset><text>The spread of AI and black-box machine learning models made it necessary to explain their behavior. Consequently, the research field of Explainable AI was born. The main objective of an Explainable AI system is to be understood by a human as the final beneficiary of the model. In our research, we frame the explainability problem from the crowds point of view and engage both users and AI researchers through a gamified crowdsourcing framework. We research whether it's possible to improve the crowds understanding of black-box models and the quality of the crowdsourced content by engaging users in a set of gamified activities through a gamified crowdsourcing framework named EXP-Crowd. While users engage in such activities, AI researchers organize and share AI- and explainability-related knowledge to educate users. We present the preliminary design of a game with a purpose (G.W.A.P.) to collect features describing real-world entities which can be used for explainability purposes. Future works will concretise and improve the current design of the framework to cover specific explainability-related needs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1180</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1196</offset><text>Over the last decades, the development of new Artificial Intelligence (AI) technologies brought forth the necessity of improving their understandability. In Explainable AI (XAI), most researchers develop algorithms to either explain models or improve their intrinsic explainability. The main problem associated with the understandability of an AI system is the gap between the explanation and the level of understanding of non-expert people. Such a gap is mainly influenced by the shape of the explanation (i.e., textual, visual, low-level details, etc.), its complexity, the persons level of knowledge, and many other factors associated with both the model and human side. In particular, while sometimes it is possible to re-shape the explanation to improve its understandability for non-experts, it is challenging to leverage people's knowledge as they are usually engaged in validation and data collection activities.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2117</offset><text>Alongside the development of AI systems, the need for training and labeled data has grown as well. Therefore, resorting to crowdsourcing has become essential to collect knowledge at scale. As such processes can sometimes be tedious and repetitive, researchers developed strategies to improve their design and effectiveness. In particular, Von Ahn proposed a human computation paradigm influencing the design of crowdsourcing activities, the so-called “Games With A Purpose” (G.W.A.P.). Such a paradigm enhances crowdsourcing endeavors through Gamification (Hamari,), making them more entertaining for the people to partake.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2745</offset><text>Our research longs for envisioning an open gamified crowdsourcing framework with the final aim of (1) improving the capability of the crowd to understand black-box AI models explanations, (2) improving the quality of the explanations provided by a black-box model by engaging the crowd to provide helpful content to AI practitioners, and (3) evaluating whether providing structured AI-related knowledge and engaging the crowd in explainability-related activities is an efficient way to achieve these objectives. As a first use case, our research covers image classification models. We explore user engagement, gamification, and knowledge collection and structuring to answer our research questions. Ultimately, we strive to create an open community through which users learn to understand the behavior of black-box models, therefore, providing value for both the developers and themselves.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3635</offset><text>The rest of this article is structured as follows. Section 2 provides an overview of explainability, crowdsourcing, and gamification. Section 3 outlines the preliminary framework design we envision, including a use case of a gamified activity for data collection and structuring and some use cases. Section 4 discusses the main advantages and limitations of the framework and explains how to overcome such restraints. A discussion on the gamified activity is also provided. Finally, Section 5 summarizes the critical contributions featured within this article and discusses the following research steps.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>4239</offset><text>2. Related Works and Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>4271</offset><text>2.1. Explainability</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4291</offset><text>One of the most well-known Artificial Intelligence (AI) branches is Machine Learning (ML). In ML, algorithms train models to perform predictions, classifications, groupings, and other tasks by learning from data. The development of Deep Learning (DL) and Deep Neural Networks (DNN) increased Machine Learning models' accuracy and performance at the expense of their interpretability. Indeed, most DNNs are referred to as “black-box” (or opaque) models. The input and output of a black-box model are known, while it is complex to understand its internal logic. They are opposed to “white-box” models, in which the internal logic is either known or easily understandable.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4969</offset><text>As of today, there is no unique definition of model explainability (Vilone and Longo,). Despite the ongoing research efforts to define the fundamentals of an explainable AI system, most definitions are either domain- or problem-specific and are usually used interchangeably across different research fields (Guidotti et al.,). In the definitions provided by Barredo Arrieta et al., the notion of “human understandability” is the most important concept associated with Explainable AI. At the same time, other scholars consider different concepts depending on their research focus, like transparency (Belle and Papantonis,) and explainability (Guidotti et al.,, Hu et al.,). In their definition of Explainable AI, (Barredo Arrieta et al.,) highlight that the understandability of an explanation is influenced by the ones to whom it is provided, i.e., the audience. In particular, depending on the person's knowledge about AI and ML, an explanation can be shaped differently. For example, an AI expert would probably prefer a detailed model description. On the other side, an inexperienced user would favor a small set of examples describing the system's behavior. Moreover, the authors state that an AI must generate an explanation “clear or easy to understand,” even though the concept of being easy to understand is not the same for everyone.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6320</offset><text>Regardless of the variety of explainability-related definitions provided in the literature, the researchers' community agreed that the main objective of an Explainable AI system is to be understood by a human as the final beneficiary of the model. Despite such an objective, XAI studies mainly approach the problem from a model-centric perspective rather than a user-centric one, overshadowing the level of users' understanding of the model. In particular, end-users and experts are frequently engaged in the later validation stages to evaluate the level of understandability of the model either directly or through simulated user experiments (Ribeiro et al.,; Lundberg and Lee,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>7001</offset><text>2.2. Crowdsourcing and Gamification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7037</offset><text>Artificial Intelligence methods—especially Deep Learning approaches—require a large amount of high-quality data, whose collection is demanding and challenging. The widespread use of the internet allows researchers to engage virtually unlimited people to cover their data needs. Indeed, crowdsourcing has become common and encompasses academic studies and private companies' interests. Crowdsourcing can be defined as a participative online activity in which a group of individuals with varying features is engaged in undertaking a task as part of a process mutually benefiting participants and crowdsourcers (Estellés-Arolas and de Guevara,). This methodology's advantages include lower costs, greater speed, and a higher degree of diversity by engaging a large and heterogeneous pool of people. This open-source practice either allows the collection of a wide variety of data, including peoples ideas and preferences (Balayn et al.,), or the accomplishment of a task (e.g., labeling a large number of images) (Mishra and Rzeszotarski,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8080</offset><text>Sometimes, crowdsourcing is enhanced with gamification (Hamari,) to make such a process more engaging, drive users' behaviors, and structure the collected data. Gamification uses people's motivations to achieve such objectives. Ryan and Deci accurately describe the influence of motivations on human decisions, mainly distinguishing between intrinsic and extrinsic motivations. Their definitions can be summarized as “the motivation to perform a behavior or engage in an activity for our own sake rather than the desire for some external reward” and “the motivation to perform a behavior or engage in an activity due to a separable outcome” (Lee et al.,), respectively. Following such a dichotomy, gamified approaches can be organized based on the kind of motivation they leverage. For example, pointification, leaderboards, etc. affects extrinsic motivation while receiving feedback (Hamari and Koivisto,) and learning (Cerasoli et al.,) influence the intrinsic one. Moreover, an extrinsic-oriented design results in a good initial level of engagement, while it is necessary to apply an intrinsic-oriented design to achieve a long-lasting engagement (Rapp,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9248</offset><text>Gamification and G.W.A.P. have also been widely applied in computer science. Lu et al. developed a Peek-a-boom-based XAI evaluation, demonstrating the presence of differences between crowd-based and automatic assessment. Balayn et al. developed FindItOut, a game with a purpose based on the GuessWho game with the final aim of collecting and organizing knowledge for researchers and AI practitioners. Speer et al. presented a gamified interface to acquire common sense knowledge through a 20 Questions game which motivates contributions and improves the throughput of new knowledge. Other than contributing to data collection, it has also been demonstrated that Gamification can be effective in education and learning (Buckley and Doyle,, Welbers et al.,). In particular, leveraging intrinsic motivation through feedback cycles is an effective way to enhance learning (Lee and Hammer,).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>10135</offset><text>3. Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10146</offset><text>The main actors engaged within our explainability-oriented crowdsourcing framework fall into two categories: users, who get involved by playing gamified activities, and AI practitioners/researchers, who set up games and share knowledge about AI, ML, and explainability, since they exhibit a high level of understanding of these fields. Figure 1 provides a simple overview of the interaction flow proposed within the framework.</text></passage><passage><infon key="file">frai-05-826499-g0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>10573</offset><text>Interaction flows of researchers (dashed cyan arrows) and users (orange plain arrows) with the activities devised within our framework, as described in Section 3. Researchers organize users' knowledge and set up activities to collect data. As users engage with such activities, they provide Content to researchers. In turn, researchers give the user feedback about the activity they performed. Such feedback aims to improve users' understanding of the activity itself, the knowledge and the context provided within it.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11092</offset><text>The following sections describe each part of the framework and provide some use cases to clarify their structure. These will be mainly associated with the researcher side since most of the activities described for the user side are simple. We use a persona named “Bill” to represent our researcher. We will illustrate how he explores and interacts with the final implementation of our framework, i.e., a web-based platform.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11520</offset><text>3.1. Knowledge Assessment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11546</offset><text>As one of the main objectives of our framework is to improve the capability of the crowd to understand black-box models' explanations, educating users on AI-related topics is essential. Therefore, the first step is an assessment questionnaire through which their knowledge about AI and explainability will be assessed. Users will be asked to answer a series of multiple-choice questions. Depending on their results, they will be assigned a category representing their level of expertise. Users can improve their category by engaging with the proposed activities and enhancing their skills.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12136</offset><text>The research community will be requested to build a collection of multiple-choice questions employed in the assessment questionnaire and within the activities. Each question is made of (1) a set of texts through which the question is asked, (2) a set of correct answers, (3) the explanation associated with each correct answer, (4) a set of incorrect answers, (5) a difficulty score, and (6) a category. Questions must receive the approval of the community to guarantee the quality of the content provided. Therefore, each question must undergo a period of evaluation in which the community members can improve them by suggesting updates and proposing new answers and explanations. After this period, it is approved if the question received enough positive evaluations. Approved questions will be openly available to the whole research community as researchers may want to re-assess the users' knowledge as they engage with one of their activities. After a question is approved, researchers can still improve it by providing new content for elements (1–4).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13195</offset><text>Use Case—Researcher. Bill is a researcher who needs data about real-world entities for his research. When exploring the platform, Bill discovers a picture-based activity that would fit his needs. Even though he would like to set it up immediately, he also wants to evaluate the knowledge of the users who will perform his activity beforehand. Therefore, he explores the section dedicated to creating multiple-choice questions about AI, looking for questions that fit the context of his research. Unable to find questions that suit his needs, he submits new questions. A few days after his submission, he noticed that the researcher community proposed some improvements for the questions (e.g. by providing new answers). Bill approves a few of them. After a few more days, the question is approved.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13995</offset><text>3.2. Education</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14010</offset><text>Following the initial assessment, users will be schooled while engaging with the framework. In particular, knowledge will be provided in different shapes. The following list describes how knowledge about AI, ML, and explainability will be provided to users.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14268</offset><text>Questionnaire: Researchers may set up a small quiz before their activity made of an arbitrary number of approved questions. For each question, they choose its text, the list of answers, and the explanation of the correct answer. Such a quiz would provide knowledge to users through the questions' explanations while allowing researchers to evaluate the level of education of the people playing the activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14676</offset><text>Knowledge Sharing: Researchers can summarize, organize, and share knowledge by setting up tailored content for the users to read and study (i.e., the summary of a paper, the outline of the knowledge related to a specific AI topic, etc.). Each publication is made of a title, the topic it discusses, a brief description of the content, and the content itself. Researchers can also share scientific articles for the users to read. Only minimal information will be collected and shared like title, authors, and DOI. Users should access such articles by themselves.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15238</offset><text>Debating: Researchers and users can discuss subjects of interest in a forum-like fashion. We argue that debating with knowledgeable people would improve the users' knowledge.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15413</offset><text>Use Case—Researcher. Bill would like users to understand how machine learning models learn so that users performing his activity can provide better inputs. Therefore, he collects knowledge from scientific documents, summarizes it, and shares it in the “Education” section. Bill achieves his first publication entitled “Understanding the way ML models learn from pictures: A simplified overview.” He also provides a custom picture and a few references to the articles he used to write it within the publication. Bill reads an exciting article about his research topic a few days later. As it may improve the users' knowledge even further, he shares it by providing the necessary information.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16114</offset><text>3.3. Gamification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16132</offset><text>Gamified activities are the core elements of our framework. The following sections discuss the steps a researcher must accomplish to set up and evaluate the outcomes of an activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16314</offset><text>Activity Setup. AI practitioners can pick between pre-defined activities and set up the necessary content depending on their needs. These activities range between data collection, explainability evaluation, etc.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16526</offset><text>Setting up an activity involves a set of passages, depending on the activity. In general, all setup processes share a questionnaire setup step, a context setup step, and an activity setup step. In the first setup step, researchers decide whether to include a Questionnaire (as described in “Education”) and potentially organize its questions. In the second step, the researcher is asked to set up the content provided to the users to understand the context of their research, relevant concepts to know while carrying out the activity, etc. Finally, they have to provide all the necessary material to set up the actual activity. Practitioners can include additional control questions to the questionnaire and the actual activity to keep track of the user's level of attention. Practitioners can also select an advised knowledge level to provide an overview of the complexity of the concepts presented within the activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17451</offset><text>Use Case—Researcher. Bill is finally ready to set up his first activity. In the questionnaire setup step, he picks the questions (including the ones he got approved before), their answers and their explanations. In the context setup step, he provides the context of its research, describing what it consists of. Bill also provides some of the content from the knowledge summary he shared for those who didn't read it. As the last step, he gives the pictures, labels, and necessary content for the picture-based activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17974</offset><text>Activity Evaluation. Users are only asked to play gamified activities while researchers perform many different tasks regarding the gamified activities. In particular, they can visualize relevant statistics about the users that partook in the activity they set up, including the answers to the questionnaire (if present), the outcome of the activity, whether the user successfully answered the questions, the knowledge level of the users, etc. The role of the researcher in this final step is to evaluate the users and potentially provide feedback. They have to identify those users who stood out, like those who answered correctly to a high number of questions (compared to their level of knowledge), those who carried out a high-quality activity, etc. These users will be consequently awarded. In particular, these users will be awarded status-based awards that will make them distinguished community members.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18885</offset><text>Use Case—Researcher. After a few weeks from publishing his activity, Bill overviews its outcomes. He notices that most users performed well while others outlined the pictures improperly. He picks the users who performed outstandingly and awards them. These users will be notified, and the award will be exhibited on their profiles. As one of the users answered most of the questions incorrectly and provided poor activity outcomes, Bill wrote them some advice on how to carry out the activity, also explaining some details related to how ML is applied in his research.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19456</offset><text>3.4. Gamified Activity: A Case Study</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19493</offset><text>Finally, we describe a case study on image classification and understanding, which we use as proof of concept of a gamified activity to collect data to be employed in the field of explainable AI.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19689</offset><text>When addressing the explainability of image classification models, the crowd is usually engaged to highlight, label, and detail pictures. We assert that the outcome of such a task strictly depends on the images supplied, i.e., a person describing different pictures of the same entity may provide different details. In particular, we argue that the description of a subject, provided its picture, may be limited to or by the features displayed. Therefore, we claim it would be possible to improve the collected features by unbinding the images from the process since the person won't be limited by the representation of the entity they describe. In particular, we would like to answer to the following questions</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20401</offset><text> (Q1) Is the picture displayed to the annotator causing bias when asked to describe the entity in the image? </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20511</offset><text> (Q2) Are we able to collect more features with respect to the standard annotation methods? </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20604</offset><text>Therefore, we design and evaluate the effectiveness of a Game With A Purpose (G.W.A.P.) to collect knowledge in terms of relevant features and descriptions of the analyzed content. Such features are organized in three categories, namely “abstract” (identified with “A,”) “not represented in the picture” (identified with “NR,”) and “represented in the picture” (identified with “R.”) “R” features and “NR” features both represents “concrete features.”</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21092</offset><text>Inspired by Ahn et al., we designed a gamified activity where a pair of people play a guessing game. The game involves the following steps.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21232</offset><text>Initial Setup step (Figure 2). Player 1 is provided with the entity category they have to guess. Player 2 is shown the picture of the entity and its name.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21387</offset><text>Basic Turn (Figure 3, on the left). Player 1 asks closed questions about the features of the entity to guess. Player 2 answers the questions. Player 1 may either ask questions freely or fill in predefined question templates (i.e., “Does it have ...?,” “Does it ...?,” etc.). If the answer is affirmative, Player 2 is asked to carry out the Annotation Step.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21752</offset><text>Annotation Step (Figure 3, on the right). Whenever the answer to a question is affirmative, Player 2 is asked to perform a series of simple tasks to identify the guessed feature in the picture they were provided with, if possible. First, they are asked whether the element is displayed in the image. If so, they are requested to outline them in the picture. Otherwise, they are asked whether the feature is an abstract one.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22176</offset><text>Hint Step. If Player 1 guessed no features of the unknown entity in the last few questions, Player 2 provides a bit of advice by providing a feature of the entity to Player 1. If possible, Player 2 should provide a feature that Player 1 already tried to guess. Therefore, Player 1 will be able to proceed with the activity. Player 2 is still required to carry out the Annotation Step for the hinted feature as it will still be considered in the final set of features.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22644</offset><text>Game Conclusion. Finally, after Player 1 has collected enough clues on the entity they are trying to guess, they can provide their final answer. If the answer is correct, the game is over; otherwise, the game moves on. When the game ends, Player 1 is shown both the original picture and the ones with the outlined features to check that Player 2 performed their task correctly. If any element has been improperly outlined or any question has been incorrectly answered, Player 1 can provide their solution (i.e., answer and annotation). Such an action generates a conflict the researcher will resolve when the outcomes of the activity are provided.</text></passage><passage><infon key="file">frai-05-826499-g0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23292</offset><text>The setup step of the gamified activity. Player 1 is provided with the category of the entity they have to guess (in this case, they have to guess an animal). Player 2 is supplied with a picture of the entity and its name (in this case, they are provided with the picture of a zebra).</text></passage><passage><infon key="file">frai-05-826499-g0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23577</offset><text>On the left, the Basic Turn of the gamified activity is displayed. Player 1 asks yes or no questions about the entity. Player 2 answers such questions. On the right, the Annotation Step is summarized. Player 2 is asked to complete a series of simple tasks to identify the guessed feature by answering questions and potentially annotating the picture.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23928</offset><text>Such an activity can be set up to have players mainly focus their questions on concrete features, abstract features or both. Moreover, such a gamified activity could be extended by applying the following changes, enhancing various steps of the activity:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24182</offset><text>It would be possible to introduce a further step at the end of the activity where Player 2 provides an additional picture of the same entity and outline the missing features on the new image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24374</offset><text>It would also be possible to introduce a further Annotation Step for Player 1 at the end of the game to improve the reliability of the results, allowing the comparison of both players' annotation to identify inconsistencies in the provided outcomes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>24624</offset><text>4. Preliminary Evaluation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24650</offset><text>In this section, we report on a preliminary study on the effectiveness and impact of our approach. The experiments have been performed by selecting one entity category and by asking participants to interact over it. In particular, we picked “animals” as a category. We selected parrots and crocodiles as relevant representatives, and we collected a picture from Google Images for each of them. We purposely selected an image partially representing the crocodile (i.e., only its head was visible) and a complete one for the parrot. We engaged 30 people aged between 24 and 30, mostly (60%) employed in IT-related sectors. Most of them (75%) achieved an educational level superior or equal to a bachelor's degree. The participants were randomly organized into three groups:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25426</offset><text>The “annotation” group (comprising 6 people), focusing on outlining features on images;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25518</offset><text>The “gamified activity (concrete)” group (comprising 12 people) focusing their questions on concrete features (i.e., “R” and “NR” features);</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25671</offset><text>And the “gamified activity (generic)” group (comprising 12 people), where members were allowed to ask questions about any features.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25807</offset><text>Depending on such a division, each person was provided with a document describing their activity. The members of each of the gamified activity groups have been internally organized in pairs to carry out the game, thus generating 6 pairs per group. Each player was given one picture to play with. Players were asked to follow the same procedure described in 3.4, depending on their role and group. Each member of the pairs alternately played both roles. Overall, each group carried out 12 matches, (i.e., 6 matches per picture). Additionally, we asked people to keep track of each question and answer when playing as Player 1, and keep track of the suggestions provided when playing as Player 2. On the other hand, each of the 6 members of the “annotation” group was given two documents containing the chosen figures. They were appointed to describe the represented animal by providing a clear and short description of their features, its possible outline on the image, and its category.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>26798</offset><text>5. Results and Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26824</offset><text>5.1. Gamified Activity</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26847</offset><text>Following the preliminary experiment, we discuss the outcomes and the feedback we collected, concerning the research questions we wanted to address.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26996</offset><text>With respect to (Q1), aiming at assessing the role of the specific picture used in generating bias in the player describing the displayed object, we observed that (as expected) most of the concrete details reported by each “annotation” group member were represented in the picture, 73% for the crocodile and 97% for the parrot (Table 1A). Within the same group, we outlined a clear tendency to report “R” features first and forget about features not represented within the picture. Indeed, 50% of the participants provided no “NR” features for the partial image. These observations are aligned with our initial thoughts and expectations. When a person is asked to describe an entity, it mainly attains to the particular representation provided in the picture rather than the actual entity, even when it is well-known. Moreover, we observed a significant difference in the ratio between the amount of “NR” and concrete features collected for the partial picture among the different experiments. In particular, such proportion grew from 27% in the “Annotation” task to 34% in the “Gamified Activity (concrete).” Such a difference is even more emphasized in the “Gamified Activity (general)” experiment. We also identify a 50% increase in the total amount of “NR” features collected by the “Gamified Activity (concrete)” group with respect to the “annotation” one. Therefore, we may argue that creating a sharp separation of roles and hiding the picture from the gamified activity contributes to reducing the bias it induces.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>28559</offset><text>The table represents the average and the sample m.s.e. per participant for each feature type and for each picture.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;(A) “Annotation” Group&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Picture&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“R” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“NR” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“A” Features&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crocodile&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.6.7 ± 0.51&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.33 ± 1.63&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.5 ± 1.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Parrot&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5 ± 2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.17 ± 0.48&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.17 ± 1.72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;(B) “Gamified Activity (concrete)” Group&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Picture&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“R” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“NR” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“A” Features&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crocodile&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.83 ± 1.94&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2 ± 0.63&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.17 ± 0.41&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Parrot&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6 ± 0.89&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0 ± 0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.83 ± 0.41&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;(C) “Gamified Activity (generic)” Group&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Picture&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“R” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“NR” Features&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;“A” Features&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crocodile&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5 ± 0.55&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.17 ± 0.75&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.33 ± 0.81&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Parrot&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.5 ± 0.55&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0 ± 0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.67 ± 1.51&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>28674</offset><text>(A) “Annotation” Group	 	Picture	“R” Features	“NR” Features	“A” Features	 	Crocodile	3.6.7 ± 0.51	1.33 ± 1.63	1.5 ± 1.38	 	Parrot	5 ± 2	0.17 ± 0.48	2.17 ± 1.72	 	(B) “Gamified Activity (concrete)” Group	 	Picture	“R” Features	“NR” Features	“A” Features	 	Crocodile	3.83 ± 1.94	2 ± 0.63	0.17 ± 0.41	 	Parrot	6 ± 0.89	0 ± 0	0.83 ± 0.41	 	(C) “Gamified Activity (generic)” Group	 	Picture	“R” Features	“NR” Features	“A” Features	 	Crocodile	0.5 ± 0.55	1.17 ± 0.75	3.33 ± 0.81	 	Parrot	1.5 ± 0.55	0 ± 0	2.67 ± 1.51	 	</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>29254</offset><text>The table is organized depending on the groups described in Section 4.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29325</offset><text>Regarding (Q2), we argue that our methodology is able to identify more features w.r.t. a state of the art annotation method. Indeed, when the participants were asked to focus on concrete features (Table 1B), we observed a 20% increase in the number of “R” features for the picture of the parrot and a 33% increase in the number of “NR” features for the crocodile one, with respect to the features identified by the “annotation” group by using traditional methods. When analysing the outcomes of the “gamified activity (general)” group, we identified a clear tendency to ask questions about abstract features (e.g., “Is it carnivorous?,” “is it oviparous?,” “Does it live in the Jungle?,” “Is it able to speak?,” etc.) resulting in a 55% increase of abstract features collected with respect to the “Annotation” task (Table 1C). We believe such a behavior is strictly related to humans' capability to abstract concrete concepts and distinguish similar entities through peculiar and selective features, which (sometimes) are abstract. Questions on such selective features even played a fundamental role in the “Gamified Activity (concrete)” group, in which most people who had already collected a lot of concrete features, at the end of the process expressed the need to ask a few abstract questions to consolidate and finalize the identification of the animal. Furthermore, we believe that several descriptive dimensions, e.g., the selectivity of the features, and the category of the entity affect such behaviors.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30878</offset><text>We also collected some comments from the participants, whose feedback would lead to a significant improvement of the gamified activity. In particular, the following changes could be applied</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31068</offset><text>Player 2 won't provide annotations for the collected features during the activity but only at the end. Such a change would smooth the flow of the activity, making it quicker and even more enjoyable for both players.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31284</offset><text>At the end of the activity, both Player 1 and Player 2 will carry out the Annotation Step, improving the consistency of the results and the amount of data collected.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31450</offset><text>At the end of the activity, both players will be shown the picture of the entity to further enrich the collection of the features they already identified by describing those they can derive from the entity's image.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31665</offset><text>In conclusion, we argue that our methodology extends gamified visual annotation and labeling methods, like the ones proposed in Runge et al. and Balayn et al., mitigating the bias caused by pictures by hiding them, allowing an even more complete collection of features. Furthermore, our methodology can be easily extended by introducing further rules to shape and enhance its outcomes. Such an activity can be employed to collect data about what the model should know or should have learned about the entity. Such knowledge can be compared with the outcomes of other explainability methods to evaluate the difference between what the model knows and what it should know. Such a comparison can be carried out both for models learning from pictures of the entity - by comparing the heat maps derived from the model and the annotated “R” (and optionally “NR”) features—and textual descriptions of the entity—comparing the outcomes of saliency-based analyses and the collected features. Moreover, the collected knowledge could be further combined to enhance the outcomes of non-textual, local explainability methods or improve the textual description of textual ones. In particular, non-abstract features annotated by the crowd would be useful to describe pictures in which the same feature is detected by other methods (e.g., heat maps, etc.), while abstract details would be useful to complete textual descriptions, making them more human-understandable and human-like.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>33145</offset><text>5.2. Framework</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33160</offset><text>We argue that our framework would facilitate and structure the exchange of knowledge between the research community and the crowd, leading to an overall improvement of the content provided and the level of understanding of the engaged users. Moreover, the presented crowdsourcing framework engages the users on a different level with respect to other platforms mainly based on extrinsic rewards. In particular, user education would improve users' awareness of what kind of knowledge an AI system needs, learns, and produce, enhancing their efficiency and shaping their mindset. Such a statement would also be amplified when a long-term engagement of the users is achieved.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33833</offset><text>We are aware of the limitations implied by our framework, namely the initial engagement gap, the necessity of keeping the users and the researchers engaged, and the high level of flexibility required to cover all the explainability-related aspects. Gamification will be helpful to compensate for the first two aspects, while the last one will be covered through an accurate design of the proposed activities. In particular, the design will include both extrinsic and intrinsic design elements to account for both the initial and long-term engagement, respectively. In particular, users' side extrinsic design elements will consist of points, activity leaderboards, achievements (i.e., status as a reward), etc. Intrinsic design elements will be mainly associated with the education aspect as it is strictly related to one of the three innate psychological needs (Ryan and Deci,), namely Competence (i.e., people are wishful to learn new skills and mastery tasks). On the other hand, we expect researchers to be engaged as they trade their scientific knowledge for data for their research. Moreover, developing a cooperative framework is challenging, especially when users and researchers must be engaged. We plan to engage users using renowned crowdsourcing platforms for testing purposes, while the initial engagement on the final release will be performed through the university and researchers' network.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>35240</offset><text>6. Conclusions and Future Works</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>35272</offset><text>We presented the preliminary design of a crowdsourcing framework to create a cooperative cycle in which the crowd is taught about explainability-related topics and provides valuable content to AI practitioners. Gamification is applied to empower engagement and drive user behavior. The design and the preliminary evaluation of a gamified data collection activity is also provided. We argue that our research would improve the quality of the data collected to evaluate and enhance the explainability of black-box models. Future work will involve the improving of the design of both the presented activity—following the discussed changes—and the framework. We plan to execute further experiments to generalize the results on effectiveness and efficiency of our method, and to release an opensource crowdsourcing platform, which may be adopted by the broader research community.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>36152</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>36180</offset><text>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>36347</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>36368</offset><text>All authors listed have made a substantial, direct, and intellectual contribution to the work and approved it for publication.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>36495</offset><text>Conflict of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>36516</offset><text>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>36689</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>36706</offset><text>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>37053</offset><text>References</text></passage><passage><infon key="fpage">75</infon><infon key="lpage">78</infon><infon key="name_0">surname:Ahn;given-names:L. V.</infon><infon key="name_1">surname:Kedia;given-names:M.</infon><infon key="name_2">surname:Blum;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">In Proceedings of ACM CHI 2006 Conference on Human Factors in Computing Systems, volume 1 of Games</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>37064</offset><text>“‘Verbosity: A game for collecting common-sense facts,”</text></passage><passage><infon key="fpage">190</infon><infon key="name_0">surname:Balayn;given-names:A.</infon><infon key="name_1">surname:He;given-names:G.</infon><infon key="name_2">surname:Hu;given-names:A.</infon><infon key="name_3">surname:Yang;given-names:J.</infon><infon key="name_4">surname:Gadiraju;given-names:U.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021a</infon><offset>37126</offset><text>Finditout: A multiplayer gwap for collecting plural knowledge</text></passage><passage><infon key="fpage">1937</infon><infon key="lpage">1948</infon><infon key="name_0">surname:Balayn;given-names:A.</infon><infon key="name_1">surname:Soilis;given-names:P.</infon><infon key="name_2">surname:Lofi;given-names:C.</infon><infon key="name_3">surname:Yang;given-names:J.</infon><infon key="name_4">surname:Bozzon;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Web Conference 2021 WWW '21</infon><infon key="type">ref</infon><infon key="year">2021b</infon><offset>37188</offset><text>“What do you mean? interpreting image classification with crowdsourced concept extraction and analysis,”</text></passage><passage><infon key="fpage">82</infon><infon key="lpage">115</infon><infon key="name_0">surname:Barredo Arrieta;given-names:A.</infon><infon key="name_1">surname:D'ıaz-Rodr'ıguez;given-names:N.</infon><infon key="name_2">surname:Del Ser;given-names:J.</infon><infon key="name_3">surname:Bennetot;given-names:A.</infon><infon key="name_4">surname:Tabik;given-names:S.</infon><infon key="name_5">surname:Barbado;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.inffus.2019.12.012</infon><infon key="section_type">REF</infon><infon key="source">Inf. Fusion</infon><infon key="type">ref</infon><infon key="volume">58</infon><infon key="year">2020</infon><offset>37297</offset><text>Explainable artificial intelligence (xai): concepts, taxonomies, opportunities and challenges toward responsible ai</text></passage><passage><infon key="name_0">surname:Belle;given-names:V.</infon><infon key="name_1">surname:Papantonis;given-names:I.</infon><infon key="pub-id_pmid">34278297</infon><infon key="section_type">REF</infon><infon key="source">CoRR</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>37413</offset><text>Principles and practice of explainable machine learning</text></passage><passage><infon key="fpage">1162</infon><infon key="lpage">1175</infon><infon key="name_0">surname:Buckley;given-names:P.</infon><infon key="name_1">surname:Doyle;given-names:E.</infon><infon key="pub-id_doi">10.1080/10494820.2014.964263</infon><infon key="section_type">REF</infon><infon key="source">Interact. Learn. Environ.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2016</infon><offset>37469</offset><text>Gamification and student motivation</text></passage><passage><infon key="fpage">980</infon><infon key="lpage">1008</infon><infon key="name_0">surname:Cerasoli;given-names:C.</infon><infon key="name_1">surname:Nicklin;given-names:J.</infon><infon key="name_2">surname:Ford;given-names:M.</infon><infon key="pub-id_doi">10.1037/a0035661</infon><infon key="pub-id_pmid">24491020</infon><infon key="section_type">REF</infon><infon key="source">Psychol. Bull.</infon><infon key="type">ref</infon><infon key="volume">140</infon><infon key="year">2014</infon><offset>37505</offset><text>Intrinsic motivation and extrinsic incentives jointly predict performance: A 40-year meta-analysis</text></passage><passage><infon key="fpage">189</infon><infon key="lpage">200</infon><infon key="name_0">surname:Estellés-Arolas;given-names:E.</infon><infon key="name_1">surname:de Guevara;given-names:F. G.-L.</infon><infon key="pub-id_doi">10.1177/0165551512437638</infon><infon key="section_type">REF</infon><infon key="source">J. Inf. Sci.</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2012</infon><offset>37604</offset><text>Towards an integrated crowdsourcing definition</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">42</infon><infon key="name_0">surname:Guidotti;given-names:R.</infon><infon key="name_1">surname:Monreale;given-names:A.</infon><infon key="name_2">surname:Ruggieri;given-names:S.</infon><infon key="name_3">surname:Turini;given-names:F.</infon><infon key="name_4">surname:Giannotti;given-names:F.</infon><infon key="name_5">surname:Pedreschi;given-names:D.</infon><infon key="pub-id_doi">10.1145/3236009</infon><infon key="section_type">REF</infon><infon key="source">ACM Comput. Surv.</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2018</infon><offset>37651</offset><text>A survey of methods for explaining black box models</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">3</infon><infon key="name_0">surname:Hamari;given-names:J..</infon><infon key="pub-id_doi">10.1002/9781405165518.wbeos1321</infon><infon key="section_type">REF</infon><infon key="source">The Blackwell Encyclopedia of Sociology</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>37703</offset><text>Gamification</text></passage><passage><infon key="name_0">surname:Hamari;given-names:J.</infon><infon key="name_1">surname:Koivisto;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">ECIS 2013 - Proceedings of the 21st European Conference on Information Systems</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>37716</offset><text>Social motivations to use gamification: An empirical study of gamifying exercise</text></passage><passage><infon key="name_0">surname:Hu;given-names:Z. F.</infon><infon key="name_1">surname:Kuflik;given-names:T.</infon><infon key="name_2">surname:Mocanu;given-names:I. G.</infon><infon key="name_3">surname:Najafian;given-names:S.</infon><infon key="name_4">surname:Shulner Tal;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization UMAP '21</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>37797</offset><text>“Recent studies of xai - review,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Lee;given-names:J.</infon><infon key="name_1">surname:Hammer;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Acad. Exchange Quarter.</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2011</infon><offset>37835</offset><text>Gamification in education: what, how, why bother?</text></passage><passage><infon key="fpage">68</infon><infon key="lpage">72</infon><infon key="name_0">surname:Lee;given-names:W.</infon><infon key="name_1">surname:Reeve;given-names:J.</infon><infon key="name_2">surname:Xue;given-names:Y.</infon><infon key="name_3">surname:Xiong;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.neures.2012.02.010</infon><infon key="pub-id_pmid">27040684</infon><infon key="section_type">REF</infon><infon key="source">Neurosci. Res.</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>37885</offset><text>Neural differences between intrinsic reasons for doing versus extrinsic reasons for doing: An fMRI study</text></passage><passage><infon key="name_0">surname:Lu;given-names:X.</infon><infon key="name_1">surname:Tolmachev;given-names:A.</infon><infon key="name_2">surname:Yamamoto;given-names:T.</infon><infon key="name_3">surname:Takeuchi;given-names:K.</infon><infon key="name_4">surname:Okajima;given-names:S.</infon><infon key="name_5">surname:Takebayashi;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">CoRR</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>37990</offset><text>Crowdsourcing evaluation of saliency-based XAI methods</text></passage><passage><infon key="name_0">surname:Lundberg;given-names:S. M.</infon><infon key="name_1">surname:Lee;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">CoRR</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>38045</offset><text>A unified approach to interpreting model predictions</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">26</infon><infon key="name_0">surname:Mishra;given-names:S.</infon><infon key="name_1">surname:Rzeszotarski;given-names:J. M.</infon><infon key="pub-id_doi">10.1145/3449213</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM Hum.-Comput. Interact.</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2021</infon><offset>38098</offset><text>Crowdsourcing and evaluating concept-driven explanations of machine learning models</text></passage><passage><infon key="fpage">67</infon><infon key="lpage">82</infon><infon key="name_0">surname:Rapp;given-names:A..</infon><infon key="pub-id_doi">10.4018/ijthi.2015010105</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Technol. Hum. Interact.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2015</infon><offset>38182</offset><text>A qualitative investigation of gamification: motivational factors in online gamified services and applications</text></passage><passage><infon key="fpage">1135</infon><infon key="lpage">1144</infon><infon key="name_0">surname:Ribeiro;given-names:M. T.</infon><infon key="name_1">surname:Singh;given-names:S.</infon><infon key="name_2">surname:Guestrin;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD '16</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>38293</offset><text>“‘why should i trust you?': explaining the predictions of any classifier,”</text></passage><passage><infon key="fpage">301</infon><infon key="lpage">314</infon><infon key="name_0">surname:Runge;given-names:N.</infon><infon key="name_1">surname:Wenig;given-names:D.</infon><infon key="name_2">surname:Zitzmann;given-names:D.</infon><infon key="name_3">surname:Malaka;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">14th International Conference on Entertainment Computing (ICEC)</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>38374</offset><text>“Tags you don't forget: gamified tagging of personal images,”</text></passage><passage><infon key="fpage">68</infon><infon key="lpage">78</infon><infon key="name_0">surname:Ryan;given-names:R. M.</infon><infon key="name_1">surname:Deci;given-names:E. L.</infon><infon key="pub-id_doi">10.1037//0003-066x.55.1.68</infon><infon key="pub-id_pmid">11392867</infon><infon key="section_type">REF</infon><infon key="source">Am. Psychol.</infon><infon key="type">ref</infon><infon key="volume">55</infon><infon key="year">2000</infon><offset>38440</offset><text>Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being</text></passage><passage><infon key="name_0">surname:Speer;given-names:R.</infon><infon key="name_1">surname:Krishnamurthy;given-names:J.</infon><infon key="name_2">surname:Havasi;given-names:C.</infon><infon key="name_3">surname:Smith;given-names:D. A.</infon><infon key="name_4">surname:Lieberman;given-names:H.</infon><infon key="name_5">surname:Arnold;given-names:K. C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 14th International Conference on Intelligent User Interfaces</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>38547</offset><text>“An interface for targeted collection of common sense knowledge using a mixture model,”</text></passage><passage><infon key="name_0">surname:Vilone;given-names:G.</infon><infon key="name_1">surname:Longo;given-names:L.</infon><infon key="pub-id_pmid">35390650</infon><infon key="section_type">REF</infon><infon key="source">CoRR</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>38639</offset><text>Explainable artificial intelligence: a systematic review</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">94</infon><infon key="name_0">surname:Von Ahn;given-names:L..</infon><infon key="pub-id_doi">10.1109/MC.2006.196</infon><infon key="section_type">REF</infon><infon key="source">Computer</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2006</infon><offset>38696</offset><text>Games with a purpose</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">109</infon><infon key="name_0">surname:Welbers;given-names:K.</infon><infon key="name_1">surname:Konijn;given-names:E. A.</infon><infon key="name_2">surname:Burgers;given-names:C.</infon><infon key="name_3">surname:de Vaate;given-names:A. B.</infon><infon key="name_4">surname:Eden;given-names:A.</infon><infon key="name_5">surname:Brugman;given-names:B. C.</infon><infon key="pub-id_doi">10.1177/2042753018818342</infon><infon key="section_type">REF</infon><infon key="source">E-Learn. Digit. Media</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2019</infon><offset>38717</offset><text>Gamification as a tool for engaging student learning: A field experiment with a gamified app</text></passage></document></collection>
