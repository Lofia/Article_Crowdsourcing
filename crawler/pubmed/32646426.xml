<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210131</date><key>pmc.key</key><document><id>7346345</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1186/s12911-020-1122-3</infon><infon key="article-id_pmc">7346345</infon><infon key="article-id_pmid">32646426</infon><infon key="article-id_publisher-id">1122</infon><infon key="elocation-id">125</infon><infon key="issue">Suppl 3</infon><infon key="kwd">Intent classification Dataset Word segmentation Name entity recognition</infon><infon key="license">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</infon><infon key="name_0">surname:Chen;given-names:Nan</infon><infon key="name_1">surname:Su;given-names:Xiangdong</infon><infon key="name_2">surname:Liu;given-names:Tongyang</infon><infon key="name_3">surname:Hao;given-names:Qizhi</infon><infon key="name_4">surname:Wei;given-names:Ming</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>0</offset><text>A benchmark dataset and case study for Chinese medical question intent classification</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>86</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>97</offset><text>To provide satisfying answers, medical QA system has to understand the intentions of the users’ questions precisely. For medical intent classification, it requires high-quality datasets to train a deep-learning approach in a supervised way. Currently, there is no public dataset for Chinese medical intent classification, and the datasets of other fields are not applicable to the medical QA system. To solve this problem, we construct a Chinese medical intent dataset (CMID) using the questions from medical QA websites. On this basis, we compare four intent classification models on CMID using a case study.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>709</offset><text>Methods</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>717</offset><text>The questions in CMID are obtained from several medical QA websites. The intent annotation standard is developed by the medical experts, which includes four types and 36 subtypes of users’ intents. Besides the intent label, CMID also provides two types of additional information, including word segmentation and named entity. We use the crowdsourcing way to annotate the intent information for each Chinese medical question. Word segmentation and named entities are obtained using the Jieba and a well-trained Lattice-LSTM model. We loaded a Chinese medical dictionary consisting of 530,000 for word segmentation to obtain a more accurate result. We also select four popular deep learning-based models and compare their performances of intent classification on CMID.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1486</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1494</offset><text>The final CMID contains 12,000 Chinese medical questions and is organized in JSON format. Each question is labeled the intention, word segmentation, and named entity information. The information about question length, number of entities, and are also detailed analyzed. Among Fast Text, TextCNN, TextRNN, and TextGCN, Fast Text and TextCNN models have achieved the best results in four types and 36 subtypes intent classification, respectively.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1939</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1951</offset><text>In this work, we provide a dataset for Chinese medical intent classification, which can be used in medical QA and related fields. We performed an intent classification task on the CMID. In addition, we also did some analysis on the content of the dataset.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2207</offset><text>Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2218</offset><text>In the early years, users mainly employ search engines to obtain answers to medical questions. They enter keywords in the search box and get many pages involving the keywords. However, it is difficult for users to filter out irrelevant information and judge the correctness when a considerable amount of information is available. Although more and more sophisticate retrieval and sorting methods were proposed, it is still an overload for users to extract the expected answers from massive web pages.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2719</offset><text>With the development of knowledge representation, knowledge graph-based medical QA systems gradually attract more and more attention. They not only allow users to ask medical questions in natural language but also returns accurate answers to users without answer selection anymore. This offers great convenience for people’s access to health knowledge.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3074</offset><text>The process of knowledge graph-based medical QA systems can be divided into three steps when using a knowledge graph based medical QA systems, including intent understanding, answer retrieval and answer generation. Intent understanding is to analyze what the users want and plays a core role in the whole answer process. It can be treated as a classification problem if we restrict the intended scope in advance. Traditional methods of intent classification include keyword matching, template matching, etc. The disadvantage of these methods is their poor generalization ability. Recently, more deep-learning-based approaches gradually attract more and more attention and have achieved excellent performance in intention classification.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3811</offset><text>In the field of medical QA, there is a demand to understand the user’s question accurately before answer generation. Besides, compared with English, Chinese has more diverse expressions. Therefore, we should introduce intent comprehension to refine the areas of Chinese medical questions further to provide better answers. For Chinese medical intent understanding, it requires a high-quality dataset to train a deep-learning-based approach in a supervised fashion. However, such a dataset is unavailable for Chinese medical intent understanding, and other areas datasets for intent understanding are not suitable for the medical QA system.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4453</offset><text>To solve the above problem, we constructed a Chinese medical intent dataset (CMID) using the questions from medical QA websites. The questions in CMID data are obtained from several medical QA websites. The intent annotation standard is developed by the medical experts, which includes four types and 36 subtypes of users’ intents. Besides the intent label, CMID also provides two types of additional information, including word segmentation and named entity. We use the crowdsourcing way to annotate the intent information for each Chinese medial question. Word segmentation and named entities are obtained using the Jieba and a well-trained Lattice-LSTM model. To obtain more accurate results, we loaded a Chinese medical dictionary consisting of 530,000 medical terms for word segmentation. Finally, we select four popular deep-learning-based models and compare their performances of intent classification on CMID.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5373</offset><text>We provide a large-scale, high-quality Chinese medical question dataset which contains the intent, word segmentation, and name entities labels. It can be used in the intent classification of the Chinese medical QA system. We will publish the dataset CMID at http://www.github.com/liutongyang/CMID.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5671</offset><text>We compared four deep learning based models for intent classification on CMID and found the best model for Chinese medical intent classification.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5817</offset><text>The contributions are mainly as follows: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>5859</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>5867</offset><text>Constructing the dataset CMID</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>5897</offset><text>Creating the standard: Before getting into the details of how to annotate the questions, we need to create a standard of medical intent. In the project of CMID, the intent annotation standard is developed by the medical experts, which includes four types and 36 subtypes of users’ intents. It details the definition of each intent type and the guidelines of how to deal with the annotation inconsistency. Besides the intent label, CMID also provides two types of additional information, including word segmentation and named entity. The entity tags fellow the classification standard of the ccks2019 evaluation task1.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6517</offset><text>Data preprocessing: We first crawl the web pages from several Chinese medical QA websites. Then, we use regular expressions to eliminate all emoticons, garbled, HTML code, and hyperlinks, leaving only numbers, punctuation, Chinese, and English characters. To avoid data bias, we removed the questions whose lengths are less than four words and more than 255 words at last. In order to ensure the balance of the data, our original data is equally selected according to departments, and repeated questions are removed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7034</offset><text>Intent annotation: We adopt crowdsourcing to annotate the intent type for the questions in CMID manually. The advantage of crowdsourcing involves taking a tremendous job and breaking it into many smaller jobs that a crowd of people can work on separately. According to the annotation standard, the user’s intent is divided into four types, including “disease,” “medicine,” “therapeutic schedule,” and “other.” Each type is further divided into several subtypes. According to the experience of experts, more additional information can help understand the intent of the question. We also provide the annotator with department information and doctor’s answers to each question to facilitate intentional discrimination. Each question is annotated by two people simultaneously. If there is a conflict, we resort to the medical experts. Due to the particularity of the medical field, labelers should also have medical expertise. This limits the number of labelers and annotation progress. The resulting CMID contains 12000 questions in total.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8091</offset><text>Word segmentation: To provide additional useful information, we use Jieba (https://github.com/fxsjy/jieba) to segment the sentences in precise mode, which is one of the three modes of Jieba. It can produce a more accurate result at the cost of more running time. In word segmentation, it is difficult to distinguish the word boundaries of Chinese medical terminology. To ensure that terminology can be accurately segmented, we loaded a medical dictionary containing 534,983 medical terms for the Jieba.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8594</offset><text>Named entity recognition: For the QA task, the named entities in questions can provide vital clues for answer generation. Therefore, we label the named entities for the questions in CMID. In this project, we use Lattice-LSTM for entity recognition, which is the best performing model on the Chinese named entity recognition task. It can take full advantage of input character and word order information without word segmentation errors. The inputs are single Chinese character embeddings and word embeddings, and the outputs are the entity labels.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9142</offset><text>As mentioned above, the questions in CMID have labeled the intention, word segmentation, and named entity information. All questions were collected from 20 online professional medical QA websites. On these websites, users can ask such a variety of questions about medical matters in Chinese, and doctors provide professional diagnosis and advice under the questions. We extract the questions from the web pages using regular expression and manually annotate the intent types through crowdsourcing. Automatic tools fulfill word segmentation and named entities of the questions. The processing workflow is as follows: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9759</offset><text>Text classification model</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9785</offset><text>TextCNN: The feature of CNN is the shared convolution kernel, which can be paralleled in the calculation, thus significantly reducing the time for model training. We use the traditional CNN structure to extract text information, which consists of an input layer, convolution layer, pooling layer, and fully connected layer. Firstly, the sentence enters the input layer through the word segmentation, and then the input layer embeds the word into the word vector. In the convolutional layer, we use four different convolution kernel sizes to extract information. The width of the convolution kernel is the length of the word vector, 200 dimensions. The length of the convolution kernel is set to 1, 2, 3, 4, respectively, corresponding to the single word feature, the 2-gram feature, the 3-gram feature, and the 4-gram feature in the sentence. In the pooling layer, we use max-pooling to process the data and concatenate the four features. Finally, we output the most probable category as the final prediction.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10795</offset><text>TextRNN: The advantage of RNN is capturing longer distance dependencies in the sequence. It is unnecessary to adjust the cumbersome hyperparameters like CNN. We use bidirectional LSTM to capture the context information of the sentence entirely. Firstly, the sentences are mapped into the word vector after being segmented. The input layer then enters the words into the model in chronological order. In order to improve the robustness of the model, we add the dropout operation in each layer of LSTM. Finally, the model takes the hidden state of the last layer of LSTM as the input to the fully connected softmax layer and obtains the probability value for each category. We output the category with the highest probability value as the result of the final prediction.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11564</offset><text>Fast Text: Fast Text was Facebook’s open-source text classification model in 2016. Its model structure is very similar to the CBOW model structure in word2vec. There are three layers in the Fast Text model, the input layer, the hidden layer, and the output layer. First, each question is segmented, and then its N-gram feature is used as the input layer, embedding in the hidden layer and averaged to get the hidden layer output. Finally, the maximum probability label is calculated by the hierarchical softmax classifier. Due to its simple model structure, Fast Text is very fast in training compared to other neural network models. At the same time, it has achieved high classification accuracy and is widely used in the industry. Therefore, we use Fast Text as one of the intent classification models on CMID.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12379</offset><text>TextGCN: TextGCN builds a large and heterogeneous text graph on the entire corpus that contains word nodes and document nodes. It can explicitly model the global word co-occurrence. When the model is initialized, the feature matrix only needs to be set to an identity matrix. In other words, each word or document is input to the TextGCN as a one-hot vector. There are two types of edges in the graph, document-word is constructed with TF-IDF information, and word-word is constructed with point-wise mutual information (PMI). When the text graph is built, we feed the graph into a simple two-layer GCN, such as Kipf’s work. In the hidden layer, the node and the neighbor node represent each other to complete the information transfer, and finally, send it into a softmax classifier to calculate the maximum probability label.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13208</offset><text>In this paper, we offer a case study of intent classification on CMID. Four deep-learning-based models were compared, including TextCNN, TextRNN, Fast Text, and TextGCN. We used default parameter settings as their papers or open-source implementations. For TextCNN and TextRNN, we used pre-trained word embeddings provided by Tencent-AILab. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>13550</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>13558</offset><text>The dataset CMID</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>13575</offset><text>Four types and 36 subtypes classification of CMID</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Type&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Subtype&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Definition&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Definition of disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Pathogeny&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Causes of disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Clinical manifestation&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Manifestations of the disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Related diseases&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Sequelae, complications, etc&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Treatment method&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Treatment of disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Recommended hospital&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Good at treating the disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Prevention&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;How to prevent disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Subordinate departments&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Belongs to which departments&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Disease taboo&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;What should not be done&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Infectivity&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Whether the disease is contagious&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Cure rate&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Whether it can be cured&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Severity&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Severity of disease&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Effect&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Role of medicine&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Applicable disease&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Which diseases are suitable for&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Price&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Medicine price&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Drug contraindication&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Conflict with drug&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Usage&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Drug usage&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Side effect&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Side effects of drugs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Ingredient&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Pharmaceutical ingredients&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Method&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Method of the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Cost&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Cost of the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Effective time&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Effective time after the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Inspection purpose&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;X-ray, CT, etc&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Treatment time&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Time of the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Curative effect&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Operation effect&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Recovery Time&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Recovery time after the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Normal indicator&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Normal indicators of the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Physical examination program&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Including various checks&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Recovery&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;recover health after the operation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Device usage&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Use of various instruments&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Multi-questions&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;More than one question&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>13625</offset><text>Type	Subtype	Description	 	Disease	Definition	Definition of disease	 	Disease	Pathogeny	Causes of disease	 	Disease	Clinical manifestation	Manifestations of the disease	 	Disease	Related diseases	Sequelae, complications, etc	 	Disease	Treatment method	Treatment of disease	 	Disease	Recommended hospital	Good at treating the disease	 	Disease	Prevention	How to prevent disease	 	Disease	Subordinate departments	Belongs to which departments	 	Disease	Disease taboo	What should not be done	 	Disease	Infectivity	Whether the disease is contagious	 	Disease	Cure rate	Whether it can be cured	 	Disease	Severity	Severity of disease	 	Medicine	Effect	Role of medicine	 	Medicine	Applicable disease	Which diseases are suitable for	 	Medicine	Price	Medicine price	 	Medicine	Drug contraindication	Conflict with drug	 	Medicine	Usage	Drug usage	 	Medicine	Side effect	Side effects of drugs	 	Medicine	Ingredient	Pharmaceutical ingredients	 	Treatment programs	Method	Method of the operation	 	Treatment programs	Cost	Cost of the operation	 	Treatment programs	Effective time	Effective time after the operation	 	Treatment programs	Inspection purpose	X-ray, CT, etc	 	Treatment programs	Treatment time	Time of the operation	 	Treatment programs	Curative effect	Operation effect	 	Treatment programs	Recovery Time	Recovery time after the operation	 	Treatment programs	Normal indicator	Normal indicators of the operation	 	Treatment programs	Physical examination program	Including various checks	 	Treatment programs	Recovery	recover health after the operation	 	Other	Device usage	Use of various instruments	 	Other	Multi-questions	More than one question	 	</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>15273</offset><text>Four types and 36 subtypes classification of CMID (Continued)</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Type&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Subtype&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Health preservation&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;All about health preservation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Cosmetic surgery&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;All about Cosmetic surgery&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Gender issues&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;All about Gender issues&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Contrast&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Medicine comparisons, hospital,etc&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Uncertainty&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Custom intentions&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>15335</offset><text>Type	Subtype	Description	 	Other	Health preservation	All about health preservation	 	Other	Cosmetic surgery	All about Cosmetic surgery	 	Other	Gender issues	All about Gender issues	 	Other	Contrast	Medicine comparisons, hospital,etc	 	Other	Uncertainty	Custom intentions	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>15609</offset><text>In CMID, there are four types and 36 subtypes of intent labels, as listed in Table 1. The descriptions of the subtypes are also provided. </text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>15749</offset><text>An example in CMID</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Key&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Value&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Example&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;question&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;user question&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;&lt;inline-graphic xlink:href=&quot;12911_2020_1122_Figai_HTML.gif&quot; id=&quot;d30e911&quot;/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;entities&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;entity extraction results&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;[“label-type”: “Desease”, “start”: 0, &quot;end&quot;: 5]&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;seg-result&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;word segmentation results&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;[&quot;&lt;inline-graphic xlink:href=&quot;12911_2020_1122_Figaj_HTML.gif&quot; id=&quot;d30e933&quot;/&gt;“, “&lt;inline-graphic xlink:href=&quot;12911_2020_1122_Figak_HTML.gif&quot; id=&quot;d30e936&quot;/&gt;”, “&lt;inline-graphic xlink:href=&quot;12911_2020_1122_Figal_HTML.gif&quot; id=&quot;d30e939&quot;/&gt;“, “&lt;inline-graphic xlink:href=&quot;12911_2020_1122_Figam_HTML.gif&quot; id=&quot;d30e942&quot;/&gt;, ?”]&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;intent&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;intent label of the question&lt;/td&gt;&lt;td align=&quot;justify&quot;&gt;Definition&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>15768</offset><text>Key	Value	Example	 	question	user question		 	entities	entity extraction results	[“label-type”: “Desease”, “start”: 0, &quot;end&quot;: 5]	 	seg-result	word segmentation results	[&quot;“, “”, ““, “, ?”]	 	intent	intent label of the question	Definition	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16034</offset><text>To facilitate user’s access and modification, the dataset CMID is stored in JSON format. The field &quot;questions&quot; stores the user’s question. The field &quot;entities&quot; stores all the entities in the sentence together with their types (“label-type”), the starting position (“start-pos”), and the ending positions (“end-pos”) of the entity in the sentence. The field “seg-result” represents the result of the word segmentation. The field “intent” is the manually annotated intent label. Table 2 presents an example in CMID. </text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16574</offset><text>Details of four types of intent classification</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Type name&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Number&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Max length&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Min length&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Average length&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Disease&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6264&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;217&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Medicine&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2755&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;205&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Treatment programs&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;904&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;213&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Other&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2332&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;221&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;37&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16621</offset><text>Type name	Number	Max length	Min length	Average length	 	Disease	6264	217	5	30	 	Medicine	2755	205	4	27	 	Treatment programs	904	213	5	32	 	Other	2332	221	5	37	 	</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>16783</offset><text>Details of 36 subtypes of intent classification</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Subtype name&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Number&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Max length&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Min length&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Average length&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Definition&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;440&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;131&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;24&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Pathogeny&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;970&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;157&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Clinical manifestation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1313&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;217&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Related diseases&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;242&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;139&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Treatment method&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2101&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;185&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Recommended hospital&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;337&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;146&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;43&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Prevention&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;138&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;111&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Subordinate departments&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;60&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;22&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Disease taboo&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;271&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;122&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Infectivity&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;43&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;111&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;42&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Cure rate&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;144&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;135&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Severity&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;263&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;107&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;40&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Effect&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;555&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;205&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Applicable disease&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;962&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;110&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Price&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;75&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;109&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Drug contraindication&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;235&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;102&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Usage&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;420&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;142&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Side effect&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;332&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Ingredient&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;104&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;83&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Method&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;423&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;213&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Cost&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;102&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;42&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Effective time&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;31&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;68&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Inspection purpose&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;45&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;163&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Treatment time&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;41&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;119&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Curative effect&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;64&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;87&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Recovery Time&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;89&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Normal indicator&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;72&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Physical examination program&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;80&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;107&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Recovery&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;80&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;117&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Device usage&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;121&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Multi-questions&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1132&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;221&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;45&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Health preservation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;139&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;87&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Cosmetic surgery&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;17&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Gender issues&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;127&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Contrast&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;114&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;justify&quot;&gt;Uncertainty&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;835&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;196&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16831</offset><text>Subtype name	Number	Max length	Min length	Average length	 	Definition	440	131	5	24	 	Pathogeny	970	157	5	26	 	Clinical manifestation	1313	217	5	32	 	Related diseases	242	139	8	33	 	Treatment method	2101	185	5	30	 	Recommended hospital	337	146	8	43	 	Prevention	138	111	6	29	 	Subordinate departments	26	60	6	22	 	Disease taboo	271	122	7	26	 	Infectivity	43	111	5	42	 	Cure rate	144	135	6	36	 	Severity	263	107	7	40	 	Effect	555	205	6	27	 	Applicable disease	962	110	7	28	 	Price	75	109	9	31	 	Drug contraindication	235	102	7	28	 	Usage	420	142	6	28	 	Side effect	332	88	8	29	 	Ingredient	104	83	5	25	 	Method	423	213	5	33	 	Cost	86	102	6	42	 	Effective time	31	68	10	26	 	Inspection purpose	45	163	9	36	 	Treatment time	41	119	9	48	 	Curative effect	64	87	5	26	 	Recovery Time	30	89	9	29	 	Normal indicator	24	72	10	26	 	Physical examination program	80	107	7	29	 	Recovery	80	117	9	30	 	Device usage	20	121	10	38	 	Multi-questions	1132	221	6	45	 	Health preservation	139	87	5	26	 	Cosmetic surgery	8	36	6	17	 	Gender issues	84	127	5	31	 	Contrast	114	100	9	26	 	Uncertainty	835	196	5	31	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>17921</offset><text>According to statistical analyses, we list intent type and subtype information of CMID in Tables 3 and 4, respectively. Table 3 reports the number of each intent type, the maximum, the minimum, and the average length of the questions in each type. Table 4 is similar to Table 3. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>18205</offset><text>Intent classification result on CMID</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18242</offset><text>In this paper, we carried out a case study on CMID to explore the performance of the deep learning-based models on Chinese medical intent classification. Two group experiments were implemented for four intent types and 36 subtypes, respectively. The training set and test set are divided by a ratio of 4:1 according to each category.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18576</offset><text>The evaluation metric is the accuracy p. The reason we focus on accuracy is that the accuracy of the intent classification plays a decisive factor in the subsequent steps of the medical QA task. The more accurate that the intent classification is, the better that the system performs. The accuracy p is defined as follows: </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18900</offset><text>where T is the number that predicts the correct labels in the test set, F is the number that predicts the error labels in the test set.</text></passage><passage><infon key="file">Tab6.xml</infon><infon key="id">Tab6</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>19036</offset><text>Four types intent classification result</text></passage><passage><infon key="file">Tab6.xml</infon><infon key="id">Tab6</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextCNN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.859&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextRNN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.814&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Fast Text&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.860&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextGCN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.683&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>19076</offset><text>Model	Accuracy	 	TextCNN	0.859	 	TextRNN	0.814	 	Fast Text	0.860	 	TextGCN	0.683	 	</text></passage><passage><infon key="file">Tab7.xml</infon><infon key="id">Tab7</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>19160</offset><text>36 subtypes intent classification result</text></passage><passage><infon key="file">Tab7.xml</infon><infon key="id">Tab7</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextCNN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.473&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextRNN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.424&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Fast Text&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.462&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;TextGCN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.357&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>19201</offset><text>Model	Accuracy	 	TextCNN	0.473	 	TextRNN	0.424	 	Fast Text	0.462	 	TextGCN	0.357	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19285</offset><text>In the experiment, the deep-learning models include TextCNN, TextRNN, Fast Text, and TextGCN. They have shown very competitive performance in text classification tasks on other datasets. The intent classification result of four types on CMID is shown in Table 5, and that of 36 subtypes is shown in Table 6. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19596</offset><text>From Tables 5 and 6, we can clearly see that Fast Text performs the best among the four models on Chinese medical intent classification. The performance of TextGCN is worse than any other model.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>19792</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>19803</offset><text>The discussion of the intent classification result</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>19854</offset><text>From the intent classification results, we can see that the results of the four types intent classification are significantly better than the 36 subtypes intent classification, and the accuracy of the four types intent classification is more than 80%, but the results of the 36 subtypes intent classification are relatively poor. There are two reasons for this result. First, the four types of intent classification models are easier to train than the 36 subtypes’ intent classification model. Second, because the number of each category in the four types is large, the data of each category can be fully trained. The number of each category in the 36 subtypes is relatively small, resulting in a reduction in accuracy. Therefore, augmenting the data set is very important for intent recognition.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>20653</offset><text>For the intent classification model, TextGCN performs worst of all models. Fast Text and TextCNN achieve the best accuracy in 4 types, and 36 subtypes classification tasks, respectively, and their results in each classification task are very close. The Text RNN has a large decrease in accuracy compared to them. This is because the average length of the questions is 30 characters, which is a short text. TextCNN and Fast Text are more powerful than TextRNN in short text feature extraction.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>21146</offset><text>The discussion of dataset content</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>21180</offset><text>Based on the CMID, we also did some analysis on the content of the dataset. The first noteworthy is that in CMID, 52% of the intentional label is &quot;disease,&quot; and further, the &quot;treatment method&quot; accounts for 33% of all disease categories. When we delved into the question of such intentions, we found an interesting thing. 30% of users’ intentions are not clear. For example: “,,”,“, ,,,,,”. As we have seen, these two sentences have no obvious intentional keywords &quot;why,&quot; &quot;how to do&quot; or &quot;how to treat,&quot; etc. Their problem is only to describe the condition, but the patient wants to ask the doctor how to treat. Obviously, methods based on template matching and keywords are unable to obtain the semantics of such sentences. However, some patients believe that such a questioning method is effective. Therefore, we believe that in the pipeline of the intelligent medical QA system, it is necessary to train an intent recognition model.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>22124</offset><text>The second thing worth discussing is that we find patients not only describe the condition when asking for treatment but at any time. In CMID, 65% of patients like to describe the condition first, and then show the intention, for example, “, ”,“, , , , , ? () , , ?”. Obviously, the intent of the previous sentence is the applicability of the drug, but the first half of the sentence is describing the illness, and the task of intent classification is invalid information. The intent of the latter sentence is the contraindication of the drug. The first half of the sentence is still describing its symptoms, which is invalid information for the intended classification task. We suggest all authors who use this dataset: it is necessary to include location information in the model when text categorization, paying particular attention to the tail of the sentence, with 65% of real user intent appearing here.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>23042</offset><text>Thirdly, in all the categories, we define the “Uncertainty” category in the 36 subtypes as Out-of-scope data, and it accounts for 6% of the CMID. We think it is necessary to set Out-of-scope data to ensure the robustness of the QA system. For this phenomenon, we suggest that the QA system add multiple rounds of dialogue to determine user intent further.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>23402</offset><text>Finally, there is a bit of thinking about a special intent called &quot;multiple questions.&quot; In our intent category, there is a type of problem that we call &quot;multiple questions,&quot; that is, a question contains two or more intents. For example: “? ? ? ? ? ?”, “, ? ? ?”. We recommend that when dealing with such sentences in a real-life scenario, supervised automated methods are used to complement the subject of the sub-question. Finally, the questions are divided into independent sub-problems and treated as complete problems.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>23933</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>23945</offset><text>In this paper, we construct CMID, which is a large-scale, high-quality question-intentional annotation dataset for the QA system and a variety of applications in related fields. With the help of expert knowledge, we extracted four types and 36 subtypes of question intent in the real medical QA scene, and finally got 12000 questions with tag information through crowdsourcing. Besides, we also provided accurate word segmentation information and named entity recognition information. We used word-based input to evaluate the performance of four text classification baseline models. The experimental results show that the Fast Text is superior to other models. Through these experiments, we provided a strong benchmark dataset for the intent of understanding the task of medical QA. We hope that our research and dataset will promote the development of smarter, more powerful medical QA systems.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>24841</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24855</offset><text>TF-IDF</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24862</offset><text>Term frequency-inverse document frequencies</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24906</offset><text>QA</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24909</offset><text>Question and answer</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24929</offset><text>NER</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24933</offset><text>Named entity recognition</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24958</offset><text>GCN</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24962</offset><text>Graph convolution neural network</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24995</offset><text>CNN</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>24999</offset><text>Convolutional neural networks</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>25029</offset><text>RNN</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>25033</offset><text>Recurrent neural network</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>25058</offset><text>LSTM</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>25063</offset><text>Long short-term memory</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>25086</offset><text>Publisher’s Note</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>25105</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>25224</offset><text>Authors’ contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>25249</offset><text>NC leads the method application, experiment conduction, and the result analysis. TYL, QZH, and MW participated in data extraction and preprocessing. MW participated in the manuscript revision. XDS provided theoretical guidance and the revision of this paper. All authors read and approved the final manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>25560</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>25568</offset><text>Publication costs are funded by grants from the Natural Science Foundation of China (No. 61762069, 61563040, 61773224) and Natural Science Foundation of Inner Mongolia Autonomous Region (No. 2017BS0601, 2016ZD06, 2018MS06006).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>25795</offset><text>Availability of data and materials</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25830</offset><text>The dataset is publically available via http://www.github.com/liutongyang/CMID.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>25910</offset><text>Ethics approval and consent to participate</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25953</offset><text>Not applicable.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>25969</offset><text>Consent for publication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25993</offset><text>Not applicable.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>26009</offset><text>Competing interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>26029</offset><text>The authors declare that they have no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>26088</offset><text>References</text></passage><passage><infon key="name_0">surname:Büttcher;given-names:S</infon><infon key="name_1">surname:Clarke;given-names:CL</infon><infon key="name_2">surname:Cormack;given-names:GV</infon><infon key="section_type">REF</infon><infon key="source">Information Retrieval: Implementing and Evaluating Search Engines</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>26099</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26100</offset><text>Nishida K, Nishida K, Nagata M, Otsuka A, Saito I, Asano H, Tomita J. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction. arXiv preprint. 2019. arXiv:1905.08511.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26302</offset><text>Kratzwald B, Eigenmann A, Feuerriegel S. Rankqa: Neural question answering with answer re-ranking. arXiv preprint. 2019. arXiv:1906.03008.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26441</offset><text>Cabrio E, Cojan J, Aprosio AP, Magnini B, Lavelli A, Gandon F. Qakis: an open domain qa system based on relational patterns. In: Proc. of the 11th International Semantic Web Conference (ISWC 2012), demo paper: 2012. https://hal.inria.fr/hal-01171115.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26692</offset><text>Hu J, Wang G, Lochovsky F, Sun J-t, Chen Z. Understanding user’s query intent with wikipedia. In: Proceedings of the 18th International Conference on World Wide Web. ACM: 2009. p. 471–80. 10.1145/1526709.1526773.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>26909</offset><text>Ji Z, Xu F, Wang B, He B. Question-answer topic model for question retrieval in community question answering. In: Proceedings of the 21st ACM International Conference on Information and Knowledge Management. ACM: 2012. p. 2471–4. 10.1145/2396761.2398669.</text></passage><passage><infon key="fpage">426</infon><infon key="issue">2</infon><infon key="lpage">41</infon><infon key="name_0">surname:Nie;given-names:L</infon><infon key="name_1">surname:Wang;given-names:M</infon><infon key="name_2">surname:Gao;given-names:Y</infon><infon key="name_3">surname:Zha;given-names:Z-J</infon><infon key="name_4">surname:Chua;given-names:T-S</infon><infon key="pub-id_doi">10.1109/TMM.2012.2229971</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Multimed</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2012</infon><offset>27166</offset><text>Beyond text qa: multimedia answer generation by harvesting web information</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27241</offset><text>Tan C, Wei F, Yang N, Du B, Lv W, Zhou M. S-net: From answer extraction to answer generation for machine reading comprehension. arXiv preprint. 2017. arXiv:1706.04815.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27409</offset><text>Mendoza M, Zamora J. Building decision trees to identify the intent of a user query. In: International Conference on Knowledge-Based and Intelligent Information and Engineering Systems. Springer: 2009. p. 285–92. 10.1007/978-3-642-04595-0_35.</text></passage><passage><infon key="fpage">8887</infon><infon key="name_0">surname:Dhar;given-names:S</infon><infon key="name_1">surname:Swain;given-names:S</infon><infon key="name_2">surname:Mishra;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Int J Comput Appl</infon><infon key="type">ref</infon><infon key="volume">975</infon><infon key="year">2014</infon><offset>27654</offset><text>Query intent classification using semi-supervised learning</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27713</offset><text>Yu C, Sheng Y. https://www.chunyuyisheng.com/. Accessed 9 Mar 2011.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27781</offset><text>Jieba project. https://github.com/fxsjy/jieba. Accessed 14 Sept 2017.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27851</offset><text>Zhang Y, Yang J. Chinese ner using lattice lstm. arXiv preprint. 2018. arXiv:1805.02023.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>27940</offset><text>Kim Y. Convolutional neural networks for sentence classification. arXiv preprint. 2014. arXiv:1408.5882.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28045</offset><text>Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning. arXiv preprint. 2016. arXiv:1605.05101.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28183</offset><text>Joulin A, Grave E, Bojanowski P, Mikolov T. Bag of tricks for efficient text classification. arXiv preprint. 2016. arXiv:1607.01759.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28316</offset><text>Yao L, Mao C, Luo Y. Graph convolutional networks for text classification. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33: 2019. p. 7370–7. 10.1609/aaai.v33i01.33017370.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28516</offset><text>Song Y, Shi S, Li J, Zhang H. Directional skip-gram: Explicitly distinguishing left and right context for word embeddings. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers): 2018. p. 175–80. 10.18653/v1/n18-2028.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28853</offset><text>Boureau Y-L, Bach F, LeCun Y, Ponce J. Learning mid-level features for recognition. In: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Citeseer: 2010. p. 2559–66. 10.1109/cvpr.2010.5539963.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29080</offset><text>Ma X, Hovy E. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint. 2016. arXiv:1603.01354.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29197</offset><text>Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint. 2013. arXiv:1301.3781.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29336</offset><text>Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In: Advances in Neural Information Processing Systems: 2013. p. 3111–19.</text></passage><passage><infon key="fpage">22</infon><infon key="issue">1</infon><infon key="lpage">9</infon><infon key="name_0">surname:Church;given-names:KW</infon><infon key="name_1">surname:Hanks;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Comput Linguist</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">1990</infon><offset>29540</offset><text>Word association norms, mutual information, and lexicography</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29601</offset><text>Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. arXiv preprint. 2016. arXiv:1609.02907.</text></passage></document></collection>
