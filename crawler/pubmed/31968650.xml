<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201223</date><key>pmc.key</key><document><id>7014516</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/s20020569</infon><infon key="article-id_pmc">7014516</infon><infon key="article-id_pmid">31968650</infon><infon key="article-id_publisher-id">sensors-20-00569</infon><infon key="elocation-id">569</infon><infon key="issue">2</infon><infon key="kwd">social robotics coaching social conversation crowdsourcing human computation real-time crowd-powered systems stress</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Abbas;given-names:Tahir</infon><infon key="name_1">surname:Khan;given-names:Vassilis-Javed</infon><infon key="name_2">surname:Gadiraju;given-names:Ujwal</infon><infon key="name_3">surname:Barakova;given-names:Emilia</infon><infon key="name_4">surname:Markopoulos;given-names:Panos</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>0</offset><text>Crowd of Oz: A Crowd-Powered Social Robotics System for Stress Management</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>74</offset><text>Coping with stress is crucial for a healthy lifestyle. In the past, a great deal of research has been conducted to use socially assistive robots as a therapy to alleviate stress and anxiety related problems. However, building a fully autonomous social robot which can deliver psycho-therapeutic solutions is a very challenging endeavor due to limitations in artificial intelligence (AI). To overcome AI’s limitations, researchers have previously introduced crowdsourcing-based teleoperation methods, which summon the crowd’s input to control a robot’s functions. However, in the context of robotics, such methods have only been used to support the object manipulation, navigational, and training tasks. It is not yet known how to leverage real-time crowdsourcing (RTC) to process complex therapeutic conversational tasks for social robotics. To fill this gap, we developed Crowd of Oz (CoZ), an open-source system that allows Softbank’s Pepper robot to support such conversational tasks. To demonstrate the potential implications of this crowd-powered approach, we investigated how effectively, crowd workers recruited in real-time can teleoperate the robot’s speech, in situations when the robot needs to act as a life coach. We systematically varied the number of workers who simultaneously handle the speech of the robot (N = 1, 2, 4, 8) and investigated the concomitant effects for enabling RTC for social robotics. Additionally, we present Pavilion, a novel and open-source algorithm for managing the workers’ queue so that a required number of workers are engaged or waiting. Based on our findings, we discuss salient parameters that such crowd-powered systems must adhere to, so as to enhance their performance in response latency and dialogue quality.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1846</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1862</offset><text>Psychological stress is an extent to which persons observe that their demands exceed their ability to manage. Coping with stress is crucial for a healthy lifestyle. Prolonged and high level of stress in humans can affect several physiological and psychological functions including change in heart rate and heart rate variability, muscle tension in the neck, hormonal changes and cause negative feelings. Stress can also affect memory processes and can impact the ability to learn and remember.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2356</offset><text>University students are more susceptible to develop stress and anxiety related problems due to highly technical and emotional demands of their studies. Sometimes, psychological suffering is so severe that it could lead them to suicidal ideation or dropout from university. The most common stressors that could persuade mental illness and emotional distress include but are not limited to academic overload (e.g., assignments, exams), sleeping problems, transition to a new environment, not being able to enjoy day-to-day activities, loneliness, being away from home, lack of self-efficacy and optimism, and not being able to make new friends.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2999</offset><text>In the past, a great deal of research has been conducted to leverage socially assistive robots as a therapy to alleviate stress and anxiety related problems among children, older adults, and teens. However, building a fully autonomous social agent which can deliver psycho-therapeutic solutions is a very challenging endeavor which requires emotional intelligence, affect analysis, computational psychology, automated speech recognition, natural language processing (NLP), prediction and planning, and advanced computer vision techniques to automatically recognize non-verbal cues such as gestures, facial expression, and objects. Consider, for example, holding a supportive discussion with someone who feels stressed—we will refer to this act as ‘stress mitigation’. For a stress mitigation task, the robot, acting as a coach, should be able to establish a sympathetic relationship with the user. Namely the robot must: help the user to understand his/her feelings through asking open questions; reflect and validate the client’s problems; and exhibit some action skills by stating opinions, facts and solutions based on the user’s situation. This requires robots to go beyond the current AI methods and acquire specialized skills in affect analysis and computational psychology. Therefore, fully automated, open-ended discussion with social robots remains elusive.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4376</offset><text>Crowdsourcing seems to be viable solution that can overcome the aforementioned problems. In the past, researchers have investigated the use of crowdsourcing for delivering positive psychological interventions to people who are stressed. This concept is known as affective crowdsourcing. For instance, Emotional Bag Check is an emotional support tool that rely on a cohort of volunteers and trained people to give therapeutic support to students. Panoply is a crowd-powered system that leverages the wisdom of crowd to provide on-demand emotional support to people anytime and across the globe though cognitive reappraisal technique. Nevertheless, these emotional support tools take enormous amount of time to give empathetic responses to stressed peoples. For instance, Panoply takes on average 9 min to 2 h to receive a first response from crowd workers and other registered users, which is not suitable for a two-way live synchronous conversation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5326</offset><text>This paper investigates rapid crowdsourcing techniques to summon a synchronous group of workers in real time to provide therapeutic support through two-way live synchronous conversation. It was made possible thanks to the development of new methods that help to reduce the latency of the crowd’s input from hours to seconds. Such systems have introduced the notion of real-time, synchronous crowdsourcing (RTC) where the crowd engages synchronously in processes that are characterized by real-time constraints. Recently, RTC based teleoperation methods have emerged as a plausible alternative for teleoperating robots and for supporting machine learning. For instance, Legion crowdsourced a toy robot’s locomotion to a real-time synchronous crowd. Another example is EURECA where online crowds assisted robots to identify unknown objects in a scene in real-time. Nevertheless, prior research has focused only on enabling RTC to support the robot in navigation and manipulation tasks. However, to this point it has not been attempted to apply RTC to enable two-way live communication for social robots to handle complex therapeutic tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6468</offset><text>Furthermore, previous emotional support tools rely on a text based medium for exchanging messages which is not perceived as enjoyable and interactive as compared to an embodied conversational agent. For example, Berry et al. compared an animated embodied agent (GRETA), with a voice-based agent and text only agent. Their participants rated GRETA as more likeable and helpful as compared to the text-based and voice-based agent. In another study, Powers et al. compared a collocated robot with a remote projected robot and an agent on the monitor screen. Their results show that the collocated robot was more engaging, more helpful, gave better advice and was more effective communicator than the projected robot and animated agent. Hence, these studies motive us to use the embodied conversational agent (Softbank’s Pepper Robot) as a life coach to deliver a psycho-therapeutic solution.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7359</offset><text>This paper presents the first attempt in applying RTC to facilitate an embodied conversational agent to provide emotional support to people who are stressed. This presents several challenges from a crowdsourcing perspective, pertaining to sustaining the context of the conversation and ensuring the speed and fluency of the robot’s behaviors. We have developed an open-source system called Crowd of Oz (CoZ) to enable crowd-powered social robotics for therapeutic tasks. Firstly, CoZ broadcasts a live audio–video (AV) stream of an interlocutor, to anonymous workers online who can then communicate with a stressed person through the embodied conversational agent in real time. In this version of CoZ, we only focused on controlling the speech of the robot and gestures were controlled by Pepper’s built-in animated speech API (application programming interface), which automatically produces appropriate gestures based on the text input. Secondly, CoZ features a crowd user interface (UI) that supports them in promptly typing utterances based on the context of the conversation. Finally, CoZ supports a novel workflow to handle the asynchronous arrival and departure of workers during the task.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8562</offset><text>To demonstrate the feasibility of this approach, we investigated how effectively a crowd of workers recruited in real-time, can act as a coach by teleoperating the Pepper robot in a stress mitigation task. To investigate this, we invited a professional actress who played the role of a stressed Master’s student and improvised dialogues with the Pepper robot (supported by CoZ) in different sessions. For each session, we systematically varied the number of workers who simultaneously controlled the robot’s speech. Our results indicate that crowd input was both meaningful and fast enough to hold a sensible conversation. Finally, we asked two professional psychologists to assess how effectively the crowd was able to provide life coaching to the user. Their assessment indicates that the crowds’ performance was satisfactory. Based on the series of experiments and the psychologists’ feedback, we propose guidelines for enabling RTC to support complex conversational tasks for social robotics.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9567</offset><text>present CoZ, a system designed to crowdsource the teleoperation of a social robot for conversational tasks. CoZ does that by: (a) live streaming the robot’s AV feed to workers, thereby enhancing their contextual and social awareness, (b) managing workers’ asynchronous arrival and departure during the conversational task, (c) supporting workers’ task performance through its UI. Our entire code base is open source and is available on Github ().</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10020</offset><text>evaluate the trade-off between response latency and dialogue quality by systematically varying the number of workers. We release our data set () containing all dialogues between CoZ and the actress to promote further research.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10247</offset><text>provide RTC-specific guidelines for social robots operating in complex life-coaching tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10339</offset><text>Contributions: We contribute to the body of literature a proof of concept demonstration of how RTC can enable a social robot to handle a social conversation. Specifically, we:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>10515</offset><text>2. Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>10531</offset><text>2.1. Remote Teleoperation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10557</offset><text>Remote teleoperation will remain an important subject of research due to limitations in the current levels of autonomy offered by artificial intelligence. Teleoperating a humanoid or social robot can be formally defined as:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10781</offset><text>⋯ the process through which a human directs remote sensors and manipulators using a communication channel subject to latency and bandwidth limitations in such a way that robot behaviors are physically, emotionally, socially, and task appropriate.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11030</offset><text>Several methods have been used to achieve realistic and meaningful conversations with robots. Foremost, the Wizard of Oz (WoZ) method is widely applied in experimental settings to test hypothesis without the need of extensive programming, and has been used in education, and elderly care centers and in public spaces. In applications that specialised knowledge and expertise is needed, previously developed interactive scripts with specific purpose are used and supported by, for instance a physical game, to streamline the interaction. This method can only facilitate a restricted range of conversations, and is not sustainable for longer term interactions. To improve the interactivity, the predefined scenarios can be connected to an interface that allows adding in real-time of new speech utterances if an unexpected situation occurs. In addition, the direction of the interaction, and thus the conversation can be done by an operator. A third group of applications attempt to use natural speech recognition and generation, however these attempts are used with a limited success in real life applications, since they can only achieve a limited understanding of the context of the interaction.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12227</offset><text>In this study, we used limited robot autonomy (Pepper’s animated speech) and crowd workers were allowed to manually control the conversation of the robot through a web interface. Furthermore, we permitted cooperative control where multiple workers were invoked to control the speech of a single robot simultaneously.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12546</offset><text>2.2. Crowdsourcing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12565</offset><text>Crowdsourcing is an emergent approach to collaborative work that provides easy access to the skills and talent of large number of individuals spread geographically, to solve tasks that would otherwise be difficult or costly to solve when using traditional approaches. The typical form of crowdsourcing is known as microtask crowdsourcing, which allows people to globally distribute short and simple tasks that do not require any particular expertise to various online workers (also known as crowd-workers). Workers then solve those human intelligence tasks. Examples of such tasks include: image labeling; writing and editing; protein folding; and generating creative contents, among others.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13257</offset><text>2.3. Real-Time Crowd-Powered Systems</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13294</offset><text>Real-time crowd-powered systems use recruiting, rewarding and user interface techniques to reduce latency of crowd input from hours to few seconds. In traditional crowdsourcing systems, tasks are often completed in batches and take hours or days making the collaboration between requesters and workers essentially asynchronous. In recent years, researchers have developed methods to recruit workers on demand from crowdsourcing platforms and created techniques to reduce latency. Such methods enable real-time crowdsourcing (RTC) and the resulting systems that build on top of the RTC concept are known as real-time crowd-powered systems. VizWiz is one of the earliest real-time crowd-powered system which helps blind users in answering visual questions about their surroundings by sending object’s photos and audio questions from their phones to crowd workers. Chorus is a text-based chatbot that assists end-users with information retrieval tasks by conversing with online synchronous group of workers. To automate conversation, Evorus builds on Chorus by employing both machine learning and human computation to enable a group of crowd-workers to collaborate with chatbots. Although effective, this approach is constrained by the availability of suitable domain specific chatbots and to purely text-based communication. Chorus: view extends VizWiz by providing real time video stream and a recorded audio question to workers in addition to text based chat to further improve the capability of VizWiz in answering general questions to visually-impaired users. InstructableCrowd is a conversational system which allows end users to verbally communicate their problems regarding IF-THEN rules to a synchronous group of workers. In return, crowd workers assist end-users by collectively programming their sensors and actuators on their mobile phones. Finally, CrowdBoard allows collocated ideators to develop ideas on a digital whiteboard with the help of synchronous remote crowd workers sharing the same digital whiteboard. We have highlighted some key differences between CoZ and other related systems from real-time crowd-powered systems domain in Table 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15456</offset><text>2.4. Crowd or Web Robotics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15483</offset><text>RTC has been used in the past as a training and teleoperation method for robotics. For example, Legion allows end-users to crowdsource the real-time synchronous control of the locomotion of a toy robot through keyboard commands, while observing a real-time video feed. Likewise, CrowdDrone replaces a human operator with a cohort of workers to control unmanned aerial vehicles in an unknown environment and supports novel mediation strategies for aggregating commands by crowd-workers. EURECA (Enhanced Understanding of Real Environments via Crowd Assistance) helped robots understand unknown objects from a scene in real-time by leveraging online crowds of human contributors. Robot management system (RMS) is an open-source web-based framework which allows researchers to outsource navigation and manipulation tasks of the PR2 robot to anonymous and diverse users from across multiple demographics. In another similar work, Crick et al. leveraged crowdsourcing to teach the robot through demonstrations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16489</offset><text>The case of controlling socially assisted robots that we examine here has several similarities but also crucial differences with the aforementioned applications. The similarities start and finish in the real-time nature of the task. The crucial differences are that in contrast to a task that has a specific answer or requires choosing among a small set of specific answers (e.g., which direction to move towards), holding a conversation is an open task with many options that unfolds over time. Furthermore, beyond a low latency of responses the robot must exhibit empathy, ask relevant questions and provide meaningful suggestions in the light of what has been said throughout the course of the dialogue exchange. We address all of these differences in our investigation of CoZ in this paper. We have highlighted some key differences between CoZ and other related systems from crowd or web robotics domain in Table 2.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>17409</offset><text>2.5. Social Robotics and Stress</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17441</offset><text>In the past, a great deal of research has been conducted to use socially assistive robots as a therapy to alleviate stress and anxiety related problems among children, older adults, and teens. In a recent study, it was reported that children who interacted with the robot showed an increase in positive mood as compared to two other conditions (where the robot was turned off, or was waiting quietly). More recent review of social robots for the well-being of older adults revealed that social robots can reduce loneliness and stress and can enhance engagement. Furthermore, teens were also found to show strong engagement and expressions of empathy even with a low-fidelity prototype, and also having participated as a designer of a robot that could overcome their stress.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18215</offset><text>2.6. Latency in Crowdsourced Tasks</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18250</offset><text>In addition to task latency, which is domain specific, key to enabling RTC is to reduce the recruitment and arrival latency of workers. There have been various strategies proposed for doing so. For example, quikTurkit reduces worker’s arrival latency by recruiting workers in advance. It posts in batches many more tasks than are actually required to keep tasks at the top of MTurk’s list of tasks for crowd-workers to choose from. While in a waiting pool, workers are kept engaged by performing old tasks posted by the requester. It also posts tasks with slightly different titles and payments to keep tasks at the top of MTurk’s list.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18893</offset><text>Rather than engaging workers with old tasks, an alternative strategy is the Retainer where workers are paid fixed fee for waiting and a small reward (3 cents) for quickly responding to alerts. Nevertheless, waiting in a retainer for a fixed duration and then responding quickly is not appreciated by workers. Furthermore, if payment for waiting is lower than the US minimum hourly wage, workers can perceive it as unethical and unjust.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19329</offset><text>More recently, the Ignition recruitment process proposes a combination of both on-demand recruitment and waiting pool mechanisms to reduce recruitment latency. Ignition recruits more workers than needed, keeping extra workers in a retainer pool. Our recruitment algorithm Pavilion is inspired by Ignition but differs in the way it handles a worker’s turnover conditions during the execution of task (e.g., when workers leave the task either by submitting their work or returning the human intelligence task (HIT)) and in that it hires extra workers parsimoniously only when needed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>19913</offset><text>3. Crowd of Oz System</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19935</offset><text>CoZ conceptually consists of four major components: the Pepper robot, the middleware, the Flask web app, and the crowd interfaces (see Figure 1). Before delving into the details of the system, we present a high level illustration of CoZ. When a user speaks to the robot using the microphone, the speech recognition script running on the middleware detects the user’s voice and converts it to text. At the same time, the middleware sends a signal to the application server to activate the controls on the crowd interface as we allow workers to send a message only after a user says something. When the transcribed text is available, it is also sent through the same channels to the server. On the server side, the Flask app receives the message, displays it on the crowd interface for workers and displays it to the Pepper robot’s tablet for the user.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20790</offset><text>When a crowd-worker wants to reply, instead of typing she can use the speech to text service. After her spoken message is automatically transcribed, she can click the button “Send”. On the server side, the Flask app receives the message(s), selects a message based on the first-response strategy (for this version of CoZ, we implement first-come-first-serve strategy to speak out the first available message on the Pepper robot), displays that message to the Pepper robot’s tablet through middleware and rewards the worker with a bonus. Most importantly, the Flask app also sends a message to the Pepper robot through middleware. It then uses Pepper robot’s text-to-speech (TTS) API to execute animated speech—Pepper’s animated speech includes gestures based on the text input.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21580</offset><text>3.1. Pepper Robot</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21598</offset><text>We used Softbank’s semi-humanoid Pepper robot (). Pepper’s execution environment is a customized version of Linux called Gentoo. The Pepper robot is equipped with a camera, microphone, speakers, and a tablet for providing textual and graphical output to the user. Pepper can exhibit some social behavior, e.g., gestures, head movements, and adjust its head pose to track users.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21980</offset><text>3.2. Middleware</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21996</offset><text>The middleware runs on a Windows PC and acts as a bridge between the application server and Pepper. All audio–video (AV) and text communication are handled by the middleware. The middleware runs two main components: the media manager and the communication adaptor.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>22263</offset><text>3.2.1. Media Manager</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22284</offset><text>The media manager includes audio and video publishers. The audio publisher implements OpenTOK API () and broadcasts audio of a user from the collar microphone. The video manager also implements OpenTOK API and broadcasts video feed from the Pepper’s front camera to the crowd interfaces. For real time AV transmission, we use the WebRTC protocol (). WebRTC is a free, open source technology that enables real-time AV and text communication to modern browsers and mobile apps. In this system, we use OpenTOK APIs which have built-in support for WebRTC technology.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>22849</offset><text>3.2.2. Communication Adaptor</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22878</offset><text>The communication adaptor runs four major programs: Speech recognition, the OOCSI sender/receiver clients (), the Flask based SocketIO client () and a script to display messages on the Pepper’s tablet. The speech recognition module converts speech into text and forwards it to the OOCSI client. OOCSI is a prototyping middleware for designing distributed products and supports connecting multiple heterogeneous client programs through the WebSocket protocol. In this project, we used OOCSI clients to connect the middleware with the Pepper robot through WebSocket for exchanging text messages. The SocketIO client connects the middleware with the application server. It handles all external messages with application server through a WebSocket secure connection.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23643</offset><text>3.3. Flask Web App</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23662</offset><text>We developed the server application using Flask (), a Python-based web framework, while for the client, we used Bootstrap, JQuery, and CSS. We built the application modular using Flask blueprints (). Each blueprint is considered as an independent application serving its own functionality. We have three major blueprints in our application: (1) admin panel; (2) authorization; (3) crowd control. Crowd control is the crux of the application and it includes the necessary logic to handle crowd interfaces and manage crowd workers throughout the task.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>24212</offset><text>3.4. Crowd Interfaces</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24234</offset><text>First, the administrator creates a new project and a new HIT on the Amazon Mechanical Turk (hereafter: MTurk) through the admin control panel. Creating a HIT requires several attributes: title, description, fixed price, qualification requirements, maximum number of workers etc. After that, the HIT is posted to MTurk; this is transparent to the admin as it is done through CoZ’s interface. Once the HIT is posted on MTurk, crowd workers can read the instructions and accept it. After accepting, they are directed to the waiting page (Figure 2) where they remain until sufficient number of workers have been recruited. They can also read the detailed instructions (point B on Figure 2). If the session has already been initiated, workers can see the on-going chat between the user and robot on the waiting page (point C), thereby becoming aware of the context of the conversation. Workers are financially compensated according to average US minimum wage ($7.25/h) while waiting (point A).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25225</offset><text>The main page contains a task ribbon at the top of the page (cf. Figure 3) showing a worker’s task-related information including accumulated bonuses based on their contributions (point A). Furthermore, it displays a chat history (point B) between user (red) and workers (grey). On the left side, we display the real-time view of the robot’s 3D model, an AV feed (point C) and a button for using speech-to-text based on IBM’s Watson API (point D). We encourage workers to use this speech-to-text functionality to reduce latency. To nudge workers for swift responses, we implemented a progress bar (point E). When the user stops speaking, a progress bar sets off for a maximum of 7 s with additional 2 s to determine the pause. This signaled workers to compose and submit their message during these 9 s. Furthermore, they could also see messages from other workers on the chat box interface to avoid asking same questions and to maintain the continuity and coherence of the discussion.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>26215</offset><text>4. Pavilion Algorithm</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26237</offset><text>As mentioned earlier, our open-source (Our implementation of Pavilion and installation details are available at: ) Pavilion algorithm extends the one introduced with Ignition, with new features to handle turnover conditions (i.e., returning the task) both in the waiting and active queue. Pavilion initially hires a fixed number of workers (based on the total number of active and waiting workers required) and then prudently hires more workers if needed. Hence, the total number of workers that we need to hire from MTurk initially are equal to:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26784</offset><text> denotes the maximum number of active workers who can work on the actual task simultaneously.  denotes the maximum number of target workers who can wait in the waiting queue simultaneously. Similarly,  represents the minimum number of workers allowed in the waiting while  represents the minimum number of active workers required to initiate the dialogue. In the experiment we report below, we wanted to initiate the conversation when  workers were hired, so we set  = . Nevertheless, this condition can be changed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27300</offset><text>The total number of workers in the waiting queue () will always be greater than the total number of workers in the active queue () by 1:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27437</offset><text>Our rationale behind choosing one extra worker for the waiting queue was to keep at least one worker in the waiting queue when the dialogue was initiated. Thus, the minimum number of waiting workers () is always equal to 1. Furthermore, we aim to reduce the chance that the waiting queue will be empty.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>27740</offset><text>4.1. How It Works</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27758</offset><text>Initially, Pavilion keeps adding workers in the waiting queue until the current count of waiting workers becomes equal to . At that point, when a new worker joins the session, all previous workers who were waiting are transferred to the active queue to initiate the conversation (except the one who has most recently joined the session who stays in the waiting queue). For the task to start we expect at least one worker to be in the waiting queue .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28208</offset><text>On the background, Pavilion continues adding workers in the waiting queue during the execution of the task until the maximum condition for the waiting queue is reached: .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28379</offset><text>When a worker leaves the active queue (from the actual conversation task) either by submitting the task or returning it. (There is a difference between submitting and returning a HIT. In submitting, a worker leaves the task by actually submitting the HIT to MTurk for reviewing and rewarding. While in returning, a worker leaves the task but is not interested in the monetary reward and the returned HIT is available for other workers on MTurk.), Pavilion immediately pushes one worker from the waiting queue to the active queue to keep the target number of workers fixed. If the task is actually submitted by the worker, then Pavilion also posts one extra HIT (with one assignment) to fulfil the deficiency in the waiting queue.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29109</offset><text>When a worker leaves the waiting queue by submitting the task, Pavilion hires a new worker. Nevertheless, when a worker leaves the waiting queue by returning the HIT, Pavilion does nothing because the returned HIT is immediately available to new workers on MTurk.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29373</offset><text>In the worst case, when the waiting queue only contains one worker or none and a worker leaves from the active queue, then Pavilion waits until another worker(s) joins the session and then it moves a worker(s) from the waiting to the active queue until the following condition is false:  where  and  represents the current number of waiting and active workers respectively.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>29747</offset><text>4.2. Differences between Pavilion and Ignition</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29794</offset><text>Pavilion is designed for supporting short-term tasks that require a specified number of workers throughout a session. Ignition on the other hand is intended for long-term jobs and can initiate the dialogue as long as there is one worker in the session.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30047</offset><text>Ignition only manages the active queue when the total number of active workers falls below a minimum threshold, while Pavilion manages both the waiting and active queues and posts new HITs during the task to fulfil the deficiency of workers in both queues.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30304</offset><text>Ignition does not allow workers to leave the task throughout the session; workers have to commit a lot of time and wait until Ignition auto-submits their HITs. In contrast, Pavilion adopts a more flexible approach by letting workers leave the task by actually submitting their HITs both from the waiting and the active queues when they do not want to continue, e.g., due to fatigue or boredom or because they are satisfied with the bonus they earned. We also aimed to safeguard for the possibility that workers would get upset or anxious from the discussion, for which an easy exit with compensation was deemed necessary and appropriate.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>30942</offset><text>5. Materials and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30967</offset><text>We conducted experiments to investigate how varying the number of workers () affects the quality of conversation and the real-time performance of CoZ. Workers were asked to type-in what the robot would say in a conversation with a student suffering from study-related stress. Our focus was on the effectiveness of the coaching and the human computation aspects of CoZ, so the student was enacted by a professional actress (Figure 4). We opted for this approach for methodological, ethical, and practical reasons. First, given that this is the very first study to investigate leveraging crowdsourcing to tackle psychological distress, our focus was the performance of the crowd in relation to the system we build (CoZ). Therefore, we wanted to have as similar as possible interactions from the user’s point of view but at the same time engage the crowd workers in a realistic interaction. Second, we took care to recruit an actress that specialize in improvisational theatre precisely because we wanted to offer crowd workers a realistic interaction. Having our actress as a single user also offered us her perspective in terms of her experience of the different conditions of our study, when we debriefed her. Thus, the above two reasons address the methodological dimension of our choice.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32259</offset><text>Third, given that the topic is a sensitive one and given that we did not precisely know how the crowd will perform, we did not want to put in an even more difficult situation actually stressed students. In other words, we were very careful in not potentially causing distress or harming any participant in our study. Thus, this was the ethical dimension of our choice.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32628</offset><text>Finally, there is also a practical dimension of our choice, namely efficiently executing the study since having the actress as a single user could allow us to execute several sessions throughout a day.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>32830</offset><text>5.1. Task for Crowd Workers</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32858</offset><text>On MTurk, we described the task as:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32894</offset><text>You are asked to act as a teleoperator of a robot and chat with a university master student who is experiencing stress due to study burden and not being able to keep work-life balance. Your task is to empathize with the student through conversing and try to find out why the student is stressed by asking open questions. Only after having a good understanding of the context and only when you have asked several open questions think about politely suggesting solutions to student to get out of this stressful situation.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33414</offset><text>Furthermore, crowd workers did not receive any formal training about cognitive therapy, but they were given detailed instructions about the nature of the task and how to use the user interface accurately.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>33619</offset><text>5.2. Participants</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33637</offset><text>We recruited workers on MTurk and restricted the study to US workers, but had one session with Indian workers, with over 98% approval rating and 1K HITs approved. We made sure to compensate each worker with at least the average US hourly wage ($7.25). Specifically, we had a $1.0 fixed reward, plus the following bonuses: $7.25/h while waiting in the queue; 2 cents for providing a response; and an extra 3 cents if their response was selected by our algorithm. We recruited a total of 245 workers. Of these, 102 workers stayed till the end of the task, while the remaining workers left during the course of task completion. The total cost of the study was $335.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>34300</offset><text>5.3. Procedure</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34315</offset><text>We had four conditions for recruiting workers to input the utterances spoken by our robot: (C1) single worker; (C2) a pair of workers; (C3) a group of four; (C4) a group of eight. Our reason for using this geometric sequence (doubling the number of workers in different conditions) was that in the absence of prior knowledge as to the number of workers needed, we wanted to ensure sufficient variation between the number of workers recruited in the different conditions. We randomized all conditions and each condition was executed five times (sessions) with our actress. This results in total of 20 interactions of our actress with our CoZ-enabled robot. We conducted the sessions across two different days and at different times of the day to ensure that unique workers contributed in each session, and that differences due to tiredness or time would balance out across conditions. The average dialogue duration was 11.27 min (SD = 1.74). Before initiating the experiments, we discussed the study’s purpose and the role that was needed to be played with the actress in detail. We also informed her about the live AV broadcast to the anonymous workers. The actress did not prepare or follow a predefined script and she was allowed to control the flow of discussion based on the crowd workers’ responses.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>35624</offset><text>5.4. Measures</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35638</offset><text>(I) Response latency: the time (in seconds) between our actress’ utterance and the first response received from the crowd for each dialogue in all conditions. (II) Dialogue quality: we used appropriateness as a measure for dialogue quality. Appropriateness is a measure that computes a score at each exchange level or dialogue turn rather than for the entire dialogue. Each dialogue turn is given a single label (e.g., appropriate/inappropriate) which translates to a numeric value. We coded each dialogue turn (robot utterance) as “appropriate” when the robot responds appropriately to an utterance or request (with score: +2). On the other hand, we coded each dialogue turn as “inappropriate” which was off-topic, irrelevant, or confusing (with score: ). Finally, we penalize those responses when the workers fail to understand the actress’ utterance and requests to repeat it. We did this because workers can ’listen’ to the user well and can also ’see’ the text-based response on their chat window screen unlike AI which could fail for many reasons (e.g., due to noise in the background). Hence, it was unlikely that they misunderstood the user except in cases when they could not respond for unforeseen reasons. Therefore, we treated this as a ’separate’ measure to penalize the dialogue (with score: ) when this occurred. (III) Assessing the quality of robot utterances through linguistic inquiry and word count (LIWC): LIWC is a simple text analysis program that reads the given text and counts the percentage of words that reflect emotionality, social relationships, thinking style etc.. Currently, LIWC searches for words across more than 80 built-in categories. We choose the following variables: emotionality (positive (posemo) and negative emotions (negemo)), word count (WC), words per sentences (WPSs), and difficult words (words comprise of more than six letters (Sixltr)). (IV) Average waiting time for eliciting multiple responses: The waiting time before a sufficient number of responses to choose from has been received. (V) Effect of progress bar and speech-to-text (STT) on latency: Since we implemented a progress bar in the crowd interface (with a maximum elapsing time of 9 s) and the STT service, we were interested in understanding the effect of these on the overall response latency. (VI) Experts’ evaluation and professional feedback: We consulted two professional clinical psychologists, who we recruited from Fiverr (), and were experts in depression and anxiety related disorders. The purpose was to evaluate workers’ overall life coaching skills through the eyes of specialists. In this way, we can ensure the validity of ratings received from therapists who were well-versed with cognitive behavioural therapy (CBT) and motivational interviewing (MI) techniques. We also want to assess the qualitative aspects of life coaching delivered by crowd workers. For instance, what were the strengths and weaknesses of responses provided by workers? Did the workers unwittingly follow some good medical practices based on CBT or MI? Finally, based on the qualitative feedback, we want to establish some guidelines for training workers in future for these therapeutic tasks. The psychologists rated the quality of the dialogues on a seven-point Likert scale (1: highly unprofessional to 7: highly professional). Furthermore, they also provided detailed feedback on four randomly selected dialogues—one from each condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>39115</offset><text>6. Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>39126</offset><text>6.1. Summary of Crowd Responses</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39158</offset><text>As we mentioned before, the actress did not use any script or predefined contents to control the flow of the dialogue, though by reviewing the dialogues, we noticed some repetitively occurring problems that actress mentioned. In Table 3, we have summarized those problems or situations and corresponding crowd responses. In addition to crowd workers’ empathetic responses to actress’ worries, they provided some practical solutions to resolve them. Table 4 shows the summary of solutions provided by crowd workers.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>39677</offset><text>6.2. Response Latency</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39699</offset><text>A one-way analysis of variance (ANOVA) showed a significant difference in mean latency scores between the four conditions; , . Post-hoc Bonferroni comparisons indicated that this difference in latency was significant () between the one- and eight-worker conditions (cf. Figure 5 and Table 5). We did not find any difference between the other conditions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40053</offset><text>Since we observed a linear drop in the latency in relation to the number of workers (Figure 5), we investigated whether the number of workers can predict the latency. A linear regression model confirmed that the number of workers in the active queue were able to predict the variance in response latency scores. The linear regression model explained 39.3% (medium) of the overall variance in the mean latency scores ( = 0.393), which was found to significantly predict the outcome, , , effect size (d). The linear equation was:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40581</offset><text>Though we experience a reduction in latency of about 0.6 (0.589) seconds per new worker added, we expect that latency should level out after a certain number of workers have been added. Hence, investing more money would be superfluous beyond such a limit. Future studies should determine this limit.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>40881</offset><text>6.3. Dialogue Quality</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40903</offset><text>We evaluated response appropriateness as a measure of the dialogue’s quality. Since each dialogue had a different length, we normalized the appropriateness scores by length (dividing quality scores by the dialogue’s length). We also ran a one-way ANOVA analysis to test for differences in the mean scores between all four conditions (Cf. Figure 6 and Table 5). We found no significant difference in quality scores between all four conditions: F(3,16) = 0.50, p = 0.687. Hence, we found that having more workers in queue does not significantly improve the dialogue’s quality.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>41484</offset><text>6.4. Cost</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41494</offset><text>The average cost of experiments was $4.93 (SD = 1.76) for one-worker, $6.60 (SD = 1.62) for two-worker, $8.59 (SD = 1.82) for four-worker, and $18.0 (SD = 4.02) for eight-worker conditions. We also conducted one-way ANOVA, results indicated that cost differed significantly: F(3,16) = 27.20, p &lt; 0.0005. Post hoc Bonferroni comparisons indicated that this difference in cost was prominent between one- and eight-worker (p &lt; 0.0005), two- and eight-worker (p &lt; 0.0005) and four- and eight-worker (p &lt; 0.0005) conditions. There was no difference between one, two, and four-worker conditions. The linear regression model explained 81.4% of the overall variance in the cost ( = 0.814) based on the number of workers in the active queue which was significant (F(1,18) = 78.9, p &lt; 0.0005, effect size (d) = 4.38). The linear equation was:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>42327</offset><text>6.5. Assessing the Quality of Robot Utterances Through Liwc</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42387</offset><text>One-way analysis of variance (ANOVA) was conducted to find the difference in mean scores for five chosen variables across four conditions. We did not find any difference in the mean scores for WC (F(3,16) = 0.345, p = 0.793), WPS (F(3,16) = 0.524, p = 0.672), Sixltr (F(3,16) = 0.593, p = 0.629), posemo (F(3,16) = 1.032, p = 0.405) and negemo (F(3,16) = 0.496, p = 0.690)(cf. Table 6 and Figure 7).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42787</offset><text>Across all conditions, it was revealed that the choice of words used by crowd workers while talking to an actress resulted in overall positive emotions. Figure 8 shows differences in scores between positive and negative emotions across all conditions. Using an independent t-test, we confirmed that this difference was significant between positive and negative emotions for one-worker (t(8) = 5.13, p = 0.001), two-worker (t(5.14) = 2.87, p = 0.034), four-worker (t(4.44) = 2.25, p = 0.05) and eight-worker (t(8) = 3.05, p = 0.016) conditions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>43331</offset><text>6.6. Average Waiting Time for Eliciting Multiple Responses</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43390</offset><text>Future work can improve CoZ by combining crowd input with AI-generated input in a hybrid fashion. As there are multiple workers simultaneously controlling the speech of the robot, it is possible that more than one worker responds to the same user query. Therefore, an important question arises: How long would an algorithm need to wait before it receives enough responses to choose the best one from? To answer this question, we gathered all dialogue exchanges from the 2, 4 and 8-worker conditions. Here, corresponding to each user query we received more than one response on average from crowd workers in each of the conditions (cf. Table 7, column 3). Then we consider the time difference between the last robot utterance and the first robot utterance for the same user query. Finally, we calculated the mean scores for each condition (Table 7). To find the difference in mean scores between all three conditions, we conducted a one-way analysis of variance (ANOVA). Results indicated that there was no difference in mean scores: F(2,12) = 0.720, p = 0.507. Based on these findings, we conclude that the maximum time an algorithm should wait is around 6 s. During this duration, we can expect more than one crowd response.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>44616</offset><text>6.7. Effect of Progress Bar and STT on Response Latency</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44672</offset><text>To check whether a message was sent by a worker within the 9 s duration (maximum elapsing time of progress bar), we counted all such instances from our corpus (Table 8). A  analysis (4 conditions x 2 (&lt;9 s or above)) indicated that increasing the number of workers who engage with the conversational task would possibly increase the chance of messages to be composed and sent in less than 9 s,  (3) = 26.65, p &lt; 0.0005.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45092</offset><text>To check whether workers used the speech to text service while composing a response, we logged this information. There were three response cases for a worker: using speech to text (STT), simply typing a response (TYPED) or using both (MIXED), e.g., in case of STT failures, workers could edit the response. Out of 551 workers’ utterances, only 16 responses (2.9%) used speech to text, 13 used (2.36%) mixed while 522 (94.74%) responses were typed. This was surprising, since we had advised workers to use the STT in our instructions with an aim to yield a shorter response latency.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>45676</offset><text>6.8. Experts’ Evaluation and Detailed Feedback</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45725</offset><text>We hired two clinical psychologists to rate the quality of the dialogues on a seven-point Likert scale, where a score of 7 would indicate that the dialogue that transpired was highly professional. We found a strong Cronbach’s alpha value ( = 0.82) among our two raters. We also computed the Spearman’s rank–order correlation to determine the relationship between two ratings. We found a strong, positive correlation which was statistically significant ((20) = 0.723, p &lt; 0.001).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46210</offset><text>Next, we took the mean score of the two ratings given by our two experts and executed an ANOVA test to investigate differences in mean scores between the four conditions. We did not find any differences in the mean scores (F(3,16) = 0.123, p = 0.945). The average score was 3.9 (SD = 2.07) for one-worker, 3.7 (SD = 1.95) for two-worker, 3.3 (SD = 0.75) for four-worker, and 3.4 (SD = 1.91) for the eight-worker condition. The maximum score given by the first rater for each condition was 5, while the maximum score given by the second rater was 7 for one and two-worker condition, 4 for the four-worker condition and 6 for the eight-worker condition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46862</offset><text>Given the fact that workers were not trained in clinical psychology and did not have prior experience in mitigating stress, these results are satisfactory and quite promising in terms of potential. We believe that with some training and implementing more sophisticated workflows, these results can be improved.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>47173</offset><text>6.9. Qualitative Feedback from Psychologists</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>47218</offset><text>The psychologists provided the following comments regarding the performance of CoZ in powering this robot-human dialogue. We asked the professional psychologists to go through the whole dialogue and they chose the exemplary responses that we present below.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>47475</offset><text>6.9.1. Building upon User Strengths</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>47511</offset><text>CoZ tried to explore the user strengths and then built upon those to provide advice against stress: User: “I love cooking. Mostly I like cooking for other people. I don’t know many people yet”. Robot: “What is the last meal you cooked?” Asking her what the last meal she cooked reminds her that there is more to life than just academic studies and the stress that develops when studying. CoZ could build upon this ‘strength’ of the user (i.e., cooking) later on during the session by helping her to acknowledge her strengths. It is important, especially among people experiencing stress, to help them remember their own strengths. Especially when academics get difficult, people often internalize their struggle and begin to believe they are failures and generalize this belief to several areas of their life.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>48335</offset><text>6.9.2. Assessing Coping Skills</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48366</offset><text>CoZ asked what the user has tried to do to overcome his anxiety and stress. An indicative excerpt: “What have you tried till now to overcome it?” In this way it assessed the user’s coping skills. In another indicative example, CoZ asked what does she like to do when not studying (e.g., “What do you like to do when not working on your research?”). This is also a desirable question during a coaching session to get the user thinking and talking about coping skills to manage stress. It is better to focus on one or two coping skills and make sure the user has a good grasp on how to apply it in their lives.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>48985</offset><text>6.9.3. Let the User Express Herself</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>49021</offset><text>CoZ let the user talk more and express herself before jumping to solutions. For instance, CoZ asked her questions about her experience in a new city (e.g., “Where did you move from? How long have you been in your new city?”). This is a great way to begin a session that allows the user a chance to express herself, her feelings, and her present causes of stress. It might seem like it will take too long and extend the session, but in reality, it is a time saver because allowing her to talk more and having CoZ reflect back and show empathy will help the user reveal the real root cause of her stress.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>49628</offset><text>6.9.4. Recalling Positive Things about Life</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>49672</offset><text>When it comes to people dealing with stress and anxiety, it is important to help them recall the factors in their lives that are positive and that they enjoy because often-times, stress and anxiety lead to and/or are fueled by negative/dysfunctional thoughts. For instance, CoZ asked: “What activity gives you the most joy in your life?”). Here CoZ is trying to guide the user out of that mindset and helping the user consider positive things about her life.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>50135</offset><text>6.10. Suggestions for Improvements</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>50170</offset><text>Beyond the positive aspect of the crowd’s input, the professional psychologists had suggestions for further improving the quality of the coaching. We synthesized their suggestions and briefly report the most important ones.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>50396</offset><text>6.10.1. Coz Should Introduce Itself as a Coach</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>50443</offset><text>According to the feedback of the professional psychologists, a coach is neither a therapist or counselor nor a friend, but something in between like a ‘life guide’ or ‘mentor’ for the user. The psychologists suggested that it would be a good idea for the coach (here CoZ) to introduce itself as a life coach and (if possible) greet the user by her name. For example, saying something like this: “Hi John, I’m Pepper, your life coach. I’m ready to talk with you. What brings you to life coaching today?” According to the psychologists this would sound more professional and provide the user with confidence that she is doing more than just chatting with someone online.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>51128</offset><text>6.10.2. Reflect Back and Validate</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>51162</offset><text>Another suggestion was that it is important for a coach to reflect back and validate what the user is saying. For instance, instead of CoZ saying, “What are you stressed about?” it would be better here for CoZ to reflect back what the user is saying. For example, CoZ could say, “It sounds like your stress has gotten to the point that it’s starting to affect your life, even your studies. What in particular are you stressed about?”. Here, CoZ would show that not only is he listening to the user and is concerned about her, but is also validating the user’s stress. This will increase the user’s comfort in the session and help her to continue to open up.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>51834</offset><text>6.10.3. Avoid Unnecessary Small Talk</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>51871</offset><text>A final suggestion was that a coach should avoid unnecessary small talk. For instance, CoZ asked the user her age and whether she has a partner and the user immediately called her out when she stated, “ah personal question.” It is not appropriate for a life coach to ask such a question, especially asking whether she has ever had a partner in her life. Furthermore, during a session with Indian workers, CoZ mentioned prayer and went on to say to the user that God is always with her so she should “be happy”. Due its personal nature this feedback would not be appropriate for life coaching. Also, when mentioning God or religion, CoZ should first wait for the user to bring up this topic because CoZ does not know the user’s spiritual beliefs.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>52627</offset><text>6.11. Discontinuities/Non-Cohesiveness in the Crowd Generated Conversations</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>52703</offset><text>In most of the dialogue exchanges, crowd workers maintained coherence and continuity in the conversation, nonetheless there were some cases where conversation diverged from the one topic to the new topic prematurely. There were various reasons that we will explore in this analysis. The purpose of presenting these findings is to show that such discontinuities are expected in real time crowd-powered dialogue systems and proper mechanisms should be devised to resolve them.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>53178</offset><text>In order to find incoherence in conversations, we established some criteria after reviewing conversations in all four conditions. This includes abruptly switching a topic due to lack of experience, arrival of new workers, unnecessary small talk, discussion about the task, technical difficulties, and presence of spam workers who deviated the discussion. Based on this criteria, we counted all such instances from the dialogue. Table 9 shows total and mean discontinuities across all conditions.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>53674</offset><text>6.11.1. Switching Topics Prematurely</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>53711</offset><text>There were some dialogue exchanges where workers made an abrupt and odd transition without concluding the ongoing topic. One possible reason for deviating the conversation in an unnecessary way is the lack of experience of workers for this therapeutic task. An indicative excerpt of this form of disjunction is shown below. Here workers first propose short activities to relieve stress and then abruptly jumped to herbal remedies.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54142</offset><text>Robot: Doing short activities to get your mind off of your studies might help, and then you can come back with a clearer mind.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54269</offset><text>User: yeah, I can see how that’s work. What do you think of when you say short activities like what maybe?</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54378</offset><text>Robot: Perhaps trying some herbal remedies</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54421</offset><text>Workers can be trained to collaborate with fellow workers to communicate consistent messages unless a user deliberately switches to a new topic.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>54566</offset><text>6.11.2. Newly Joined Workers</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54595</offset><text>Occasionally, when a new worker joined the session, he/she initiated the discussion with either greeting (’hello’, ’how are you?’) or requested a user to repeat her last utterance. This resulted in an odd transition and our actress had to restate her worries again. We observed this form of disjunction in the 4 and 8-worker conditions where a greater number of workers left and joined the session more frequently. This was surprising since we had shown the ongoing conversation to workers who were in the waiting state and they were well aware that they had to pick up the ongoing discussion. Furthermore, after they were redirected to the main conversational task, they could also see messages from other workers. In future work, simple automated methods can be employed to filter out such repeat requests or greetings.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>55425</offset><text>6.11.3. Small Talk</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>55444</offset><text>At times, workers engaged with the actress in unnecessary small talk which resulted in non-cohesive dialogues. The most common pattern that we observed was personal questions about the actress’ private life (’what is your age?’, or ’do you have any partner in your life?’). Workers can be easily trained to avoid asking personal questions to the user.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>55806</offset><text>6.11.4. Side Chatter</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>55827</offset><text>Sometimes, workers wrote their confusions about the task and instructions directly into the chat interface which resulted in an odd transition. Sample quotes of this form of disjunction are shown below:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56030</offset><text>Robot: Do you allow repeats? I feel like we didn’t finish our conversation properly.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56117</offset><text>Robot: where are the instructions?</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>56152</offset><text>6.11.5. Technical Problems</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56179</offset><text>Another form of disjunction occurred due to some technical issues experienced by workers during the task and as a result, they wrote their responses as complaints directly into the chat interface.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56376</offset><text>Robot: I have not heard voice if I can hear I will help to you.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56440</offset><text>Robot: video is OK but not have a sound</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>56480</offset><text>6.11.6. Spam Worker</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56500</offset><text>We had two spam workers that included out of context, worthless and rude responses. But when this happened, other trustworthy and serious workers took over and handled the conversation. Examples include:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56704</offset><text>Robot: who cares about your studies?</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56741</offset><text>Another honest worker handled this situation by reflecting and validating the actress’ problem as follows:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56850</offset><text>Robot: You’re having trouble studying, but you can’t stop using your phone? I understand.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>56944</offset><text>User: well I should have started with saying that I did get rid of my phone.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>57021</offset><text>6.12. Performance of Pavilion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>57051</offset><text>High departure rate: Figure 9 shows that more workers left during the task execution as compared to those who stayed till the end of the task. Since we were not sure about the exact length of a dialogue, we auto submitted all HITs from workers who stayed till the end of the task through admin control (workers were informed). We found a strong positive correlation between HITs that were submitted or returned during task execution and the ones that were submitted automatically at the end of the task (r = 0.98, p = 0.01). This further confirms the volatile nature of workers on MTurk and emphasizes how mechanisms for handling this asynchronous turnover of workers during a session is a key feature for enabling RTC.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>57771</offset><text>Worker retention: Pavilion was able to retain the required number of workers in the active queue during the task execution (active workers at the start of session and at the end were the same) despite the high turnover rate of workers. When a worker left the active queue, Pavilion moved workers from the waiting to the active queue instantly or waited until a sufficient number of workers were available. We calculated the average time that Pavilion took to regain the required number of workers in the active queue when workers left during the task execution. The average was 0.07 s (SD = 0.04) for one-worker, 55 s (SD = 85.8) for two workers, 49.8 s (SD = 69.9) for four workers and 29.2 s (SD = 68.1) for eight workers. The reason it took longer for Pavilion to refill the active queue when more than one worker was needed was due to some workers leaving the task at the start of conversation and not having enough workers in the waiting queue to migrate straight away.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>58746</offset><text>Recall requests and new HITs: In our experiments, a total of 123 recall requests were made to move workers from the waiting to the active queue for all conditions. Out of 123 recall requests, 47 were directly associated with reinstating the required number of workers in the active queue. To fulfill the deficiency of workers in waiting queue, Pavilion posted 24 new HITs during the study.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>59136</offset><text>Active workers: Other than the first condition where exactly one worker was responsible for controlling the dialogue, we computed the average number of active workers in three conditions. We considered a worker as active if he/she contributed at least one message during the dialogue. We found that for one and two-worker conditions, the average number of active workers remained 1 and 2 respectively throughout the session. However, for four and eight-worker condition, the average was 3.2 (SD = 1.64) and 6.8 (SD = 2.39) respectively. This shows that apart from sustaining the target number of workers in the active queue, mechanisms are required to regularly prompt the idle workers or replace them with other workers.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>59858</offset><text>7. Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59872</offset><text>Our results indicate that increasing the number of workers in the queue did not improve the quality of the dialogue, but it did shorten the response latency. A linear regression analysis confirmed that adding a new worker in the queue can decrease the response latency by 0.6 s but at the expense of extra $1.87. Cost and latency remain inversely proportional to each other when we hire more workers while quality remains constant. Future research should explore the limits of this finding: namely the number of workers after which we observe diminishing returns for the extra investment. Our stringent qualification requirements on MTurk (98% approval rate and 1000 HITs submitted), can explain the results regarding the dialogue’s good quality. Costs can further be reduced by paying a fixed bonus for waiting tasks. For example, in a survey, a crowd worker suggested  for 10-min waiting time. In the imminent future, we will explore the impact of answer aggregation approaches on quality-related outcomes of CoZ.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>60890</offset><text>7.1. Guidelines for Enabling Social Robotics to Handle Stress Management via RTC</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>60971</offset><text>7.1.1. Handling Quality</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>60995</offset><text>Allow the user to express his/her stressors/concerns without interruption (at least initially without interrupting or changing the topic).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61134</offset><text>Avoid simplistic answers/solutions that are also too general or not realistically applicable for the user (e.g., “just try to think positive” or “maybe you can just move to another apartment”).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61336</offset><text>Use empathy, understanding, and reflection to let the user know the robot is listening and cares about the users concerns.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61459</offset><text>Respond on topic and in line with the users expressed concerns/stressors.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61533</offset><text>Avoid jumping from topic to topic rather than maintaining an organization and structure to the session</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61636</offset><text>Assess the coping skills that the user is currently employing/applying to try to manage stressors. This avoids giving advice about coping skills that the user may have already attempted.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61823</offset><text>(a) Training workers: The quality of the conversation can potentially be improved by training the crowd workers before they engage in the conversation. More specifically, for the case of managing stress that we studied in our experiment the psychologists suggested that the crowd workers could be advised to:</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>62132</offset><text>We are currently exploring how can we structure the instructions given to workers based on Motivational interviewing (MI). MI is very useful tool which could enhance the performance of non-experts from a crowdsourcing platform to provide reasonable coaching to people who are stressed. MI is defined as: “a collaborative conversation style for strengthening a person’s own motivation and commitment to change”. It includes four processes: engaging, focusing, evoking and planning. In addition to that, we can add reflective listening based on the psychologists’ feedback. Reflective listening is very useful tool to understand the inner struggle and commiserate with the stressed person.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>62828</offset><text>(b) Automated response selection: An important measure to improve the quality of responses is to elicit multiple responses from crowd workers in a short span of time and then to use an automatic method to select the best response. In our study the average time it took to elicit multiple responses was about 6 s. One way to ensure quality is to select the best response provided by the crowd within at least such a time span. Nevertheless, this is a complex NLP problem which requires further investigation. For instance, given more than one response to a single user utterance, one can select the one that is most appropriate.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>63456</offset><text>(c) Structuring the dialogue: Cross-checking the responses generated by the crowd, we found that workers in the four- and eight- workers conditions (who joined the session later) said ‘hello’ in the middle of a conversation which resulted in re-starting the conversation. The system could support workers in structuring the dialogue by providing some suggestions of the conversation structure and context, e.g., whether currently the dialogue is about greeting, exploring (asking questions) suggesting and proposing solutions, or closing. Also, workers could be made explicitly aware of the robot’s attributes (e.g., its name) and its environment (e.g., weather, time, location) to avoid situations such as (indicative excerpt): Robot: Yeah, it is time for breakfast—User: Time for breakfast? Well it’s one o’clock in the afternoon here.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>64306</offset><text>7.1.2. Handling Latency:</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>64331</offset><text>There is an important question that remains: How could the system handle a failure of the crowd to produce a response within an acceptable delay? The most effective approach for reducing the latency involves combining AI with human computation techniques. In social robotics, it is known that if a robot takes more than two seconds to respond then it can induce frustration in users. One solution that has been developed to support artificial conversational agents is to ‘buy time’ with conversational fillers or acknowledgement tokens. These conversational fillers can be simple (e.g., “well” and “uh”) or complex (e.g., “that is an interesting question, let me think about it”). This way, the crowd would still be handling the essence of the dialogue while CoZ would aim to yield a more positive user experience and a better perception of system latency.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>65204</offset><text>Another way to reduce the latency can be to support workers with auto-completion; when a worker starts typing a message, CoZ could propose a list of suitable replies derived from predefined messages from a corpus of dialogues. The role of workers would then be to select the responses that they think are more appropriate or modify them to generate new ones. Given the fact that workers preferred to compose messages by typing instead of using speech to text, such interventions would allow them to swiftly compose and send messages.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>65738</offset><text>7.1.3. Handling Privacy</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>65762</offset><text>If one would want to deploy a social robot operated by anonymous crowd-workers in a realistic scenario, then one needs to address privacy issues. A prior study that favors our crowd-based approach showed that participants felt more trust for the robot which was controlled by multiple teleoperators than a robot controlled by a single operator. One possible reason could be that users interacting with the robot might treat an anonymous crowd as less threatening than teleoperators who are collocated and known to experimenters. To protect the privacy of end-users, different filtering techniques can be used. For example, color-skewed super-pixel filter was considered acceptable for preserving privacy for end-users while obscured view filtering method was considered good for recognizing objects comfortably for teleoperators. Furthermore, we recommend that end-users should be given full control to switch between a privacy-preserving and a “normal” mode. For example, when teleoperators need fine-grained information to execute a task, they can request users, by using Pepper’s tablet, to grant them permission to temporarily suspend privacy veils.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>66922</offset><text>7.2. Potential for Re-Purposing</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>66954</offset><text>From a technical perspective, developers can integrate CoZ by other robots than Pepper. For example, no changes in the implementation are needed to use CoZ for the NAO robot (). Since we have chosen a modular approach to develop CoZ, one can replace an OOCSI script which communicates with the Pepper robot to any other robot by writing a script using relevant API commands. No other changes are needed in the rest of the application.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>67389</offset><text>CoZ can enable researchers to run experiments to collect conversational dialogue data that can be used to train a fully automated dialogue management system in different contexts (e.g., chitchat, mental health). This method is quite affordable and scalable than traditional Wizard of Oz experiments.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>67689</offset><text>8. Future Work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>67704</offset><text>For our next steps, we are planning to replace the actress and see how this crowdsourcing technique (with coaching and instructions for workers) works in the wild. We have planned two studies after training workers based on the guidelines presented in Section 7.1.1 and employing conversational fillers to buy some time when workers are busy in typing their responses.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>68073</offset><text>First, we plan to deploy our CoZ powered robot to our department for students who are feeling stress due to study burden and problems with their private life. Since university students are highly vulnerable population, there is also an opportunity to investigate about the effect of media (audio and video) on the quality of conversations, engagement (time spent with the robot), disclosure of sensitive information and privacy aspects. For this purpose, we plan to conduct a between-subjects user study, wherein the robot broadcasts both audio and video to crowd workers in one condition, as opposed to broadcasting only the participants’ audio cues in the other condition. If the study shows no significant difference between quality of conversations, engagement and privacy aspects, then perhaps we do not need to broadcast video of the users to the anonymous workers.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>68947</offset><text>Second, we plan to deploy CoZ in an elderly care house to act as a companion for older adults to treat their loneliness and stress. A prior study shows that older adults felt more concerned about their privacy-enhancing behaviours when they were monitored by a camera (67%) as compared to an anthropomorphic robot (17%). Hence, this study will help us to focus more on the quality of interaction and crowd-based life-coaching than addressing privacy issues.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>69405</offset><text>To further reduce the risks in these interactions, especially for a vulnerable population, we will include the supervision of trained staff to carefully evaluate the technique and intervene whenever necessary to make sure that there is no risk in these interactions. We can also train workers to avoid requesting personal information and we can employ automated methods to filter out queries containing sensitive information. In addition, computer vision techniques such as silhouetting or avatars and altering the voice of the person using audio modulation techniques can be used to hide the identity of the person talking to the robot.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>70043</offset><text>9. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>70058</offset><text>We introduce Crowd-of-Oz (CoZ)—a system designed to enable real time crowdsourcing for teleoperating a social robot in complex social conversation. CoZ extends earlier works in crowdsourcing and human–robot interaction where the response time of a couple of seconds is desirable, in situations where crowd-workers have to apply human intelligence skills exceeding the current state of the art in AI. In this study, we leveraged social communication skills of crowd workers in MTurk to provide meaningful answers that take into account the rich contextual information for coaching someone with stress. Our experimental evaluation of CoZ systematically examined how the number of workers affects the quality, cost and latency of CoZ. Our results indicate that increasing the number of workers did not improve the quality of the dialogue, but did shorten the response latency at the expense of cost. Regarding the effectiveness of dialogues, the feedback from professional clinical psychologists shows that crowd workers recruited in real-time provided a satisfactory coaching performance. Opportunities to further improve CoZ’s performance include: creating a hybrid system combining both AI and crowd wisdom to (a) reduce latency by employing conversational fillers and auto-completion features; (b) improve dialogue quality with new workflows and UIs to improve the structure of the dialogue and enriching context awareness of workers; (c) creating new workflows for training workers as per recommendations received from professional psychologists.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>71613</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>71634</offset><text>Conceptualization, P.M.; methodology, P.M. and V.-J.K.; software, T.A.; formal analysis, T.A. and V.-J.K.; investigation, T.A.; writing—original draft preparation, T.A.; supervision, V.-J.K. and P.M.; writing—review and editing, U.G., V.-J.K., E.B. and P.M. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>71973</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>71981</offset><text>This work was supported by Higher Education Commission of Pakistan (HEC) in cooperation with Mirpur University of Science &amp; Technology (MUST), Mirpur (AJK)-10250, Pakistan under grant number: 611-19/P.D/2017.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>72190</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>72212</offset><text>The authors declare no conflict of interest.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>72257</offset><text>Abbreviations</text></passage><passage><infon key="file">no_id_0.xml</infon><infon key="id">no_id_0</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CoZ&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crowd of Oz&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MTurk&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Amazon Mechanical Turk&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HIT&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Human intelligence task&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AI&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Artificial intelligence&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RTC&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Real time crowdsourcing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AV&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Audio–video&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;STT&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Speech to text&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MI&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Motivational interviewing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LIWC&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Linguistic inquiry and word count&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WC&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Word count&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WPS&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words per sentences&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sixltr&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Six letter&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;posemo&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive emotion&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;negemo&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative emotion&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;coproc&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cognitive process&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;percept&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Perceptual&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;
</infon><offset>72271</offset><text>CoZ	Crowd of Oz	 	MTurk	Amazon Mechanical Turk	 	HIT	Human intelligence task	 	AI	Artificial intelligence	 	RTC	Real time crowdsourcing	 	AV	Audio–video	 	STT	Speech to text	 	MI	Motivational interviewing	 	LIWC	Linguistic inquiry and word count	 	WC	Word count	 	WPS	Words per sentences	 	Sixltr	Six letter	 	posemo	Positive emotion	 	negemo	Negative emotion	 	coproc	Cognitive process	 	percept	Perceptual	 	</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>72684</offset><text>The following abbreviations are used in this manuscript: </text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>72742</offset><text>References</text></passage><passage><infon key="fpage">1366</infon><infon key="lpage">1369</infon><infon key="name_0">surname:Taelman;given-names:J.</infon><infon key="name_1">surname:Vandeput;given-names:S.</infon><infon key="name_2">surname:Spaepen;given-names:A.</infon><infon key="name_3">surname:Van Huffel;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 4th European Conference of the International Federation for Medical and Biological Engineering</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>72753</offset><text>Influence of mental stress on heart rate and heart rate variability</text></passage><passage><infon key="fpage">152</infon><infon key="lpage">158</infon><infon key="name_0">surname:Joëls;given-names:M.</infon><infon key="name_1">surname:Pu;given-names:Z.</infon><infon key="name_2">surname:Wiegert;given-names:O.</infon><infon key="name_3">surname:Oitzl;given-names:M.S.</infon><infon key="name_4">surname:Krugers;given-names:H.J.</infon><infon key="pub-id_doi">10.1016/j.tics.2006.02.002</infon><infon key="pub-id_pmid">16513410</infon><infon key="section_type">REF</infon><infon key="source">Trends Cogn. Sci.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2006</infon><offset>72821</offset><text>Learning under stress: How does it work?</text></passage><passage><infon key="fpage">369</infon><infon key="lpage">388</infon><infon key="name_0">surname:Eskin;given-names:M.</infon><infon key="name_1">surname:Sun;given-names:J.M.</infon><infon key="name_2">surname:Abuidhail;given-names:J.</infon><infon key="name_3">surname:Yoshimasu;given-names:K.</infon><infon key="name_4">surname:Kujan;given-names:O.</infon><infon key="name_5">surname:Janghorbani;given-names:M.</infon><infon key="name_6">surname:Flood;given-names:C.</infon><infon key="name_7">surname:Carta;given-names:M.G.</infon><infon key="name_8">surname:Tran;given-names:U.S.</infon><infon key="name_9">surname:Mechri;given-names:A.</infon><infon key="pub-id_doi">10.1080/13811118.2015.1054055</infon><infon key="pub-id_pmid">26954847</infon><infon key="section_type">REF</infon><infon key="source">Arch. Suicide Res.</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2016</infon><offset>72862</offset><text>Suicidal behavior and psychological distress in university students: A 12-nation study</text></passage><passage><infon key="fpage">223</infon><infon key="lpage">237</infon><infon key="name_0">surname:Nam;given-names:B.</infon><infon key="name_1">surname:Hilimire;given-names:M.R.</infon><infon key="name_2">surname:Jahn;given-names:D.</infon><infon key="name_3">surname:Lehmann;given-names:M.</infon><infon key="name_4">surname:DeVylder;given-names:J.E.</infon><infon key="pub-id_doi">10.1080/15332985.2017.1380742</infon><infon key="section_type">REF</infon><infon key="source">Soc. Work. Ment. Health</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2018</infon><offset>72949</offset><text>Predictors of suicidal ideation among college students: A prospective cohort study</text></passage><passage><infon key="fpage">505</infon><infon key="lpage">525</infon><infon key="name_0">surname:Denovan;given-names:A.</infon><infon key="name_1">surname:Macaskill;given-names:A.</infon><infon key="pub-id_doi">10.1007/s10902-016-9736-y</infon><infon key="section_type">REF</infon><infon key="source">J. Happiness Stud.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2017</infon><offset>73032</offset><text>Stress and subjective well-being among first year UK undergraduate students</text></passage><passage><infon key="fpage">646</infon><infon key="lpage">655</infon><infon key="name_0">surname:Elias;given-names:H.</infon><infon key="name_1">surname:Ping;given-names:W.S.</infon><infon key="name_2">surname:Abdullah;given-names:M.C.</infon><infon key="pub-id_doi">10.1016/j.sbspro.2011.11.288</infon><infon key="section_type">REF</infon><infon key="source">Procedia-Soc. Behav. Sci.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2011</infon><offset>73108</offset><text>Stress and academic achievement among undergraduate students in Universiti Putra Malaysia</text></passage><passage><infon key="fpage">55</infon><infon key="lpage">57</infon><infon key="name_0">surname:Chowdhury;given-names:R.</infon><infon key="name_1">surname:Mukherjee;given-names:A.</infon><infon key="name_2">surname:Mitra;given-names:K.</infon><infon key="name_3">surname:Naskar;given-names:S.</infon><infon key="name_4">surname:Karmakar;given-names:P.R.</infon><infon key="name_5">surname:Lahiri;given-names:S.K.</infon><infon key="pub-id_doi">10.4103/0019-557X.200253</infon><infon key="pub-id_pmid">28218165</infon><infon key="section_type">REF</infon><infon key="source">Indian J. Public Health</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">2017</infon><offset>73198</offset><text>Perceived psychological stress among undergraduate medical students: Role of academic factors</text></passage><passage><infon key="elocation-id">2</infon><infon key="name_0">surname:Shah;given-names:M.</infon><infon key="name_1">surname:Hasan;given-names:S.</infon><infon key="name_2">surname:Malik;given-names:S.</infon><infon key="name_3">surname:Sreeramareddy;given-names:C.T.</infon><infon key="pub-id_doi">10.1186/1472-6920-10-2</infon><infon key="pub-id_pmid">20078853</infon><infon key="section_type">REF</infon><infon key="source">BMC Med. Educ.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2010</infon><offset>73292</offset><text>Perceived stress, sources and severity of stress among medical undergraduates in a Pakistani medical school</text></passage><passage><infon key="fpage">468</infon><infon key="lpage">473</infon><infon key="name_0">surname:Ward-Griffin;given-names:E.</infon><infon key="name_1">surname:Klaiber;given-names:P.</infon><infon key="name_2">surname:Collins;given-names:H.K.</infon><infon key="name_3">surname:Owens;given-names:R.L.</infon><infon key="name_4">surname:Coren;given-names:S.</infon><infon key="name_5">surname:Chen;given-names:F.S.</infon><infon key="pub-id_doi">10.1002/smi.2804</infon><infon key="pub-id_pmid">29528189</infon><infon key="section_type">REF</infon><infon key="source">Stress Health</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2018</infon><offset>73400</offset><text>Petting away pre-exam stress: The effect of therapy dog sessions on student well-being</text></passage><passage><infon key="fpage">48</infon><infon key="lpage">56</infon><infon key="name_0">surname:Crossman;given-names:M.K.</infon><infon key="name_1">surname:Kazdin;given-names:A.E.</infon><infon key="name_2">surname:Kitt;given-names:E.R.</infon><infon key="pub-id_doi">10.1037/pro0000177</infon><infon key="section_type">REF</infon><infon key="source">Prof. Psychol. Res. Pract.</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2018</infon><offset>73487</offset><text>The influence of a socially assistive robot on mood, anxiety, and arousal in children</text></passage><passage><infon key="name_0">surname:Pu;given-names:L.</infon><infon key="name_1">surname:Moyle;given-names:W.</infon><infon key="name_2">surname:Jones;given-names:C.</infon><infon key="name_3">surname:Todorovic;given-names:M.</infon><infon key="pub-id_doi">10.1093/geront/gny046</infon><infon key="section_type">REF</infon><infon key="source">Gerontologist</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>73573</offset><text>The Effectiveness of Social Robots for Older Adults: A Systematic Review and Meta-Analysis of Randomized Controlled Studies</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Rose;given-names:E.J.</infon><infon key="pub-id_doi">10.1145/3121113.3121212</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 35th ACM International Conference on the Design of Communication</infon><infon key="type">ref</infon><offset>73697</offset><text>Designing for engagement: Using participatory design to develop a social robot to measure teen stress</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:Kim;given-names:J.</infon><infon key="name_1">surname:Kim;given-names:Y.</infon><infon key="name_2">surname:Kim;given-names:B.</infon><infon key="name_3">surname:Yun;given-names:S.</infon><infon key="name_4">surname:Kim;given-names:M.</infon><infon key="name_5">surname:Lee;given-names:J.</infon><infon key="pub-id_doi">10.1145/3170427.3188548</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems-CHI ’18</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>73799</offset><text>Can a Machine Tend to Teenagers’ Emotional Needs?</text></passage><passage><infon key="fpage">762</infon><infon key="lpage">769</infon><infon key="name_0">surname:Kang;given-names:H.</infon><infon key="name_1">surname:Hebert;given-names:M.</infon><infon key="name_2">surname:Kanade;given-names:T.</infon><infon key="pub-id_doi">10.1109/ICCV.2011.6126314</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 IEEE International Conference on Computer Vision</infon><infon key="type">ref</infon><offset>73851</offset><text>Discovering object instances from scenes of Daily Living</text></passage><passage><infon key="fpage">509</infon><infon key="lpage">516</infon><infon key="name_0">surname:Morris;given-names:R.R.</infon><infon key="name_1">surname:Picard;given-names:R.</infon><infon key="pub-id_doi">10.1080/17439760.2014.913671</infon><infon key="section_type">REF</infon><infon key="source">J. Posit. Psychol.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>73908</offset><text>Crowd-powered positive psychological interventions</text></passage><passage><infon key="name_0">surname:Morris;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 Annual Conference Extended Abstracts on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>73959</offset><text>Crowdsourcing workshop: The emergence of affective crowdsourcing</text></passage><passage><infon key="comment">Available online: https://emotionalbaggagecheck.com/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>74024</offset><text>Emotional Baggage Check</text></passage><passage><infon key="fpage">33</infon><infon key="name_0">surname:Bernstein;given-names:M.S.</infon><infon key="name_1">surname:Brandt;given-names:J.</infon><infon key="name_2">surname:Miller;given-names:R.C.</infon><infon key="name_3">surname:Karger;given-names:D.R.</infon><infon key="pub-id_doi">10.1145/2047196.2047201</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 24th annual ACM Symposium on User Interface Software and Technology, Proc. UIST’11</infon><infon key="type">ref</infon><offset>74048</offset><text>Crowds in two seconds</text></passage><passage><infon key="fpage">6</infon><infon key="lpage">26</infon><infon key="name_0">surname:Gadiraju;given-names:U.</infon><infon key="name_1">surname:Möller;given-names:S.</infon><infon key="name_2">surname:Nöllenburg;given-names:M.</infon><infon key="name_3">surname:Saupe;given-names:D.</infon><infon key="name_4">surname:Egger-Lampl;given-names:S.</infon><infon key="name_5">surname:Archambault;given-names:D.</infon><infon key="name_6">surname:Fisher;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</infon><infon key="type">ref</infon><infon key="volume">Volume 10264 LNCS</infon><infon key="year">2017</infon><offset>74070</offset><text>Crowdsourcing versus the laboratory: Towards human-centered experiments using the crowd</text></passage><passage><infon key="fpage">23</infon><infon key="name_0">surname:Lasecki;given-names:W.S.</infon><infon key="name_1">surname:Murray;given-names:K.I.</infon><infon key="name_2">surname:White;given-names:S.</infon><infon key="name_3">surname:Miller;given-names:R.C.</infon><infon key="name_4">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_doi">10.1145/2047196.2047200</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 24th annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>74158</offset><text>Real-time crowd control of existing interfaces</text></passage><passage><infon key="name_0">surname:Gouravajhala;given-names:S.R.</infon><infon key="name_1">surname:Yim;given-names:J.</infon><infon key="name_2">surname:Desingh;given-names:K.</infon><infon key="name_3">surname:Huang;given-names:Y.</infon><infon key="name_4">surname:Jenkins;given-names:O.C.</infon><infon key="name_5">surname:Lasecki;given-names:W.S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Sixth AAAI Conference on Human Computation and Crowdsourcing</infon><infon key="type">ref</infon><offset>74205</offset><text>EURECA: Enhanced Understanding of Real Environments via Crowd Assistance</text></passage><passage><infon key="fpage">304</infon><infon key="lpage">327</infon><infon key="name_0">surname:Berry;given-names:D.C.</infon><infon key="name_1">surname:Butler;given-names:L.T.</infon><infon key="name_2">surname:De Rosis;given-names:F.</infon><infon key="pub-id_doi">10.1016/j.ijhcs.2005.03.006</infon><infon key="section_type">REF</infon><infon key="source">Int. J. -Hum.-Comput. Stud.</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">2005</infon><offset>74278</offset><text>Evaluating a realistic agent in an advice-giving task</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">152</infon><infon key="name_0">surname:Powers;given-names:A.</infon><infon key="name_1">surname:Kiesler;given-names:S.</infon><infon key="name_2">surname:Fussell;given-names:S.</infon><infon key="name_3">surname:Fussell;given-names:S.</infon><infon key="name_4">surname:Torrey;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>74332</offset><text>Comparing a computer agent with a humanoid robot</text></passage><passage><infon key="comment">Available online: https://www.softbankrobotics.com/emea/en/pepper</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>74381</offset><text>Pepper | Softbank</text></passage><passage><infon key="fpage">1231</infon><infon key="lpage">1245</infon><infon key="name_0">surname:Chen;given-names:J.Y.</infon><infon key="name_1">surname:Haas;given-names:E.C.</infon><infon key="name_2">surname:Barnes;given-names:M.J.</infon><infon key="pub-id_doi">10.1109/TSMCC.2007.905819</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Syst. Man Cybern. Part C Appl. Rev.</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2007</infon><offset>74399</offset><text>Human performance issues and user interface design for teleoperated robots</text></passage><passage><infon key="fpage">27</infon><infon key="lpage">34</infon><infon key="name_0">surname:Butler;given-names:D.J.</infon><infon key="name_1">surname:Huang;given-names:J.</infon><infon key="name_2">surname:Roesner;given-names:F.</infon><infon key="name_3">surname:Cakmak;given-names:M.</infon><infon key="pub-id_doi">10.1145/2696454.2696484</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction</infon><infon key="type">ref</infon><offset>74474</offset><text>The Privacy-Utility Tradeoff for Remotely Teleoperated Robots</text></passage><passage><infon key="fpage">175</infon><infon key="lpage">226</infon><infon key="name_0">surname:Goodrich;given-names:M.A.</infon><infon key="name_1">surname:Crandall;given-names:J.W.</infon><infon key="name_2">surname:Barakova;given-names:E.</infon><infon key="pub-id_doi">10.1177/1557234X13502463</infon><infon key="section_type">REF</infon><infon key="source">Rev. Hum. Factors Ergon.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2013</infon><offset>74536</offset><text>Teleoperation and Beyond for Assistive Humanoid Robots</text></passage><passage><infon key="fpage">259</infon><infon key="lpage">295</infon><infon key="name_0">surname:Van den Berghe;given-names:R.</infon><infon key="name_1">surname:Verhagen;given-names:J.</infon><infon key="name_2">surname:Oudgenoeg-Paz;given-names:O.</infon><infon key="name_3">surname:van der Ven;given-names:S.</infon><infon key="name_4">surname:Leseman;given-names:P.</infon><infon key="pub-id_doi">10.3102/0034654318821286</infon><infon key="section_type">REF</infon><infon key="source">Rev. Educ. Res.</infon><infon key="type">ref</infon><infon key="volume">89</infon><infon key="year">2019</infon><offset>74591</offset><text>Social Robots for Language Learning: A Review</text></passage><passage><infon key="name_0">surname:Torta;given-names:E.</infon><infon key="name_1">surname:Oberzaucher;given-names:J.</infon><infon key="name_2">surname:Werner;given-names:F.</infon><infon key="name_3">surname:Cuijpers;given-names:R.H.</infon><infon key="name_4">surname:Juola;given-names:J.F.</infon><infon key="pub-id_doi">10.5898/JHRI.1.2.Torta</infon><infon key="section_type">REF</infon><infon key="source">J. -Hum.-Robot. Interact.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2013</infon><offset>74637</offset><text>Attitudes towards Socially Assistive Robots in Intelligent Homes: Results From Laboratory Studies and Field Trials</text></passage><passage><infon key="fpage">530</infon><infon key="lpage">544</infon><infon key="name_0">surname:Glas;given-names:D.F.</infon><infon key="name_1">surname:Kanda;given-names:T.</infon><infon key="name_2">surname:Ishiguro;given-names:H.</infon><infon key="name_3">surname:Hagita;given-names:N.</infon><infon key="pub-id_doi">10.1109/TSMCA.2011.2164243</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Syst. Man Cybern. Part A Syst. Humans</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2012</infon><offset>74752</offset><text>Teleoperation of multiple social robots</text></passage><passage><infon key="fpage">345</infon><infon key="lpage">356</infon><infon key="name_0">surname:Huskens;given-names:B.</infon><infon key="name_1">surname:Verschuur;given-names:R.</infon><infon key="name_2">surname:Gillesen;given-names:J.</infon><infon key="name_3">surname:Didden;given-names:R.</infon><infon key="name_4">surname:Barakova;given-names:E.</infon><infon key="pub-id_doi">10.3109/17518423.2012.739212</infon><infon key="section_type">REF</infon><infon key="source">Dev. Neurorehabilit.</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2013</infon><offset>74792</offset><text>Promoting question-asking in school-aged children with autism spectrum disorders: Effectiveness of a robot intervention compared to a human-trainer intervention</text></passage><passage><infon key="fpage">379</infon><infon key="lpage">390</infon><infon key="name_0">surname:Van Straten;given-names:C.L.</infon><infon key="name_1">surname:Smeekens;given-names:I.</infon><infon key="name_2">surname:Barakova;given-names:E.</infon><infon key="name_3">surname:Glennon;given-names:J.</infon><infon key="name_4">surname:Buitelaar;given-names:J.</infon><infon key="name_5">surname:Chen;given-names:A.</infon><infon key="pub-id_doi">10.1007/s00779-017-1060-y</infon><infon key="section_type">REF</infon><infon key="source">Pers. Ubiquitous Comput.</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2018</infon><offset>74953</offset><text>Effects of robots’ intonation and bodily appearance on robot-mediated communicative treatment outcomes for children with autism spectrum disorder</text></passage><passage><infon key="name_0">surname:Sono;given-names:T.</infon><infon key="name_1">surname:Satake;given-names:S.</infon><infon key="name_2">surname:Kanda;given-names:T.</infon><infon key="name_3">surname:Imai;given-names:M.</infon><infon key="pub-id_doi">10.1080/01691864.2019.1610062</infon><infon key="section_type">REF</infon><infon key="source">Adv. Robot.</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>75101</offset><text>Walking partner robot chatting about scenery</text></passage><passage><infon key="fpage">57</infon><infon key="lpage">84</infon><infon key="name_0">surname:Mao;given-names:K.</infon><infon key="name_1">surname:Capra;given-names:L.</infon><infon key="name_2">surname:Harman;given-names:M.</infon><infon key="name_3">surname:Jia;given-names:Y.</infon><infon key="pub-id_doi">10.1016/j.jss.2016.09.015</infon><infon key="section_type">REF</infon><infon key="source">J. Syst. Softw.</infon><infon key="type">ref</infon><infon key="volume">126</infon><infon key="year">2017</infon><offset>75146</offset><text>A Survey of the Use of Crowdsourcing in Software Engineering</text></passage><passage><infon key="fpage">319</infon><infon key="lpage">326</infon><infon key="name_0">surname:Von Ahn;given-names:L.</infon><infon key="name_1">surname:Dabbish;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><offset>75207</offset><text>Labeling images with a computer game</text></passage><passage><infon key="fpage">313</infon><infon key="lpage">322</infon><infon key="name_0">surname:Bernstein;given-names:M.S.</infon><infon key="name_1">surname:Little;given-names:G.</infon><infon key="name_2">surname:Miller;given-names:R.C.</infon><infon key="name_3">surname:Hartmann;given-names:B.</infon><infon key="name_4">surname:Ackerman;given-names:M.S.</infon><infon key="name_5">surname:Karger;given-names:D.R.</infon><infon key="name_6">surname:Crowell;given-names:D.</infon><infon key="name_7">surname:Panovich;given-names:K.</infon><infon key="pub-id_doi">10.1145/1866029.1866078</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM Symposium on User Interface Software and Technology-UIST’10</infon><infon key="type">ref</infon><offset>75244</offset><text>Soylent: A Word Processor with a Crowd Inside</text></passage><passage><infon key="fpage">756</infon><infon key="lpage">760</infon><infon key="name_0">surname:Cooper;given-names:S.</infon><infon key="name_1">surname:Khatib;given-names:F.</infon><infon key="name_2">surname:Treuille;given-names:A.</infon><infon key="name_3">surname:Barbero;given-names:J.</infon><infon key="name_4">surname:Lee;given-names:J.</infon><infon key="name_5">surname:Beenen;given-names:M.</infon><infon key="name_6">surname:Leaver-Fay;given-names:A.</infon><infon key="name_7">surname:Baker;given-names:D.</infon><infon key="name_8">surname:Popović;given-names:Z.</infon><infon key="name_9">surname:Players;given-names:F.</infon><infon key="pub-id_doi">10.1038/nature09304</infon><infon key="pub-id_pmid">20686574</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">466</infon><infon key="year">2010</infon><offset>75290</offset><text>Predicting protein structures with a multiplayer online game</text></passage><passage><infon key="fpage">1393</infon><infon key="name_0">surname:Yu;given-names:L.</infon><infon key="name_1">surname:Nickerson;given-names:J.V.</infon><infon key="pub-id_doi">10.1145/1978942.1979147</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 Annual Conference on Human Factors In Computing Systems-CHI ’11</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>75351</offset><text>Cooks or cobblers?</text></passage><passage><infon key="fpage">67</infon><infon key="lpage">93</infon><infon key="name_0">surname:Lasecki;given-names:W.S.</infon><infon key="name_1">surname:Homan;given-names:C.</infon><infon key="name_2">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_doi">10.15346/hc.v1i1.5</infon><infon key="section_type">REF</infon><infon key="source">Hum. Comput.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2014</infon><offset>75370</offset><text>Architecting Real-Time Crowd-Powered Systems</text></passage><passage><infon key="fpage">333</infon><infon key="lpage">342</infon><infon key="name_0">surname:Bigham;given-names:J.P.</infon><infon key="name_1">surname:Jayant;given-names:C.</infon><infon key="name_2">surname:Ji;given-names:H.</infon><infon key="name_3">surname:Little;given-names:G.</infon><infon key="name_4">surname:Miller;given-names:A.</infon><infon key="name_5">surname:Miller;given-names:R.C.</infon><infon key="name_6">surname:Miller;given-names:R.</infon><infon key="name_7">surname:Tatarowicz;given-names:A.</infon><infon key="name_8">surname:White;given-names:B.</infon><infon key="name_9">surname:White;given-names:S.</infon><infon key="pub-id_doi">10.1145/1866029.1866080</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><offset>75415</offset><text>VizWiz: Nearly Real-Time Answers to Visual Questions</text></passage><passage><infon key="fpage">151</infon><infon key="lpage">162</infon><infon key="name_0">surname:Lasecki;given-names:W.S.</infon><infon key="name_1">surname:Wesley;given-names:R.</infon><infon key="name_2">surname:Nichols;given-names:J.</infon><infon key="name_3">surname:Kulkarni;given-names:A.</infon><infon key="name_4">surname:Allen;given-names:J.F.</infon><infon key="name_5">surname:Bigham;given-names:J.</infon><infon key="pub-id_doi">10.1145/2501988.2502057</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><offset>75468</offset><text>Chorus: A Crowd-powered Conversational Assistant</text></passage><passage><infon key="name_0">surname:Huang;given-names:T.H.K.</infon><infon key="name_1">surname:Chang;given-names:J.C.</infon><infon key="name_2">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_doi">10.1145/3173574.3173869</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><offset>75517</offset><text>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself over Time</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Lasecki;given-names:W.S.</infon><infon key="name_1">surname:Thiha;given-names:P.</infon><infon key="name_2">surname:Zhong;given-names:Y.</infon><infon key="name_3">surname:Brady;given-names:E.</infon><infon key="name_4">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_doi">10.1145/2513383.2517033</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>75601</offset><text>Answering visual questions with conversational crowd assistants</text></passage><passage><infon key="name_0">surname:Huang;given-names:T.H.</infon><infon key="name_1">surname:Azaria;given-names:A.</infon><infon key="name_2">surname:Romero;given-names:O.J.</infon><infon key="name_3">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_arxiv">1909.05725</infon><infon key="pub-id_doi">10.15346/hc.v6i1.7</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>75665</offset><text>InstructableCrowd: Creating IF-THEN Rules for Smartphones via Conversations with the Crowd</text></passage><passage><infon key="fpage">106</infon><infon key="lpage">118</infon><infon key="name_0">surname:Andolina;given-names:S.</infon><infon key="name_1">surname:Schneider;given-names:H.</infon><infon key="name_2">surname:Chan;given-names:J.</infon><infon key="name_3">surname:Klouche;given-names:K.</infon><infon key="name_4">surname:Jacucci;given-names:G.</infon><infon key="name_5">surname:Dow;given-names:S.</infon><infon key="pub-id_doi">10.1145/3059454.3059477</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition</infon><infon key="type">ref</infon><offset>75756</offset><text>Crowdboard: Augmenting in-person idea generation with real-time crowds</text></passage><passage><infon key="fpage">841</infon><infon key="lpage">849</infon><infon key="name_0">surname:Salisbury;given-names:E.</infon><infon key="name_1">surname:Stein;given-names:S.</infon><infon key="name_2">surname:Ramchurn;given-names:S.D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems</infon><infon key="type">ref</infon><infon key="volume">Volume 2</infon><offset>75827</offset><text>Real-time opinion aggregation methods for crowd robotics</text></passage><passage><infon key="fpage">25</infon><infon key="name_0">surname:Toris;given-names:R.</infon><infon key="name_1">surname:Kent;given-names:D.</infon><infon key="name_2">surname:Chernova;given-names:S.</infon><infon key="pub-id_doi">10.5898/JHRI/3.2.Toris</infon><infon key="section_type">REF</infon><infon key="source">J. Hum. Robot. Interact.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2014</infon><offset>75884</offset><text>The Robot Management System: A Framework for Conducting Human-Robot Interaction Studies Through Crowdsourcing</text></passage><passage><infon key="fpage">339</infon><infon key="lpage">346</infon><infon key="name_0">surname:Crick;given-names:C.</infon><infon key="name_1">surname:Osentoski;given-names:S.</infon><infon key="name_2">surname:Jenkins;given-names:O.C.</infon><infon key="name_3">surname:Jay;given-names:G.</infon><infon key="pub-id_doi">10.1145/1957656.1957788</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the HRI 2011, 6th ACM/IEEE International Conference on Human-Robot Interaction</infon><infon key="type">ref</infon><offset>75994</offset><text>Human and robot perception in large-scale learning from demonstration</text></passage><passage><infon key="fpage">69</infon><infon key="lpage">70</infon><infon key="name_0">surname:Björling;given-names:E.A.</infon><infon key="name_1">surname:Rose;given-names:E.</infon><infon key="name_2">surname:Ren;given-names:R.</infon><infon key="pub-id_doi">10.1145/3173386.3177068</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction</infon><infon key="type">ref</infon><infon key="volume">Volume Part F1351</infon><offset>76064</offset><text>Teen-Robot Interaction: A Pilot Study of Engagement with a Low-fidelity Prototype</text></passage><passage><infon key="name_0">surname:Huang;given-names:T.H.K.</infon><infon key="name_1">surname:Bigham;given-names:J.P.</infon><infon key="pub-id_doi">10.1007/978-3-642-32787-2</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Fifth AAAI Conference on Human Computation and Crowdsourcing</infon><infon key="type">ref</infon><offset>76146</offset><text>A 10-Month-Long Deployment Study of On-Demand Recruiting for Low-Latency Crowdsourcing</text></passage><passage><infon key="fpage">84</infon><infon key="lpage">92</infon><infon key="name_0">surname:Webb;given-names:N.</infon><infon key="name_1">surname:Benyon;given-names:D.</infon><infon key="name_2">surname:Hansen;given-names:P.</infon><infon key="name_3">surname:Mival;given-names:O.</infon><infon key="section_type">REF</infon><infon key="source">LREC Conf. Proc.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2010</infon><offset>76233</offset><text>Evaluating Human-Machine Interaction for Appropriateness</text></passage><passage><infon key="fpage">24</infon><infon key="lpage">54</infon><infon key="name_0">surname:Tausczik;given-names:Y.R.</infon><infon key="name_1">surname:Pennebaker;given-names:J.W.</infon><infon key="pub-id_doi">10.1177/0261927X09351676</infon><infon key="section_type">REF</infon><infon key="source">J. Lang. Soc. Psychol.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2010</infon><offset>76290</offset><text>The psychological meaning of words: LIWC and computerized text analysis methods</text></passage><passage><infon key="fpage">482</infon><infon key="name_0">surname:Miller;given-names:W.R.</infon><infon key="name_1">surname:Rollnick;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Motivational Interviewing: Helping People Change</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>76370</offset></passage><passage><infon key="fpage">153</infon><infon key="name_0">surname:Shiwa;given-names:T.</infon><infon key="name_1">surname:Kanda;given-names:T.</infon><infon key="name_2">surname:Imai;given-names:M.</infon><infon key="name_3">surname:Ishiguro;given-names:H.</infon><infon key="name_4">surname:Hagita;given-names:N.</infon><infon key="pub-id_doi">10.1145/1349822.1349843</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Third International Conference on Human Robot Interaction-HRI ’08</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>76371</offset><text>How quickly should communication robots respond?</text></passage><passage><infon key="fpage">179</infon><infon key="lpage">194</infon><infon key="name_0">surname:Zimmerman;given-names:D.H.</infon><infon key="pub-id_doi">10.1207/s15327973rlsi2602_4</infon><infon key="section_type">REF</infon><infon key="source">Res. Lang. Soc. Interact.</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">1993</infon><offset>76420</offset><text>Acknowledgment Tokens and Speakership Incipiency Revisited</text></passage><passage><infon key="fpage">4945</infon><infon key="lpage">4955</infon><infon key="name_0">surname:Srinivasan;given-names:V.</infon><infon key="name_1">surname:Takayama;given-names:L.</infon><infon key="pub-id_doi">10.1145/2858036.2858217</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><offset>76479</offset><text>Help Me Please</text></passage><passage><infon key="fpage">343</infon><infon key="name_0">surname:Caine;given-names:K.</infon><infon key="name_1">surname:Sabanovic;given-names:S.</infon><infon key="name_2">surname:Carter;given-names:M.</infon><infon key="pub-id_doi">10.1145/2157689.2157807</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction</infon><infon key="type">ref</infon><offset>76494</offset><text>The effect of monitoring by cameras and robots on the privacy enhancing behaviors of older adults</text></passage><passage><infon key="file">sensors-20-00569-g001.jpg</infon><infon key="id">sensors-20-00569-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76592</offset><text>High-level system architecture of Crowd of Oz.</text></passage><passage><infon key="file">sensors-20-00569-g002.jpg</infon><infon key="id">sensors-20-00569-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76639</offset><text>Waiting page where workers are kept to ensure availability.</text></passage><passage><infon key="file">sensors-20-00569-g003.jpg</infon><infon key="id">sensors-20-00569-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76699</offset><text>Crowd worker’s main task page. It also shows an actress interacting with the CoZ from one of the sessions.</text></passage><passage><infon key="file">sensors-20-00569-g004.jpg</infon><infon key="id">sensors-20-00569-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76808</offset><text>An actress interacting with a CoZ powered Pepper robot during a session.</text></passage><passage><infon key="file">sensors-20-00569-g005.jpg</infon><infon key="id">sensors-20-00569-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76881</offset><text>Mean latency: significant difference in the average response time between one-crowd and eight-crowd conditions.</text></passage><passage><infon key="file">sensors-20-00569-g006.jpg</infon><infon key="id">sensors-20-00569-f006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>76993</offset><text>Dialogue quality: no significant difference in quality scores between all conditions.</text></passage><passage><infon key="file">sensors-20-00569-g007.jpg</infon><infon key="id">sensors-20-00569-f007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>77079</offset><text>Using one-way ANOVA, we found no significant difference between mean scores of five categories across all conditions.</text></passage><passage><infon key="file">sensors-20-00569-g008.jpg</infon><infon key="id">sensors-20-00569-f008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>77197</offset><text>t-test revealed that words chosen by crowd workers contained more positive emotions than negative emotions across all conditions.</text></passage><passage><infon key="file">sensors-20-00569-g009.jpg</infon><infon key="id">sensors-20-00569-f009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>77327</offset><text>Number of workers who left during the conversational task vs. who stayed till the end of the task.</text></passage><passage><infon key="file">sensors-20-00569-t001.xml</infon><infon key="id">sensors-20-00569-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>77426</offset><text>Summary of the key differences between Crowd of Oz (CoZ) and other crowd-powered conversational agents.</text></passage><passage><infon key="file">sensors-20-00569-t001.xml</infon><infon key="id">sensors-20-00569-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reference&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Use Case&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Input&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Output&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Device&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Chorus &lt;xref rid=&quot;B41-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;41&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Information retrieval&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User queries in natural language&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Text message&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile phones or PC&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Evorus &lt;xref rid=&quot;B42-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;42&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Information retrieval&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User queries in natural language&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Text message&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile phones or PC&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VizWiz &lt;xref rid=&quot;B40-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;40&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Assisting blind users to interact with devices&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Video stream and recorded audio question + text&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Audio through voice-over screen reader&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile phones&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Chorus:view &lt;xref rid=&quot;B43-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;43&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Assisting blind users to interact with devices&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Video stream and recorded audio question + text&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Audio through voice-over screen reader&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile phones&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CrowdBoard &lt;xref rid=&quot;B45-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;45&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Creativity&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Write ideas on sticky notes&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Textual ideas from crowd workers&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Digital whiteboard&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;InstructableCrowd &lt;xref rid=&quot;B44-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;44&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Programming&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User’s problem in natural language&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IF-THEN rules&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile phones&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CoZ&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Live conversational task for stress management&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Real time audio and video feed + transcribed text messages&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Animated Speech by Pepper robot and text message displayed on the Pepper robot’s Tablet&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pepper or NAO robot&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>77530</offset><text>Reference	Use Case	Input	Output	Device	 	Chorus 	Information retrieval	User queries in natural language	Text message	Mobile phones or PC	 	Evorus 	Information retrieval	User queries in natural language	Text message	Mobile phones or PC	 	VizWiz 	Assisting blind users to interact with devices	Video stream and recorded audio question + text	Audio through voice-over screen reader	Mobile phones	 	Chorus:view 	Assisting blind users to interact with devices	Video stream and recorded audio question + text	Audio through voice-over screen reader	Mobile phones	 	CrowdBoard 	Creativity	Write ideas on sticky notes	Textual ideas from crowd workers	Digital whiteboard	 	InstructableCrowd 	Programming	User’s problem in natural language	IF-THEN rules	Mobile phones	 	CoZ	Live conversational task for stress management	Real time audio and video feed + transcribed text messages	Animated Speech by Pepper robot and text message displayed on the Pepper robot’s Tablet	Pepper or NAO robot	 	</text></passage><passage><infon key="file">sensors-20-00569-t002.xml</infon><infon key="id">sensors-20-00569-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>78520</offset><text>Summary of the key differences between CoZ and other systems from crowd or web robotics.</text></passage><passage><infon key="file">sensors-20-00569-t002.xml</infon><infon key="id">sensors-20-00569-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reference&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Use Case&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Input&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Output&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Legion &lt;xref rid=&quot;B20-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;20&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot navigation&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Video stream of rovio robot + arrow key presses&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot movement&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rovio robot&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CrowdDrone &lt;xref rid=&quot;B46-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;46&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drone navigation&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Simulated or real imagery from drone’s camera + arrow key presses&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot movement&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Drone robot&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EURECA &lt;xref rid=&quot;B21-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;21&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Scene manipulation&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Natural language query + scene manipulation (zoom, pan, orbit) + selection tools&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Segmented and labelled objects&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fetch robot&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot Management System (RMS) &lt;xref rid=&quot;B47-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;47&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot navigation + manipulation&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Arrow keys for changing direction + camera feeds + 2D map + slider control to alter speed + arm controls&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot movement and object retrieval&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PR2 robot&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Learning from demonstration &lt;xref rid=&quot;B48-sensors-20-00569&quot; ref-type=&quot;bibr&quot;&gt;48&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot learning&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Web interface for controlling a robot&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot movement&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;iRobot&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CoZ&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Live conversational task for stress management&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Real time audio and video feed + transcribed text messages&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Animated speech by Pepper robot and text message displayed on the Pepper robot’s Tablet&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pepper or NAO robot&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>78609</offset><text>Reference	Use Case	Input	Output	Robot	 	Legion 	Robot navigation	Video stream of rovio robot + arrow key presses	Robot movement	Rovio robot	 	CrowdDrone 	Drone navigation	Simulated or real imagery from drone’s camera + arrow key presses	Robot movement	Drone robot	 	EURECA 	Scene manipulation	Natural language query + scene manipulation (zoom, pan, orbit) + selection tools	Segmented and labelled objects	Fetch robot	 	Robot Management System (RMS) 	Robot navigation + manipulation	Arrow keys for changing direction + camera feeds + 2D map + slider control to alter speed + arm controls	Robot movement and object retrieval	PR2 robot	 	Learning from demonstration 	Robot learning	Web interface for controlling a robot	Robot movement	iRobot	 	CoZ	Live conversational task for stress management	Real time audio and video feed + transcribed text messages	Animated speech by Pepper robot and text message displayed on the Pepper robot’s Tablet	Pepper or NAO robot	 	</text></passage><passage><infon key="file">sensors-20-00569-t003.xml</infon><infon key="id">sensors-20-00569-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>79579</offset><text>Problems or situations mentioned by an actress during sessions and examples of corresponding crowd responses.</text></passage><passage><infon key="file">sensors-20-00569-t003.xml</infon><infon key="id">sensors-20-00569-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Problem/Topic&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Indicative Excerpt&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Opening a discussion&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: I have been stressed for the past few weeks really and it’s very difficult to focus on my studies and it would be nice just to talk about that.&lt;break/&gt;Robot: What is stressing you about your studies?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Moving to a new city&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: I moved from Belgium to the Netherlands 3 months ago to do this minor so its new city and new house I am living in.&lt;break/&gt;Robot: Are there any language barriers you have to deal with?&lt;break/&gt;User: Not really because in Belgium we speak Flounderish and in Holland, they speak Dutch and it’s the same language.&lt;break/&gt;Robot: So, the differences between your studies and studying life are feeling overwhelming?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Difficulty in study&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: So, I’m doing a minor here in Eindhoven and it’s new subjects and it’s so different working than before…I am anxious about it⋯ it’s a new style of working…new content, so I do not know how to deal with that.&lt;break/&gt;Robot: Why did you chose this subject?&lt;break/&gt;User: I thought it was interesting… it is more focused on user experience.&lt;break/&gt;Robot: But learning it is not giving you happiness?&lt;break/&gt;User: I think it will give me… we disagree maybe because I think my study gives me fulfillment.&lt;break/&gt;Robot: How come fulfilment and stress at same time?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Difficulty in finding people for an interview&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: I need to find people to interview and I don’t know so many people. So, I have tried putting an Ad up, but it didn’t work. It mixes up with all my planning.&lt;break/&gt;Robot: Try social media for getting connections.&lt;break/&gt;User: so, what would you put up on social media who would like to go out for a coffee with me then.&lt;break/&gt;Robot: Yes, they have groups for locals on places like Meetups.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Noisy roommates&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: I was hoping I would get some sleep, but I have 5 extremely noisy roommates.&lt;break/&gt;Robot: Can you try playing low volume rain sounds or something to drown them out?&lt;break/&gt;User: What is white noise?&lt;break/&gt;Robot: A background sound that’s relaxing to listen to that will drown out the loud sounds that your roommates make.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Showing empathy&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;User: I am saying that I am very stressed and that I like to speak about that, and I can’t focus on my studies&lt;break/&gt;Robot: Oh, that’s terrible what can I do to help?&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>79689</offset><text>Problem/Topic	Indicative Excerpt	 	Opening a discussion	User: I have been stressed for the past few weeks really and it’s very difficult to focus on my studies and it would be nice just to talk about that.Robot: What is stressing you about your studies?	 	Moving to a new city	User: I moved from Belgium to the Netherlands 3 months ago to do this minor so its new city and new house I am living in.Robot: Are there any language barriers you have to deal with?User: Not really because in Belgium we speak Flounderish and in Holland, they speak Dutch and it’s the same language.Robot: So, the differences between your studies and studying life are feeling overwhelming?	 	Difficulty in study	User: So, I’m doing a minor here in Eindhoven and it’s new subjects and it’s so different working than before…I am anxious about it⋯ it’s a new style of working…new content, so I do not know how to deal with that.Robot: Why did you chose this subject?User: I thought it was interesting… it is more focused on user experience.Robot: But learning it is not giving you happiness?User: I think it will give me… we disagree maybe because I think my study gives me fulfillment.Robot: How come fulfilment and stress at same time?	 	Difficulty in finding people for an interview	User: I need to find people to interview and I don’t know so many people. So, I have tried putting an Ad up, but it didn’t work. It mixes up with all my planning.Robot: Try social media for getting connections.User: so, what would you put up on social media who would like to go out for a coffee with me then.Robot: Yes, they have groups for locals on places like Meetups.	 	Noisy roommates	User: I was hoping I would get some sleep, but I have 5 extremely noisy roommates.Robot: Can you try playing low volume rain sounds or something to drown them out?User: What is white noise?Robot: A background sound that’s relaxing to listen to that will drown out the loud sounds that your roommates make.	 	Showing empathy	User: I am saying that I am very stressed and that I like to speak about that, and I can’t focus on my studiesRobot: Oh, that’s terrible what can I do to help?	 	</text></passage><passage><infon key="file">sensors-20-00569-t004.xml</infon><infon key="id">sensors-20-00569-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>81867</offset><text>Different solutions provided by crowd workers to allivate the stressful thoughts of an actress.</text></passage><passage><infon key="file">sensors-20-00569-t004.xml</infon><infon key="id">sensors-20-00569-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Solutions&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Indicative Excerpt&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mindfulness&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: I just do mindfulness, where I’m no judgmentally aware of my thoughts and feelings&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Prayers&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Do prayer&lt;break/&gt;Robot: God is always with u&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Exercise&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Walking is good to clear your mind.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Having a dog&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: maybe get a dog&lt;break/&gt;Robot: Do you like dogs?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Socializing&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Sometimes talking to your family helps too. They can give you advice and love&lt;break/&gt;Robot: Maybe you could do some volunteer work in the area to try to meet some new people&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Playing sports&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: you can try playing sports as well&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sleeping aid&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: I’d definitely say sleep will help you a lot. You’ll be able to focus a lot more.&lt;break/&gt;Maybe try taking a nap during the day if you can find a time when your roommates are gone.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Taking a break from work&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Perhaps clearing some free time in your schedule to relax while going through this transition would help.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Voicing one’s thoughts aloud&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: voice your thoughts out loud.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Focusing on one thing&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: It sounds like you need to break down your bigger problems into smaller parts to begin with.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Using calming teas&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: I drink hot tea and think about my past. Robot: take some calming teas.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Using ear plug or white noise machine&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Have you tried ear plugs or a white noise machine?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Watching something interesting&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: watch something that you find funny or interesting&lt;break/&gt;Robot: watch YouTube&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Listen to music&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Another thing you could do is try and listen to music when stressed it is a great way to relax.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yoga&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Have you tried exercise or yoga?&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Miscellaneous&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Robot: Short walks or exercise, writing, meditation, watching a TV show, or talking to a friend.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>81963</offset><text>Solutions	Indicative Excerpt	 	Mindfulness	Robot: I just do mindfulness, where I’m no judgmentally aware of my thoughts and feelings	 	Prayers	Robot: Do prayerRobot: God is always with u	 	Exercise	Robot: Walking is good to clear your mind.	 	Having a dog	Robot: maybe get a dogRobot: Do you like dogs?	 	Socializing	Robot: Sometimes talking to your family helps too. They can give you advice and loveRobot: Maybe you could do some volunteer work in the area to try to meet some new people	 	Playing sports	Robot: you can try playing sports as well	 	Sleeping aid	Robot: I’d definitely say sleep will help you a lot. You’ll be able to focus a lot more.Maybe try taking a nap during the day if you can find a time when your roommates are gone.	 	Taking a break from work	Robot: Perhaps clearing some free time in your schedule to relax while going through this transition would help.	 	Voicing one’s thoughts aloud	Robot: voice your thoughts out loud.	 	Focusing on one thing	Robot: It sounds like you need to break down your bigger problems into smaller parts to begin with.	 	Using calming teas	Robot: I drink hot tea and think about my past. Robot: take some calming teas.	 	Using ear plug or white noise machine	Robot: Have you tried ear plugs or a white noise machine?	 	Watching something interesting	Robot: watch something that you find funny or interestingRobot: watch YouTube	 	Listen to music	Robot: Another thing you could do is try and listen to music when stressed it is a great way to relax.	 	Yoga	Robot: Have you tried exercise or yoga?	 	Miscellaneous	Robot: Short walks or exercise, writing, meditation, watching a TV show, or talking to a friend.	 	</text></passage><passage><infon key="file">sensors-20-00569-t005.xml</infon><infon key="id">sensors-20-00569-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>83648</offset><text>Mean latency and quality scores for all conditions.</text></passage><passage><infon key="file">sensors-20-00569-t005.xml</infon><infon key="id">sensors-20-00569-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Condition&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Latency&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Quality&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SD&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8.82&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.69&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.68&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.51&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.35&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.40&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.67&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.19&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>83700</offset><text>Condition	Latency	Quality	 	Mean	SD	Mean	Mean	 	1-worker	8.82	2.95	0.69	0.23	 	2-worker	6.79	2.25	0.68	0.27	 	4-worker	6.79	1.96	0.51	0.35	 	8-worker	4.12	0.40	0.67	0.19	 	</text></passage><passage><infon key="file">sensors-20-00569-t006.xml</infon><infon key="id">sensors-20-00569-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>83873</offset><text>Mean scores for five chosen categories calculated through linguistic inquiry and word count (LIWC) tool.</text></passage><passage><infon key="file">sensors-20-00569-t006.xml</infon><infon key="id">sensors-20-00569-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Condition&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WC&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WPS&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sixltr&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Posemo&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negemo&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm37&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8.5&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;4.4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm38&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.6&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;2.9&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm39&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12.0&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;3.1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm40&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;9.1&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;2.6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm41&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2.8&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;0.9&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm42&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;9.2&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;4.2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm43&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.8&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;2.8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm44&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;14.0&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;4.6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm45&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10.8&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;5.8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm46&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2.8&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;2.2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm47&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.3&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;1.2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm48&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;6.2&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;0.9&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm49&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12.1&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;5.3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm50&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;6.5&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;4.4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm51&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1.9&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;1.1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm52&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.6&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;1.7&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm53&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.0&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;1.1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm54&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15.2&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;5.2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm55&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7.6&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;2.9&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm56&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3.0&lt;/mml:mn&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;mml:mn&gt;1.8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>83978</offset><text>Condition	WC	WPS	Sixltr	Posemo	Negemo	 	1-worker						 	2-worker						 	4-worker						 	8-worker						 	</text></passage><passage><infon key="file">sensors-20-00569-t007.xml</infon><infon key="id">sensors-20-00569-t007</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>84083</offset><text>Average waiting time for eliciting multiple responses (first column), avg. responses per user query and maximum responses per condition (Max).</text></passage><passage><infon key="file">sensors-20-00569-t007.xml</infon><infon key="id">sensors-20-00569-t007</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Condition&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Avg. Waiting Time&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Avg. Responses/User Query&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Max.&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.88 ± 2.81&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.25 ± 0.17&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.60 ± 1.29&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.40 ± 0.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.56 ± 0.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.53 ± 0.20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>84226</offset><text>Condition	Avg. Waiting Time	Avg. Responses/User Query	Max.	 	2-worker	6.88 ± 2.81	1.25 ± 0.17	4	 	4-worker	6.60 ± 1.29	1.40 ± 0.23	6	 	8-worker	5.56 ± 0.71	1.53 ± 0.20	5	 	</text></passage><passage><infon key="file">sensors-20-00569-t008.xml</infon><infon key="id">sensors-20-00569-t008</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>84405</offset><text>Number of messages sent before and after 9 s.</text></passage><passage><infon key="file">sensors-20-00569-t008.xml</infon><infon key="id">sensors-20-00569-t008</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Condition&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Number of Workers&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&amp;lt;9 s&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;65&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;130&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&amp;gt;9 s&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;38&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>84451</offset><text>Condition	Number of Workers	 	1	2	4	8	 	&lt;9 s	65	88	79	130	 	&gt;9 s	38	31	30	14	 	</text></passage><passage><infon key="file">sensors-20-00569-t009.xml</infon><infon key="id">sensors-20-00569-t009</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>84531</offset><text>Summary of discontinuities in the crowd generated conversations.</text></passage><passage><infon key="file">sensors-20-00569-t009.xml</infon><infon key="id">sensors-20-00569-t009</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Condition&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean (SD)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.2 (2.9)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.0 (2.9)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.6 (4.3)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8-worker&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;36&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.2 (4.8)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>84596</offset><text>Condition	Total	Mean (SD)	 	1-worker	11	2.2 (2.9)	 	2-worker	15	3.0 (2.9)	 	4-worker	28	5.6 (4.3)	 	8-worker	36	7.2 (4.8)	 	</text></passage></document></collection>
