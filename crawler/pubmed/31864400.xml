<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201216</date><key>pmc.key</key><document><id>6925844</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1186/s13104-019-4858-z</infon><infon key="article-id_pmc">6925844</infon><infon key="article-id_pmid">31864400</infon><infon key="article-id_publisher-id">4858</infon><infon key="elocation-id">820</infon><infon key="kwd">Crowdsourcing Text highlighting Classification Question answering Extractive summarization</infon><infon key="license">Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</infon><infon key="name_0">surname:Ramírez;given-names:Jorge</infon><infon key="name_1">surname:Baez;given-names:Marcos</infon><infon key="name_2">surname:Casati;given-names:Fabio</infon><infon key="name_3">surname:Benatallah;given-names:Boualem</infon><infon key="name_4">surname:Casati;given-names:Fabio</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">12</infon><infon key="year">2019</infon><offset>0</offset><text>Crowdsourced dataset to study the generation and impact of text highlighting in classification tasks</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>101</offset><text>Objectives</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>112</offset><text>Text classification is a recurrent goal in machine learning projects and a typical task in crowdsourcing platforms. Hybrid approaches, leveraging crowdsourcing and machine learning, work better than either in isolation and help to reduce crowdsourcing costs. One way to mix crowd and machine efforts is to have algorithms highlight passages from texts and feed these to the crowd for classification. In this paper, we present a dataset to study text highlighting generation and its impact on document classification.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>629</offset><text>Data description</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>646</offset><text>The dataset was created through two series of experiments where we first asked workers to (i) classify documents according to a relevance question and to highlight parts of the text that supported their decision, and on a second phase, (ii) to assess document relevance but supported by text highlighting of varying quality (six human-generated and six machine-generated highlighting conditions). The dataset features documents from two application domains: systematic literature reviews and product reviews, three document sizes, and three relevance questions of different levels of difficulty. We expect this dataset of 27,711 individual judgments from 1851 workers to benefit not only this specific problem domain, but the larger class of classification problems where crowdsourced datasets with individual judgments are scarce.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1478</offset><text>Objective</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1488</offset><text>In this paper, we introduce datasets derived from multiple crowdsourcing experiments for document classification tasks. These experiments resemble a two-step pipeline that first highlights relevant passages and then classifies the documents. The datasets include the individual judgments provided by the workers for both steps of our pipeline, totaling 27,711 judgments from 1851 workers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1877</offset><text>Research has shown the feasibility of leveraging non-expert annotators in complex NLP tasks. Text classification, in particular, is a recurrent goal of machine learning (ML) projects, and a typical task in crowdsourcing platforms. Hybrid approaches, combining ML and crowd efforts, have been proposed to boost accuracy and reduce costs. One possibility is to use automatic techniques for highlighting relevant excerpts in the text and then ask workers to classify. And in doing so, workers could rely on the highlights, and avoid reading parts of the text, or ignore the highlighting and read the full text. In this context, we run crowdsourcing experiments to study the effects that text highlighting has on human performance in classification tasks. In these experiments, we focused on two crowdsourcing tasks: gathering the text highlights, and classification. The highlighting gathering task produced a dataset containing crowd-generated highlights that could serve, for example, researchers in studying automatic techniques such as text summarizers and question-answering models. The classification datasets could benefit researchers from the human computation community working on problems such as assessing and assuring quality, budget optimization, and worker behavior, as well as further investigating highlighting support.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>3210</offset><text>Data description</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3227</offset><text>In the following we described the crowdsourcing experiments that generated the dataset as well as the dataset structure.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>3348</offset><text>Task</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3353</offset><text>In our experiments, we asked workers to assess whether a document is relevant to a given question (predicate), augmenting the task design found in the literature. The documents come from two different domains systematic literature reviews (SLR) and amazon reviews. For the SLR domain, we considered two predicates “Does the paper describe a study that involves older adults (60+)?” (OA), and “Does the paper describe a study that involves technology for online social interactions?” (Tech). For Amazon reviews, we asked, “Is this review written on a book?” (AMZ).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3929</offset><text>All tasks were run in the crowdsourcing platform Figure Eight (https://www.figure-eight.com/). And personal information was not requested to workers; we only collected class labels and statistics related to effort.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>4144</offset><text>Gathering text highlights</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>4170</offset><text>Overview of data files/data sets</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Label&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Name of data file/data set&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;File types (file extension)&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Data repository and identifier (DOI or accession number)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;crowdsourced_highlights.csv: the dataset containing highlighted passages provided by workers from Figure Eight&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ML highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ml_highlights.csv: the dataset containing the highlighted passages produced by automatic techniques&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification OA crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_oa-crowd-highlights.csv: first dataset from Experiment 1. OA predicate using crowd-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification tech crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_tech-crowd-highlights.csv: second dataset from Experiment 1. Tech predicate using crowd-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification Amazon crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_amazon-crowd-highlights.csv: third dataset from Experiment 1. AMZ predicate using crowd-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification tech 3 × 12 crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_tech-3 × 12-crowd-highlights.csv: first dataset from Experiment 2. tech predicate using crowd-generated highlights. Layout 3 × 12&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification tech 6 × 6 crowd highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_tech-6 × 6-crowd-highlights.csv: second dataset from Experiment 2. tech predicate using crowd-generated highlights. layout 6 × 6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification OA ML highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_oa-ML-highlights.csv: first dataset from Experiment 3. OA predicate using machine-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification Tech ML highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_tech-ML-highlights.csv: second dataset from Experiment 3. Tech predicate using machine-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Classification Amazon ML highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;classification_amazon-ML-highlights.csv: third dataset from Experiment 3. AMZ predicate using machine-generated highlights&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Comma-separated values (.csv)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10.6084/m9.figshare.9917162.v4&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>4203</offset><text>Label	Name of data file/data set	File types (file extension)	Data repository and identifier (DOI or accession number)	 	Crowd highlights	crowdsourced_highlights.csv: the dataset containing highlighted passages provided by workers from Figure Eight	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	ML highlights	ml_highlights.csv: the dataset containing the highlighted passages produced by automatic techniques	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification OA crowd highlights	classification_oa-crowd-highlights.csv: first dataset from Experiment 1. OA predicate using crowd-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification tech crowd highlights	classification_tech-crowd-highlights.csv: second dataset from Experiment 1. Tech predicate using crowd-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification Amazon crowd highlights	classification_amazon-crowd-highlights.csv: third dataset from Experiment 1. AMZ predicate using crowd-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification tech 3 × 12 crowd highlights	classification_tech-3 × 12-crowd-highlights.csv: first dataset from Experiment 2. tech predicate using crowd-generated highlights. Layout 3 × 12	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification tech 6 × 6 crowd highlights	classification_tech-6 × 6-crowd-highlights.csv: second dataset from Experiment 2. tech predicate using crowd-generated highlights. layout 6 × 6	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification OA ML highlights	classification_oa-ML-highlights.csv: first dataset from Experiment 3. OA predicate using machine-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification Tech ML highlights	classification_tech-ML-highlights.csv: second dataset from Experiment 3. Tech predicate using machine-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	Classification Amazon ML highlights	classification_amazon-ML-highlights.csv: third dataset from Experiment 3. AMZ predicate using machine-generated highlights	Comma-separated values (.csv)	10.6084/m9.figshare.9917162.v4	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6546</offset><text>The first step is to generate highlights. This step serves as the basis of our study on text highlighting as an aid to workers in the classification tasks. We considered crowdsourcing and ML to generate the highlighted excerpts. For crowd-generated highlights, we asked workers to classify documents and to justify their decisions by highlighting passages from the text. For machine-generated highlights we used state-of-the-art extractive summarization and question-answering models. Two experts judged the quality of the highlights provided by the crowd and automatic techniques (Kappa was 0.87 for OA, 0.72 for Tech and 0.66 for AMZ). Table 1 shows the files containing the generated highlights (crowd and ML); both datasets include the individual highlights and associated quality. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>7334</offset><text>Classification with highlighting support</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>7375</offset><text>Experiment 1</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7388</offset><text>In this experiment, we asked workers to classify documents, giving additional support by highlighting passages from the text. Workers proceeded on pages of three documents each, up to six pages (3 × 6 layout). We categorized the available crowdsourced highlights according to their quality and derived six experimental conditions for our study. The baseline condition does not show any highlighted text. The 0%, 33%, 66% and 100% show highlights of varying quality. For example, on a page with three documents, the 33% condition shows one high-quality highlight and two low-quality ones. Finally, the aggregation condition combines multiple highlights similar to aggregating votes in crowdsourcing tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>8098</offset><text>Experiment 2</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8111</offset><text>This experiment focused on longer documents and pages, using 6 × 6 and 3 × 12 layouts and crowd-generated highlights. We keep the baseline as one experimental condition, and we introduce the 83% quality as the other.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>8338</offset><text>Experiment 3</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8351</offset><text>This experiment used machine-generated highlights, using a 3 × 6 layout and six experimental conditions: BertSum, Refresh, Bert-QA, AggrML, 100%ML, baseline. BertSum and Refresh, are extractive summarization techniques, while Bert-QA is a question-answering model. AggrML aggregates the output from the three algorithms, and 100%ML only uses machine-generated highlighting assessed by experts as being of good quality.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8775</offset><text>We encourage readers to check for a more in-depth explanation of the experimental settings. Table 1 overviews the available datasets derived from our experiments.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>8939</offset><text>Limitations</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>8951</offset><text>The dataset described in this paper features a set of dimensions that allow for an exploration of approaches, but that cannot be considered comprehensive. The dataset is still limited to two types of classification tasks, includes only the most widely used state-of-the-art algorithms for highlight generation, and relies on two task designs for crowd classification. Besides, the experiments with longer pages and documents (Experiment 2) are extensions of the first experiment and focus only on one relevance question.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>9472</offset><text>These alternatives have been carefully selected, but more systematic studies will require a more in-depth investigation of each of these dimensions.</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">title</infon><offset>9621</offset><text>Abbreviations</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9635</offset><text>ML</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9638</offset><text>machine learning</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9655</offset><text>SLR</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9659</offset><text>systematic literature reviews</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9689</offset><text>OA</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9692</offset><text>relevance question: “Does the paper describe a study that involves older adults (60+)?”</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9784</offset><text>Tech</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9789</offset><text>relevance question: “Does the paper describe a study that involves technology for online social interactions?”</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9904</offset><text>AMZ</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">paragraph</infon><offset>9908</offset><text>relevance question: “Is this review written on a book?”</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>9968</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">ABBR</infon><infon key="type">footnote</infon><offset>9985</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>10104</offset><text>Authors’ contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>10129</offset><text>All authors contributed to the conceptualization of the work and research questions. JR and MB prepared the experimental design. JR and MB designed the crowdsourcing tasks, which were revised by FC and BB. All authors provided input on strategies to avoid bias. JR and MB provided expert annotations (gold labels) and qualitative analysis of crowd contributions. JR prepared the first version of the draft. All authors prepared the final version of the manuscript. All authors read and approved the final manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>10646</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>10654</offset><text>The study was supported by the Russian Science Foundation (Project No. 19-18-00282).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>10739</offset><text>Availability of data and materials</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10774</offset><text>The data described in this Data note can be freely and openly accessed on Figshare. Please see Table 1 and reference list for details and links to the data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>10932</offset><text>Ethics approval and consent to participate</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10975</offset><text>Not applicable.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title</infon><offset>10991</offset><text>Consent for publication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11015</offset><text>Not applicable.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>11031</offset><text>Competing interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>11051</offset><text>The authors declare that they have no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>11110</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>11121</offset><text>Snow R, O’Connor B, Jurafsky D, Ng AY. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In: 2008 conference on empirical methods in natural language processing, EMNLP 2008, proceedings of the conference, 25–27 October 2008, Honolulu, Hawaii, USA, a meeting of SIGDAT, a special interest group of the ACL. 2008. p. 254–63. http://www.aclweb.org/anthology/D08-1027.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>11537</offset><text>Cheng J, Bernstein MS. Flock: hybrid crowd-machine learning classifiers. In: CSCW 2015. 2015. 10.1145/2675133.2675214.</text></passage><passage><infon key="name_0">surname:Wallace;given-names:BC</infon><infon key="name_1">surname:Noel-Storr;given-names:A</infon><infon key="name_2">surname:Marshall;given-names:IJ</infon><infon key="name_3">surname:Cohen;given-names:AM</infon><infon key="name_4">surname:Smalheiser;given-names:NR</infon><infon key="name_5">surname:Thomas;given-names:J</infon><infon key="pub-id_doi">10.1093/jamia/ocx053</infon><infon key="pub-id_pmid">28541493</infon><infon key="section_type">REF</infon><infon key="source">JAMIA</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>11656</offset><text>Identifying reports of randomized controlled trials (RCTs) via a hybrid machine learning and crowdsourcing approach</text></passage><passage><infon key="fpage">1</infon><infon key="issue">CSCW</infon><infon key="lpage">18</infon><infon key="name_0">surname:Krivosheev;given-names:Evgeny</infon><infon key="name_1">surname:Casati;given-names:Fabio</infon><infon key="name_2">surname:Baez;given-names:Marcos</infon><infon key="name_3">surname:Benatallah;given-names:Boualem</infon><infon key="pub-id_doi">10.1145/3274366</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM on Human-Computer Interaction</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2018</infon><offset>11772</offset><text>Combining Crowd and Machines for Multi-predicate Item Screening</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>11836</offset><text>Ramírez J, Baez M, Casati F, Benatallah B. Understanding the impact of text highlighting in crowdsourcing tasks. In: Proceedings of the seventh AAAI conference on human computation and crowdsourcing, HCOMP 2019, vol. 7. AAAI. 2019. p. 144–52.</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">7</infon><infon key="name_0">surname:Daniel;given-names:F</infon><infon key="name_1">surname:Kucherbaev;given-names:P</infon><infon key="name_2">surname:Cappiello;given-names:C</infon><infon key="name_3">surname:Benatallah;given-names:B</infon><infon key="name_4">surname:Allahbakhsh;given-names:M</infon><infon key="pub-id_doi">10.1145/3148148</infon><infon key="section_type">REF</infon><infon key="source">ACM Comput Surv</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2018</infon><offset>12082</offset><text>Quality control in crowdsourcing: a survey of quality attributes, assessment techniques, and assurance actions</text></passage><passage><infon key="fpage">52</infon><infon key="lpage">85</infon><infon key="name_0">surname:Dai;given-names:P</infon><infon key="name_1">surname:Lin;given-names:CH</infon><infon key="name_2">surname:Weld;given-names:DS</infon><infon key="pub-id_doi">10.1016/j.artint.2013.06.002</infon><infon key="section_type">REF</infon><infon key="source">Artif Intell</infon><infon key="type">ref</infon><infon key="volume">202</infon><infon key="year">2013</infon><offset>12193</offset><text>POMDP-based control of workflows for crowdsourcing</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>12244</offset><text>Lan D, Reed K, Shin A, Trushkowsky B. Dynamic filter: adaptive query processing with the crowd. In: Proceedings of the fifth AAAI conference on human computation and crowdsourcing, HCOMP 2017, 23–26 October 2017, Quebec City, Quebec, Canada. 2017. p. 118–27. https://aaai.org/ocs/index.php/HCOMP/HCOMP17/paper/view/15932.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>12570</offset><text>Alagarai Sampath H, Rajeshuni R, Indurkhya B.. Cognitively inspired task design to improve user performance on crowd-sourcing platforms. In: CHI conference on human factors in computing systems, CHI’14, Toronto, ON, Canada—April 26–May 01, 2014. 2014. p. 3665–374. 10.1145/2556288.2557155.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>12868</offset><text>Krivosheev E, Casati F, Caforio V, Benatallah B. Crowdsourcing paper screening in systematic literature reviews. In: Proceedings of the fifth AAAI conference on human computation and crowdsourcing, HCOMP 2017, 23–26 October 2017, Quebec City, Quebec, Canada. 2017. p. 108–17</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>13147</offset><text>Ramirez J, Krivosheev E, Baez M, Casati F, Benatallah B. Crowdrev: a platform for crowd-based screening of literature reviews. In: Collective intelligence, CI 2018. 2018.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>13318</offset><text>Liu Y. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318. 2019.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>13409</offset><text>Narayan S, Cohen SB, Lapata M. Ranking sentences for extractive summarization with reinforcement learning. In: NAACL 2018.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>13532</offset><text>Devlin J, Chang MW, Lee K, Toutanova K. Bert. Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018.</text></passage><passage><infon key="name_0">surname:Ramírez;given-names:J</infon><infon key="name_1">surname:Baez;given-names:M</infon><infon key="name_2">surname:Casati;given-names:F</infon><infon key="name_3">surname:Benatallah;given-names:B</infon><infon key="pub-id_doi">10.6084/m9.figshare.9917162.v4</infon><infon key="section_type">REF</infon><infon key="source">Figshare</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>13693</offset></passage></document></collection>
