<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220630</date><key>pmc.key</key><document><id>9068224</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1007/s00779-022-01684-y</infon><infon key="article-id_pmc">9068224</infon><infon key="article-id_pmid">35528273</infon><infon key="article-id_publisher-id">1684</infon><infon key="fpage">1</infon><infon key="kwd">Human-robot interaction Affective computing Telepresence Neural networks Crowdsourcing</infon><infon key="license">This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</infon><infon key="lpage">17</infon><infon key="name_0">surname:Suguitan;given-names:Michael</infon><infon key="name_1">surname:Hoffman;given-names:Guy</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="year">2022</infon><offset>0</offset><text>What is it like to be a bot? Variable perspective embodied telepresence for crowdsourcing robot movements</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>106</offset><text>Movement and embodiment are communicative affordances central to social robotics, but designing embodied movements for robots often requires extensive knowledge of both robotics and movement theory. More accessible methods such as learning from demonstration often rely on physical access to the robot which is usually limited to research settings. Machine learning (ML) algorithms can complement hand-crafted or learned movements by generating new behaviors, but this requires large and diverse training datasets, which are hard to come by. In this work, we propose an embodied telepresence system for remotely crowdsourcing emotive robot movement samples that can serve as ML training data. Remote users control the robot through the internet using the motion sensors in their smartphones and view the movement either from a first-person or a third-person perspective. We evaluated the system in an online study where users created emotive movements for the robot and rated their experience. We then utilized the user-crafted movements as inputs to a neural network to generate new movements. We found that users strongly preferred the third-person perspective and that the ML-generated movements are largely comparable to the user-crafted movements. This work supports the usability of telepresence robots as a movement crowdsourcing platform.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1453</offset><text>Introduction</text></passage><passage><infon key="file">779_2022_1684_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>1466</offset><text>Roboticists can complement their initially small set of hand-crafted movements by crowdsourcing new samples from users. Machine learning techniques can then further expand the available movements by generating new samples. This work focuses on the crowdsourcing and generation aspects</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1751</offset><text>Duffy et al. define a “social robot” as “a physical entity embodied in a complex, dynamic, and social environment sufficiently empowered to behave in a manner conducive to its own goals and those of its community”. Social robots can communicate through their embodiment and movements, which serve to not only achieve utilitarian functions but also to convey affective states . Movement is an important nonverbal communication modality that differentiates robots from graphics- or voice-based agents. However, designing robot movements is often a costly process that requires expertise in robotics and movement theory. Accessible methods such as learning from demonstration (LfD) enable lay-users to provide movement samples by either physically manipulating the robot or controlling its degrees-of-freedom (DoFs) . In some cases, larger sample libraries can be elicited using crowdsourcing methods . Movement libraries, whether hand-generated, crowdsourced, or learned, can be further expanded with generative models that analyze existing samples and synthesize new realistic movements  (Fig. 1). For example, deep neural networks can learn important data features given a sufficient diversity of samples, thus relaxing the need for expert knowledge in movement generation . As a result, human-robot interaction (HRI) researchers have begun applying neural networks for generating robot movements , but these approaches are limited by the availability of data. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3228</offset><text>Restrictions on in-person experiments due to the COVID-19 pandemic forced HRI researchers to shift towards remote technologies, such as simulators or telepresence robots, and this shift could prove beneficial for robot movement generation. Researchers have also used these remote technologies to conduct online evaluations and crowdsource data. Services such as Amazon Mechanical Turk and Prolific have enabled the collection of data from a diverse user base. Paired with telepresence platforms, crowdsourcing could also enable the collection of user-crafted demonstrations for robots. A machine learning model could then use the collected data to generate new samples and further expand the robot’s behavior library. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3949</offset><text>An accessible system for remotely motion controlling a robot in either the first- or third-person, requiring no specialized hardware.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4083</offset><text>An evaluation of the system as an embodied telepresence platform. We conducted a remote study for users to control the robot, create emotive movements, and rate their experience using the platform comparing the first- and third-person views.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4325</offset><text>An evaluation of the quality of the user-crafted movements as a data set for ML generation, first by using a generative neural network to synthesize new movement samples, then by deploying a survey to compare the user-crafted and generated samples.</text></passage><passage><infon key="file">779_2022_1684_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>4574</offset><text>In the first-person view (1PV, left) the camera feed is transmitted from the local robot to the remote phone. In the third-person view (3PV, right) the local computer camera feed capturing the robot is transmitted to the desktop. In both cases, the remote phone’s motion data is transmitted to the local computer to control the robot’s motors</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4921</offset><text>In this work, we present a system for remotely crowdsourcing emotive robot movements through a telepresence robot. The robot is controlled with a smartphone, a widely accessible device that enables a direct mapping from the user’s body to the robot using the phone’s built-in motion sensors. We compared two alternate viewpoints for the interface: a through-the-robot first-person view (1PV) seen on the phone, and a whole-body third-person view (3PV) seen on an external monitor (Fig. 2). We performed an evaluation where users controlled the robot and recorded emotive movements to collect a diverse user-crafted data set. To validate the usability of the collected data set for ML movement generation, we trained a neural network to generate new movements, and deployed a survey to subjectively compare the user-crafted and generated movements. Our contributions are:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5796</offset><text>Related work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5809</offset><text>In this section we review related works in affective telepresence, teleoperation control methods, and crowdsourcing for robot learning.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>5945</offset><text>Affective telepresence</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5968</offset><text>The physicality of objects can promote nonverbal and ludic interactions beyond the affordances of visual or auditory communication modalities. Strong and Gaver’s Feather, Scent, and Shaker were minimally expressive home objects for technologically mediated sociality between remote users . More specifically within robotics, Goldberg’s early telepresence robots emphasized playful interactions, such as tending to a garden or uncovering treasures in a sandbox by remotely controlling a robot through the internet . Sirkin and Ju found that augmenting a screen-based telepresence robot with motion improved the sense of presence on both ends . Tanaka et al. compared video, avatar, and robot communications and found that the presence and movements of a robot improved the conversation partner’s sense of social presence . The teddy bear Huggable robot enabled remote users to control its gaze and appendages through a web interface . Gomez et al. used the Haru robot for transmitting “robomojis,” emojis that are embodied by the robot’s motion, animations, and sounds . The MeBot telepresence robot features controllable appendages in addition to a screen displaying the remote user . Similarly, Tsoi et al. created a phone application to turn the Anki Vector robot into a telepresence platform controlled with game-like touchscreen joysticks; this work was a direct response to the sudden isolation of children due to COVID-19 safety restrictions . While these embodied platforms afford an additional dimension of engagement beyond virtual agents, most use button- or joystick-centric controllers that abstract remote users away from their own bodies as a communicative medium. Employing the user’s own embodied movement is possible through motion control.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>7748</offset><text>Motion control</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7763</offset><text>Rather than use text inputs or game controllers as proxies for controlling robots, proprioceptive motion controls afford a more direct translation between the embodiments of the user and robot, enhancing the sense of self-location and agency . Ainasoja et al. compared motion- and touch-based smartphone interfaces for controlling a Beam telepresence robot, and found that users preferred a hybrid motion-touch interface (motion for left-right steering, touch for forward-reverse) . Jonggil et al. compared touch and motion controls for a mobile camera robot, and found that motion controls improved the user’s sense of presence, synchronicity, and understanding of the remote space . In a more affective application, Sakashita et al. used a virtual reality system with head and arm tracking to remotely embody and puppeteer robots . Many of these robots were utilitarian in design and function, require specialized hardware, and the user perspectives were constrained to first-person views.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>8761</offset><text>Viewpoint control</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8779</offset><text>In traditional video chat applications, the remote user’s view is controllable only by their interaction partner. Müller et al. created a panoramic stitching application to enable remote users to freely adjust their view by panning their phone around the environment, and found that this significantly improved measures of spatial and social presence and slightly improved copresence . Tang et al. extended this work by replacing the panoramic stitching with a 360° camera . They recommended improvement to collocation, such as indicators to dictate gaze direction or ways to convey remote gestures. Young et al. combined the panoramic stitching and 360° camera into a single evaluation while also adding the user’s hand into the shared view as a gesture indicator, and found that both implementations increased spatial presence and copresence . Free choice between first- and third-person is a common interface setting in video games, and several works have shown that first-person perspectives increase immersion and the sense of body ownership while third-person offers heightened spatial awareness . To our knowledge, viewpoint effects on experiential factors of telerobotics operation have not been thoroughly explored.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>10017</offset><text>Crowdsourcing demonstrations for robots</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10057</offset><text>Robotic systems can implement LfD systems that enable lay-users to provide high-fidelity data for machine learning models. However, collecting demonstrations is still time-consuming and often constrained by physical proximity to a robot. Mandlekar et al. created a system for remotely crowdsourcing grasping task demonstrations for simulated and physical robot arms, and found that more data improves model performance . Among various input devices ranging from mice to virtual reality controllers, they found smartphones to be the best compromise of accessibility and functionality. The primary performance metric was grasp success, with completion time as a secondary measure. Timing is an important feature for affective expression, specifically the arousal dimension on the circumplex model of emotions . Rakita et al. found that while users could adapt to a teleoperated robot’s physical slowness, latency between the user’s movement and the robot executing the motion reduced performance, further emphasizing the importance of timing .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11106</offset><text>Research questions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11125</offset><text>Would affective telepresence be better achieved with a first- or third-person perspective?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11216</offset><text>Are crowdsourcing movement demonstrations and generative neural networks viable methods for expanding a robot’s behavior library?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11348</offset><text>There are several gaps in existing works. Prior works focused primarily on the usability of different control methods, but were either constrained to first-person perspectives or designed for utilitarian, nonaffective functions. Alternatively, we are interested in fixing the control input and instead varying the viewpoints. Although prior works measured subjective experiential responses from the users as both operators and interactors with the robot, many did not focus on the affective quality of the robot’s movements. Additionally, there are few prior works in enabling remote crowdsourcing of robot movement demonstrations. We address these gaps by designing a robot telepresence system with accessible motion controls and variable viewpoints. We perform user evaluations to assess the subjective usability of the system for creating emotive robot movements. We then use the movements to train a neural network to generate new movement samples, and perform another evaluation to compare the user-crafted and generated movements. This work probes the following research questions:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>12438</offset><text>Technical implementation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12463</offset><text>In this section we detail the technical implementation of the system, including the robot and user interfaces.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12574</offset><text>Robot</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12580</offset><text>We used the Blossom robot, an open-source social robot (Fig. 2, left) . Blossom’s internal mechanisms consist of a head platform suspended from a tower structure that rotates about its base platform. Blossom features four DoFs: yaw, pitch, roll, and vertical translation, though we disable vertical translation to simplify the control interface. The robot achieves motion with four actuators: tower motors 1, 2, and 3 control the front, left, and right sides of the head, respectively, and a motor in the base rotates the tower left and right. The robot’s head can pitch up and down and roll left and right  and can yaw  left and right about its base. Although the robot’s DoFs are limited compared to more complex embodiments, it features a large range of motion (RoM) and head movements alone can convey complex affective information . For 1PV, we embedded a small USB camera inside the robot’s head, in front of one of its ears. The camera has a wide-angle lens (21 mm equivalent,  diagonal angle of view) to maximize the viewing range.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13630</offset><text>User interfaces</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13646</offset><text>To bolster the system’s accessibility, we built the application as a mobile browser experience instead of creating a standalone application. This enabled us to iterate quickly and access a rich library of functionality through APIs while obviating the need for external downloads on the user’s device. We created two interfaces to accommodate the two viewpoints (Fig. 2, right): a mobile interface showing 1PV from the camera in the robot’s head, and a desktop interface showing 3PV from the host computer’s webcam. Users access both interfaces from a public URL. 1PV for the mobile interface acts as a “window” through the robot; 3PV for the desktop interface acts as a “mirror” at the robot.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>14356</offset><text>Mobile interface</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14373</offset><text>The mobile interface consists of a video feed showing 1PV and a simple layout of buttons for controlling the robot (Fig. 2, center). The layout was inspired by existing controlling and recording interfaces, such as camera applications and voice recorders. Control of the robot is toggled with a slider switch. Users can record and save movements with a large microphone-style recording button. The robot can be reoriented using a calibration button; this resets the robot’s yaw orientation relative to the phone’s current compass heading, setting it to face towards the external camera. If the user rotates to the endpoints of the base RoM, indicator arrows appear on the interface to direct the user back towards the center.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>15103</offset><text>Desktop interface</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15121</offset><text>The desktop interface consists of a video screen showing 3PV (Fig. 2, right). The mobile interface still controls the robot, but 1PV is hidden to force users to look at the robot instead of through the robot. For the evaluation (described later in Section 4.1), the interface also features a YouTube video player, controls for displaying a video from a given URL, and a Qualtrics survey at the bottom of the page.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15536</offset><text>Back end</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>15545</offset><text>Communication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15559</offset><text>The robot is connected to the host computer, which also serves the interfaces. We use ngrok to enable communication across the internet from the user to the host computer and robot1. We open two ngrok tunnels: one for accessing the user interfaces, and another for transmitting the phone orientation data to motion control the robot.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>15893</offset><text>Motion control</text></passage><passage><infon key="file">779_2022_1684_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15908</offset><text>The alignment of the robot and phone reference frames when controlling in 1PV. In 3PV, the motion is mirrored to accommodate the perspective of looking straight at the robot (e.g., motion towards the phone’s left moves the robot to its right)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16153</offset><text>Kinematic models of the phone and robot translate the phone’s orientation into the angular poses of the robot’s head (Fig. 3). The mobile interface uses the DeviceOrientation API to report motion events2. The phone’s inertial measurement unit (IMU) records its pose as Tait-Bryan angles about the phone’s reference frame. In 1PV, the phone and robot axes are aligned as if the phone’s camera were looking through the robot’s eyes. When switching from 1PV to 3PV, the motion is mirrored horizontally to accommodate the front-facing view of the robot, as if the user were facing a physical mirror. In 3PV, yawing or rolling the phone to the left from the user’s perspective moves the robot to its right, and vice versa. Assuming a stable connection, motion data is transferred at a rate of approximately 10 Hz.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16977</offset><text>Video</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16983</offset><text>For the video streams, we use WebRTC, the standard for online audiovisual communication3. WebRTC manages the handshaking for broadcasting the local video stream to remote viewers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>17163</offset><text>Experiments</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17175</offset><text>In this section we detail the evaluations of the interface and the crowdsourced dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17264</offset><text>Interface evaluation</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>17285</offset><text>Interface evaluation survey questions, displayed after every video and again at the end of the survey to compare 1PV and 3PV. Note: scales for mental and physical tiredness are reversed from how they were displayed in the evaluation (1 = not tiring, 7 = tiring) to better match the other factors</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Question&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;1 (low rating)&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;7 (high rating)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How synchronized with the robot did you feel?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unsynchronized&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Synchronized&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How much did you feel present in the remote location?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Separate&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Present&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How easy was controlling the robot?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Difficult&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Easy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How enjoyable was controlling the robot?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Not enjoyable&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Enjoyable&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How engaging was controlling the robot?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Not engaging&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Engaging&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How mentally tiring was controlling the robot?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Tiring&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Not tiring&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How physically tiring was controlling the robot?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Tiring&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Not tiring&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;How do you feel about the quality of the movement you created?&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Low quality&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;High quality&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>17581</offset><text>Question	1 (low rating)	7 (high rating)	 	How synchronized with the robot did you feel?	Unsynchronized	Synchronized	 	How much did you feel present in the remote location?	Separate	Present	 	How easy was controlling the robot?	Difficult	Easy	 	How enjoyable was controlling the robot?	Not enjoyable	Enjoyable	 	How engaging was controlling the robot?	Not engaging	Engaging	 	How mentally tiring was controlling the robot?	Tiring	Not tiring	 	How physically tiring was controlling the robot?	Tiring	Not tiring	 	How do you feel about the quality of the movement you created?	Low quality	High quality	 	</text></passage><passage><infon key="file">779_2022_1684_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18183</offset><text>Evaluation setup showing the fields of view of 1PV (yellow) and 3PV (green). The evaluation proctor (right) acts as a focal point when controlling the robot in 1PV</text></passage><passage><infon key="file">779_2022_1684_Fig5_HTML.jpg</infon><infon key="id">Fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18347</offset><text>Interface evaluation flow. Users first access the interfaces and test the robot’s motion. In the main movement creation task, users watch videos of cartoon characters emoting, then create movements for the robot corresponding to the conveyed emotions (happy, sad, or angry). The evaluation concludes with a comparative assessment of the perspectives for user experience factors and overall preferences</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18751</offset><text>We measured the usability of the system and compared 1PV and 3PV through an online user evaluation for a movement creation task (Figs. 4, 5). We recruited participants through the Prolific online survey platform4. Apart from limiting enrollment to users within the United States (for latency concerns), we did not record any demographic information. We first instructed the user to navigate to the interfaces on both their phone and desktop. The user connected to the robot and tested the controller by looking around the environment in 1PV, then in 3PV. Only one viewpoint (1PV on the phone, 3PV on the desktop) is visible at a time. Because of the importance of timing for the task, we measured the latency between when the orientation data packet is sent from the user’s phone and when it is received by the robot’s host computer. This latency is only “one-way” and is exacerbated by the video latency, so the user will experience a longer delay from their perspective. Latency below 100 ms is very good and around 1,000 ms (1 s) is serviceable, but exceeding 2,000 ms (2 s) noticeably degrades usability. If the user’s latency exceeded the 2-s threshold, we would end the study prematurely and compensate the user proportionally to their time spent.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20017</offset><text>For the main movement creation task, we had the users record examples of emotive gestures. We prompted the users with short videos, between 5 and 10 s in length, of cartoon characters (either SpongeBob, Pikachu, or Homer Simpson) displaying either happiness, sadness, or anger. We then had the users control the robot to express the emotion from the video and record the movement. We urged users to not simply mimic the motion of the characters, but rather to move the robot as if it were conveying the overall emotion from the scene. Users could rehearse and re-record the movements until they were satisfied, but could not redo the movement once they moved on to the next video. We introduced two trial videos to acclimate the user to the task, followed by nine actual videos (three emotions for each of three different characters). To account for learning effects, we randomized the video orders and perspectives so that each would be equally represented (e.g., four 1PV and five 3PV, or vice versa). We measured the latency during recording for post-analysis of its effect on the user experience.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21118</offset><text>H1.1</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21123</offset><text>1PV will increase the sense of synchronization with the robot due to a heightened sense of embodiment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21226</offset><text>H1.2</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21231</offset><text>1PV will increase the sense of presence in the remote location due to higher immersion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21319</offset><text>H1.3</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21324</offset><text>3PV will be easier to use due to heightened spatial awareness.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21387</offset><text>H1.4</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21392</offset><text>1PV will be more enjoyable due to being a unique experience.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21453</offset><text>H1.5</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21458</offset><text>1PV will be more engaging due to having to move around in one’s physical space.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21540</offset><text>H1.6</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21545</offset><text>1PV will be more mentally tiring due to having to embody a remote system with latency.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21632</offset><text>H1.7</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21637</offset><text>1PV will be more physically tiring due to having to move one’s whole body to maintain a view of the video.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21746</offset><text>H1.8</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21751</offset><text>3PV will increase the self-reported quality of created movements due to being able to see the full robot.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21857</offset><text>We used surveys throughout and after the evaluation to collect user-reported metrics. After each video, we asked for subjective 7-point Likert scale responses to measure experiential factors (Table 1). After all of the videos, we again asked for Likert scale responses for each factor, but asked for comparative responses for both 1PV and 3PV. We also asked for overall preferences between the perspectives and included a free response field for any additional feedback. Due to the limited expressiveness of the robot platform, we expected differences across the different emotion classes (e.g., sadness will be more homogeneous but easier to convey than anger). We preregistered hypotheses regarding the experiential factors5: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22587</offset><text>We enrolled 30 participants through the Prolific platform and offered US $10 as compensation. We prescreened by participants with access to both a mobile device and desktop. In the interest of minimizing latency, we restricted enrollment to participants living in the United States. We proctored the evaluation through an audio-only Zoom call and took approximately 30 min to complete: 10 min for the introduction and 20 min for creating the movements. We occasionally encountered incompatibilities with certain Android devices, often stemming from access permissions for the orientation sensor. In cases where we were unable to troubleshoot the problem, we ended the study prematurely and compensated the participants proportionally to their time spent; this led us to eventually prescreen to users with Apple devices. We did not have to reject any participants on the basis of high latency.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23480</offset><text>Movement kinematic evaluation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23510</offset><text>H2.1</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23515</offset><text>1PV will yield longer movements due to having to move around in one’s physical space.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23603</offset><text>H2.2</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23608</offset><text>3PV will yield faster movements due to requiring less full-body motions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23681</offset><text>H2.3</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23686</offset><text>3PV will yield wider, more exaggerated movements due to requiring less full-body motions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23776</offset><text>We calculated kinematic features for each movement: length, speed, and range. Length is the overall duration of the movement, measured in seconds. Because there may have been delays between when the user pressed the record button and actually began or stopped moving, we trimmed the “whitespace” of no motion at the beginning and end of each movement. Speed is the angular velocity of the motors, measured in radians per second. Range is the wideness of the motion in each DoF, measured in radians. We averaged the speed and range across all DoFs for the entire movement. We preregistered hypotheses for the movement features: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24408</offset><text>Dataset evaluation</text></passage><passage><infon key="file">779_2022_1684_Fig6_HTML.jpg</infon><infon key="id">Fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24427</offset><text>Neural network architecture for generating movements. The user-crafted movements (4.8 s at 10 Hz with four DoFs ) are used as inputs and encoded into a 36D embedding space (left). The embeddings are both decoded to reconstruct the original input (bottom path) and classified into one of the three emotion classes (happy, sad, or angry) (top path)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24774</offset><text>To appraise the validity and usability of the system as a data collection platform, we used the user-crafted movements to train a neural network to generate new movements. The network architecture consists of a convolutional variational autoencoder (VAE) with an additional emotion classifier (Fig. 6) . The VAE encodes the movement samples into a compressed lower-dimension latent embedding space (Fig. 6, left), then decodes these embeddings back into a reconstruction of the original samples (Fig. 6, bottom path). The classifier operates on the embeddings and separates the latent space by emotions (happy, sad, or angry) (Fig. 6, top path). We split the collected dataset by perspective (1PV and 3PV) and trained the network with identical parameters on both subsets. The technical results can be objectively evaluated in terms of the network training metrics, quality of the movement reconstructions, and separability of the emotion classes in the latent embedding space.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25757</offset><text>H3.1</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25762</offset><text>The generated movements will be as realistic as the user-crafted movements.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25838</offset><text>H3.2</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25843</offset><text>The generated movements will be as emotive as the user-crafted movements.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25917</offset><text>H3.3</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25922</offset><text>The generated movements will be recognized with the same accuracy as the user-crafted movements.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26019</offset><text>We compared the user-crafted and generated movements in a survey to appraise realism, emotiveness, and emotional legibility. We recorded the robot performing the movements from an external perspective similar to 3PV in the first evaluation, and thus used only movements created or generated with the 3PV dataset. We randomly selected subsets of user-crafted movements from a held-out test set and generated movements from the neural network. To avoid using several similar or static movements, we further manually curated the movements to four diverse and representative examples for each condition, resulting in a set of 24 movements (3 emotions  [User, Generated]  4 examples). Users watched the movements and gave ratings for realism, emotiveness, and which emotion was conveyed (Table 2). We preregistered hypotheses for the movement comparison: </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>26871</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26879</offset><text>Interface evaluation results</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>26908</offset><text>Movement comparison survey questions for comparing the user-crafted and generated movements</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;1 (low rating)&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;7 (high rating)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Fake&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Natural&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Emotionless&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Emotional&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Please select the emotion&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Happy, Sad, or Angry&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;that best describes the&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;robot’s movement&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27000</offset><text>1 (low rating)	7 (high rating)	 	Fake	Natural	 	Emotionless	Emotional	 	Please select the emotion	Happy, Sad, or Angry	 	that best describes the		 	robot’s movement		 	</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27171</offset><text>p-values of H1 tested with two-sided t-tests within each emotion, calculated from the average of the scores after each video. Slight support is suggested only for sadness being more physically tiring in 1PV</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Factor&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Happy&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Sad&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Anger&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sync (1PV&amp;gt;3PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.706&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.995&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.681&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Presence (1PV&amp;gt;3PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.365&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.667&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.911&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Ease (3PV&amp;gt;1PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.428&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.665&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.430&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Enjoyment (1PV&amp;gt;3PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.750&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.637&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.558&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Engagement (1PV&amp;gt;3PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.881&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.382&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.630&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Mental tired (3PV&amp;gt;1PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.567&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.960&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.619&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Physical tired (3PV&amp;gt;1PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.938&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.088&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.718&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Quality (3PV&amp;gt;1PV)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.908&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.744&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.609&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27378</offset><text>Factor	Happy	Sad	Anger	 	Sync (1PV&gt;3PV)	0.706	0.995	0.681	 	Presence (1PV&gt;3PV)	0.365	0.667	0.911	 	Ease (3PV&gt;1PV)	0.428	0.665	0.430	 	Enjoyment (1PV&gt;3PV)	0.750	0.637	0.558	 	Engagement (1PV&gt;3PV)	0.881	0.382	0.630	 	Mental tired (3PV&gt;1PV)	0.567	0.960	0.619	 	Physical tired (3PV&gt;1PV)	0.938	0.088	0.718	 	Quality (3PV&gt;1PV)	0.908	0.744	0.609	 	</text></passage><passage><infon key="file">779_2022_1684_Fig7_HTML.jpg</infon><infon key="id">Fig7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27720</offset><text>Likert scale responses from the interface evaluation end-survey questions. Color indicates level: blue = 1 (low), gray = 4 (neutral), red = 7 (high). Width indicates proportion of responses for a given level. Black bars indicate means and standard deviations. p-values of H1 tested with two-sided t-tests are displayed on the right, and the means indicate preferences for 3PV in all factors except presence andtiredness. Note: as in Table 2, the scales for mental and physical tiredness are reversed from what was displayed in the survey</text></passage><passage><infon key="file">779_2022_1684_Fig8_HTML.jpg</infon><infon key="id">Fig8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28258</offset><text>Overall preferences reported at the end of the evaluation, showing strong preferences for 3PV</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28352</offset><text>We used two-sided t-tests to test H1 from the end-survey Likert scale responses, and found that many results were significant in the opposite direction of our hypotheses favoring 1PV (Fig. 7). H1.3 and H1.8 were supported in the hypothesized direction. H1.1, H1.4, and H1.5 were supported opposite of the hypothesized direction. We found overwhelming preference for 3PV, with significant results in synchronization, ease, enjoyment, engagement, and quality. Even increased presence, which we assumed would be decisively in favor of 1PV, is not supported. We also tested the hypotheses within each emotion class using the responses after every video, and only found slight support for sadness being more physically tiring in 1PV (Table 3). Interestingly, the within-emotion scores do not correlate with the comparative end-survey scores. The overall preferences are also favorable toward 3PV (Fig. 8). </text></passage><passage><infon key="file">779_2022_1684_Fig9_HTML.jpg</infon><infon key="id">Fig9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29257</offset><text>Interface evaluation scores versus latency for each user for each perspective. The horizontal axes are truncated to 300 ms (maximum 900 ms) and vertical jitter is applied for legibility. The low  values suggest no correlation between latency and any of the experiential factors</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29535</offset><text>We compared the end-survey scores against the average latencies for each user and for each perspective to analyze latency’s effect on the experience (Fig. 9). As suggested by the low  values, we found no correlation between latency and any factors, suggesting that latency did not noticeably affect the user experience.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>29858</offset><text>Movement kinematic evaluation results</text></passage><passage><infon key="file">779_2022_1684_Fig10_HTML.jpg</infon><infon key="id">Fig10</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29896</offset><text>Comparison of kinematic features between 1PV and 3PV, testing H2 with two-sided t-tests. Movement length did not significantly vary between perspectives, but 3PV yielded faster and wider movements compared to 1PV</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30109</offset><text>We computed the average kinematic features for each user and for each perspective, and used two-sided t-tests to test H2 (Fig. 10). We found support for 3PV yielding faster and wider movements (H2.2-3), but no support for 1PV yielding longer movements. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>30364</offset><text>Dataset evaluation results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30391</offset><text>The interface evaluation yielded approximately 135 movement samples from each perspective. We prepared the data by chunking the 4-DoF 10 Hz movements into samples of 4.8 s with a sliding window of 0.3 s, resulting in  data samples. We then performed an 80-20 train-test split and augmented the training data by mirroring (flipping left-right), shearing (nudging the timing of DoFs relative to each other), shifting the center (adding small variation to the left-right direction that the robot is looking), and decoupling the left and right tower motors (preventing these DoFs from copying each other), yielding over 150, 000 training samples for each perspective. We tuned the neural network architecture and parameters until satisfactory results could be achieved on the datasets from both perspectives.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31197</offset><text>We empirically found that an embedding size of 36 was the lowest before noticeably degrading reconstruction performance. The encoder convolutions have a stride of 2 to progressively increase the effective receptive field. We trained the network for 10 epochs with a learning rate of  and a batch size of 32. We used Leaky ReLU activations ( = 0.01), batch normalization , and 10% dropout after the convolutional and dense layers, as well as a mixup parameter of 0.2 . For the reconstruction loss, we used mean absolute error for the front (tower 1) and base DoFs, mean squared error for the side (towers 2 and 3) DoFs, and weighed the errors as 5, 7, and 10 for the front, side, and base DoFs, respectively. For the classification loss, we used categorical cross entropy on the softmax output of the classifier. For the overall loss, we applied weights of 5 and 7 for the reconstruction and classification losses, respectively, and implemented a  = 0.1 weight for the VAE’s Kullback-Leibler divergence .</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>32206</offset><text>Network training results</text></passage><passage><infon key="file">779_2022_1684_Fig11_HTML.jpg</infon><infon key="id">Fig11</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>32231</offset><text>Network training results on the test sets. Color indicates perspective, line style indicates data size. Using more data generally lowers the overall loss (top), but only slightly improves classification accuracy (bottom). The small improvement indicates that the network “overfits” to the smaller test set when using less data</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32562</offset><text>We trained the networks on both datasets with varying dataset sizes as an ablation study (Fig. 11). We found that the 3PV dataset required less tuning to achieve better results. There is a noticeable improvement for the overall loss compared to using only 10% of the dataset, but only marginal improvement compared to using 50%. While it appears that smaller training datasets do not dramatically impact classification accuracy, the testing dataset sizes were also decreased; the high classification accuracies with smaller datasets are actually “overfit” and thus less generalizable to unseen samples.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>33170</offset><text>Movement reconstruction results</text></passage><passage><infon key="file">779_2022_1684_Fig12_HTML.jpg</infon><infon key="id">Fig12</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>33202</offset><text>Movement reconstructions with varying 3PV dataset sizes. Reconstruction fidelity is proportional to dataset size</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33315</offset><text>We compared movement reconstruction accuracy with varying dataset sizes (Fig. 12). Reconstruction fidelity increases with more data, most noticeably in the base motion. The network captures the overall trajectories but has difficulty achieving the same level of exaggeration and reconstructing granular motions, such as low-amplitude high-frequency jitter.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>33673</offset><text>Embedding separability results</text></passage><passage><infon key="file">779_2022_1684_Fig13_HTML.jpg</infon><infon key="id">Fig13</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>33704</offset><text>Embedding space visualization using t-SNE for 1PV (left) and 3PV (right), color-coded by emotion (happy = green, sad = blue, angry = red). 3PV is more separable, suggesting more diversity and legibility</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33907</offset><text>We used t-SNE to further compress the 36D embeddings into visualizable 2D representations (Fig. 13). As corroborated by the classification accuracies, the emotion clusters are more separable in the 3PV dataset than the 1PV dataset. This suggests that the 3PV movements are more diverse and will yield more emotionally legible generated movements.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>34255</offset><text>Movement generation results</text></passage><passage><infon key="file">779_2022_1684_Fig14_HTML.jpg</infon><infon key="id">Fig14</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34283</offset><text>Sample trajectories of user-crafted (top) and network-generated (bottom) movements from the 3PV dataset. The generated movements retain the characteristics of the original user-crafted movements</text></passage><passage><infon key="file">779_2022_1684_Fig15_HTML.jpg</infon><infon key="id">Fig15</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34478</offset><text>Kinematic comparison between user-crafted and generated movements, with equivalence test scores (bounds set to  of the range for a given feature) annotated on each graph. The tests show similarities primarily in pitch (the amount of upward and downward tilt, measured in radians)</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34758</offset><text>To generate new movements, we first randomly sampled about the embedding distributions of each emotion (e.g., for a new happy movement, we sampled a 36D embedding about the mean and standard deviation of the happy embeddings), then passed these embeddings through the VAE decoder to generate full  movements. Upon inspection, the generated movements look comparable to the user-crafted movements (Fig. 14). We performed an objective comparison for calculable kinematic features (Fig. 15). Equivalence tests with bounds of  of the range for a given feature show similarities in pitch (the amount of upward and downward tilt, measured in radians) for all emotions, acceleration for sadness, and range for happiness and sadness. We subjectively evaluated the comparability through a user survey.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>35552</offset><text>Movement comparison survey results</text></passage><passage><infon key="file">779_2022_1684_Fig16_HTML.jpg</infon><infon key="id">Fig16</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35587</offset><text>Likert scale responses from the movement comparison survey. As in Fig. 7, color indicates level, width indicates proportion of responses for a given level, and black bars indicate means and standard deviations. For each user, the scores for each emotion (happy, sad, or angry) and source (user-crafted or generated) are calculated and rounded to the nearest integer (e.g., a given user’s responses for realism for all happy user-crafted videos they saw are averaged and rounded into a single Likert score, which represents one data point used in the top left bar). p-values of H3 tested with equivalence tests (two one-sided t-tests, equivalence bound of 0.6) are displayed on the right sides. The two sources are largely comparable, except for user-crafted happy movements being more emotive and angry movements being more realistic</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36424</offset><text>We deployed the movement comparison survey on Prolific, offered US $2 in compensation for approximately 10 min of work, and received 100 responses. Each user watched and rated 15 random movements out of the total set of 24 movements. We averaged each user’s responses for each emotion, source, and measure, then rounded to the nearest integer on the Likert scale (e.g., a given user’s responses for realism for all happy user-crafted videos they saw are averaged and rounded into a single Likert score, which represents one data point used in the top left bar of Fig. 16). On the unrounded per-user averages, we used equivalence tests (two one-sided t-tests) with an equivalence bound of 0.6 (1/10th of the 7-point Likert scale) to test H3. The results show that the generated movements are comparable to the user-crafted movements in many measures, supporting H3.1-2, except for user-crafted happy movements being more emotive and angry movements being more realistic. In the context of the prior objective kinematic comparison, these results suggest that pitch is a particularly important feature in conveying affect. </text></passage><passage><infon key="file">779_2022_1684_Fig17_HTML.jpg</infon><infon key="id">Fig17</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37549</offset><text>Confusion matrices for user-crafted (left) and generated movements (right) using 3PV. Overall and within-emotion accuracies accompany the vertical labels. Happiness and sadness are largely correctly matched in both sources, but anger is rarely chosen</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37800</offset><text>We compared the recognition rates between the actual and interpreted emotions (Fig. 17). The recognition accuracies are well above chance (33%) for both the user-crafted and generated movements. Looking at the row-wise results, happiness and sadness are recognized with high accuracies, supporting H3.3, though generated happy movements are more ambiguous. Anger has low recognition rates in both sources, and the column-wise responses indicate that users selected anger much less frequently than the other emotions.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>38318</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38329</offset><text>The interface evaluation revealed strong preferences for 3PV, suggesting that an external perspective may be more useful for conveying affect remotely. The dataset evaluations showed that the user-crafted movements are usable as inputs to the neural network for generating new movements. The movement comparison survey supported movement generation as a valid approach for expanding a robot’s behavior library.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38742</offset><text>Feedback to the interface evaluation was largely positive, with many participants commenting on the uniqueness and enjoyability of the experience. Several participants also commented on the robot’s design, remarking on its cuteness and the fun factor in controlling the robot remotely. The robot’s aesthetic appeal may explain the strong preferences for being able to watch it move in 3PV.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39136</offset><text>Latency can explain the lower than expected synchronization and presence measures in 1PV. Compared to viewing the external robot in 3PV, 1PV may heighten the expectation of synchrony between motion and the video updating. Latency lands 1PV in a temporal uncanny valley, exacerbating the delay and negatively affecting the experience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39470</offset><text>Latency can also explain the slower, smaller movements in 1PV. Although we did not view the users during the evaluation, it is reasonable to posit that 1PV employs more of the user’s body as they must turn their head to maintain a view of the video. In contrast, control in 3PV requires only hand and arm movements, which enables users to create faster and wider movements.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>39846</offset><text>The neural network training results support performance increasing with more data, though our dataset is still magnitudes smaller than publicly available datasets for common modalities such as images or text. There are relatively few works in generative affective robot movements that generalize across different robot platforms and machine learning methods. Establishing standardized comparisons for generative movement algorithms is important for future research to build upon prior works; the GENEA Project is a recent development that aims to address this issue by providing common datasets for benchmarking .</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40461</offset><text>The subjective comparisons of the user-crafted and generated movements show that they are largely comparable, but also indicate limitations of the robot’s embodiment, particularly when emoting anger. The low survey responses for anger and user feedback regarding the robot’s limitations, specifically its lack of appendages and difficulty in tracking finer motions, indicate that more DoFs are necessary for delineating subtleties in affect. Interestingly, the network classifier can outperform the human classifications (&gt;70% compared to &gt;60%), suggesting that the network learns latent features that are not legible from the movement videos.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>41109</offset><text>Limitations and future work</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>41137</offset><text>Latency</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>41145</offset><text>Latency is the largest bottleneck in the system, but is the hardest to mitigate. Although the latency measurements for the trip from the user’s phone to the robot’s host computer could reach as low as 10 ms, we cannot accurately measure the return latency between when the robot moves and when the video updates on the user’s device. WebRTC benchmarks measured round-trip times from 400 ms on a cellular network down to below 100 ms on a dedicated university connection . By contrast, virtual reality systems are expected to perform with latency below 50 ms, and ideally below 20 ms . Future technical work could involve optimizing the underlying technologies to minimize the latency, and perhaps even freely adjust latency as a controlled variable to investigate its effects on the user experience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>41953</offset><text>Embodiment</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>41964</offset><text>While the simplicity of the robot’s design enabled novice users to quickly learn the control scheme, it also limited its expressive capabilities to three DoFs. Several users noted feeling that many of their movements were very similar and expressed wanting arms to convey strong emotions, particularly anger; additionally, the robot’s “cuteness” may have limited its expressive range. The robot’s vertical translation and ear DoFs were removed to simplify the interface, but these motions may be significantly important for affording more expressiveness.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>42529</offset><text>Remote evaluation paradigm</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42556</offset><text>Due to the social distancing restrictions that were in place at the time of this work, we designed the interface evaluation to focus solely on the experience of the remote participant. This neglects studying the experience of a local participant interacting with the robot, and how a remote participant would use the system accordingly. A two-sided scenario may reveal favorable situations for 1PV, such as tasks requiring joint attention or communication in a real-time environment.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>43040</offset><text>Design implications</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>43060</offset><text>Research</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>43069</offset><text>Through this work, we gathered a dataset of affective movements from novice users, who provided usable samples after a short trial to acclimate to the system. The results of the interface evaluation suggest that 3PV is more enjoyable and useful for the movement generation task; future affective telepresence systems may benefit from this external perspective. The comparison survey results showed that these movements are still legible to other users, and support crowdsourcing and generation as viable methods for expanding a robot’s given behavior library. Other researchers can adopt this accessible crowdsourcing approach for their own systems. For example, video-based pose trackers (e.g., OpenPose, VideoPose3D ) can translate human motions into movements for humanoid robots , emancipating these systems from specialized motion capture environments. In the vein of RoboTurk , the remote control scheme could be adapted to source demonstrations for other LfD tasks such as locomotion or manipulation. Such open-access systems will require enforceable review policies to ensure the quality and usability of the samples, such as the two-survey approach with independent populations that we undertook in this work.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_3</infon><offset>44293</offset><text>Fictional scenario</text></passage><passage><infon key="file">779_2022_1684_Fig18_HTML.jpg</infon><infon key="id">Fig18</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>44312</offset><text>Scenario depicting remote communication through pairs of robots in separate locations. Each user remotely controls their conversation partner’s robot and can record behaviors, which are stored in a personal repository on each robot and in a collective database. Coupled with behavior generation algorithms, these behaviors imbue the robot with personalities that either reflect a specific user or represent the robot as a unique individual character</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>44764</offset><text>We imagine robots as a communicative medium that affords a transmission of one’s physicality, adding an extra dimension beyond voice- or video-based mediums (Fig. 18). In one example scenario6, two family members in separate locations communicate through their conversation partner’s respective robot, transmitting their voice, movement, and, optionally, their face through screens implemented on the robots. The remote users can record their movements and save them to their personal repository on their communication partner’s robot. These movements are tied to a unique individual user, but are also added to a collective database of all user-crafted movements. The backend movement generation algorithm trains on both the individual and collective samples. With the individual samples, the robot learns to act as a proxy of a specific user by generating movements in their personal idiosyncratic style. With the collective samples, the robot learns to act as a unique individual character. While movement is seemingly more innocuous than incendiary imagery or text, future work may involve safeguarding against such adversarial content. </text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>45912</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>45923</offset><text>We presented a variable perspective telepresence system for motion controlling a social robot and crowdsourcing affective movement samples. The system uses a smartphone as an accessible motion-based input device. Users controlled the robot from one of two perspectives: either embodying the robot from a first-person perspective through a camera in the robot’s head, or a third-person perspective with an external camera looking at the whole body of the robot. To crowdsource robot movements and assess the experiential quality of the system, we performed an evaluation where lay-users created emotive movement samples for the robot. The subjective responses showed strong preferences for the third-person perspective in self-reported measures of synchronization, ease, enjoyment, engagement, and quality of the created movements. The third-person perspective also yielded movements that were faster and wider than those created in the first-person. To evaluate the usefulness of the collected dataset, we used the user-crafted movements as inputs to a neural network to generate new movements. Through a second user survey, we found that the user-crafted and generated movements were largely comparable. This work supports the use of affective telepresence systems as crowdsourcing platforms for robot demonstrations, and hopefully inspires creative approaches for conducting remote human-robot interaction research.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47343</offset><text>https://ngrok.com/</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47362</offset><text>https://www.w3.org/TR/orientation-event/</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47403</offset><text>https://www.w3.org/TR/webrtc/</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47433</offset><text>https://www.prolific.co/</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47458</offset><text>Preregistration link: https://aspredicted.org/pu8p3.pdf</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>47514</offset><text>This assumes that such social robotic systems are adopted on a similar scale as modern computing devices, either through commercial viability or open-sourcing.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title</infon><offset>47674</offset><text>Declarations</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>47687</offset><text>Conflict of interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>47708</offset><text>The authors declare no competing interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>47752</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47763</offset><text>Adalgeirsson S, Breazeal C (2010) MeBot: A robotic platform for socially embodied telepresence. In: 2010 5th ACM/IEEE International conference on human-robot interaction (HRI). pp 15–22. 10.1109/HRI.2010.5453272</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>47977</offset><text>Adams A, Mahmoud M, Baltrušaitis T, Robinson P (2015) Decoupling facial expressions and head motions in complex emotions. In: 2015 International conference on affective computing and intelligent interaction (ACII). pp 274–280. 10.1109/ACII.2015.7344583</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48233</offset><text>Ainasoja AE, Pertuz S, Kämäräinen J-K (2019) Smartphone teleoperation for self-balancing telepresence robots. In: VISIGRAPP (4: VISAPP). pp 561–568 10.5220/0007406405610568</text></passage><passage><infon key="fpage">469</infon><infon key="issue">5</infon><infon key="lpage">483</infon><infon key="name_0">surname:Argall;given-names:BD</infon><infon key="name_1">surname:Chernova;given-names:S</infon><infon key="name_2">surname:Veloso;given-names:M</infon><infon key="name_3">surname:Browning;given-names:B</infon><infon key="pub-id_doi">10.1016/j.robot.2008.10.024</infon><infon key="section_type">REF</infon><infon key="source">Robotics and Autonomous Systems</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2009</infon><offset>48411</offset><text>A survey of robot learning from demonstration</text></passage><passage><infon key="fpage">82</infon><infon key="issue">1</infon><infon key="lpage">111</infon><infon key="name_0">surname:Breazeal;given-names:C</infon><infon key="name_1">surname:DePalma;given-names:N</infon><infon key="name_2">surname:Orkin;given-names:J</infon><infon key="name_3">surname:Chernova;given-names:S</infon><infon key="name_4">surname:Jung;given-names:M</infon><infon key="pub-id_doi">10.5898/JHRI.2.1.Breazeal</infon><infon key="section_type">REF</infon><infon key="source">J Hum-Robot Interact.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2013</infon><offset>48457</offset><text>Crowdsourcing human-robot interaction: new methods and system evaluation in a public environment</text></passage><passage><infon key="fpage">172</infon><infon key="issue">1</infon><infon key="lpage">186</infon><infon key="name_0">surname:Cao;given-names:Z</infon><infon key="name_1">surname:Hidalgo;given-names:G</infon><infon key="name_2">surname:Simon;given-names:T</infon><infon key="name_3">surname:Wei;given-names:S-E</infon><infon key="name_4">surname:Sheikh;given-names:Y</infon><infon key="pub-id_doi">10.1109/TPAMI.2019.2929257</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Pattern Analysis and Machine Intelligence</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2019</infon><offset>48554</offset><text>OpenPose: realtime multi-person 2D pose estimation using part affinity fields</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48632</offset><text>Denisova A, Cairns P (2015) First person vs. third person perspective in digital games: do player preferences affect immersion?. In: Proceedings of the 33rd annual acm conference on human factors in computing systems. (CHI ’15). association for computing machinery, New York, NY, USA, 145–148. 10.1145/2702123.2702256</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48954</offset><text>Desai R, Anderson F, Matejka J, Coros S, McCann J, Fitzmaurice G, Grossman T (2019) Geppetto: enabling semantic design of expressive robot behaviors. In: Proceedings of the 2019 CHI conference on human factors in computing systems. (CHI ’19). ACM, New York, NY, USA, Article 369, 14 pages. 10.1145/3290605.3300599</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49270</offset><text>Duffy BR, Colm Rooney GMP, O’Hare, GMP, O’Donoghue, R (1999) What is a social robot?</text></passage><passage><infon key="fpage">1</infon><infon key="issue">12</infon><infon key="lpage">19</infon><infon key="name_0">surname:Debarba;given-names:HG</infon><infon key="name_1">surname:Bovet;given-names:S</infon><infon key="name_2">surname:Salomon;given-names:R</infon><infon key="name_3">surname:Blanke;given-names:O</infon><infon key="name_4">surname:Herbelin;given-names:B</infon><infon key="name_5">surname:Boulic;given-names:R</infon><infon key="pub-id_doi">10.1371/journal.pone.0190109</infon><infon key="section_type">REF</infon><infon key="source">PLOS ONE</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2017</infon><offset>49359</offset><text>Characterizing first and third person viewpoints and their alternation for embodied interaction in virtual reality</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49474</offset><text>Goldberg K (2001) The robot in the garden: telerobotics and telepistemology in the age of the internet. MIT Press</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49588</offset><text>Gomez R, Szapiro D, Merino L, Brock H, Nakamura K, Sabanovic S (2020) Emoji to robomoji: exploring affective telepresence through haru. In: International Conference on Social Robotics. Springer, pp 652–663. 10.1007/978-3-030-62056-1_54</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>49826</offset><text>Gorisse G, Christmann O, Amato EA, Richir S (2017) First- and third-person perspectives in immersive virtual environments: presence and performance analysis of embodied users. Frontiers in Robotics and AI. 4:33. 10.3389/frobt.2017.00033</text></passage><passage><infon key="fpage">6</infon><infon key="issue">5</infon><infon key="name_0">surname:Higgins;given-names:I</infon><infon key="name_1">surname:Matthey;given-names:L</infon><infon key="name_2">surname:Pal;given-names:A</infon><infon key="name_3">surname:Burgess;given-names:C</infon><infon key="name_4">surname:Glorot;given-names:X</infon><infon key="name_5">surname:Botvinick;given-names:M</infon><infon key="name_6">surname:Mohamed;given-names:S</infon><infon key="name_7">surname:Lerchner;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Iclr</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2017</infon><offset>50063</offset><text>Beta-vae: learning basic visual concepts with a constrained variational framework</text></passage><passage><infon key="fpage">89</infon><infon key="issue">1</infon><infon key="lpage">122</infon><infon key="name_0">surname:Hoffman;given-names:G</infon><infon key="name_1">surname:Ju;given-names:W</infon><infon key="pub-id_doi">10.5898/JHRI.3.1.Hoffman</infon><infon key="section_type">REF</infon><infon key="source">Journal of Human-Robot Interaction</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2014</infon><offset>50145</offset><text>Designing robots with movement in mind</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50184</offset><text>Ioffe S, Szegedy C (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. In: Bach F, Blei D (eds) Proceedings of the 32nd international conference on machine learning (proceedings of machine learning research), vol 37. PMLR, Lille, France, 448–456. http://proceedings.mlr.press/v37/ioffe15.html</text></passage><passage><infon key="fpage">537</infon><infon key="issue">4</infon><infon key="lpage">552</infon><infon key="name_0">surname:Jonggil;given-names:A</infon><infon key="name_1">surname:Kim;given-names:GJ</infon><infon key="pub-id_doi">10.1007/s12369-017-0463-2</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Social Robotics</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2018</infon><offset>50528</offset><text>SPRinT: A mixed approach to a hand-held robot interface for telepresence</text></passage><passage><infon key="fpage">373</infon><infon key="issue">4</infon><infon key="lpage">387</infon><infon key="name_0">surname:Kilteni;given-names:K</infon><infon key="name_1">surname:Groten;given-names:R</infon><infon key="name_2">surname:Slater;given-names:M</infon><infon key="pub-id_doi">10.1162/PRES_a_00124</infon><infon key="section_type">REF</infon><infon key="source">Presence: Teleoperators and Virtual Environments</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2012</infon><offset>50601</offset><text>The sense of embodiment in virtual reality</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50644</offset><text>Kingma DP, Welling M (2013) Auto-encoding variational bayes. arXiv:1312.6114 [stat.ML]</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>50731</offset><text>Komiyama R, Miyaki T, Rekimoto J (2017) JackIn space: designing a seamless transition between first and third person view for effective telepresence collaborations. In: Proceedings of the 8th augmented human international conference (AH ’17). association for computing machinery, New York, NY, USA, Article 14. 10.1145/3041164.3041183</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51068</offset><text>Kucherenko T, Jonell P, Yoon Y, Wolfert P, Henter GE (2021) A large, crowdsourced evaluation of gesture generation systems on common data: the GENEA challenge 2020. In: International conference on intelligent user interfaces. 10.1145/3397481.3450692</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51318</offset><text>Mandlekar A, Zhu Y, Garg A, Booher J, Spero M, Tung A, Gao J, Emmons J, Gupta A, Orbay E, Savarese S, Fei-Fei L (2018) RoboTurk: A crowdsourcing platform for robotic skill learning through imitation. In: proceedings of the 2nd conference on robot learning (proceedings of machine learning research), vol 87. PMLR, pp 879-893. http://proceedings.mlr.press/v87/mandlekar18a.html</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51695</offset><text>Marmpena M, Lim A, Dahl TS, Hemion N(2019) Generating robotic emotional body language with variational autoencoders. In: 2019 8th international conference on affective computing and intelligent interaction (ACII). IEEE, pp 545–551</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>51928</offset><text>Müller J, Langlotz T, Regenbrecht H (2016) PanoVC: Pervasive telepresence using mobile phones. In: 2016 IEEE international conference on pervasive computing and communications. pp 1–10. 10.1109/PERCOM.2016.7456508</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52145</offset><text>Pavllo D, Feichtenhofer C, Grangier D, Auli M (2019) 3D Human pose estimation in video with temporal convolutions and semi-supervised training. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</text></passage><passage><infon key="fpage">457</infon><infon key="lpage">462</infon><infon key="name_0">surname:Raaen;given-names:K</infon><infon key="name_1">surname:Kjellmo;given-names:I</infon><infon key="section_type">REF</infon><infon key="source">Entertainment computing - ICEC 2015</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>52382</offset><text>Measuring latency in virtual reality systems</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52427</offset><text>Rakita D, Mutlu B, Gleicher M (2020) Effects of onset latency and robot speed delays on mimicry-control teleoperation. In: Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction (HRI ’20). Association for computing machinery, New York, NY, USA, 519–527. 10.1145/3319502.3374838</text></passage><passage><infon key="fpage">273</infon><infon key="issue">2</infon><infon key="lpage">282</infon><infon key="name_0">surname:Rhodin;given-names:H</infon><infon key="name_1">surname:Tompkin;given-names:J</infon><infon key="name_2">surname:Kim;given-names:KI</infon><infon key="name_3">surname:Varanasi;given-names:K</infon><infon key="name_4">surname:Seidel;given-names:H-P</infon><infon key="pub-id_doi">10.1111/cgf.12325</infon><infon key="section_type">REF</infon><infon key="source">Computer Graphics Forum</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2014</infon><offset>52739</offset><text>Theobalt C (2014) Interactive motion mapping for real-time character control</text></passage><passage><infon key="fpage">1161</infon><infon key="issue">6</infon><infon key="name_0">surname:Russell;given-names:JA</infon><infon key="pub-id_doi">10.1037/h0077714</infon><infon key="section_type">REF</infon><infon key="source">Journal of Personality and Social Psychology</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">1980</infon><offset>52816</offset><text>A circumplex model of affect</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52845</offset><text>Sakashita M, Minagawa T, Koike A, Suzuki I, Kawahara K, Ochiai Y (2017) You as a puppet: evaluation of telepresence user interface for puppetry. In: Proceedings of the 30th annual ACM symposium on user interface software and technology (UIST ’17). Association for Computing Machinery, New York, NY, USA, 217–228. 10.1145/3126594.3126608</text></passage><passage><infon key="fpage">85</infon><infon key="issue">2015</infon><infon key="lpage">117</infon><infon key="name_0">surname:Schmidhuber;given-names:J</infon><infon key="pub-id_doi">10.1016/j.neunet.2014.09.003</infon><infon key="pub-id_pmid">25462637</infon><infon key="section_type">REF</infon><infon key="source">Neural Netw</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">2015</infon><offset>53186</offset><text>Deep learning in neural networks: an overview</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53232</offset><text>Seol Y, O’Sullivan C, Lee J (2013) Creature features: online motion puppetry for non-human characters. In: Proceedings of the 12th ACM SIGGRAPH/eurographics symposium on computer animation (SCA ’13). ACM, New York, NY, USA, 213–221. 10.1145/2485895.2485903</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53495</offset><text>Sirkin D, Ju W (2012) Consistency in physical and on-screen action improves perceptions of telepresence robots. In: Proceedings of the seventh annual ACM/IEEE international conference on human-robot interaction (HRI ’12). Association for Computing Machinery, New York, NY, USA, 57–64. 10.1145/2157689.2157699</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53808</offset><text>Slyper R, Hoffman G, Shamir A (2015) Mirror puppeteering: animating toy robots in front of a webcam. In: Proceedings of the ninth international conference on tangible, embedded, and embodied interaction. ACM, pp. 241–248</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54031</offset><text>Stiehl WD, Lee JK, Breazeal C, Nalin M, Morandi A, Sanna A (2009) The huggable: a platform for research in robotic companions for pediatric care. In: Proceedings of the 8th international conference on interaction design and children (IDC ’09). Association for Computing Machinery, New York, NY, USA, 317–320. 10.1145/1551788.1551872</text></passage><passage><infon key="fpage">29</infon><infon key="lpage">30</infon><infon key="name_0">surname:Strong;given-names:R</infon><infon key="name_1">surname:Gaver;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of CSCW</infon><infon key="type">ref</infon><infon key="volume">96</infon><infon key="year">1996</infon><offset>54368</offset><text>Feather, scent and shaker: supporting simple intimacy</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54422</offset><text>Suguitan M, Gomez R, Hoffman G (2020) MoveAE: modifying affective robot movements using classifying variational autoencoders. In: Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction. pp 481–489. 10.1145/3319502.3374807</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54676</offset><text>Suguitan M, Hoffman G (2019) Blossom: a handcrafted open-source robot. ACM Transactions on Human-Robot Interaction 8(1) Article 2 (March 2019), 27. 10.1145/3310356</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54840</offset><text>Taheri S, Beni LA, Veidenbaum AV, Nicolau A, Cammarota R, Qiu J, Lu Q, Haghighat MR (2015) WebRTCbench: a benchmark for performance assessment of webRTC implementations. In: 2015 13th IEEE Symposium on embedded systems for real-time multimedia (ESTIMedia). pp 1–7. 10.1109/ESTIMedia.2015.7351769</text></passage><passage><infon key="fpage">96</infon><infon key="lpage">110</infon><infon key="name_0">surname:Tanaka;given-names:K</infon><infon key="name_1">surname:Nakanishi;given-names:H</infon><infon key="name_2">surname:Ishiguro;given-names:H</infon><infon key="name_3">surname:Yuizono;given-names:T</infon><infon key="name_4">surname:Zurita;given-names:G</infon><infon key="name_5">surname:Baloian;given-names:N</infon><infon key="name_6">surname:Inoue;given-names:T</infon><infon key="name_7">surname:Ogata;given-names:H</infon><infon key="section_type">REF</infon><infon key="source">Collaboration technologies and social computing</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>55138</offset><text>Comparing video, avatar, and robot mediated communication: pros and cons of embodiment</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55225</offset><text>Tang A, Fakourfar O, Neustaedter C, Bateman S (2017) Collaboration in  videochat: challenges and opportunities. 10.11575/PRISM/10182</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55358</offset><text>Tsoi N, Connolly J, Adéníran E, Hansen A, Pineda KT, Adamson T, Thompson S, Ramnauth R, Vázquez M, Scassellati B (2021) Challenges deploying robots during a pandemic: an effort to fight social isolation among children. In: Proceedings of the 2021 ACM/IEEE international conference on human-robot interaction (HRI ’21). Association for Computing Machinery, New York, NY, USA, 234-242. 10.1145/3434073.3444665</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55771</offset><text>Yamane K, Ariki Y, Hodgins J (2010) Animating Non-Humanoid Characters With Human Motion Data. In: Proceedings of the 2010 ACM SIGGRAPH/eurographics symposium on computer animation (SCA ’10). Eurographics Association, Goslar Germany, Germany, 169–178. http://dl.acm.org/citation.cfm?id=1921427.1921453</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56076</offset><text>Yoon Y, Cha B, Lee J-H, Jang M, Lee J, Kim J, Lee G (2020) Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Transactions on Graphics 39(6) Article 222. 10.1145/3414685.3417838</text></passage><passage><infon key="fpage">1908</infon><infon key="issue">5</infon><infon key="lpage">1918</infon><infon key="name_0">surname:Young;given-names:J</infon><infon key="name_1">surname:Langlotz;given-names:T</infon><infon key="name_2">surname:Cook;given-names:M</infon><infon key="name_3">surname:Mills;given-names:S</infon><infon key="name_4">surname:Regenbrecht;given-names:H</infon><infon key="pub-id_doi">10.1109/TVCG.2019.2898737</infon><infon key="pub-id_pmid">30762552</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Visualization and Computer Graphics</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2019</infon><offset>56297</offset><text>Immersive telepresence and remote collaboration using mobile and wearable devices</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56379</offset><text>Zhang H, Cisse M, Dauphin YN, Lopez-Paz D (2017) Mixup: beyond empirical risk minimization. arXiv:1710.09412 [cs.LG]</text></passage></document></collection>
