<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201221</date><key>pmc.key</key><document><id>7059806</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/fninf.2020.00007</infon><infon key="article-id_pmc">7059806</infon><infon key="article-id_pmid">32180712</infon><infon key="elocation-id">7</infon><infon key="kwd">quality control fMRI brain registration crowdsourcing visual inspection inter-rater agreement</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Benhajali;given-names:Yassine</infon><infon key="name_1">surname:Badhwar;given-names:AmanPreet</infon><infon key="name_2">surname:Spiers;given-names:Helen</infon><infon key="name_3">surname:Urchs;given-names:Sebastian</infon><infon key="name_4">surname:Armoza;given-names:Jonathan</infon><infon key="name_5">surname:Ong;given-names:Thomas</infon><infon key="name_6">surname:Pérusse;given-names:Daniel</infon><infon key="name_7">surname:Bellec;given-names:Pierre</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">14</infon><infon key="year">2020</infon><offset>0</offset><text>A Standardized Protocol for Efficient and Reliable Quality Control of Brain Registration in Functional MRI Studies</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>115</offset><text>Automatic alignment of brain anatomy in a standard space is a key step when processing magnetic resonance imaging for group analyses. Such brain registration is prone to failure, and the results are therefore typically reviewed visually to ensure quality. There is however no standard, validated protocol available to perform this visual quality control (QC). We propose here a standardized QC protocol for brain registration, with minimal training overhead and no required knowledge of brain anatomy. We validated the reliability of three-level QC ratings (OK, Maybe, Fail) across different raters. Nine experts each rated N = 100 validation images, and reached moderate to good agreement (kappa from 0.4 to 0.68, average of 0.54 ± 0.08), with the highest agreement for “Fail” images (Dice from 0.67 to 0.93, average of 0.8 ± 0.06). We then recruited volunteers through the Zooniverse crowdsourcing platform, and extracted a consensus panel rating for both the Zooniverse raters (N = 41) and the expert raters. The agreement between expert and Zooniverse panels was high (kappa = 0.76). Overall, our protocol achieved a good reliability when performing a two level assessment (Fail vs. OK/Maybe) by an individual rater, or aggregating multiple three-level ratings (OK, Maybe, Fail) from a panel of experts (3 minimum) or non-experts (15 minimum). Our brain registration QC protocol will help standardize QC practices across laboratories, improve the consistency of reporting of QC in publications, and will open the way for QC assessment of large datasets which could be used to train automated QC systems.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1729</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1742</offset><text>Aligning individual anatomy across brains is a key step in the processing of structural magnetic resonance imaging (MRI) for functional MRI (fMRI) studies. This brain registration process allows for comparison of local brain measures and statistics across subjects. A visual quality control (QC) of brain registration is crucial to minimize incorrect data in downstream analyses of fMRI studies. However, no standardized, validated protocol has yet been developed to streamline this QC. Here, we present a standardized procedure for visual QC of brain registration and describe the reliability of QC ratings from both expert raters and a large panel of non-experts recruited through an online citizen science platform1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>2462</offset><text>Brain Registration</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2481</offset><text>Magnetic resonance imaging is a non-invasive technique that can be applied to study brain structure (sMRI) and function (fMRI). Multiple steps are required to transform raw MRI data to processed images ready for downstream statistical analyses. One critical preprocessing step is brain registration; this involves aligning 3D brain images to a standard stereotaxic space, such as the MNI152/ICBM2009c template. State-of-the-art registration procedures use non-linear optimization algorithms such as ANIMAL, DARTEL, or ANTS. compared five publicly available, widely used brain registration algorithms in medical image analysis and found a failure rate of 16.8 ± 3.13% on their benchmarks. This lack of robustness is mainly due to differences in image quality, shape and cortical topology between individual brains. A visual QC of registered brain images is thus required to ensure good data quality for subsequent analyses.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>3405</offset><text>Visual QC</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3415</offset><text>The specific focus of the visual QC for sMRI registration depends on the intended use of the data. Voxel-based analysis of brain morphology typically calls for a highly accurate registration, as this step can impact brain tissue segmentation. In contrast, fMRI studies usually rely on larger voxel size and spatial blurring, and are less likely to be affected by small registration errors. To our knowledge, as of yet, there are no standardized criteria for tolerable errors in sMRI registration for fMRI processing pipelines. Many fMRI analytical software packages present users with images to assess the quality of T1 image registration. In one of the most recent packages developed by the community, fMRIprep, the registered T1 image is presented across 21 brain slices, along with images for three other processing steps (skull stripping, tissue segmentation, and surface reconstruction), yielding a total of 84 brain slices for visual inspection. Established processing tools like FMRIB Software Library or the Statistical Parametric Mapping MATLAB package also present users with reports that often include more than ten brain slices for visual inspection for each subject. This makes visual inspection tedious and time-consuming. Critically, none of these packages offer guidelines on how to assess the quality of structural brain registration for fMRI studies. Without such guidelines and with a large number of images to review, QC is likely to vary significantly across raters.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>4903</offset><text>Inter-Rater Agreement</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4925</offset><text>Quality control studies of preprocessed images rarely report inter-rater reliability, and no such study examined brain registration to our knowledge. applied a support vector machine algorithm on visually rated (N = 1457 usable/unusable) sMRI data from 5 to 9 investigators who rated the same 630 images, but did not report agreement metrics. compared automated QC metrics and manual QC from 6662 sMRI data from 4 different cohorts/sites, merging visual inspection across sites, raters, protocols and scan quality but without presenting agreement statistics. Studies that do report inter-rater agreement mostly focus on issues related to raw MRI images (e.g. signal-to-noise ratio or susceptibility artifacts), head motion (e.g. ghosting or blurring), brain extraction, and tissue segmentation. Inter-rater agreement in these studies is found to vary considerably. For example, reported high agreement for two trained raters who visually inspected the same 88 sMRI, achieving an intra-class correlation of 0.931 for two categories of quality (pass-fail) on issues related to MRI acquisition and head motion. reported a kappa of 0.39 between two raters for three quality categories (Exclude/Doubtful/Accept) on 100 images when ratings were based on the quality of the MRI acquisition, head motion, brain extraction and tissue segmentation. Table 1 shows recent (2010 onward) studies reporting inter-rater agreements on visual QC of sMRI for a variety of issues. Only one study,, included brain registration for visual QC assessment. These authors reported a test-retest Dice similarity of 0.96% from one expert rater who evaluated as pass or fail 1000 images twice, but no inter-rater reliability estimate. Variability in reliability across studies may be due to two types of factors: user- and protocol-related factors. Protocol-related factors (e.g. clarity, levels of rating or training set) can be addressed by multiple iteration and refinement of the protocol. Factors related to the rater (e.g. level of expertise, fatigue, motivation, etc.) are more difficult to constrain or control. One solution to circumvent individual rater variability is to aggregate multiple ratings from a large pool of raters.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>7134</offset><text>Reported agreement in visual inspection of sMRI data on QC studies.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;Reported visual inspection issue related to&lt;hr/&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Study&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;QC agreement details&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MRI Aquisition&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Head motion&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Brain extraction&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Tissue segmentation&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Brain registration&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B4&quot; ref-type=&quot;bibr&quot;&gt;Backhausen et al., 2016&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Images&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Image sharpness, ringing. Contrast to noise ratio (subcortical structures and gray/white matter) and susceptibility artifacts&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ghosting or blurring&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Raters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rating scale&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Include/Exclude&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QC Manual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref ref-type=&quot;supplementary-material&quot; rid=&quot;DS1&quot;&gt;Supplementary Material&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreement&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ICC = 0.93&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B15&quot; ref-type=&quot;bibr&quot;&gt;Esteban et al., 2017&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Images&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;signal-to-noise ratio. Image contrast and Ringing&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Head motion artifacts&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gray/white matter and the pial delineation&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gray–white matter segmentation&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb Raters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rating scale&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Exclude/Doubtful/Accept&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QC Manual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreement&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cohen’s Kappa = 0 39&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B39&quot; ref-type=&quot;bibr&quot;&gt;Rosen et al., 2018&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Images&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Phasel = l00, Phase2 = 100&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Raters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Phase1 = 2, Phase2 = 3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rating scale&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/1/2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QC Manual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreement&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Phasel = 100%, Phase2 = 85%&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B19&quot; ref-type=&quot;bibr&quot;&gt;Fonov et al., 2018&lt;/xref&gt; (preprint)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Images&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9693 (1000 rated twice)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Effect of noise and image intensity non-uniformity&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Incorrect estimates of, translation, scaling in all directions and rotation.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Raters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rating scale&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accept/Fail&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QC Manual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B13&quot; ref-type=&quot;bibr&quot;&gt;Dadar et al., 2018&lt;/xref&gt; paper&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreement&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;intra-rater Dice similarity = 0.96&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B32&quot; ref-type=&quot;bibr&quot;&gt;Klapwijk et al., 2019&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Images&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ringing&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Division between gray/white matter and pial surface&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gray–white matter segmentation&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N.R&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nb. Raters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Rating scale&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Excellent/Good/Doubtful/Failed&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QC Manual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref ref-type=&quot;supplementary-material&quot; rid=&quot;DS1&quot;&gt;Supplementary Material&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreement&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reliability = 0.53&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;justify&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>7202</offset><text>			Reported visual inspection issue related to				 	Study	QC agreement details	MRI Aquisition	Head motion	Brain extraction	Tissue segmentation	Brain registration	 		Nb. Images	88	Image sharpness, ringing. Contrast to noise ratio (subcortical structures and gray/white matter) and susceptibility artifacts	Ghosting or blurring	N.R	N.R	N.R	 		Nb. Raters	2						 		Rating scale	Include/Exclude						 		QC Manual	Supplementary Material						 		Agreement	ICC = 0.93						 		Nb. Images	100	signal-to-noise ratio. Image contrast and Ringing	Head motion artifacts	Gray/white matter and the pial delineation	Gray–white matter segmentation	N.R	 		Nb Raters	2						 		Rating scale	Exclude/Doubtful/Accept						 		QC Manual	N.R						 		Agreement	Cohen’s Kappa = 0 39						 		Nb. Images	Phasel = l00, Phase2 = 100	N.R	N.R	N.R	N.R	N.R	 		Nb. Raters	Phase1 = 2, Phase2 = 3						 		Rating scale	0/1/2						 		QC Manual	N.R						 		Agreement	Phasel = 100%, Phase2 = 85%						 	 (preprint)	Nb. Images	9693 (1000 rated twice)	Effect of noise and image intensity non-uniformity	N.R	N.R	N.R	Incorrect estimates of, translation, scaling in all directions and rotation.	 		Nb. Raters	1						 		Rating scale	Accept/Fail						 		QC Manual	 paper						 		Agreement	intra-rater Dice similarity = 0.96						 		Nb. Images	80	N.R	Ringing	Division between gray/white matter and pial surface	Gray–white matter segmentation	N.R	 		Nb. Raters	5						 		Rating scale	Excellent/Good/Doubtful/Failed						 		QC Manual	Supplementary Material						 		Agreement	Reliability = 0.53						 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>8757</offset><text>Crowdsourced QC</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8773</offset><text>Crowdsourcing can be used to achieve multiple QC ratings on large collections of images rapidly. Crowdsourcing, as first defined by Howe in 2016, is “the act of taking a job traditionally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call”. Crowdsourcing can be used in citizen science research projects where a large number of non-specialists take part in the scientific workflow to help researchers. Crowdsourcing labor-intensive tasks across hundreds or thousands of individuals has proven to be effective in a number of citizen science research projects, such as modeling complex protein structures, mapping the neural circuitry of the mammalian retina, and discovering new astronomical objects.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9576</offset><text>In brain imaging, recent work by showed the advantages of using citizen science to rate brain images for issues related to head motion and scanner artifacts. These authors were able to gather 80,000 ratings on slices drawn from 722 brains using a simple web interface. A deep learning algorithm was then trained to predict data quality, based on the gathered rating from citizen science. The deep learning network performed as well as a the specialized algorithm MRIQC for quality control of T1-weighted images. QC of large open access databases like HCP, UKbiobank or ABCD is challenging and time consuming task if done manually. Using crowdsourced rating could be a key element to rate huge databases and eventually use these ratings to efficiently train a machine learning models to perform QC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10374</offset><text>Here, we propose a novel, standardized visual QC protocol for the registration of T1 images by non-experts. We formally assessed protocol reliability, first with “expert” raters familiar with visual inspection of brain registration, and second with a large pool of “non-expert” raters with no specific background in brain imaging. These citizen scientists contributed via the world’s largest online citizen science platform, called Zooniverse. Zooniverse enabled the enrollment of more than 2000 volunteers from around the globe, thus enabling the evaluation of consensus between non-expert raters on a large scale. Specific aims and hypotheses of the study were as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11059</offset><text>To establish a QC procedure for MRI brain registration that does not require extensive training or prior knowledge of brain anatomy. Our hypothesis was that such a procedure would help raters achieve more reliable visual QC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11284</offset><text>To quantify the agreement between a consensus panel composed of non-expert raters and that of experts. Our hypothesis was that the consensus of non-experts would be consistent with experts’ assessments, since the protocol requires no knowledge of brain anatomy.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>11548</offset><text>Method</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11555</offset><text>Quality Control Protocol Building</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11589</offset><text>The QC protocol was developed iteratively over the past 5 years, with several rounds of feedback from users. Initially, the protocol was used internally in our laboratory, and required a visual comparison of T1 slices against a template using the Minctool register. Although the protocol achieved good consistency of ratings between two expert users (kappa = 0.72), it was time consuming and hard to teach. We then switched from an interactive brain viewer to a static mosaic comprised of 9 different slices (3 axial, 3 sagittal, 3 coronal, see Figure 1B), and we highlighted anatomical landmarks using a precomputed mask. These landmarks were selected because we expected all of them to align well in the case of a successful registration, and the precomputed mask served as an objective measure to decide on the severity of a misalignment. We established guidelines on how to rate a registered image on a three-level scale (“OK,” “Maybe,” or “Fail”) using these landmarks. The new protocol limited the need for extensive training for new users and potentially reduced the subjectivity of decision, notably for edge cases. The following sections describe the details and the validation of the final protocol (brain slices, landmarks and rating guidelines).</text></passage><passage><infon key="file">fninf-14-00007-g001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12859</offset><text>QC protocol for brain registration. (A) Brain slices. The rater is presented with two sets of brain slices (3 axial, 3 sagittal and 3 coronal), one of them showing the template in stereotactic space and the other showing an individual T1 brain after registration. In the interface, the two images are superimposed and the rater can flip between them to visually assess the registration. (B) Anatomical landmarks. The landmarks for QC included: the outline of the brain (A), tentorium cerebelli (B), cingulate sulcus (C), parieto-cingulate sulcus occipital fissure (D), calcarine fissure (E), the lateral ventricles (F), central sulcus (G) and the hippocampal formation (H) bilaterally. The landmarks were outlined in stereotaxic space. (C) Rating guidelines. The boundaries of red landmarks act as “confidence interval” for registration: an area is tagged as a misregistration only if the target structure falls outside the boundaries. (D) Tags. Raters put tags on each misregistered brain structure. (E) Final rating. A final decision is reached on the quality of registration: an image with no tags is rated OK, one or more non-adjacent tags are rated Maybe, two or more adjacent tags are rated Fail. An image that is excessively blurry is also rated Fail.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14122</offset><text>Brain Slices</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14135</offset><text>A mosaic view of nine brain slices was extracted from each registered brain. The x, y, and z coordinates, corresponding to axial, coronal and sagittal views, were as follows:</text></passage><passage><infon key="file">d35e856.xml</infon><infon key="id">d35e856</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;x&lt;/italic&gt; (sagitai)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;y&lt;/italic&gt;
(coronal)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;italic&gt;z&lt;/italic&gt; (axial)&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−50&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−65&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−20&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;54&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>14310</offset><text>x (sagitai)	y	 	(coronal)	z (axial)	 	−50	−65	−6	 	−8	−20	13	 	30	54	58	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14395</offset><text>Two images were generated: one using the individual T1 image of a subject, after brain registration, and one using the MNI2009c MRI T1 template averaged from 152 adults after iterative non-linear registration, see Figure 1A.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14620</offset><text>Anatomical Landmarks</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14641</offset><text>Notable anatomical landmarks included the central sulcus, cingulate sulcus, parieto-occipital fissure, calcarine fissure, tentorium cerebelli, lateral ventricles, bilateral hippocampal formation and the outline of the brain (see Figure 1B). To highlight these landmarks, we hand-drew a red transparent outline inside the brain with the MRIcron drawing tool using the MNI 2009 gray matter atlas as a reference. For the outline of the brain, we substracted a 4-mm eroded brain mask (MNI2009c release) from a 4-mm dilated brain mask. This process resulted in a roughly 8-mm thick mask centered on the outline of the brain in template space. The landmark boundaries served as the “confidence interval” of acceptable registration. The width of this confidence interval was somewhat arbitrary, but critically helped raters to consistently assess what amount of misregistration was acceptable. The scripts to generate the mosaic brain images with highlighted landmarks have been made available in the GitHub repository2.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15659</offset><text>Rating Guidelines</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15677</offset><text>We instructed raters to focus on the brain structures within the red anatomical landmarks, comparing the individual brain, after registration, with the MNI 2009c template. The two images were presented superimposed with each other, and raters were able to flip manually or automatically between the individual and the template brain. For a given anatomical landmark, raters were asked to tag any part of the brain structure that fell outside of the anatomical landmark for the individual brain. The template acted as a reference for what the structure looked like, and where it was supposed to be. Figure 1C provides examples of acceptable and unacceptable registration of brain structures within the landmarks. Raters were instructed to click all misregistered brain structures, which resulted in a series of tag spheres with 4 mm radius (Figure 1D). After an image was fully tagged, the overall registration quality was evaluated by the rater as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16634</offset><text>“OK” if no tag was reported,</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16667</offset><text>“Maybe” if one or several regions were tagged, yet no tag spheres overlapped (less than 8 mm apart),</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16772</offset><text>“Failed” if two tag spheres overlapped, meaning that an extensive brain area (&gt;8 mm) was misregistered. Alternatively, a “Failed” rating was also issued if the entire image was of poor quality due to motion or a ringing artifact (Figure 1E).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17022</offset><text>Zooniverse Platform</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17042</offset><text>We used the online citizen science platform Zooniverse) as an interface to perform the validation of our QC protocol3. Zooniverse offers a web-based infrastructure for researchers to build citizen science projects that require a human visual inspection and possibly recruit a large number of zooniverse volunteers, who are not familiar with neuroimaging and have no formal requirements to participate (. Our project, called “Brain match” was developed with the support of the Zooniverse team, to ensure compliance with Zooniverse policies and appropriate task design for an online audience4, and the project was also approved by our institutional review board. Note that the raters were considered part of the research team, and not participants of the research project, and thus they were not required to sign an informed consent form. The project underwent a “beta review” phase on zooniverse, where we collected feedback on the clarity and difficulty level of the task. Rating was performed by Zooniverse raters and expert raters. All ratings were performed on the zooniverse platform through the Brain Match dashboard5. The rating workflow was the same for the two types of types raters. Note that individuals participating in Zooniverse choose to voluntarily dedicate some of their time to science and thus do not constitute a representative sample of the general population.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18431</offset><text>Brain Images Validation and Training Sets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18473</offset><text>We used a combination of two publicly available datasets, COBRE and ADHD-200, for both the beta and the full launch of the project. These datasets have been made available after anonymization by consortia of research team, each of which received ethics approval at their local institutional review board, as well as informed consent from all participants. Each individual sMRI scan was first corrected for intensity non-uniformities and the brain extracted using a region growing algorithm. Individual scans were then linearly registered (9 parameters) with the T1 MNI symmetric template. The sMRI scans were again corrected for intensity non-uniformities in stereotaxic space, this time restricted to the template brain mask. An individual brain mask was extracted a second time on this improved image and combined with template segmentation priors. An iterative non-linear registration was estimated between the linearly registered sMRI and the template space, restricted to the brain mask. The processed data were finally converted into mosaics and merged with a mask of anatomical landmarks using in-house scripts. Two expert raters (PB,YB) rated each 954 preprocessed images in ADHD-200, achieving a kappa of 0.72 (substantial agreement) from a random subset of 260 images. The COBRE dataset was rated by YB only.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19792</offset><text>On Zooniverse, raters were first invited to read a tutorial (Supplementary Figure S2) explaining the protocol, and then completed a QC training session, featuring 15 selected images (5 rated OK, 5 rated Maybe and 5 rated Fail, as rated by YB). Because the COBRE structural images were of higher quality, OK images were selected from COBRE while Maybe and Fail were selected from ADHD-200. For each training image, the rater was first asked to assess the image, and was then able to see the tags and the final ratings by an expert rater (YB).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20334</offset><text>After completing the training session, raters were presented a series of 100 “open label” cases, and were free to rate as many of these images as they wanted. We chose to present only 100 images in order to ensure we would have many ratings by different raters for each image, within a relatively short time frame. We arbitrary selected a subset of 100 images with a ratio of 35 Fail, 35 Maybe, and 30 OK images based on one expert rater (YB). Once again, the OK images were drawn from COBRE, while the Fail and Maybe were drawn from ADHD-200.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20882</offset><text>Raters</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20889</offset><text>More than 2500 volunteers took part in our Brain Match project. They performed approximately 21,600 ratings of individual images over 2 beta-testing phases and two full workflows for a total of 260 registered brain images (see Brain images section). We used a retirement of 40 ratings, which means each image was rated by 40 different Zooniverse raters before being removed from the workflow. Only individuals who rated more than 15 images were kept in the final study. After data cleaning, 41 Zooniverse volunteer raters were kept. The distribution of rating per image showed a mean number of ratings of 21.76 ± 2.75 (see Supplementary Figure S1).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21539</offset><text>A group of 9 experts raters were also recruited for this study and each asked to rate all of the 100 validation images using the Brain Match interface. They were instructed to first start with the training session and to carefully read the tutorial before starting the main QC workflow. All raters had prior experience with QC of brain registration in the past. Each rater was free to perform the QC task at her pace without any specific direction on how to do it. The process was completed once all ratings were submitted.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22063</offset><text>Finally, a radiologist was also recruited for the study. He rated the same 100 images using Brain Match interface, also undergoing the training session before the rating process. Although the radiologist had no prior experience in QC of brain registration, that participant had very extensive experience in examining brain images following a standardized protocol, and served as a gold standard about what to expect from a fully compliant rater, trained on QC solely through available online documentation.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22570</offset><text>Agreement Statistics</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22591</offset><text>We used Cohen’s kappa to assess inter-rater reliability across all nine experts (ratings R1–R9). The kappa metric measures the agreement between two raters who rate the same amount of items into N mutually exclusives categories. The kappa is based on the difference between the observed agreement (po, i.e. the proportion of rated images for which both raters agreed on the category) and the probability of chance or expected agreement (pe). Kappa (k) is computed as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23071</offset><text>In this work we used a weighted kappa metric, which assigns less weight to agreement as categories are further apart. In our QC cases disagreements between OK and Maybe, and between Maybe and Fail count as partial disagreements; disagreements between OK and Fail, however, count as complete disagreements. We used the R package irr to estimate the weighted kappa and interpretation of the strength of agreement for κ ≤ 0 = poor, 0.01–0.20 = slight, 0.21–0.40 = fair, 0.41–0.60 = moderate, 0.61–0.80 = substantial, and 0.81–1 = almost perfect.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23627</offset><text>We also used the Sørensen–Dice coefficient (Dice) to assess the agreement within the rating categories of OK, Maybe and Fail, as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23768</offset><text>where X is the set of images rated “OK” by one rater and Y is the set of images rated “OK” by a second rater, ⋂ is the intersection between two sets, and |X| is the number of images. In plain English, the Dice between two raters for the OK category is the number of images that both raters rated “OK,” divided by the average number of images rated “OK” across the two raters. The same Dice measure was generated as well for “Maybe” and “Fail” images. We interpreted Dice coefficients using the same range of strength of agreement as for the Kappa coefficient (≤0 = poor, 0.01–0.20 = slight, 0.21–0.40 = fair, 0.41–0.60 = moderate, 0.61–0.80 = substantial, and 0.81–1 = almost perfect).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24492</offset><text>Consensus Panels</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24509</offset><text>We also evaluated the reliability of QC ratings after pooling several raters into a consensus panel. The panel consensus was generated by counting the number of OK, Maybe and Fail attributed to an image from different raters (number of votes). The category with the highest vote count was selected as the consensus on that specific image for the panel. If there was a tie between 2 or 3 categories, the worst category was selected (Fail &lt; Maybe &lt; OK).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24961</offset><text>We tested different panel configurations, large and small, for expert and Zooniverse raters separately. Large panels were composed either by all 9 experts (panel Ec) or 41 Zooniverse users (panel Zc). We compared the agreement between Ec and Zc versus each individual expert rater (R1 to R9) as well as the ratings from the radiologist (Ra). For small panel, experts were arbitrarily split into three panels of three raters (panels Ec1, Ec2, and Ec3). The Zooniverse users were also arbitrarily split into two independent consensus panels of roughly equal size (Zc1 and Zc2). We quantified the agreement between small panels, as well as small vs. large panels.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>25622</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>25630</offset><text>Expert Raters Achieved Moderate Agreement, With “Fail” Rating Being the Most Reliable</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25720</offset><text>Kappa agreement between expert raters across the three classes (OK, Maybe, Fail) was moderate to substantial (range 0.4–0.68, average of 0.54 ± 0.08), see Figure 2. However, there were marked differences in agreement across the three rating classes. The highest reliability was for “Fail,” with between-rater Dice agreement ranging from substantial to almost perfect (0.67–0.93, average of 0.8 ± 0.06). The second class in terms of reliability was “OK,” with Dice ranging from fair to strong (0.38–0.76), and the least reliable class was “Maybe,” with Dice agreements ranging from slight to strong (0.23–0.72).</text></passage><passage><infon key="file">fninf-14-00007-g002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26353</offset><text>Between-expert agreement. (A) Matrix of Kappa agreement between raters (top). Note that R1 to R9 are identification codes for the different expert raters. The distribution of agreement is also presented (bottom). For example, the boxplot for R1 shows the agreement between R1 and R2-R9. (B–D) Matrix and distribution for the Dice agreement between raters in the OK (B), Maybe (C), and Fail (D) categories.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26761</offset><text>Large Panel of Experts or Zooniverse Raters Give Convergent, Reliable QC Ratings</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26842</offset><text>We found that the kappa between Ec and individual expert raters was, as expected, improved over comparison between pairs of individual experts, with a range from moderate to strong (0.56–0.82), see Figure 3. As observed before, the Dice scores for Ec were highest in the “Fail” category (almost perfect agreement, range of 0.76–0.98), followed by the “OK” category (from substantial to almost perfect: range 0.66–0.85) and finally “Maybe” (fair to almost perfect, ranging from 0.38 to 0.8). These findings confirmed our previous expert inter-rater analysis, with “Fail” being a reliable rating, “Maybe” being a noisy rating, and “OK” being a moderately reliable rating. When comparing the individual experts with the Zooniverse panel Zc, we only observed a slight decrease in average Kappa compared with the Expert panel (0.61 for Zc vs. 0.7 for Ec), mostly driven by the “Fail” (0.82 for Zc vs. 0.88 for Ec) and “Maybe” (0.58 for Zc vs. 0.68 for Ec) ratings. When directly computing the agreement between the two consensus ranels Ec and Zc, the kappa was substantial (0.76), with almost perfect agreement for “Fail” (Dice 0.9) and “OK” (0.82), and substantial agreement for “Maybe” (0.77), see Figure 3. This comparison demonstrated that aggregating multiple ratings improved the overall quality, and that expert and zooniverse raters converged to similar ratings. The radiologist achieved a level of agreement with panels similar to what was observed with expert raters, and was substantially lower than the agreement between panels. This shows that the QC training material alone was enough for a radiologist to agree with QC experts, but a single user can likely not achieve high quality QC ratings by herself.</text></passage><passage><infon key="file">fninf-14-00007-g003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28607</offset><text>Zooniverse, expert and radiologist agreements. (A) Matrix of Kappa agreement between consensus of experts (Ec), zooniverse users (Zc) and radiologist (Ra) raters, in rows, vs. individual experts (R1–R9), in column (top). The distribution of agreement is also presented (bottom). (B–D) Matrix and distribution for the Dice agreement in the OK (B), Maybe (C), and Fail (D) categories.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>28994</offset><text>Small Consensus Panels of Expert (N = 3) or Zooniverse (N = 20) Raters Achieve Reliable QC Ratings</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29093</offset><text>Once we established that large panels of raters lead to high levels of agreements, our next question was to determine whether small panels could also lead to reliable assessments. The small expert panels Ec1-3 reached lower agreement with Zc than the full Ec. Specifically, kappa was 0.64, 0.64, and 0.73 for Ec1 to Ec3 (with respect to Zc), compared to kappa of 0.76 for Ec vs. Zc. Similar observations were done when breaking down the comparison per category with Dice, with a decrease of 5% to 10% in this coefficient (see Figure 4). Comparing small zooniverse panels Zc1-2 with the full expert panel Ec, a slight decrease in reliability was observed, very similar in magnitude with comparisons between Ec1-3 and Zc. The agreements Ec1-3 vs. Zc, as well as Zc1-2 vs. Ec, remained substantial. This suggests that reliable three-level QC assessments can be performed by small panels of three experts (n = 3), or moderate panels of zooniverse users, with roughly 20 assessments by image (see Supplementary Figure S1 for distribution).</text></passage><passage><infon key="file">fninf-14-00007-g004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30128</offset><text>Agreement between small panels of raters for both experts and Zooniverse panels. (A) Matrix of Kappa agreement between large panel consensus of experts (Ec), zooniverse users (Zc) and a small panel of expert (Ec1 = 3 rater, Ec2 = 3 rater, Ec3 = 3 rater) and small panel of Zooniverse raters (Zc1 = 20 rater, Zc2 = 21 rater) (top). The distribution of agreement is also presented (bottom). (B–D) Dice distribution between group consensus in the OK (B), Maybe (C), and Fail (D) categories.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>30618</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30629</offset><text>This project proposes a standardized QC protocol with minimal training overhead and no required knowledge of brain anatomy. Our goal was to quantify the reliability of QC ratings between expert raters, as well as panels of expert or Zooniverse raters. Overall, our results demonstrated that our protocol leads to good reliability across individual expert raters, in particular for “Fail” images, and good reliability across panels of raters (both experts and Zooniverse), even for panels featuring only three experts. To our knowledge, this is the first quantitative assessment of between-rater agreement on QC of brain registration.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>31267</offset><text>Visual QC</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31277</offset><text>Our protocol was designed to be simple enough that even a rater without brain anatomy knowledge or prior QC experience could generate meaningful ratings. The mosaic view of 9 slices used in our protocol is similar to display images used in fMRI preprocessing tools like MRIQC, fMRIPrep or CONN. These QC tools also use an overlay that highlights brain borders or tissues segmentation. Differentiating aspects of our protocol are (1) fewer number of brain slices in the mosaic view, so that raters can more easily examine all presented images and (2) the overlay provides an objective confidence interval to assess the severity of misregistration in key anatomical landmarks. We believe that these two design principles helped reduce the subjectivity of brain registration QC, and increase between-rater agreement, although we did not formally test these hypotheses.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>32143</offset><text>Inter-Rater Agreement</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32165</offset><text>Table 1 shows that the visual QC agreement reported in recent studies ranged from 0.39 to 0.9. Interestingly, the studies which reached high levels of agreement (0.93–0.96) used ratings with only two levels (ex: pass, fail). Studies with three or more rating levels reported lower agreement scores (0.39–0.85), which were in line with our findings (average of 0.54 for experts). The most challenging rating in our protocol appeared to be the “Maybe” class, featuring mild, spatially limited registration errors. In contrast, good and failed registrations were easily detectable by expert raters. When working with three levels of ratings, the reliability of our protocol is not high enough to work with a single rater. We found that a consensus panel of three experts was sufficient to reach a good level of agreement (average of 0.64), which appears as a minimum panel size to generate high quality QC scores. Aggregating rating between expert or non-expert is a good solution to overcome the variability among human observers on the QC task.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>33217</offset><text>Crowd Sourced QC</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33234</offset><text>Crowdsourcing QC rating could be one solution to generate high quality QC ratings in big datasets like the UK biobank. A recent work from showed that crowdsourced QC ratings on raw brain images can reach the performance of an automated state-of-the-art machine learning QC tool. This work relied on a large pool (N = 261) of participants, many of whom had prior experience in neuroimaging. We recruited more than 2000 zooniverse non-expert raters, and found that a consensus panel of non-experts with adequate size (about 40 ratings per image) leads to QC ratings of similar quality to a panel of three experts.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>33846</offset><text>Limitations of the Study</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33871</offset><text>Our study has a number of limitations. First, our protocol is intended to be used with anatomical brain registration in the context of fMRI analyses in volumetric space, rather than surface. Structural brain imaging studies (i.e. cortical thickness) or surface-based fMRI analyses need other protocols that examine more closely fine anatomy and tissue segmentation. Also, our primary use case is large-scale research studies, and not clinical applications. Some clinical applications may require more stringent standards being applied on brain registration. Our protocol was validated with a specific brain registration tool, the CIVET pipeline, and may not be well suited for other algorithms.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34566</offset><text>Second, we did not control for screen size, screen resolution or fidelity of color representations in our validation, be it with experts or zooniverse individuals. The main use case for our protocol is the review of thousands of brain registration [e.g. in the ABCD sample ] in a relatively short span of time. The quality control procedure only examines coarse anatomical landmarks, and the required precision of the alignment is on the order of couple of millimeters. For that reason, we think that the characteristics of the screen will not affect significantly between-rater agreement. This is however a potential source of variations which may have decreased the observed agreement, both between experts and zooniverse raters.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35298</offset><text>Third, The success rate of our registration tool varies widely as a function of the imaging protocol. The Cobre dataset has almost only OK registration, while the ADHD has a lot of Maybe and some Fail. So we decided to mix two dataset, in order to assemble representative examples of the three classes. This may influence the results by increasing the potential agreement, if subjects learned to recognize which datasets the examples originated from.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35749</offset><text>Fourth, our choice on the number of rated images (N = 100) was selected arbitrarily. We checked the appropriateness of that choice by assessing the minimum number of rated cases with a three-choice decision using the R package “irr”, that uses the minimum sample size estimation formula from. We estimated the minimum sample size under the following scenario. The vector of marginal probability was given by rates for the 3 categories, OK = 0.3, Maybe = 0.35 and Fail = 0.35. These marginal probabilities were decided by our team when designing the dataset, based on an initial QC assessment performed by YB and PB. The value of kappa under the null hypothesis was set equal to 0.5 (k0 = 0.5) – i.e. we want to demonstrate an improvement over a baseline κ of 0.5. The true kappa statistic estimated between two expert was set equal to 0.72 (k1 = 0.72), as was observed in our sample. The type I error test was set equal to 0.05 (α = 0.05). The desired power to detect the difference between the true kappa and the null kappa was investigated at 0.8 and 0.9, separately. The required number of ratings was estimated at N = 54 for a power of 0.8, and N = 72 for a power of 0.9. In our case, the number of images rated per expert was N = 100, which is more than required by the power analysis.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37048</offset><text>Fifth, We were unable to assess to what degree this protocol improves or not over current best practices in the fMRI community, in the absence of other standardized protocols available for comparison. We still produced preliminary evidence while developing the current protocol. During the beta phase of our project, we tested the agreement between consensus of Zooniverse raters and experts raters (on 29 images). The protocol used during that phase was different from the actual one. In particular, we did not instruct raters on how to take the final decision on the quality of registration (Figures 1C–E), and we did not offer a training set. The kappa measure between consensus Zooniverse raters and an expert during phase 1 was 0.34, by contrast with 0.61 using the current protocol. We regard these results as preliminary evidence that our protocol improves over our previous iteration. These results are to be interpreted with caution, as the number of images rated was low and we used only one expert rating. Note that the feedback received by beta testers helped us identify the importance of steps described in Figures 1C–E, and we suspect that protocols that do not include such detailed explanation have poor reliability. But we did not attempt to demonstrate this formally within the scope of the present study.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38377</offset><text>Finally, our protocol is missing an evaluation of another key registration step, i.e. alignment between functional images and the structural scan. We are currently working on an extension of our protocol for functional registration.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>38610</offset><text>Future Work: Impact of QC on Downstream Analyses</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38659</offset><text>Despite the ubiquity of visual brain registration QC in the neuroimaging research community, the impact of visual QC of brain registration on statistical analyses remains poorly characterized. used a multi-site dataset of structural MRI images with different age ranges to show how automated image quality metrics impacted regional gray matter volumes and their relationship with age. showed a significant impact of visual QC on the estimation of cortical trajectories. They demonstrated that, when omitting to discard subjects that did not pass QC, the developmental trajectory of cortical thickness followed a quadratic or cubic trend. By contrast, after filtering those subjects, the trajectory followed a linear trend. Standardizing the QC protocol will allow different laboratories to join their effort of rating and open up new opportunities to systematically investigate the impact of visual QC on the relationship between the brain and various phenotypes. This represents an important area of future work for brain registration.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>39696</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>39707</offset><text>Our QC protocol is the first reliable visual protocol for brain registration in fMRI studies. The protocol is easy to implement and requires minimum training effort. This protocol demonstrates a good reliability when performing a two level assessment (Fail vs. OK/Maybe) by an individual rater, or aggregating multiple three-level ratings (OK, Maybe, Fail) from a panel of experts (3 minimum) or non-experts (15 minimum). The images necessary to apply the protocol can be generated using an open-source tool, called dashQC_fmri and a live version can be tested on this link https://simexp.github.io/dashQC_BrainMatch/index.html. We hope this new protocol will help standardize the evaluation and reporting of brain registration in the fMRI community. This standardization effort will also enable the generation of high quality QC ratings on large amounts of data, which will in turn allow to train machine learning models to automatically perform brain registration QC, alleviating the need for visual review.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>40717</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>40745</offset><text>The datasets generated for this study are available on request to the corresponding author.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>40837</offset><text>Ethics Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>40854</offset><text>The studies involving human participants were reviewed and approved by the Comité d’éthique de la recherche vieillissement-neuroimagerie. Written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>41151</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>41172</offset><text>YB, AB, SU, HS, and PB contributed to the conception, revision, and design of the QC protocol. YB, SU, and PB generated and reviewed code for the study. YB, JA, and SU developed the QC interface. TO performed the rating as gold standard. All authors contributed to the manuscript revision, read, and approved the submitted version.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>41504</offset><text>Conflict of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>41525</offset><text>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>41698</offset><text>Funding. YB, AB, and PB are currently supported by the Canadian Consortium on Neurodegeneration in Aging (CCNA) and the Courtois NeuroMod Foundation. AB is currently supported by a CIHR Postdoctoral Fellowship (funding reference number #152548). At the start of the project AB was supported by the Alzheimer Society of Canada Postdoctoral Fellowship. PB is a Research Scholar from the Fonds de Recherche du Québec.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>42114</offset><text>www.zooniverse.org</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>42133</offset><text>https://github.com/SIMEXP/brain_match</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>42171</offset><text>https://www.zooniverse.org/projects/simexp/brain-match</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>42226</offset><text>https://www.zooniverse.org/lab-policies</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>42266</offset><text>https://www.zooniverse.org/projects/simexp/brain-match</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>42321</offset><text>Supplementary Material</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>42344</offset><text>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fninf.2020.00007/full#supplementary-material</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>42502</offset><text>References</text></passage><passage><infon key="fpage">400</infon><infon key="lpage">424</infon><infon key="name_0">surname:Alfaro-Almagro;given-names:F.</infon><infon key="name_1">surname:Jenkinson;given-names:M.</infon><infon key="name_2">surname:Bangerter;given-names:N. K.</infon><infon key="name_3">surname:Andersson;given-names:J. L. R.</infon><infon key="name_4">surname:Griffanti;given-names:L.</infon><infon key="name_5">surname:Douaud;given-names:G.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2017.10.034</infon><infon key="pub-id_pmid">29079522</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">166</infon><infon key="year">2018</infon><offset>42513</offset><text>Image processing and quality control for the first 10,000 brain imaging datasets from UK Biobank.</text></passage><passage><infon key="fpage">95</infon><infon key="lpage">113</infon><infon key="name_0">surname:Ashburner;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2007.07.007</infon><infon key="pub-id_pmid">17761438</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2007</infon><offset>42611</offset><text>A fast diffeomorphic image registration algorithm.</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">35</infon><infon key="name_0">surname:Avants;given-names:B. B.</infon><infon key="name_1">surname:Tustison;given-names:N.</infon><infon key="name_2">surname:Song;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Insight J.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2009</infon><offset>42662</offset><text>Advanced normalization tools (ANTS).</text></passage><passage><infon key="issue">558</infon><infon key="name_0">surname:Backhausen;given-names:L. L.</infon><infon key="name_1">surname:Herting;given-names:M. M.</infon><infon key="name_2">surname:Buse;given-names:J.</infon><infon key="name_3">surname:Roessner;given-names:V.</infon><infon key="name_4">surname:Smolka;given-names:M. N.</infon><infon key="name_5">surname:Vetter;given-names:N. C.</infon><infon key="pub-id_doi">10.3389/fnins.2016.00558</infon><infon key="pub-id_pmid">27999528</infon><infon key="section_type">REF</infon><infon key="source">Front. Neurosci.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2016</infon><offset>42699</offset><text>Quality control of structural MRI images applied using FreeSurfer-a hands-on workflow to rate motion artifacts.</text></passage><passage><infon key="fpage">275</infon><infon key="lpage">286</infon><infon key="name_0">surname:Bellec;given-names:P.</infon><infon key="name_1">surname:Chu;given-names:C.</infon><infon key="name_2">surname:Chouinard-Decorte;given-names:F.</infon><infon key="name_3">surname:Benhajali;given-names:Y.</infon><infon key="name_4">surname:Margulies;given-names:D. S.</infon><infon key="name_5">surname:Craddock;given-names:R. C.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2016.06.034</infon><infon key="pub-id_pmid">27423255</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">144(Pt B)</infon><infon key="year">2017</infon><offset>42811</offset><text>The neuro bureau ADHD-200 preprocessed repository.</text></passage><passage><infon key="fpage">5331</infon><infon key="lpage">5342</infon><infon key="name_0">surname:Calhoun;given-names:V. D.</infon><infon key="name_1">surname:Wager;given-names:T. D.</infon><infon key="name_2">surname:Krishnan;given-names:A.</infon><infon key="name_3">surname:Rosch;given-names:K. S.</infon><infon key="name_4">surname:Seymour;given-names:K. E.</infon><infon key="name_5">surname:Nebel;given-names:M. B.</infon><infon key="pub-id_doi">10.1002/hbm.23737</infon><infon key="pub-id_pmid">28745021</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2017</infon><offset>42862</offset><text>The impact of T1 versus EPI spatial normalization templates for fMRI data analyses.</text></passage><passage><infon key="fpage">1191</infon><infon key="lpage">1205</infon><infon key="name_0">surname:Cardamone;given-names:C.</infon><infon key="name_1">surname:Schawinski;given-names:K.</infon><infon key="name_2">surname:Sarzi;given-names:M.</infon><infon key="name_3">surname:Bamford;given-names:S. P.</infon><infon key="name_4">surname:Bennert;given-names:N.</infon><infon key="name_5">surname:Urry;given-names:C. M.</infon><infon key="pub-id_doi">10.1111/j.1365-2966.2009.15383.x</infon><infon key="section_type">REF</infon><infon key="source">Mon. Not. R. Astron. Soc.</infon><infon key="type">ref</infon><infon key="volume">399</infon><infon key="year">2009</infon><offset>42946</offset><text>Galaxy zoo green peas: discovery of a class of compact extremely star-forming galaxies.</text></passage><passage><infon key="fpage">43</infon><infon key="lpage">54</infon><infon key="name_0">surname:Casey;given-names:B. J.</infon><infon key="name_1">surname:Cannonier;given-names:T.</infon><infon key="name_2">surname:Conley;given-names:M. I.</infon><infon key="name_3">surname:Cohen;given-names:A. O.</infon><infon key="name_4">surname:Barch;given-names:D. M.</infon><infon key="name_5">surname:Heitzeg;given-names:M. M.</infon><infon key="pub-id_doi">10.1016/j.dcn.2018.03.001</infon><infon key="pub-id_pmid">29567376</infon><infon key="section_type">REF</infon><infon key="source">Dev. Cogn. Neurosci.</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2018</infon><offset>43034</offset><text>The adolescent brain cognitive development (ABCD) study: imaging acquisition across 21 sites.</text></passage><passage><infon key="fpage">37</infon><infon key="lpage">46</infon><infon key="name_0">surname:Cohen;given-names:J.</infon><infon key="pub-id_doi">10.1177/001316446002000104</infon><infon key="section_type">REF</infon><infon key="source">Educ. Psychol. Meas.</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">1960</infon><offset>43128</offset><text>A coefficient of agreement for nominal scales.</text></passage><passage><infon key="fpage">213</infon><infon key="lpage">220</infon><infon key="name_0">surname:Cohen;given-names:J.</infon><infon key="pub-id_doi">10.1037/h0026256</infon><infon key="pub-id_pmid">19673146</infon><infon key="section_type">REF</infon><infon key="source">Psychol. Bull.</infon><infon key="type">ref</infon><infon key="volume">70</infon><infon key="year">1968</infon><offset>43175</offset><text>Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit.</text></passage><passage><infon key="fpage">1271</infon><infon key="lpage">1294</infon><infon key="name_0">surname:Collins;given-names:D. L.</infon><infon key="name_1">surname:Evans;given-names:A. C.</infon><infon key="pub-id_doi">10.1142/s0218001497000597</infon><infon key="section_type">REF</infon><infon key="source">Intern. J. Pattern Recognit. Artif. Intell.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1997</infon><offset>43268</offset><text>Animal: validation and applications of nonlinear registration-based segmentation.</text></passage><passage><infon key="fpage">192</infon><infon key="lpage">205</infon><infon key="name_0">surname:Collins;given-names:D. L.</infon><infon key="name_1">surname:Neelin;given-names:P.</infon><infon key="name_2">surname:Peters;given-names:T. M.</infon><infon key="name_3">surname:Evans;given-names:A. C.</infon><infon key="pub-id_doi">10.1097/00004728-199403000-00005</infon><infon key="pub-id_pmid">8126267</infon><infon key="section_type">REF</infon><infon key="source">J. Comput. Assist. Tomogr.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">1994</infon><offset>43350</offset><text>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space.</text></passage><passage><infon key="fpage">191</infon><infon key="lpage">200</infon><infon key="name_0">surname:Dadar;given-names:M.</infon><infon key="name_1">surname:Fonov;given-names:V. S.</infon><infon key="name_2">surname:Collins;given-names:D. L.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2018.03.025</infon><infon key="pub-id_pmid">29548850</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">174</infon><infon key="year">2018</infon><offset>43444</offset><text>A comparison of publicly available linear MRI stereotaxic registration techniques.</text></passage><passage><infon key="fpage">267</infon><infon key="lpage">279</infon><infon key="name_0">surname:Ducharme;given-names:S.</infon><infon key="name_1">surname:Albaugh;given-names:M. D.</infon><infon key="name_2">surname:Nguyen;given-names:T.-V.</infon><infon key="name_3">surname:Hudziak;given-names:J. J.</infon><infon key="name_4">surname:Mateos-Pérez;given-names:J. M.</infon><infon key="name_5">surname:Labbe;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2015.10.010</infon><infon key="pub-id_pmid">26463175</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">125</infon><infon key="year">2016</infon><offset>43527</offset><text>Trajectories of cortical thickness maturation in normal brain development–the importance of quality control procedures.</text></passage><passage><infon key="issue">e0184661</infon><infon key="name_0">surname:Esteban;given-names:O.</infon><infon key="name_1">surname:Birman;given-names:D.</infon><infon key="name_2">surname:Schaer;given-names:M.</infon><infon key="name_3">surname:Koyejo;given-names:O. O.</infon><infon key="name_4">surname:Poldrack;given-names:R. A.</infon><infon key="name_5">surname:Gorgolewski;given-names:K. J.</infon><infon key="pub-id_doi">10.1371/journal.pone.0184661</infon><infon key="pub-id_pmid">28945803</infon><infon key="section_type">REF</infon><infon key="source">PloS One</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2017</infon><offset>43649</offset><text>MRIQC: advancing the automatic prediction of image quality in MRI from unseen sites.</text></passage><passage><infon key="comment">(accessed December, 2018)</infon><infon key="name_0">surname:Esteban;given-names:O.</infon><infon key="name_1">surname:Blair;given-names:R.</infon><infon key="name_2">surname:Markiewicz;given-names:C. J.</infon><infon key="name_3">surname:Berleant;given-names:S. L.</infon><infon key="name_4">surname:Moodie;given-names:C.</infon><infon key="name_5">surname:Ma;given-names:F.</infon><infon key="pub-id_doi">10.5281/zenodo.1897231</infon><infon key="section_type">REF</infon><infon key="source">Fmriprep.</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>43734</offset></passage><passage><infon key="fpage">111</infon><infon key="lpage">116</infon><infon key="name_0">surname:Esteban;given-names:O.</infon><infon key="name_1">surname:Markiewicz;given-names:C. J.</infon><infon key="name_2">surname:Blair;given-names:R. W.</infon><infon key="name_3">surname:Moodie;given-names:C. A.</infon><infon key="name_4">surname:Isik;given-names:A. I.</infon><infon key="name_5">surname:Erramuzpe;given-names:A.</infon><infon key="pub-id_doi">10.1038/s41592-018-0235-4</infon><infon key="pub-id_pmid">30532080</infon><infon key="section_type">REF</infon><infon key="source">Nat. Methods</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2019</infon><offset>43735</offset><text>fMRIPrep: a robust preprocessing pipeline for functional MRI.</text></passage><passage><infon key="fpage">321</infon><infon key="lpage">325</infon><infon key="name_0">surname:Flack;given-names:V. F.</infon><infon key="name_1">surname:Afifi;given-names:A. A.</infon><infon key="name_2">surname:Lachenbruch;given-names:P. A.</infon><infon key="name_3">surname:Schouten;given-names:H. J. A.</infon><infon key="pub-id_doi">10.1007/bf02294215</infon><infon key="section_type">REF</infon><infon key="source">Psychometrika</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">1988</infon><offset>43797</offset><text>Sample size determinations for the two rater kappa statistic.</text></passage><passage><infon key="comment">Preprint</infon><infon key="name_0">surname:Fonov;given-names:V.</infon><infon key="name_1">surname:Dadar;given-names:M.</infon><infon key="name_2">surname:Louis Collins;given-names:D.</infon><infon key="pub-id_doi">10.1101/303487</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>43859</offset><text>Deep learning of quality control for stereotaxic registration of human brain MRI.</text></passage><passage><infon key="fpage">313</infon><infon key="lpage">327</infon><infon key="name_0">surname:Fonov;given-names:V.</infon><infon key="name_1">surname:Evans;given-names:A. C.</infon><infon key="name_2">surname:Botteron;given-names:K.</infon><infon key="name_3">surname:Almli;given-names:C. R.</infon><infon key="name_4">surname:McKinstry;given-names:R. C.</infon><infon key="name_5">surname:Collins;given-names:D. L.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2010.07.033</infon><infon key="pub-id_pmid">20656036</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2011</infon><offset>43941</offset><text>Unbiased average age-appropriate atlases for pediatric studies.</text></passage><passage><infon key="issue">S102</infon><infon key="name_0">surname:Fonov;given-names:V. S.</infon><infon key="name_1">surname:Evans;given-names:A. C.</infon><infon key="name_2">surname:McKinstry;given-names:R. C.</infon><infon key="name_3">surname:Almli;given-names:C. R.</infon><infon key="name_4">surname:Collins;given-names:D. L.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2010.07.033</infon><infon key="pub-id_pmid">20656036</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2009</infon><offset>44005</offset><text>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood.</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">20</infon><infon key="name_0">surname:Franzoni;given-names:C.</infon><infon key="name_1">surname:Sauermann;given-names:H.</infon><infon key="pub-id_doi">10.1016/j.respol.2013.07.005</infon><infon key="section_type">REF</infon><infon key="source">Res. Policy</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2014</infon><offset>44089</offset><text>Crowd science: the organization of scientific research in open collaborative projects.</text></passage><passage><infon key="comment">(accessed September, 2016)</infon><infon key="name_0">surname:Gamer;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Irr (Version 0.84.1) [Linux].</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>44176</offset></passage><passage><infon key="name_0">surname:Gilmore;given-names:A.</infon><infon key="name_1">surname:Buser;given-names:N.</infon><infon key="name_2">surname:Hanson;given-names:J. L.</infon><infon key="pub-id_doi">10.1101/581876</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>44177</offset><text>Variations in structural MRI quality impact measures of brain anatomy: relations with age and other sociodemographic variables.</text></passage><passage><infon key="fpage">1175</infon><infon key="lpage">1187</infon><infon key="name_0">surname:Glasser;given-names:M. F.</infon><infon key="name_1">surname:Smith;given-names:S. M.</infon><infon key="name_2">surname:Marcus;given-names:D. S.</infon><infon key="name_3">surname:Andersson;given-names:J. L. R.</infon><infon key="name_4">surname:Auerbach;given-names:E. J.</infon><infon key="name_5">surname:Behrens;given-names:T. E. J.</infon><infon key="pub-id_doi">10.7554/eLife.38976</infon><infon key="pub-id_pmid">27571196</infon><infon key="section_type">REF</infon><infon key="source">Nat. Neurosci.</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2016</infon><offset>44305</offset><text>The human connectome project’s neuroimaging approach.</text></passage><passage><infon key="comment">(accessed May, 2019)</infon><infon key="name_0">surname:Howe;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">The Rise of Crowdsourcing.</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>44361</offset></passage><passage><infon key="name_0">surname:Janke;given-names:A.</infon><infon key="name_1">surname:Fonov;given-names:V. S.</infon><infon key="section_type">REF</infon><infon key="source">Register (Version 1.4.00) [Linux].</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>44362</offset></passage><passage><infon key="fpage">782</infon><infon key="lpage">790</infon><infon key="name_0">surname:Jenkinson;given-names:M.</infon><infon key="name_1">surname:Beckmann;given-names:C. F.</infon><infon key="name_2">surname:Behrens;given-names:T. E. J.</infon><infon key="name_3">surname:Woolrich;given-names:M. W.</infon><infon key="name_4">surname:Smith;given-names:S. M.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2011.09.015</infon><infon key="pub-id_pmid">21979382</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2012</infon><offset>44363</offset><text>FSL.</text></passage><passage><infon key="name_0">surname:Keshavan;given-names:A.</infon><infon key="name_1">surname:Yeatman;given-names:J. D.</infon><infon key="name_2">surname:Rokem;given-names:A.</infon><infon key="pub-id_doi">10.1101/363382</infon><infon key="pub-id_pmid">31139070</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>44368</offset><text>Combining citizen science and deep learning to amplify expertise in neuroimaging.</text></passage><passage><infon key="fpage">1175</infon><infon key="lpage">1177</infon><infon key="name_0">surname:Khatib;given-names:F.</infon><infon key="name_1">surname:DiMaio;given-names:F.</infon><infon key="name_2">surname:Cooper;given-names:S.</infon><infon key="name_3">surname:Kazmierczyk;given-names:M.</infon><infon key="pub-id_doi">10.1038/nsmb.2119</infon><infon key="pub-id_pmid">21926992</infon><infon key="section_type">REF</infon><infon key="source">Nat. Struct. Mol. Biol.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2011</infon><offset>44450</offset><text>Crystal structure of a monomeric retroviral protease solved by protein folding game players.</text></passage><passage><infon key="fpage">331</infon><infon key="lpage">336</infon><infon key="name_0">surname:Kim;given-names:J. S.</infon><infon key="name_1">surname:Greene;given-names:M. J.</infon><infon key="name_2">surname:Zlateski;given-names:A.</infon><infon key="name_3">surname:Lee;given-names:K.</infon><infon key="name_4">surname:Richardson;given-names:M.</infon><infon key="name_5">surname:Turaga;given-names:S. C.</infon><infon key="pub-id_doi">10.1038/nature13240</infon><infon key="pub-id_pmid">24805243</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">509</infon><infon key="year">2014</infon><offset>44543</offset><text>Space-time wiring specificity supports direction selectivity in the retina.</text></passage><passage><infon key="fpage">116</infon><infon key="lpage">129</infon><infon key="name_0">surname:Klapwijk;given-names:E. T.</infon><infon key="name_1">surname:van de Kamp;given-names:F.</infon><infon key="name_2">surname:van der Meulen;given-names:M.</infon><infon key="name_3">surname:Peters;given-names:S.</infon><infon key="name_4">surname:Wierenga;given-names:L. M.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2019.01.014</infon><infon key="pub-id_pmid">30633965</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">189</infon><infon key="year">2019</infon><offset>44619</offset><text>Qoala-T: a supervised-learning tool for quality control of FreeSurfer segmented MRI data.</text></passage><passage><infon key="fpage">159</infon><infon key="lpage">174</infon><infon key="name_0">surname:Landis;given-names:J. R.</infon><infon key="name_1">surname:Koch;given-names:G. G.</infon><infon key="pub-id_pmid">843571</infon><infon key="section_type">REF</infon><infon key="source">Biometrics</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">1977</infon><offset>44709</offset><text>The measurement of observer agreement for categorical data.</text></passage><passage><infon key="fpage">129</infon><infon key="lpage">140</infon><infon key="name_0">surname:Lintott;given-names:C. J.</infon><infon key="name_1">surname:Schawinski;given-names:K.</infon><infon key="name_2">surname:Keel;given-names:W.</infon><infon key="pub-id_doi">10.1111/j.1365-2966.2009.15299.x</infon><infon key="section_type">REF</infon><infon key="source">Mon. Not. R. Astron. Soc.</infon><infon key="type">ref</infon><infon key="volume">399</infon><infon key="year">2009</infon><offset>44769</offset><text>Galaxy Zoo:’Hanny’s Voorwerp’, a quasar light echo?</text></passage><passage><infon key="fpage">2302</infon><infon key="lpage">2312</infon><infon key="name_0">surname:Mayer;given-names:A. R.</infon><infon key="name_1">surname:Ruhl;given-names:D.</infon><infon key="name_2">surname:Merideth;given-names:F.</infon><infon key="name_3">surname:Ling;given-names:J.</infon><infon key="name_4">surname:Hanlon;given-names:F. M.</infon><infon key="name_5">surname:Bustillo;given-names:J.</infon><infon key="pub-id_doi">10.1002/hbm.22065</infon><infon key="pub-id_pmid">22461278</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2013</infon><offset>44827</offset><text>Functional imaging of the hemodynamic sensory gating response in schizophrenia.</text></passage><passage><infon key="fpage">1394</infon><infon key="lpage">1407</infon><infon key="name_0">surname:Park;given-names:J. G.</infon><infon key="name_1">surname:Lee;given-names:C.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2009.04.047</infon><infon key="pub-id_pmid">19389477</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2009</infon><offset>44907</offset><text>Skull stripping based on region growing for magnetic resonance brain images.</text></passage><passage><infon key="issue">52</infon><infon key="name_0">surname:Pizarro;given-names:R. A.</infon><infon key="name_1">surname:Cheng;given-names:X.</infon><infon key="name_2">surname:Barnett;given-names:A.</infon><infon key="name_3">surname:Lemaitre;given-names:H.</infon><infon key="name_4">surname:Verchinski;given-names:B. A.</infon><infon key="name_5">surname:Goldman;given-names:A. L.</infon><infon key="pub-id_doi">10.3389/fninf.2016.00052</infon><infon key="pub-id_pmid">28066227</infon><infon key="section_type">REF</infon><infon key="source">Front. Neuroinform.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2016</infon><offset>44984</offset><text>Automated quality assessment of structural magnetic resonance brain images based on a supervised machine learning algorithm.</text></passage><passage><infon key="comment">(accessed March, 2018)</infon><infon key="name_0">surname:Rorden;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">MRIcron.</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>45109</offset></passage><passage><infon key="fpage">407</infon><infon key="lpage">418</infon><infon key="name_0">surname:Rosen;given-names:A. F. G.</infon><infon key="name_1">surname:Roalf;given-names:D. R.</infon><infon key="name_2">surname:Ruparel;given-names:K.</infon><infon key="name_3">surname:Blake;given-names:J.</infon><infon key="name_4">surname:Seelaus;given-names:K.</infon><infon key="name_5">surname:Villa;given-names:L. P.</infon><infon key="pub-id_doi">10.1016/j.neuroimage.2017.12.059</infon><infon key="pub-id_pmid">29278774</infon><infon key="section_type">REF</infon><infon key="source">NeuroImage</infon><infon key="type">ref</infon><infon key="volume">169</infon><infon key="year">2018</infon><offset>45110</offset><text>Quantitative assessment of structural image quality.</text></passage><passage><infon key="fpage">1049</infon><infon key="lpage">1054</infon><infon key="name_0">surname:Simpson;given-names:R.</infon><infon key="name_1">surname:Page;given-names:K. R.</infon><infon key="name_2">surname:De Roure;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd International Conference on World Wide Web</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>45163</offset><text>Zooniverse: observing the world’s largest citizen science platform</text></passage><passage><infon key="fpage">87</infon><infon key="lpage">97</infon><infon key="name_0">surname:Sled;given-names:J. G.</infon><infon key="name_1">surname:Zijdenbos;given-names:A. P.</infon><infon key="name_2">surname:Evans;given-names:A. C.</infon><infon key="pub-id_doi">10.1109/42.668698</infon><infon key="pub-id_pmid">9617910</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Med. Imaging</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">1998</infon><offset>45232</offset><text>A nonparametric method for automatic correction of intensity nonuniformity in MRI data.</text></passage><passage><infon key="comment">(accessed March, 2017)</infon><infon key="name_0">surname:Sørensen;given-names:T. A.</infon><infon key="name_1">surname:Sørensen;given-names:T.</infon><infon key="name_2">surname:Sorensen;given-names:T.</infon><infon key="name_3">surname:Sørensen;given-names:T. J.</infon><infon key="name_4">surname:Sørensen;given-names:T. J.</infon><infon key="name_5">surname:Biering-Sørensen;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species and its Application to Analyses of the Vegetation on Danish Commons.</infon><infon key="type">ref</infon><infon key="year">1948</infon><offset>45320</offset></passage><passage><infon key="comment">(accessed December, 2019)</infon><infon key="name_0">surname:Urchs;given-names:S.</infon><infon key="name_1">surname:Armoza;given-names:J.</infon><infon key="name_2">surname:Bellec;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">dashQC: An Interactive Quality Control Dashboard — dashqc_fmri Alpha Documentation [Linux].</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>45321</offset></passage><passage><infon key="comment">n.d. (accessed April 19, 2019)</infon><infon key="section_type">REF</infon><infon key="source">SPM12 – Statistical Parametric Mapping.</infon><infon key="type">ref</infon><offset>45322</offset></passage><passage><infon key="fpage">1218</infon><infon key="lpage">1231</infon><infon key="name_0">surname:White;given-names:T.</infon><infon key="name_1">surname:Jansen;given-names:P. R.</infon><infon key="name_2">surname:Muetzel;given-names:R. L.</infon><infon key="name_3">surname:Sudre;given-names:G.</infon><infon key="name_4">surname:El Marroun;given-names:H.</infon><infon key="name_5">surname:Tiemeier;given-names:H.</infon><infon key="pub-id_doi">10.1002/hbm.23911</infon><infon key="pub-id_pmid">29206318</infon><infon key="section_type">REF</infon><infon key="source">Hum. Brain Mapp.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2018</infon><offset>45323</offset><text>Automated quality assessment of structural magnetic resonance images in children: comparison with visual inspection and surface-based reconstruction.</text></passage><passage><infon key="fpage">125</infon><infon key="lpage">141</infon><infon key="name_0">surname:Whitfield-Gabrieli;given-names:S.</infon><infon key="name_1">surname:Nieto-Castanon;given-names:A.</infon><infon key="pub-id_doi">10.1089/brain.2012.0073</infon><infon key="pub-id_pmid">22642651</infon><infon key="section_type">REF</infon><infon key="source">Brain Connect.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2012</infon><offset>45473</offset><text>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks.</text></passage><passage><infon key="name_0">surname:Yassine;given-names:B.</infon><infon key="name_1">surname:Pierre;given-names:B.</infon><infon key="pub-id_doi">10.6084/m9.figshare.4204845.v1</infon><infon key="section_type">REF</infon><infon key="source">Quality Control and Assessment of the NIAK Functional MRI Preprocessing Pipeline.</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>45563</offset></passage></document></collection>
