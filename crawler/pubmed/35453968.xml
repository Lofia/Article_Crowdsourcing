<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220611</date><key>pmc.key</key><document><id>9028864</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/diagnostics12040920</infon><infon key="article-id_pmc">9028864</infon><infon key="article-id_pmid">35453968</infon><infon key="article-id_publisher-id">diagnostics-12-00920</infon><infon key="elocation-id">920</infon><infon key="issue">4</infon><infon key="kwd">artificial intelligence COVID-19 pre-screening crowdsourcing application deep learning cough and breath sounds spectrogram</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Rahman;given-names:Tawsifur</infon><infon key="name_1">surname:Ibtehaz;given-names:Nabil</infon><infon key="name_10">surname:Mahmud;given-names:Sakib</infon><infon key="name_11">surname:Zughaier;given-names:Susu M.</infon><infon key="name_12">surname:Abbas;given-names:Tariq</infon><infon key="name_13">surname:Al-Maadeed;given-names:Somaya</infon><infon key="name_14">surname:Chowdhury;given-names:Muhammad E. H.</infon><infon key="name_15">surname:Russo;given-names:Alessandro</infon><infon key="name_2">surname:Khandakar;given-names:Amith</infon><infon key="name_3">surname:Hossain;given-names:Md Sakib Abrar</infon><infon key="name_4">surname:Mekki;given-names:Yosra Magdi Salih</infon><infon key="name_5">surname:Ezeddin;given-names:Maymouna</infon><infon key="name_6">surname:Bhuiyan;given-names:Enamul Haque</infon><infon key="name_7">surname:Ayari;given-names:Mohamed Arselene</infon><infon key="name_8">surname:Tahir;given-names:Anas</infon><infon key="name_9">surname:Qiblawey;given-names:Yazan</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">12</infon><infon key="year">2022</infon><offset>0</offset><text>QUCoughScope: An Intelligent Application to Detect COVID-19 Patients Using Cough and Breath Sounds</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>99</offset><text>Problem—Since the outbreak of the COVID-19 pandemic, mass testing has become essential to reduce the spread of the virus. Several recent studies suggest that a significant number of COVID-19 patients display no physical symptoms whatsoever. Therefore, it is unlikely that these patients will undergo COVID-19 testing, which increases their chances of unintentionally spreading the virus. Currently, the primary diagnostic tool to detect COVID-19 is a reverse-transcription polymerase chain reaction (RT-PCR) test from the respiratory specimens of the suspected patient, which is invasive and a resource-dependent technique. It is evident from recent researches that asymptomatic COVID-19 patients cough and breathe in a different way than healthy people. Aim—This paper aims to use a novel machine learning approach to detect COVID-19 (symptomatic and asymptomatic) patients from the convenience of their homes so that they do not overburden the healthcare system and also do not spread the virus unknowingly by continuously monitoring themselves. Method—A Cambridge University research group shared such a dataset of cough and breath sound samples from 582 healthy and 141 COVID-19 patients. Among the COVID-19 patients, 87 were asymptomatic while 54 were symptomatic (had a dry or wet cough). In addition to the available dataset, the proposed work deployed a real-time deep learning-based backend server with a web application to crowdsource cough and breath datasets and also screen for COVID-19 infection from the comfort of the user’s home. The collected dataset includes data from 245 healthy individuals and 78 asymptomatic and 18 symptomatic COVID-19 patients. Users can simply use the application from any web browser without installation and enter their symptoms, record audio clips of their cough and breath sounds, and upload the data anonymously. Two different pipelines for screening were developed based on the symptoms reported by the users: asymptomatic and symptomatic. An innovative and novel stacking CNN model was developed using three base learners from of eight state-of-the-art deep learning CNN algorithms. The stacking CNN model is based on a logistic regression classifier meta-learner that uses the spectrograms generated from the breath and cough sounds of symptomatic and asymptomatic patients as input using the combined (Cambridge and collected) dataset. Results—The stacking model outperformed the other eight CNN networks with the best classification performance for binary classification using cough sound spectrogram images. The accuracy, sensitivity, and specificity for symptomatic and asymptomatic patients were 96.5%, 96.42%, and 95.47% and 98.85%, 97.01%, and 99.6%, respectively. For breath sound spectrogram images, the metrics for binary classification of symptomatic and asymptomatic patients were 91.03%, 88.9%, and 91.5% and 80.01%, 72.04%, and 82.67%, respectively. Conclusion—The web-application QUCoughScope records coughing and breathing sounds, converts them to a spectrogram, and applies the best-performing machine learning model to classify the COVID-19 patients and healthy subjects. The result is then reported back to the test user in the application interface. Therefore, this novel system can be used by patients in their premises as a pre-screening method to aid COVID-19 diagnosis by prioritizing the patients for RT-PCR testing and thereby reducing the risk of spreading of the disease.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>3562</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3578</offset><text>The novel coronavirus-2019 (COVID-19) disease has infected 320 million and caused death to around 5.5 million people worldwide as of 15 January 2022. This has led to countries imposing strict lockdowns to reduce the infection rate, which has severely affected the economic and social lives of people. Mass vaccination has helped some countries, but some countries have entered into second and third waves of infection. Due to the emerging new variants, the pattern of infection and effectiveness of vaccination is still under question. The common symptoms of COVID-19 include fever, cough, shortness of breath, and pneumonia. People with a compromised immune system or elderly people are more likely to develop serious illnesses but the younger population is also affected, especially by the new variants. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4385</offset><text>Currently, diagnosis of COVID-19 is done by time-consuming, expensive, and expert-dependent reverse transcription-polymer chain reaction (RT-PCR) testing. This kit is not easily available in some regions due to a lack of adequate supplies, medical professionals, and healthcare facilities. Moreover, it requires patients to travel to a laboratory facility to be tested, thereby potentially infecting others along the way. Due to the delay in obtaining the results of RT-PCR, rapid antigen detection tests have also been used in many countries, but they suffer from low accuracy. Recently, Artificial Intelligence (AI) has been implemented in the health sector widely, such as on chest X-rays and computed tomography (CT) scans, which have also been used for early detection of COVID-19 and other lung abnormalities. Recently, electrocardiogram (ECG) trace images have been used with AI for the detection of COVID-19 and other cardiovascular diseases. Hasoon et al. proposed a method for classification and early detection of COVID-19 through image processing using X-ray images. The evaluation results showed high diagnosis accuracy, from 89.2% up to 98.66%. Alyasseri et al. provided a comprehensive review of the deep learning and machine learning (ML) techniques for COVID-19 diagnosis from studies between December 2019 and April 2021. This paper included more than 200 studies that were carefully selected from several publishers, such as IEEE, Springer, and Elsevier. It provided COVID-19 public datasets established in and extracted from different countries. Al-Waisy et al. proposed a novel hybrid multimodal deep learning system for identifying COVID-19 virus in chest X-ray (CX-R) images and termed it the COVID-DeepNet system. It aids expert radiologists in rapid and accurate image interpretation, and helps in correctly and accurately diagnosing patients with COVID-19 with an accuracy rate of 99.93%. Abdulkareem et al. proposed a model based on ML and the Internet of Things (IoT) to diagnose patients with COVID-19 in smart hospitals. Compared with benchmark studies, the proposed SVM model obtained the most substantial diagnosis performance (up to 95%). Obaid et al. proposed a prediction mechanism that uses a long short-term memory (LSTM) deep learning model that has been carried out on a coronavirus dataset that was obtained from the records of infections, deaths, and recovered cases across the world. Furthermore, they have stated that by producing a dataset which includes features (temperature and humidity) of geographic regions that have experienced severe virus outbreaks, risk factors, spatiotemporal analysis, and social behavior of people, a predictive model can be developed for areas where the virus is likely to spread. All of the above approaches would need the patient to go to a medical center to provide a sample or undergo testing. However, the asymptomatic COVID-19 patients will not undergo any test until the disease reaches a level of concern. Therefore, these COVID-19 patients can easily spread the disease. Moreover, vaccinated patients, when infected by the virus, are often asymptomatic or show very mild symptoms, and can spread the disease very easily. Thus, there is a need for an early screening tool for such patients in the convenience of their homes. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7694</offset><text>Machine learning has been used for many applications in the field of speech and audio, including machine learning techniques for spectrogram images. It is used for the screening and early detection of different life-threatening diseases. It is stated that breathing, speech, sneezing, and coughing can be used by machine learning models to diagnose different respiratory illnesses such as COVID-19. Different body signals such as respiration or heart signals have been used by researchers to automatically detect different lung and heart diseases (such as wheeze detection in asthma). The human voice has been used for early detection of several diseases such as Parkinson’s disease, coronary artery disease, traumatic brain injury, and brain disorders. Parkinson’s disease was linked to the softness of speech which can result from a lack of vocal muscle coordination. Different voice parameters such as vocal frequency, vocal tone, pitch, rhythm, rate, and volume can be correlated with coronary artery disease. Invisible illnesses such post-traumatic stress disorder, traumatic brain injury, and psychiatric conditions can be linked with audio information. Human-generated audio can be used as a biomarker for the early detection of different diseases and can be a cheap solution for mass population screening and pre-screening. This becomes even more useful and comfortable to the user if it is related to their daily activities and the data acquisition can be done non-invasively.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9184</offset><text>Recent works have showed how respiratory sounds (e.g., coughing, breathing, and voice) from patients who tested positive for COVID-19 in hospitals differ from sounds of healthy people. Digital stethoscope data from lung auscultation is used as a diagnostic signal for COVID-19, while the coughs 48 COVID-19 patients versus patients with other pathological coughs collected with phones were used to detect COVID-19 using an ensemble of CNN models. In, speech recordings from hospitalized COVID-19 patients were used to automatically detect the health status of the patients. Thus, it is possible to identify whether a person is infected by the virus or not by utilizing respiratory signals like breath and cough sounds.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9903</offset><text>Data collection from COVID-19 patients is challenging due to the possibility of getting infected and the datasets are often not publicly available. McFarlane et al. had stressed the need for a COVID-19 cough database which would help the development of an algorithm for detecting COVID from coughs. They used a database of 73 individual cough events from public media and named it NoCoCoDa. They stressed the need for uniformity/consistency in the dataset to help develop reliable algorithms. Grant et al. have utilized crowd-sourced recorded speech, breath, and cough data from 150 COVID-19-positive cases to train a machine learning model. They investigated random forest and deep neural networks using mel-frequency cepstral coefficients (MFCCs) and relative spectral perceptual linear prediction (RASTA-PLP) features and have achieved a 0.7983 area under the curve (AUC) for detecting COVID-19 using speech sound analysis and a 0.7575 AUC for detecting COVID-19 using breathing sounds. Mouawad et al. used MFCC features of cough and vowel ‘eh’ pronunciation from a dataset collected by the Corona Voice Detect project in partnership with Voca.ai and Carnegie Mellon University. They used XGBoost machine learning classifier and achieved an F1-score of 91% for cough and 89% for vowel “eh”. Erdogam and Narin discussed the features of cough spectrogram data with the help of empirical mode decomposition (EMD), discrete wavelet transform (DWT) and the ReliefF algorithm on a dataset from a free-access site, achieving a 98.06% F1-score in detecting COVID from cough sounds. Pahar et al. in have investigated machine learning classifiers, long short-term memory (LSTM), and convolutional neural network (CNNs), and found that the ResNet50 network of the Coswara dataset and Sarcos dataset achieved an AUC of 0.98. Imran et al. proposed a mobile app called AI4COVID-19, which records 3 s of cough sounds to analyze automatically for the detection of COVID-19 within 2 min using transfer learning. The pipeline consists of two stages: cough detection and collection, and COVID-19 diagnosis. In the cough detection engine, a user must record 3 s of good quality cough sounds, and a mel spectrogram image of the wave is analyzed with a convolutional neural network (CNN). After the cough is detected, the system passes to the COVID-19 diagnosis to decide the result. It consists of three AI approaches, the deep transfer learning multi-class classifier (DTL-MC), the classical machine learning multi-class classifier (CML-MC), and the deep transfer learning binary-class classifier. Some key limitations of the current AI4Covide-19 are (1) limited training data, (2) limited data to generalize the model, (3) an AI model is not publicly available. In another study by Pal and Sankarasubbu, the authors investigated deep neural networks (DNNs) on a dataset in which 328 cough sounds had been recorded from 150 patients of four different types: COVID-19, asthma, bronchitis, and healthy. In the study, Pal and Sankarasubbu’s trained DNN could distinguish the COVID-19 coughs from others with an accuracy of 96.83%. These studies confirm that COVID-19 coughs have a unique pattern. Bagad et al. found that a pre-trained ResNet18 classifier could identify COVID-19 coughs with an AUC of 0.72 using COVID-19-confirmed cough samples collected over the phone from 3621 individuals. Laguarta et al. had an AUC of 0.97 and a sensitivity of 98.5% with a pre-trained ResNet50 model for distinguishing COVID-19 coughs from non-COVID-19 patients using coughs which trained on 4256 subjects and tested on the remaining 1064 subjects. Belkacem et al. reported a complete hardware system that can be used to collect cough samples, temperature (via thermos camera) and airflow (via spirometer) and transmit this information to a database using smartphones. Next, cough samples and other health details with expert opinion were used to train a machine learning network to classify the samples as either COVID-19, bronchitis, flu, cold, or other. They used the existing motivation from recent papers that cough samples and machine learning networks are very useful in distinguishing between COVID-19 and healthy patients, but confirmed it with other data (airflow and body temperature). However, they have not mentioned the performance of their approach. A similar approach was adopted by Rahman et al. utilizing chest X-rays, CT Scans, cough samples, temperature, and symptom inputs from patients. Although both the above approaches make the final results very reliable, they cannot be used immediately due to the hardware or extra health details needed for those systems.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14566</offset><text>Brown et al. collected both cough and breathing sounds, then investigated how such data can aid with COVID-19 diagnosis. They provided handcrafted features for cough and breath sounds such as duration, onset, tempo, period, root mean square (RMS) energy, spectral centroid, roll-off frequency, zero-crossing, mel-frequency cepstrum (MFCC), and delta MFCC. Combined with deep transfer learning, VGGish, which is a convolution network designed to extract audio features, automatically achieved an accuracy of 0.80 ± 0.7 for two-class classification problems using the cough and breathing data. This dataset has also been used by Coppock et al. in a pilot study, even before the dataset was made public, with their deep learning network achieving an AUC of 0.846. Kumar et al., with their developed deep convolutional network, achieved a weighted F1-score of 96.46% in distinguishing between non-COVID and COVID-19 patients. This dataset was shared with our team under a data-sharing agreement, and was used to develop a machine learning pipeline in combination with Qatari data. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15645</offset><text>The scope for having a more reliable and robust machine learning network trained and validated using a diverse database (due to limitations in terms of inconsistency and low-quality recordings in the available datasets) has motivated the current work. This work proposes a novel machine-learning framework using the combined Cambridge and Qatari cough and breathing sound databases. Most of the previous works either used classical machine learning with hand-crafted features or used pre-trained models to classify the spectrograms. A very limited number of works used combined datasets and no work has used the novel stacking concept for increasing model performance. Moreover, none of the AI-enabled data collection applications can show instant outcomes of the users’ data. Most applications are mere crowd data collection applications. We developed an AI-enabled web application as a pre-screening tool to decrease the pressure on health centers and provide a faster and more reliable testing mechanism to reduce the spread of the virus. Our contribution can be summarized as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16736</offset><text>Conduct a literature review of related works to prove the potential applicability of the proposed solution. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16845</offset><text>Point out the limitations of related works and how the proposed solution may overcome those problems. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16948</offset><text>To the best of the authors’ knowledge, this is the first time an innovative and novel stacking-based CNN model using spectrograms of cough and breath sounds have been proposed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17127</offset><text>Experimentally prove cough sounds have latent features to distinguish COVID-19 patients from non-COVID patients.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17240</offset><text>A web application with a backend server was created that allows the user to share symptoms and cough and breath data for COVID-19 diagnosis anonymously from a computer, tablet, or Android or iOS mobile phone. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17450</offset><text>To the best of our knowledge, QUCoughScope (, accessed on 5 May 2021) is the first solution that is not just an application to collect crowd-sourced data. Rather, we have implemented a deep-learning pipeline in the backend to immediately provide the screening outcome to the user.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17731</offset><text>This article consists of six sections. In the introduction, we explained the problem of the current COVID-19 testing approach and how it can be addressed with the help of our pre-screening tool. Section II highlights related works, while Section III introduces the methodology, with details of the dataset, data preparation, and experiment, and Section IV summarizes all the results. Section V explains the implementation details while Section VI concludes the article. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>18202</offset><text>2. Methodology</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18217</offset><text>The overall methodology of the study is summarized in Figure 1. This study used cough and breath sounds of COVID-19 (symptomatic and asymptomatic) and healthy subjects after converting these sounds into spectrograms to identify COVID-19 patients. This paper discusses four different binary classification experiments: healthy and COVID-19 symptomatic (i) and asymptomatic (ii) subjects using cough sound spectrograms; healthy and COVID-19 symptomatic (iii) and asymptomatic (iv) subjects using breath sound spectrograms. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18739</offset><text>For all four experiments, novel stacking machine learning models were deployed, in which the eight CNN models were used as the base learners and then a logistic regression-based meta learner was used to detect COVID-19 from cough and breath sound spectrograms. Detailed descriptions of the dataset, preprocessing, and the experiments are presented below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19094</offset><text>2.1. Dataset Description</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19119</offset><text>Several public datasets are available such as Coswara, CoughVid, and the Cambridge dataset. However, the Cambridge dataset was not completely public, and the team has made it available upon request. Among the accessible datasets, the Cambridge dataset was the most reliable as it was acquired in a well-designed framework. Moreover, the authors have collected a similar cough and breath dataset from COVID-19-infected and healthy subjects with our proposed framework. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19588</offset><text>Cambridge dataset: The Cambridge dataset was designed for developing a diagnostic tool for COVID-19 based on cough and breath sounds. The dataset was collected through an app (Android and web application ( (accessed on 5 May 2021))) that asked volunteers for samples of their coughs and breathing as well as their medical history and symptoms. Age, gender, geographical location, current health status, and pre-existing medical conditions were also recorded. Audio recordings were sampled at 44.1 kHz and subjects were from different parts of the world. Cough and breath sound samples were collected from 582 healthy subjects and 141 COVID-19-positive patients. Among them, 264 healthy subjects and 54 COVID-19 patients had cough symptoms while 318 healthy subjects and 87 COVID-19 patients had no symptoms (Table 1). </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20407</offset><text>Qatari dataset: The QU cough dataset consists of both cough and breath data from symptomatic and asymptomatic patients. Cough and breath sound samples were collected from 245 healthy subjects and 96 COVID-19-positive, respectively. Among them, 32 healthy subjects and 18 COVID-19 patients had cough symptoms while 213 healthy subjects and 78 COVID-19 patients had no symptoms (as shown in Table 1). </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20807</offset><text>In this study, we investigated the cough and breath sounds to overcome the limitations of some related works. We have therefore investigated two different pipelines for cough and breath. Moreover, for both cough and breath, we investigated symptomatic and asymptomatic patients’ data. Both datasets were merged to train, validate, and test the models in this study. Table 2 shows the experimental pipelines used in this study. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21237</offset><text>2.2. Pre-Processing Stage</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21263</offset><text>As shown in Figure 1, the input data (i.e., user cough and breath sounds) were converted to spectrograms, which were then tested using a 5-fold cross validation approach with 80% for training and 20% for testing. The detailed pre-processing stage is mentioned below:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>21530</offset><text>2.2.1. Audio to Spectrogram Conversion</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21569</offset><text>Since the dataset was collected using web and Android platforms, it was first organized into two sub-sets: cough and breath sounds. Then, each of these subsets was subdivided into symptomatic and asymptomatic groups. Each of the symptomatic and asymptomatic breath and cough sounds for COVID-19 and healthy groups were visualized in the time domain to see potential differences among them (Figure 2). </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21971</offset><text>Firstly, we converted cough and breath sounds to spectrograms. A spectrogram is a visual representation of an audio signal that shows the evolution of the frequency spectrum over time. A spectrogram is usually generated by performing a Fast Fourier Transform (FFT) on a collection of overlapping windows extracted from the original signal. The process of dividing the signal in short-term sequences of fixed size and applying FFT on those independently is called short-time Fourier transform (STFT). The spectrogram is the squared magnitude of the STFT of the signal, s(t) for a window width, w. These are the parameters used for STFT: n_fft = 2048, hop_length = 512, win_length = n_fft, and window = ‘hann’. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>22685</offset><text>2.2.2. Five-Fold Cross-Validation </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22720</offset><text>The training dataset had to be balanced to avoid biased training. This was done with the help of the data augmentation approach, an effective method for providing reliable results evident in many of the authors’ recent publications. In this study, two augmentation strategies (scaling and translation) were utilized to balance the training images shown in Table 3. The scaling operation is the magnification or reduction of the frame size of the image; 2.5% to 10% image magnifications were used in this work. Image translation was done by translating images horizontally and vertically by 5% to 10%. The complete image set was divided into 80% training and 20% testing sub-sets for five-fold cross-validation, and 10% of training data were used for validation, whose primary purpose was to avoid model overfitting. Table 3 shows the number of training, validation, and test images used in the two experiments on symptomatic and asymptomatic patients.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23674</offset><text>As discussed earlier, eight pre-trained CNN models were used in the study and were implemented using PyTorch library with Python 3.7 on an Intel® Xeon® CPU E5-2697 v4@ 2.30 GHz and 64 GB RAM, with a 16-GB NVIDIA GeForce GTX 1080 GPU. Eight of the pre-trained CNN models were trained using the same training parameters and stopping criteria mentioned in Table 4. Five-fold cross-validation results were averaged to produce the final receiver operating characteristic (ROC) curve, confusion matrix, and evaluation matrices. Here, 80% of the images were used for training and 20% for testing per fold. Image augmentations were used in the training set, and 20% of the non-augmented training set was used for validation to avoid overfitting of the models. We also used a logistic regression classifier as a meta-learner for the final prediction in the stacking model where ‘lbfgs’ solver with L2 regularization was used and the maximum iteration was 100.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24631</offset><text>2.3. Stacking Model Development</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24663</offset><text>In this study, we used a CNN-based stacking approach in which the eight state-of-the-art CNN models (Resnet18, Resnet50, Resnet101, InceptionV3, DenseNet201, Mobilenetv2, EfficientNet_B0, and EfficientNet_B7) were used as a base learner and multiple best-performing models were used to train a logistic regression based meta learner classifier for the final decision. A single dataset A consists of data vectors () and their classification score (). At first, a set of base-level classifiers  is generated and the outputs are used to train the meta-level classifier, as illustrated in Figure 3.</text></passage><passage><infon key="file">no_id_0.xml</infon><infon key="id">no_id_0</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;tbody xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Algorithm 1:&lt;/bold&gt; Stacking classification&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;top&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Input:&lt;/bold&gt;&lt;inline-formula&gt;&lt;mml:math id=&quot;mm11&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;training&lt;/mml:mi&gt;&lt;mml:mtext&gt; &lt;/mml:mtext&gt;&lt;mml:mi&gt;data&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;break/&gt;&lt;bold&gt;Output:&lt;/bold&gt; a stacking classifier H&lt;break/&gt;1: Step 1: learn base-level classifiers&lt;break/&gt;2: &lt;bold&gt;for&lt;/bold&gt; t = 1 to T &lt;bold&gt;do&lt;/bold&gt;&lt;break/&gt;3: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm12&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi&gt;learn&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt; based on D&lt;break/&gt;4: &lt;bold&gt;end for&lt;/bold&gt;&lt;break/&gt;5: Step 2: construct new data set of predictions&lt;break/&gt;6: &lt;bold&gt;for&lt;/bold&gt; i =1 to m &lt;bold&gt;do&lt;/bold&gt;&lt;break/&gt;7: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm13&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;D&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;′&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;inline-formula&gt;&lt;mml:math id=&quot;mm14&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mtext&gt; &lt;/mml:mtext&gt;&lt;mml:mi&gt;where&lt;/mml:mi&gt;&lt;mml:mtext&gt; &lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;′&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mn&gt;…&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;T&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;break/&gt;8: &lt;bold&gt;end for&lt;/bold&gt;&lt;break/&gt;9: Step 3: learn a meta-classifier&lt;break/&gt;10: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm15&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;learn&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi&gt;based&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi&gt;on&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;D&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;break/&gt;11: &lt;bold&gt;return&lt;/bold&gt; H&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;
</infon><offset>25258</offset><text>Algorithm 1: Stacking classification	 	Input:Output: a stacking classifier H1: Step 1: learn base-level classifiers2: for t = 1 to T do3:  based on D4: end for5: Step 2: construct new data set of predictions6: for i =1 to m do7: 8: end for9: Step 3: learn a meta-classifier10: 11: return H	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25551</offset><text>We used five-fold cross-validation to generate a training set for the meta-level classifier. Among these folds, base-level classifiers were used on four folds, leaving one fold for testing. Each base-level classifier predicts a probability (0 to 1) over the possible class values. Thus, using input x, a probability distribution is created using the predictions of the base-level classifier set M: where  is the set of possible class values and  denotes the probability that example x belongs to a class  as estimated (and predicted) by classifier M in Equation (1). The class  with the highest-class probability is predicted by classifier Mj. The meta-level classifier  and attributes are thus the probabilities predicted for each possible class by each of the base-level classifiers, i.e.,  for i = 1,…., p and j = 1,…., N. The pseudo-code for the stacking approach is shown in Algorithm 1. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>26449</offset><text>2.4. Performance Metrics</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26474</offset><text>To evaluate the performance of the COVID-19 detection classifiers, we used the receiver operating characteristic (ROC) and area under the curve (AUC) along with precision, sensitivity, specificity, accuracy, and F1-Score as shown in Equations (2)–(6). Here, TP, TN, FP, and FN represent the true positive, true negative, false positive, and false negative, respectively.  where accuracy is the ratio of the correctly classified samples to all the samples.  where precision is the rate of correctly classified positive class samples among all the samples classified as positive.  where sensitivity is the rate of correctly predicted positive samples in the positive class samples,  where F1 is the harmonic average of precision and sensitivity.  where specificity is the ratio of accurately predicted negative class samples to all negative class samples. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27331</offset><text>The performance of deep CNNs was assessed using different evaluation metrics with 95% confidence intervals (CIs). Accordingly, the CI for each evaluation metric was computed, as shown in Equation (7): where N is the number of test samples, and z is the level of significance that is 1.96 for 95% CI.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27631</offset><text>In addition to the above metrics, the various classification networks were compared in terms of elapsed time per image, or the time it took each network to classify an input image, as shown in Equation (8). </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27839</offset><text>In this equation, T1 is the starting time for a network to classify a cough sound, S and T2 is the end time when the network has classified the same cough sound, S.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>28004</offset><text>3. Results and Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28030</offset><text>This section describes the performance of the different classification networks on healthy and COVID-19 cough and breath sound spectrograms for symptomatic and asymptomatic patients. As mentioned earlier, two different experiments using cough and breath sound spectrograms were conducted: (i) symptomatic COVID-19 and healthy, and (ii) asymptomatic COVID-19 and healthy. The comparative performance of different CNNs for these classification schemes is shown in Table 5A,B.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28504</offset><text>Overall accuracies for five-fold cross-validation from the top three CNN models for symptomatic and asymptomatic patients using cough sounds are 95.38%, 94.29%, and 93.25% and 98.5%, 98.28%, and 96.84%, respectively. The top three networks for symptomatic and asymptomatic patients using cough sounds are Resnet50, Resnet101, and DenseNet201 and Mobilenetv2, DenseNet201, and Resnet101, respectively. On the other hand, the overall accuracies from the top three CNN models for symptomatic and asymptomatic patients using breath sounds are 90.33%, 87.57%, and 84.53% and 75.6%, 69.72%, and 68.4%, respectively. The top three networks for symptomatic and asymptomatic patients using breath sounds are EfficientNet_B0, MobileNetv2, and ResNet101 and EfficientNet_B7, ResNet101, and MobileNetv2, respectively. It is evident from the results that cough sound-based stratification models perform better than breath sound-based models, for both symptomatic and asymptomatic patients.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29481</offset><text>Interestingly, the stacking CNN model outperformed all CNN models for both cough and breath sounds, as can be seen from Table 5. It achieved accuracies of 96.5% and 98.85% for symptomatic and asymptomatic patients’ cough sounds, respectively. On the contrary, it produced accuracies of 91.03% and 80.01% for symptomatic and asymptomatic patients’ breath data, respectively. It is clear that breath sounds were unable to classify healthy subjects and COVID-19 patients reliably, whereas cough sounds performed better for both symptomatic and asymptomatic patients. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30050</offset><text>Figure 4 shows the area under the curve (AUC)/receiver-operating characteristic (ROC) curve (also known as AUROC (area under the receiver-operating characteristic)) for the symptomatic and asymptomatic patients’ cough and breath data. These ROC curves clearly depict that the stacking model performs better than any individual CNN model for cough and breath data, however, as mentioned earlier, cough sounds can reliably distinguish COVID-19 patients from the healthy group. It can also be seen that the best-performing scheme is the asymptomatic COVID-19 patients’ stratification using cough sounds. The asymptomatic patients are the ones who are spreading the virus unknowingly, and our trained network performs well in detecting them from their cough sounds. Therefore, this COVID-19 screening framework can significantly help in screening suspected populations and reducing the risk of spread. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30953</offset><text>Figure 5 shows the confusion matrix for the outperforming-stacking model for the cough data of symptomatic and asymptomatic patients and the breath data of symptomatic and asymptomatic patients. It can be noticed that even with the best-performing model, eight out of 72 COVID-19 spectrogram images were miss-classified as healthy and 9 out of 296 healthy spectrogram images were mis-classified as COVID-19 images for symptomatic cough sound spectrogram images. On the other hand, five out of 165 COVID-19 images were mis-classified as healthy and only two out of 531 healthy spectrogram images were mis-classified as COVID-19 images for asymptomatic cough sound spectrogram images. Once again, consistent with the results from Figure 4, the cough sounds performed very well in distinguishing between the asymptomatic subjects. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31782</offset><text>For symptomatic breath sound spectrogram images, eight out of 72 COVID-19 images were miss-classified as healthy and 25 out of 296 healthy spectrogram images were mis-classified as COVID-19 images while 47 out of 165 COVID-19 images were mis-classified as healthy and 92 out of 531 healthy spectrogram images were mis-classified as COVID-19 images for asymptomatic breath sound spectrogram images. It is evident from the confusion matrices that the cough sound spectrogram outperformed the breath sound spectrogram. This outstanding performance of any computer-aided classifier using non-invasively acquirable cough sounds can significantly help with fast diagnosis of COVID-19 immediately and in the comfort of the user’s home.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32513</offset><text>Figure 6 shows a comparison of accuracy versus the inference time for each image for different CNN networks and the stacking CNN model for symptomatic and asymptomatic data. Inference times of the best-performing stacking network for symptomatic and asymptomatic cough sounds were about 0.0389 and 0.0411 s, respectively. Even though the inference time for the stacking model was higher than for most of the individual models, the inference time was still small enough to be suitable for real-time applications. Therefore, to enable real-time application, we have deployed the best-performing stacking models in a web application that can be used from any mobile browser to make it independent from Android and iOS platforms. The next section describes the development and deployment steps of the AI-enabled web application. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>33339</offset><text>AI-Enabled Application for COVID-19 Detection</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33385</offset><text>An AI-enabled application was developed using Flutter, a cross-platform app development framework maintained by Google which uses the Dart programming language. The utility of using a cross-platform framework over native frameworks like Swift or Kotlin is that we can maintain multiple platforms like Android, iOS, and even desktop using a single codebase. This will in essence provide us with the maximum coverage for users, quicker development and continuous integration, seamless deployment and maintenance, easier cloud integration, and increased stability. Furthermore, using Flutter instead of other cross-platform frameworks like Ionic comes with the benefit of developing almost near-native code with complete access to native plugins and device hardware features in device AI using built-in GPU. We deployed an application entitled QUCoughScope that allows patients to upload cough and breath sounds along with clinical history. For our purposes, the application requires access to the microphone of the smartphone and records cough and breath sounds. The mobile-recorded audio signal and symptoms, once received by the server machine, undergo an STFT operation to convert raw audio signals into spectrogram images without any pre-processing. The deployed Google computation engine-based backend AI-based server analyzes the uploaded sounds to classify them as healthy or COVID-19-positive. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34786</offset><text>In the prototype system, the user fills in some demographic data, as well as a list of confirmed symptoms. Next, once the app collects cough and breath sounds from the user, these are transferred to the server using HTTPS protocol. The server performs signal processing and machine learning classification to determine whether the cough and breath sounds like those of COVID-19 patients or not (Figure 7). Our app then notifies the users about their status. The application displays the results and also stores them in a cloud database.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35323</offset><text>Our pipeline is divided into two parts: symptomatic (cough) and asymptomatic users (no symptoms). Once the spectrogram is generated, our AI-enabled server checks whether the user has a cough or not, based on which two separate pipelines are carried out. If the user has entered that he/she has a cough, the symptomatic pipeline is activated. It was observed that differentiating between COVID-19-positive and healthy users based on symptomatic and asymptomatic patients’ cough sounds plays a more important role than breath sounds. </text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>35858</offset><text>4. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>35873</offset><text>This work presents a novel stacking approach with deep CNN models for the automatic detection of COVID-19 using cough and breath sound spectrogram images for symptomatic and asymptomatic patients. As can be seen in comparison Table 6, the proposed innovative stacking approach has provided the best performance compared to similar studies. </text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>36214</offset><text>The performance of eight different CNN models was evaluated for the classification of different studies: binary classification of healthy and COVID-19 using cough and breath sound spectrogram images for symptomatic and asymptomatic patients. The study also evaluated the performance of the stacking CNN model in which the top three models were used as a base learner, and predictions of those models were used to train a logistic regression-based meta learner classifier for the final decision. The stacking CNN model outperformed other networks and the best classification accuracy, sensitivity, and specificity for binary classification using cough sound spectrogram images with symptomatic and asymptomatic data were found to be 96.5%, 96.42%, and 95.47% and 98.85%, 97.01%, and 99.6%, respectively. The best classification accuracy, sensitivity, and specificity for binary classification with symptomatic and asymptomatic breath sound data were found to be 91.03%, 88.9%, 91.5%, 80.01%, 72.04%, and 82.67% respectively. Thus, it is clear that cough sounds spectrogram images are more reliable in detecting COVID-19 patients than breath sound spectrograms. Moreover, the network has shown the best performance in detecting the asymptomatic patients, who are unknowing super-spreaders. The proposed web application can also help in crowdsourcing more data and further increasing the robustness of the solution. Therefore, automatic COVID-19 detection using cough sound spectrogram images can play a crucial role in computer-aided diagnosis as a fast diagnostic tool, which can detect a significant number of people in the early stages and can reduce healthcare costs and burden significantly. </text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>37910</offset><text>The limitations of this work include (i) a less diverse dataset in terms of ethnicity, as the datasets are from the UK and Qatar, (ii) less intuitiveness of the application in terms of not being able to distinguish between cough or breath sounds from other sounds, even though we have an option for the user to confirm the recorded sound, (iii) the dataset has limited RT-PCR verified labelled data.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>38310</offset><text>These limitations can be minimized in future work, as the application is being proposed to many doctors and government organizations (nationally and internationally) so that the network can be trained with a more diverse dataset to improve itself. Doctors and government organizations can help in providing RT-PCR labelled datasets, as this convenient solution can be a much better replacement of low sensitivity rapid antigen test kits, which are widely used for quicker results. The authors are working to train an anomaly detection model to ensure that the user can only submit cough and breathing sounds while other sounds will not be accepted. This will improve the robustness of the proposed system.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>39016</offset><text>Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>39142</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>39163</offset><text>Conceptualization, T.R., Y.M.S.M., M.E., A.T., S.M.Z., T.A., S.A.-M. and M.E.H.C.; Data curation, N.I., E.H.B. and M.A.A.; Formal analysis, T.R., N.I., Y.M.S.M., M.E., M.A.A. and A.T.; Funding acquisition, T.A., S.A.-M. and M.E.H.C.; Investigation, T.R., N.I., A.K., M.S.A.H., Y.Q. and S.M.; Methodology, T.R., A.K., M.S.A.H., Y.M.S.M., M.E., E.H.B., M.A.A., A.T., Y.Q., S.M. and M.E.H.C.; Project administration, M.A.A., S.M.Z., T.A., S.A.-M. and M.E.H.C.; Resources, M.E.H.C.; Software, M.S.A.H. and M.E.H.C.; Supervision, S.M.Z., T.A., S.A.-M. and M.E.H.C.; Validation, M.E.H.C.; Visualization, M.E.H.C.; Writing–original draft, T.R., A.K., Y.M.S.M., M.E., E.H.B., M.A.A., A.T., Y.Q., S.M., S.M.Z., T.A., S.A.-M. and M.E.H.C.; Writing–review &amp; editing, T.R., E.H.B., S.M.Z. and M.E.H.C. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>40034</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>40042</offset><text>This work was supported by the Qatar National Research Grant: UREP28-144-3-046. The statements made herein are solely the responsibility of the authors.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>40195</offset><text>Institutional Review Board Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>40232</offset><text>The study was approved by the Institutional Review Board of Qatar University (protocol code IRB-A-QU-2020-0014 and date of approval is 8 July 2020).</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>40381</offset><text>Informed Consent Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>40408</offset><text>Patient consent was waived as the dataset was collected through crowdsourcing and there is no identification information available in the dataset and there is no way to track the user and the users has participated in the study voluntarily. </text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>40650</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>40678</offset><text>The dataset collected from QUCoughScope through crowdsourcing is available in. </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>40758</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>40780</offset><text>The authors declare no conflict of interest.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>40825</offset><text>References</text></passage><passage><infon key="comment">Available online: https://www.worldometers.info/coronavirus/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>40836</offset><text>COVID-19 CORONAVIRUS PANDEMIC</text></passage><passage><infon key="fpage">e21122020</infon><infon key="name_0">surname:Pormohammad;given-names:A.</infon><infon key="name_1">surname:Ghorbani;given-names:S.</infon><infon key="name_2">surname:Khatami;given-names:A.</infon><infon key="name_3">surname:Farzi;given-names:R.</infon><infon key="name_4">surname:Baradaran;given-names:B.</infon><infon key="name_5">surname:Turner;given-names:D.L.</infon><infon key="name_6">surname:Turner;given-names:R.J.</infon><infon key="name_7">surname:Bahr;given-names:N.C.</infon><infon key="name_8">surname:Idrovo;given-names:J.P.</infon><infon key="pub-id_doi">10.1002/rmv.2112</infon><infon key="pub-id_pmid">32502331</infon><infon key="section_type">REF</infon><infon key="source">Rev. Med. Virol.</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">2020</infon><offset>40866</offset><text>Comparison of confirmed COVID-19 with SARS and MERS cases-Clinical characteristics, laboratory findings, radiographic signs and outcomes: A systematic review and meta-analysis</text></passage><passage><infon key="fpage">e514</infon><infon key="lpage">e516</infon><infon key="name_0">surname:Felsenstein;given-names:S.</infon><infon key="name_1">surname:Hedrich;given-names:C.M.</infon><infon key="pub-id_doi">10.1016/S2665-9913(20)30212-5</infon><infon key="pub-id_pmid">32838308</infon><infon key="section_type">REF</infon><infon key="source">Lancet Rheumatol.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2020</infon><offset>41042</offset><text>COVID-19 in children and young people</text></passage><passage><infon key="fpage">1149</infon><infon key="lpage">1151</infon><infon key="name_0">surname:Sattar;given-names:N.</infon><infon key="name_1">surname:Ho;given-names:F.K.</infon><infon key="name_2">surname:Gill;given-names:J.M.</infon><infon key="name_3">surname:Ghouri;given-names:N.</infon><infon key="name_4">surname:Gray;given-names:S.R.</infon><infon key="name_5">surname:Celis-Morales;given-names:C.A.</infon><infon key="name_6">surname:Katikireddi;given-names:S.V.</infon><infon key="name_7">surname:Berry;given-names:C.</infon><infon key="name_8">surname:Pell;given-names:J.P.</infon><infon key="name_9">surname:McMurray;given-names:J.J.</infon><infon key="pub-id_doi">10.1016/j.dsx.2020.06.060</infon><infon key="pub-id_pmid">32668401</infon><infon key="section_type">REF</infon><infon key="source">Diabetes Metab. Syndr. Clin. Res. Rev.</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2020</infon><offset>41080</offset><text>BMI and future risk for COVID-19 infection and death across sex, age and ethnicity: Preliminary findings from UK biobank</text></passage><passage><infon key="fpage">n1315</infon><infon key="name_0">surname:Wise;given-names:J.</infon><infon key="pub-id_doi">10.1136/bmj.n1315</infon><infon key="pub-id_pmid">34020965</infon><infon key="section_type">REF</infon><infon key="source">BMJ Br. Med. J.</infon><infon key="type">ref</infon><infon key="volume">373</infon><infon key="year">2021</infon><offset>41201</offset><text>COVID-19: UK cases of variant from India rise by 160% in a week</text></passage><passage><infon key="fpage">1037</infon><infon key="lpage">1041</infon><infon key="name_0">surname:Jain;given-names:V.K.</infon><infon key="name_1">surname:Iyengar;given-names:K.</infon><infon key="name_2">surname:Vaish;given-names:A.</infon><infon key="name_3">surname:Vaishya;given-names:R.</infon><infon key="pub-id_doi">10.1016/j.dsx.2020.06.067</infon><infon key="section_type">REF</infon><infon key="source">Diabetes Metab. Syndr. Clin. Res. Rev.</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2020</infon><offset>41265</offset><text>Differential mortality in COVID-19 patients from India and western countries</text></passage><passage><infon key="fpage">104455</infon><infon key="name_0">surname:Scohy;given-names:A.</infon><infon key="name_1">surname:Anantharajah;given-names:A.</infon><infon key="name_2">surname:Bodéus;given-names:M.</infon><infon key="name_3">surname:Kabamba-Mukadi;given-names:B.</infon><infon key="name_4">surname:Verroken;given-names:A.</infon><infon key="name_5">surname:Rodriguez-Villalobos;given-names:H.</infon><infon key="pub-id_doi">10.1016/j.jcv.2020.104455</infon><infon key="pub-id_pmid">32485618</infon><infon key="section_type">REF</infon><infon key="source">J. Clin. Virol.</infon><infon key="type">ref</infon><infon key="volume">129</infon><infon key="year">2020</infon><offset>41342</offset><text>Low performance of rapid antigen detection test as frontline testing for COVID-19 diagnosis</text></passage><passage><infon key="elocation-id">3493</infon><infon key="name_0">surname:Khandker;given-names:S.S.</infon><infon key="name_1">surname:Nik Hashim;given-names:N.H.H.</infon><infon key="name_2">surname:Deris;given-names:Z.Z.</infon><infon key="name_3">surname:Shueb;given-names:R.H.</infon><infon key="name_4">surname:Islam;given-names:M.A.</infon><infon key="pub-id_doi">10.3390/jcm10163493</infon><infon key="pub-id_pmid">34441789</infon><infon key="section_type">REF</infon><infon key="source">J. Clin. Med.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2021</infon><offset>41434</offset><text>Diagnostic accuracy of rapid antigen test kits for detecting SARS-CoV-2: A systematic review and meta-analysis of 17,171 suspected COVID-19 patients</text></passage><passage><infon key="fpage">472.e7</infon><infon key="lpage">472.e10</infon><infon key="name_0">surname:Albert;given-names:E.</infon><infon key="name_1">surname:Torres;given-names:I.</infon><infon key="name_2">surname:Bueno;given-names:F.</infon><infon key="name_3">surname:Huntley;given-names:D.</infon><infon key="name_4">surname:Molla;given-names:E.</infon><infon key="name_5">surname:Fernández-Fuentes;given-names:M.Á.</infon><infon key="name_6">surname:Martínez;given-names:M.</infon><infon key="name_7">surname:Poujois;given-names:S.</infon><infon key="name_8">surname:Forqué;given-names:L.</infon><infon key="name_9">surname:Valdivia;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.cmi.2020.11.004</infon><infon key="section_type">REF</infon><infon key="source">Clin. Microbiol. Infect.</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2021</infon><offset>41583</offset><text>Field evaluation of a rapid antigen test (Panbio™ COVID-19 Ag Rapid Test Device) for COVID-19 diagnosis in primary healthcare centres</text></passage><passage><infon key="name_0">surname:Chowdhury;given-names:M.E.</infon><infon key="name_1">surname:Khandakar;given-names:A.</infon><infon key="name_2">surname:Qiblawey;given-names:Y.</infon><infon key="name_3">surname:Reaz;given-names:M.B.I.</infon><infon key="name_4">surname:Islam;given-names:M.T.</infon><infon key="name_5">surname:Touati;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">Sports Science and Human Health-Different Approaches</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>41719</offset><text>Machine learning in wearable biomedical systems</text></passage><passage><infon key="fpage">132665</infon><infon key="lpage">132676</infon><infon key="name_0">surname:Chowdhury;given-names:M.E.</infon><infon key="name_1">surname:Rahman;given-names:T.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Mazhar;given-names:R.</infon><infon key="name_4">surname:Kadir;given-names:M.A.</infon><infon key="name_5">surname:Mahbub;given-names:Z.B.</infon><infon key="name_6">surname:Islam;given-names:K.R.</infon><infon key="name_7">surname:Khan;given-names:M.S.</infon><infon key="name_8">surname:Iqbal;given-names:A.</infon><infon key="name_9">surname:Al Emadi;given-names:N.</infon><infon key="pub-id_doi">10.1109/ACCESS.2020.3010287</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>41767</offset><text>Can AI help in screening viral and COVID-19 pneumonia?</text></passage><passage><infon key="fpage">104319</infon><infon key="name_0">surname:Rahman;given-names:T.</infon><infon key="name_1">surname:Khandakar;given-names:A.</infon><infon key="name_2">surname:Qiblawey;given-names:Y.</infon><infon key="name_3">surname:Tahir;given-names:A.</infon><infon key="name_4">surname:Kiranyaz;given-names:S.</infon><infon key="name_5">surname:Kashem;given-names:S.B.A.</infon><infon key="name_6">surname:Islam;given-names:M.T.</infon><infon key="name_7">surname:Al Maadeed;given-names:S.</infon><infon key="name_8">surname:Zughaier;given-names:S.M.</infon><infon key="name_9">surname:Khan;given-names:M.S.</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.104319</infon><infon key="pub-id_pmid">33799220</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">132</infon><infon key="year">2021</infon><offset>41822</offset><text>Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">21</infon><infon key="name_0">surname:Tahir;given-names:A.</infon><infon key="name_1">surname:Qiblawey;given-names:Y.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Rahman;given-names:T.</infon><infon key="name_4">surname:Khurshid;given-names:U.</infon><infon key="name_5">surname:Musharavati;given-names:F.</infon><infon key="name_6">surname:Islam;given-names:M.</infon><infon key="name_7">surname:Kiranyaz;given-names:S.</infon><infon key="name_8">surname:Chowdhury;given-names:M.</infon><infon key="pub-id_doi">10.1007/s12559-021-09955-1</infon><infon key="pub-id_pmid">35035591</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Comput.</infon><infon key="type">ref</infon><infon key="year">2022</infon><offset>41922</offset><text>Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-Ray Images</text></passage><passage><infon key="name_0">surname:Tahir;given-names:A.M.</infon><infon key="name_1">surname:Chowdhury;given-names:M.E.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Rahman;given-names:T.</infon><infon key="name_4">surname:Qiblawey;given-names:Y.</infon><infon key="name_5">surname:Khurshid;given-names:U.</infon><infon key="name_6">surname:Kiranyaz;given-names:S.</infon><infon key="name_7">surname:Ibtehaz;given-names:N.</infon><infon key="name_8">surname:Rahman;given-names:M.S.</infon><infon key="name_9">surname:Al-Madeed;given-names:S.</infon><infon key="pub-id_arxiv">2103.07985</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.105002</infon><infon key="pub-id_pmid">34749094</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>42016</offset><text>COVID-19 Infection Localization and Severity Grading from Chest X-ray Images</text></passage><passage><infon key="elocation-id">893</infon><infon key="name_0">surname:Qiblawey;given-names:Y.</infon><infon key="name_1">surname:Tahir;given-names:A.</infon><infon key="name_2">surname:Chowdhury;given-names:M.E.</infon><infon key="name_3">surname:Khandakar;given-names:A.</infon><infon key="name_4">surname:Kiranyaz;given-names:S.</infon><infon key="name_5">surname:Rahman;given-names:T.</infon><infon key="name_6">surname:Ibtehaz;given-names:N.</infon><infon key="name_7">surname:Mahmud;given-names:S.</infon><infon key="name_8">surname:Maadeed;given-names:S.A.</infon><infon key="name_9">surname:Musharavati;given-names:F.</infon><infon key="pub-id_doi">10.3390/diagnostics11050893</infon><infon key="pub-id_pmid">34067937</infon><infon key="section_type">REF</infon><infon key="source">Diagnostics</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2021</infon><offset>42093</offset><text>Detection and severity classification of COVID-19 in CT images using deep learning</text></passage><passage><infon key="name_0">surname:Zhao;given-names:J.</infon><infon key="name_1">surname:Zhang;given-names:Y.</infon><infon key="name_2">surname:He;given-names:X.</infon><infon key="name_3">surname:Xie;given-names:P.</infon><infon key="pub-id_arxiv">2003.13865</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>42176</offset><text>COVID-ct-dataset: A ct scan dataset about COVID-19</text></passage><passage><infon key="name_0">surname:He;given-names:X.</infon><infon key="name_1">surname:Yang;given-names:X.</infon><infon key="name_2">surname:Zhang;given-names:S.</infon><infon key="name_3">surname:Zhao;given-names:J.</infon><infon key="name_4">surname:Zhang;given-names:Y.</infon><infon key="name_5">surname:Xing;given-names:E.</infon><infon key="name_6">surname:Xie;given-names:P.</infon><infon key="pub-id_doi">10.1101/2020.04.13.20063941</infon><infon key="section_type">REF</infon><infon key="source">medrxiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>42227</offset><text>Sample-efficient deep learning for COVID-19 diagnosis based on CT scans</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">16</infon><infon key="name_0">surname:Rahman;given-names:T.</infon><infon key="name_1">surname:Akinbi;given-names:A.</infon><infon key="name_2">surname:Chowdhury;given-names:M.E.</infon><infon key="name_3">surname:Rashid;given-names:T.A.</infon><infon key="name_4">surname:Şengür;given-names:A.</infon><infon key="name_5">surname:Khandakar;given-names:A.</infon><infon key="name_6">surname:Islam;given-names:K.R.</infon><infon key="name_7">surname:Ismael;given-names:A.M.</infon><infon key="pub-id_doi">10.1007/s13755-021-00169-1</infon><infon key="pub-id_pmid">35096384</infon><infon key="section_type">REF</infon><infon key="source">Health Inf. Sci. Syst.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2022</infon><offset>42299</offset><text>COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network</text></passage><passage><infon key="fpage">105045</infon><infon key="name_0">surname:Hasoon;given-names:N.</infon><infon key="name_1">surname:Fadel;given-names:A.H.</infon><infon key="name_2">surname:Hameed;given-names:R.S.</infon><infon key="name_3">surname:Mostafa;given-names:S.A.</infon><infon key="name_4">surname:Khalaf;given-names:B.A.</infon><infon key="name_5">surname:Mohammed;given-names:M.A.</infon><infon key="name_6">surname:Nedoma;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.rinp.2021.105045</infon><infon key="section_type">REF</infon><infon key="source">Results Phys.</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2020</infon><offset>42392</offset><text>COVID-19 anomaly detection and classification method based on supervised machine learning of chest X-ray images</text></passage><passage><infon key="fpage">e12759</infon><infon key="name_0">surname:Alyasseri;given-names:Z.A.A.</infon><infon key="name_1">surname:Al-Betar;given-names:M.A.</infon><infon key="name_2">surname:Doush;given-names:I.A.</infon><infon key="name_3">surname:Awadallah;given-names:M.A.</infon><infon key="name_4">surname:Abasi;given-names:A.K.</infon><infon key="name_5">surname:Makhadmeh;given-names:S.N.</infon><infon key="name_6">surname:Alomari;given-names:O.A.</infon><infon key="name_7">surname:Abdulkareem;given-names:K.H.</infon><infon key="name_8">surname:Adam;given-names:A.</infon><infon key="name_9">surname:Damasevicius;given-names:R.</infon><infon key="pub-id_doi">10.1111/exsy.12759</infon><infon key="section_type">REF</infon><infon key="source">Expert Syst.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2021</infon><offset>42504</offset><text>Review on COVID-19 diagnosis models based on machine learning and deep learning approaches</text></passage><passage><infon key="fpage">2409</infon><infon key="lpage">2429</infon><infon key="name_0">surname:Al-Waisy;given-names:A.</infon><infon key="name_1">surname:Mohammed;given-names:M.A.</infon><infon key="name_2">surname:Al-Fahdawi;given-names:S.</infon><infon key="name_3">surname:Maashi;given-names:M.</infon><infon key="name_4">surname:Garcia-Zapirain;given-names:B.</infon><infon key="name_5">surname:Abdulkareem;given-names:K.H.</infon><infon key="name_6">surname:Mostafa;given-names:S.A.</infon><infon key="name_7">surname:Le;given-names:D.N.</infon><infon key="pub-id_doi">10.32604/cmc.2021.012955</infon><infon key="section_type">REF</infon><infon key="source">Comput. Mater. Contin.</infon><infon key="type">ref</infon><infon key="volume">67</infon><infon key="year">2021</infon><offset>42595</offset><text>COVID-DeepNet: Hybrid multimodal deep learning system for improving COVID-19 pneumonia detection in chest X-ray images</text></passage><passage><infon key="fpage">15919</infon><infon key="lpage">15928</infon><infon key="name_0">surname:Abdulkareem;given-names:K.H.</infon><infon key="name_1">surname:Mohammed;given-names:M.A.</infon><infon key="name_2">surname:Salim;given-names:A.</infon><infon key="name_3">surname:Arif;given-names:M.</infon><infon key="name_4">surname:Geman;given-names:O.</infon><infon key="name_5">surname:Gupta;given-names:D.</infon><infon key="name_6">surname:Khanna;given-names:A.</infon><infon key="pub-id_doi">10.1109/JIOT.2021.3050775</infon><infon key="section_type">REF</infon><infon key="source">IEEE Internet Things J.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2021</infon><offset>42714</offset><text>Realizing an effective COVID-19 diagnosis system based on machine learning and IOT in smart hospital environment</text></passage><passage><infon key="fpage">11</infon><infon key="lpage">21</infon><infon key="name_0">surname:Obaid;given-names:O.I.</infon><infon key="name_1">surname:Mohammed;given-names:M.A.</infon><infon key="name_2">surname:Mostafa;given-names:S.A.</infon><infon key="section_type">REF</infon><infon key="source">J. Inf. Technol. Manag.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2020</infon><offset>42827</offset><text>Long Short-Term Memory Approach for Coronavirus Disease Predicti</text></passage><passage><infon key="name_0">surname:Khan;given-names:K.N.</infon><infon key="name_1">surname:Khan;given-names:F.A.</infon><infon key="name_2">surname:Abid;given-names:A.</infon><infon key="name_3">surname:Olmez;given-names:T.</infon><infon key="name_4">surname:Dokur;given-names:Z.</infon><infon key="name_5">surname:Khandakar;given-names:A.</infon><infon key="name_6">surname:Chowdhury;given-names:M.E.</infon><infon key="name_7">surname:Khan;given-names:M.S.</infon><infon key="pub-id_arxiv">2012.08406</infon><infon key="pub-id_doi">10.1088/1361-6579/ac1d59</infon><infon key="pub-id_pmid">34388736</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>42892</offset><text>Deep Learning Based Classification of Unsegmented Phonocardiogram Spectrograms Leveraging Transfer Learning</text></passage><passage><infon key="elocation-id">2781</infon><infon key="name_0">surname:Chowdhury;given-names:M.E.</infon><infon key="name_1">surname:Khandakar;given-names:A.</infon><infon key="name_2">surname:Alzoubi;given-names:K.</infon><infon key="name_3">surname:Mansoor;given-names:S.</infon><infon key="name_4">surname:Tahir;given-names:A.M.</infon><infon key="name_5">surname:Reaz;given-names:M.B.I.</infon><infon key="name_6">surname:Al-Emadi;given-names:N.</infon><infon key="pub-id_doi">10.3390/s19122781</infon><infon key="pub-id_pmid">31226869</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2019</infon><offset>43000</offset><text>Real-time smart-digital stethoscope system for heart diseases monitoring</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">14</infon><infon key="name_0">surname:Mu;given-names:W.</infon><infon key="name_1">surname:Yin;given-names:B.</infon><infon key="name_2">surname:Huang;given-names:X.</infon><infon key="name_3">surname:Xu;given-names:J.</infon><infon key="name_4">surname:Du;given-names:Z.</infon><infon key="pub-id_pmid">33414495</infon><infon key="section_type">REF</infon><infon key="source">Sci. Rep.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2021</infon><offset>43073</offset><text>Environmental sound classification using temporal-frequency attention based convolutional neural network</text></passage><passage><infon key="fpage">611</infon><infon key="lpage">621</infon><infon key="name_0">surname:Connolly;given-names:J.H.</infon><infon key="name_1">surname:Edmonds;given-names:E.A.</infon><infon key="name_2">surname:Guzy;given-names:J.</infon><infon key="name_3">surname:Johnson;given-names:S.</infon><infon key="name_4">surname:Woodcock;given-names:A.</infon><infon key="pub-id_doi">10.1016/S0020-7373(86)80012-8</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Man-Mach. Stud.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">1986</infon><offset>43178</offset><text>Automatic speech recognition based on spectrogram reading</text></passage><passage><infon key="fpage">423</infon><infon key="lpage">431</infon><infon key="name_0">surname:Arias-Vergara;given-names:T.</infon><infon key="name_1">surname:Klumpp;given-names:P.</infon><infon key="name_2">surname:Vasquez-Correa;given-names:J.C.</infon><infon key="name_3">surname:Noeth;given-names:E.</infon><infon key="name_4">surname:Orozco-Arroyave;given-names:J.R.</infon><infon key="name_5">surname:Schuster;given-names:M.</infon><infon key="pub-id_doi">10.1007/s10044-020-00921-5</infon><infon key="section_type">REF</infon><infon key="source">Pattern Anal. Appl.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2021</infon><offset>43236</offset><text>Multi-channel spectrograms for speech processing applications using deep learning methods</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Badshah;given-names:A.M.</infon><infon key="name_1">surname:Ahmad;given-names:J.</infon><infon key="name_2">surname:Rahim;given-names:N.</infon><infon key="name_3">surname:Baik;given-names:S.W.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 International Conference on Platform Technology and Service (PlatCon)</infon><infon key="type">ref</infon><offset>43326</offset><text>Speech emotion recognition from spectrograms with deep convolutional neural network</text></passage><passage><infon key="name_0">surname:Deshpande;given-names:G.</infon><infon key="name_1">surname:Schuller;given-names:B.</infon><infon key="pub-id_arxiv">2005.08579</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>43410</offset><text>An overview on audio, signal, speech, &amp; language processing for COVID-19</text></passage><passage><infon key="name_0">surname:Nasreddine Belkacem;given-names:A.</infon><infon key="name_1">surname:Ouhbi;given-names:S.</infon><infon key="name_2">surname:Lakas;given-names:A.</infon><infon key="name_3">surname:Benkhelifa;given-names:E.</infon><infon key="name_4">surname:Chen;given-names:C.</infon><infon key="pub-id_arxiv">2006.15469</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>43483</offset><text>End-to-End AI-Based Point-of-Care Diagnosis System for Classifying Respiratory Illnesses and Early Detection of COVID-19</text></passage><passage><infon key="name_0">surname:Schuller;given-names:B.W.</infon><infon key="name_1">surname:Schuller;given-names:D.M.</infon><infon key="name_2">surname:Qian;given-names:K.</infon><infon key="name_3">surname:Liu;given-names:J.</infon><infon key="name_4">surname:Zheng;given-names:H.</infon><infon key="name_5">surname:Li;given-names:X.</infon><infon key="pub-id_arxiv">2003.11117</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>43604</offset><text>COVID-19 and computer audition: An overview on what speech &amp; sound analysis could contribute in the SARS-CoV-2 corona crisis</text></passage><passage><infon key="elocation-id">e01779262017</infon><infon key="name_0">surname:Pramono;given-names:R.X.A.</infon><infon key="name_1">surname:Bowyer;given-names:S.</infon><infon key="name_2">surname:Rodriguez-Villegas;given-names:E.</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2017</infon><offset>43729</offset><text>Automatic adventitious respiratory sound analysis: A systematic review</text></passage><passage><infon key="elocation-id">171</infon><infon key="name_0">surname:Li;given-names:S.-H.</infon><infon key="name_1">surname:Lin;given-names:B.-S.</infon><infon key="name_2">surname:Tsai;given-names:C.-H.</infon><infon key="name_3">surname:Yang;given-names:C.-T.</infon><infon key="name_4">surname:Lin;given-names:B.-S.</infon><infon key="pub-id_doi">10.3390/s17010171</infon><infon key="pub-id_pmid">28106747</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>43800</offset><text>Design of wearable breathing sound monitoring system for real-time wheeze detection</text></passage><passage><infon key="fpage">8295</infon><infon key="lpage">8303</infon><infon key="name_0">surname:Oletic;given-names:D.</infon><infon key="name_1">surname:Bilas;given-names:V.</infon><infon key="pub-id_doi">10.1109/JSEN.2016.2585039</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sens. J.</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>43884</offset><text>Energy-efficient respiratory sounds sensing for personal mobile asthma monitoring</text></passage><passage><infon key="fpage">303</infon><infon key="lpage">334</infon><infon key="name_0">surname:Brabenec;given-names:L.</infon><infon key="name_1">surname:Mekyska;given-names:J.</infon><infon key="name_2">surname:Galaz;given-names:Z.</infon><infon key="name_3">surname:Rektorova;given-names:I.</infon><infon key="pub-id_doi">10.1007/s00702-017-1676-0</infon><infon key="pub-id_pmid">28101650</infon><infon key="section_type">REF</infon><infon key="source">J. Neural Transm.</infon><infon key="type">ref</infon><infon key="volume">124</infon><infon key="year">2017</infon><offset>43966</offset><text>Speech disorders in Parkinson’s disease: Early diagnostics and effects of medication and brain stimulation</text></passage><passage><infon key="elocation-id">e01824282017</infon><infon key="name_0">surname:Erdogdu Sakar;given-names:B.</infon><infon key="name_1">surname:Serbes;given-names:G.</infon><infon key="name_2">surname:Sakar;given-names:C.O.</infon><infon key="pub-id_doi">10.1371/journal.pone.0182428</infon><infon key="pub-id_pmid">28792979</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2017</infon><offset>44075</offset><text>Analyzing the effectiveness of vocal features in early telediagnosis of Parkinson’s disease</text></passage><passage><infon key="fpage">840</infon><infon key="lpage">847</infon><infon key="name_0">surname:Maor;given-names:E.</infon><infon key="name_1">surname:Sara;given-names:J.D.</infon><infon key="name_2">surname:Orbelo;given-names:D.M.</infon><infon key="name_3">surname:Lerman;given-names:L.O.</infon><infon key="name_4">surname:Levanon;given-names:Y.</infon><infon key="name_5">surname:Lerman;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.mayocp.2017.12.025</infon><infon key="pub-id_pmid">29656789</infon><infon key="section_type">REF</infon><infon key="source">Mayo Clin. Proc.</infon><infon key="type">ref</infon><infon key="volume">93</infon><infon key="year">2018</infon><offset>44169</offset><text>Voice signal characteristics are independently associated with coronary artery disease</text></passage><passage><infon key="fpage">1693</infon><infon key="lpage">1724</infon><infon key="name_0">surname:Banerjee;given-names:D.</infon><infon key="name_1">surname:Islam;given-names:K.</infon><infon key="name_2">surname:Xue;given-names:K.</infon><infon key="name_3">surname:Mei;given-names:G.</infon><infon key="name_4">surname:Xiao;given-names:L.</infon><infon key="name_5">surname:Zhang;given-names:G.</infon><infon key="name_6">surname:Xu;given-names:R.</infon><infon key="name_7">surname:Lei;given-names:C.</infon><infon key="name_8">surname:Ji;given-names:S.</infon><infon key="name_9">surname:Li;given-names:J.</infon><infon key="pub-id_doi">10.1007/s10115-019-01337-2</infon><infon key="section_type">REF</infon><infon key="source">Knowl. Inf. Syst.</infon><infon key="type">ref</infon><infon key="volume">60</infon><infon key="year">2019</infon><offset>44256</offset><text>A deep transfer learning approach for improved post-traumatic stress disorder diagnosis</text></passage><passage><infon key="fpage">e856</infon><infon key="name_0">surname:Faurholt-Jepsen;given-names:M.</infon><infon key="name_1">surname:Busk;given-names:J.</infon><infon key="name_2">surname:Frost;given-names:M.</infon><infon key="name_3">surname:Vinberg;given-names:M.</infon><infon key="name_4">surname:Christensen;given-names:E.M.</infon><infon key="name_5">surname:Winther;given-names:O.</infon><infon key="name_6">surname:Bardram;given-names:J.E.</infon><infon key="name_7">surname:Kessing;given-names:L.V.</infon><infon key="pub-id_doi">10.1038/tp.2016.123</infon><infon key="pub-id_pmid">27434490</infon><infon key="section_type">REF</infon><infon key="source">Transl. Psychiatry</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>44344</offset><text>Voice analysis as an objective state marker in bipolar disorder</text></passage><passage><infon key="name_0">surname:hui Huang;given-names:Y.</infon><infon key="name_1">surname:jun Meng;given-names:S.</infon><infon key="name_2">surname:Zhang;given-names:Y.</infon><infon key="name_3">surname:sheng Wu;given-names:S.</infon><infon key="name_4">surname:Zhang;given-names:Y.</infon><infon key="name_5">surname:wei Zhang;given-names:Y.</infon><infon key="name_6">surname:xiang Ye;given-names:Y.</infon><infon key="name_7">surname:feng Wei;given-names:Q.</infon><infon key="name_8">surname:gui Zhao;given-names:N.</infon><infon key="name_9">surname:ping Jiang;given-names:J.</infon><infon key="pub-id_doi">10.1101/2020.04.07.20051060</infon><infon key="section_type">REF</infon><infon key="source">medRxiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>44408</offset><text>The respiratory sound features of COVID-19 patients fill gaps between clinical data and screening methods</text></passage><passage><infon key="fpage">100378</infon><infon key="name_0">surname:Imran;given-names:A.</infon><infon key="name_1">surname:Posokhova;given-names:I.</infon><infon key="name_2">surname:Qureshi;given-names:H.N.</infon><infon key="name_3">surname:Masood;given-names:U.</infon><infon key="name_4">surname:Riaz;given-names:M.S.</infon><infon key="name_5">surname:Ali;given-names:K.</infon><infon key="name_6">surname:John;given-names:C.N.</infon><infon key="name_7">surname:Hussain;given-names:M.I.</infon><infon key="name_8">surname:Nabeel;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.imu.2020.100378</infon><infon key="pub-id_pmid">32839734</infon><infon key="section_type">REF</infon><infon key="source">Inform. Med. Unlocked</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>44514</offset><text>AI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app</text></passage><passage><infon key="fpage">154087</infon><infon key="lpage">154094</infon><infon key="name_0">surname:Cohen-McFarlane;given-names:M.</infon><infon key="name_1">surname:Goubran;given-names:R.</infon><infon key="name_2">surname:Knoefel;given-names:F.</infon><infon key="pub-id_doi">10.1109/ACCESS.2020.3018028</infon><infon key="pub-id_pmid">34786285</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>44603</offset><text>Novel coronavirus cough database: NoCoCoDa</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:Grant;given-names:D.</infon><infon key="name_1">surname:McLane;given-names:I.</infon><infon key="name_2">surname:West;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)</infon><infon key="type">ref</infon><offset>44646</offset><text>Rapid and scalable COVID-19 screening using speech, breath, and cough recordings</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">13</infon><infon key="name_0">surname:Mouawad;given-names:P.</infon><infon key="name_1">surname:Dubnov;given-names:T.</infon><infon key="name_2">surname:Dubnov;given-names:S.</infon><infon key="pub-id_doi">10.1007/s42979-020-00422-6</infon><infon key="pub-id_pmid">33458700</infon><infon key="section_type">REF</infon><infon key="source">SN Comput. Sci.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2021</infon><offset>44727</offset><text>Robust Detection of COVID-19 in Cough Sounds</text></passage><passage><infon key="fpage">104765</infon><infon key="name_0">surname:Erdoğan;given-names:Y.E.</infon><infon key="name_1">surname:Narin;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.104765</infon><infon key="pub-id_pmid">34416571</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">136</infon><infon key="year">2021</infon><offset>44772</offset><text>COVID-19 detection with traditional and deep features on cough acoustic signals</text></passage><passage><infon key="fpage">104572</infon><infon key="name_0">surname:Pahar;given-names:M.</infon><infon key="name_1">surname:Klopper;given-names:M.</infon><infon key="name_2">surname:Warren;given-names:R.</infon><infon key="name_3">surname:Niesler;given-names:T.</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.104572</infon><infon key="pub-id_pmid">34182331</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">135</infon><infon key="year">2021</infon><offset>44852</offset><text>COVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings</text></passage><passage><infon key="name_0">surname:Sharma;given-names:N.</infon><infon key="name_1">surname:Krishnan;given-names:P.</infon><infon key="name_2">surname:Kumar;given-names:R.</infon><infon key="name_3">surname:Ramoji;given-names:S.</infon><infon key="name_4">surname:Chetupalli;given-names:S.R.</infon><infon key="name_5">surname:Ghosh;given-names:P.K.</infon><infon key="name_6">surname:Ganapathy;given-names:S.</infon><infon key="pub-id_arxiv">2005.10548</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>44938</offset><text>Coswara--A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis</text></passage><passage><infon key="comment">Available online: https://coughtest.online</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>45019</offset><text>COVID-19 Screening by Cough Sound Analysis</text></passage><passage><infon key="fpage">620</infon><infon key="lpage">628</infon><infon key="name_0">surname:Pal;given-names:A.</infon><infon key="name_1">surname:Sankarasubbu;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 36th Annual ACM Symposium on Applied Computing</infon><infon key="type">ref</infon><offset>45062</offset><text>Pay attention to the cough: Early diagnosis of COVID-19 using interpretable symptoms embeddings with cough sound signal processing</text></passage><passage><infon key="name_0">surname:Bagad;given-names:P.</infon><infon key="name_1">surname:Dalmia;given-names:A.</infon><infon key="name_2">surname:Doshi;given-names:J.</infon><infon key="name_3">surname:Nagrani;given-names:A.</infon><infon key="name_4">surname:Bhamare;given-names:P.</infon><infon key="name_5">surname:Mahale;given-names:A.</infon><infon key="name_6">surname:Rane;given-names:S.</infon><infon key="name_7">surname:Agarwal;given-names:N.</infon><infon key="name_8">surname:Panicker;given-names:R.</infon><infon key="pub-id_arxiv">2009.08790</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>45193</offset><text>Cough against COVID: Evidence of COVID-19 signature in cough sounds</text></passage><passage><infon key="fpage">275</infon><infon key="lpage">281</infon><infon key="name_0">surname:Laguarta;given-names:J.</infon><infon key="name_1">surname:Hueto;given-names:F.</infon><infon key="name_2">surname:Subirana;given-names:B.</infon><infon key="pub-id_doi">10.1109/OJEMB.2020.3026928</infon><infon key="pub-id_pmid">34812418</infon><infon key="section_type">REF</infon><infon key="source">IEEE Open J. Eng. Med. Biol.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2020</infon><offset>45261</offset><text>COVID-19 artificial intelligence diagnosis using only cough recordings</text></passage><passage><infon key="fpage">372</infon><infon key="name_0">surname:Belkacem;given-names:A.N.</infon><infon key="name_1">surname:Ouhbi;given-names:S.</infon><infon key="name_2">surname:Lakas;given-names:A.</infon><infon key="name_3">surname:Benkhelifa;given-names:E.</infon><infon key="name_4">surname:Chen;given-names:C.</infon><infon key="pub-id_doi">10.3389/fmed.2021.585578</infon><infon key="section_type">REF</infon><infon key="source">Front. Med.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2021</infon><offset>45332</offset><text>End-to-End AI-Based Point-of-Care Diagnosis System for Classifying Respiratory Illnesses and Early Detection of COVID-19: A Theoretical Framework</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">24</infon><infon key="name_0">surname:Rahman;given-names:M.A.</infon><infon key="name_1">surname:Hossain;given-names:M.S.</infon><infon key="name_2">surname:Alrajeh;given-names:N.A.</infon><infon key="name_3">surname:Gupta;given-names:B.</infon><infon key="pub-id_doi">10.1145/3421725</infon><infon key="section_type">REF</infon><infon key="source">ACM Trans. Multimid. Comput. Commun. Appl.</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2021</infon><offset>45478</offset><text>A multimodal, multimedia point-of-care deep learning framework for COVID-19 diagnosis</text></passage><passage><infon key="fpage">3474</infon><infon key="lpage">3484</infon><infon key="name_0">surname:Brown;given-names:C.</infon><infon key="name_1">surname:Chauhan;given-names:J.</infon><infon key="name_2">surname:Grammenos;given-names:A.</infon><infon key="name_3">surname:Han;given-names:J.</infon><infon key="name_4">surname:Hasthanasombat;given-names:A.</infon><infon key="name_5">surname:Spathis;given-names:D.</infon><infon key="name_6">surname:Xia;given-names:T.</infon><infon key="name_7">surname:Cicuta;given-names:P.</infon><infon key="name_8">surname:Mascolo;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</infon><infon key="type">ref</infon><offset>45564</offset><text>Exploring automatic diagnosis of COVID-19 from crowdsourced respiratory sound data</text></passage><passage><infon key="fpage">356</infon><infon key="lpage">362</infon><infon key="name_0">surname:Coppock;given-names:H.</infon><infon key="name_1">surname:Gaskell;given-names:A.</infon><infon key="name_2">surname:Tzirakis;given-names:P.</infon><infon key="name_3">surname:Baird;given-names:A.</infon><infon key="name_4">surname:Jones;given-names:L.</infon><infon key="name_5">surname:Schuller;given-names:B.</infon><infon key="pub-id_doi">10.1136/bmjinnov-2021-000668</infon><infon key="section_type">REF</infon><infon key="source">BMJ Innov.</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2021</infon><offset>45647</offset><text>End-to-end convolutional neural network enables COVID-19 detection from breath and cough audio: A pilot study</text></passage><passage><infon key="fpage">1319</infon><infon key="lpage">1334</infon><infon key="name_0">surname:Kumar;given-names:L.K.</infon><infon key="name_1">surname:Alphonse;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">Alex. Eng. J.</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">2021</infon><offset>45757</offset><text>Automatic Diagnosis of COVID-19 Disease using Deep Convolutional Neural Network with Multi-Feature Channel from Respiratory Sound Data: Cough, Voice, and Breath</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Orlandic;given-names:L.</infon><infon key="name_1">surname:Teijeiro;given-names:T.</infon><infon key="name_2">surname:Atienza;given-names:D.</infon><infon key="pub-id_doi">10.1038/s41597-021-00937-4</infon><infon key="pub-id_pmid">33414438</infon><infon key="section_type">REF</infon><infon key="source">Sci. Data</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2021</infon><offset>45918</offset><text>The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms</text></passage><passage><infon key="comment">Available online: https://www.kaggle.com/tawsifurrahman/qucoughscope-covid19-cough-dataset</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46018</offset></passage><passage><infon key="fpage">191586</infon><infon key="lpage">191601</infon><infon key="name_0">surname:Rahman;given-names:T.</infon><infon key="name_1">surname:Khandakar;given-names:A.</infon><infon key="name_2">surname:Kadir;given-names:M.A.</infon><infon key="name_3">surname:Islam;given-names:K.R.</infon><infon key="name_4">surname:Islam;given-names:K.F.</infon><infon key="name_5">surname:Mazhar;given-names:R.</infon><infon key="name_6">surname:Hamid;given-names:T.</infon><infon key="name_7">surname:Islam;given-names:M.T.</infon><infon key="name_8">surname:Kashem;given-names:S.</infon><infon key="name_9">surname:Mahbub;given-names:Z.B.</infon><infon key="pub-id_doi">10.1109/ACCESS.2020.3031384</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>46019</offset><text>Reliable tuberculosis detection using chest X-ray with deep learning, segmentation and visualization</text></passage><passage><infon key="name_0">surname:Tahir;given-names:A.</infon><infon key="name_1">surname:Qiblawey;given-names:Y.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Rahman;given-names:T.</infon><infon key="name_4">surname:Khurshid;given-names:U.</infon><infon key="name_5">surname:Musharavati;given-names:F.</infon><infon key="name_6">surname:Islam;given-names:M.</infon><infon key="name_7">surname:Kiranyaz;given-names:S.</infon><infon key="name_8">surname:Chowdhury;given-names:M.E.</infon><infon key="pub-id_arxiv">2005.11524</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>46120</offset><text>Coronavirus: Comparing COVID-19, SARS and MERS in the eyes of AI</text></passage><passage><infon key="elocation-id">3233</infon><infon key="name_0">surname:Rahman;given-names:T.</infon><infon key="name_1">surname:Chowdhury;given-names:M.E.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Islam;given-names:K.R.</infon><infon key="name_4">surname:Islam;given-names:K.F.</infon><infon key="name_5">surname:Mahbub;given-names:Z.B.</infon><infon key="name_6">surname:Kadir;given-names:M.A.</infon><infon key="name_7">surname:Kashem;given-names:S.</infon><infon key="pub-id_doi">10.3390/app10093233</infon><infon key="section_type">REF</infon><infon key="source">Appl. Sci.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2020</infon><offset>46185</offset><text>Transfer learning with deep convolutional neural network (CNN) for pneumonia detection using chest X-ray</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">16</infon><infon key="name_0">surname:Chowdhury;given-names:M.E.</infon><infon key="name_1">surname:Rahman;given-names:T.</infon><infon key="name_2">surname:Khandakar;given-names:A.</infon><infon key="name_3">surname:Al-Madeed;given-names:S.</infon><infon key="name_4">surname:Zughaier;given-names:S.M.</infon><infon key="name_5">surname:Doi;given-names:S.A.</infon><infon key="name_6">surname:Hassen;given-names:H.</infon><infon key="name_7">surname:Islam;given-names:M.T.</infon><infon key="pub-id_doi">10.1007/s12559-020-09812-7</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Comput.</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>46290</offset><text>An early warning tool for predicting mortality risk of COVID-19 patients using machine learning</text></passage><passage><infon key="comment">Available online: https://elitedatascience.com/overfitting-in-machine-learning</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46386</offset><text>Overfitting in Machine Learning: What It Is and How to Prevent It</text></passage><passage><infon key="comment">Available online: https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46452</offset><text>ResNet, AlexNet, VGGNet, Inception: Understanding Various Architectures of Convolutional Networks</text></passage><passage><infon key="comment">Available online: http://www.programmersought.com/article/7780717554/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46550</offset><text>DenseNet: Better CNN Model than ResNet</text></passage><passage><infon key="fpage">4510</infon><infon key="lpage">4520</infon><infon key="name_0">surname:Sandler;given-names:M.</infon><infon key="name_1">surname:Howard;given-names:A.</infon><infon key="name_2">surname:Zhu;given-names:M.</infon><infon key="name_3">surname:Zhmoginov;given-names:A.</infon><infon key="name_4">surname:Chen;given-names:L.-C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><offset>46589</offset><text>Mobilenetv2: Inverted residuals and linear bottlenecks</text></passage><passage><infon key="fpage">6105</infon><infon key="lpage">6114</infon><infon key="name_0">surname:Tan;given-names:M.</infon><infon key="name_1">surname:Le;given-names:Q.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Machine Learning</infon><infon key="type">ref</infon><offset>46644</offset><text>Efficientnet: Rethinking model scaling for convolutional neural networks</text></passage><passage><infon key="fpage">104838</infon><infon key="name_0">surname:Khandakar;given-names:A.</infon><infon key="name_1">surname:Chowdhury;given-names:M.E.</infon><infon key="name_2">surname:Reaz;given-names:M.B.I.</infon><infon key="name_3">surname:Ali;given-names:S.H.M.</infon><infon key="name_4">surname:Hasan;given-names:M.A.</infon><infon key="name_5">surname:Kiranyaz;given-names:S.</infon><infon key="name_6">surname:Rahman;given-names:T.</infon><infon key="name_7">surname:Alfkey;given-names:R.</infon><infon key="name_8">surname:Bakar;given-names:A.A.A.</infon><infon key="name_9">surname:Malik;given-names:R.A.</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.104838</infon><infon key="pub-id_pmid">34534794</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">137</infon><infon key="year">2021</infon><offset>46717</offset><text>A machine learning model for early detection of diabetic foot using thermogram images</text></passage><passage><infon key="comment">Available online: https://nibtehaz.github.io/qu-cough-scope/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46803</offset><text>QUCoughScope</text></passage><passage><infon key="fpage">104944</infon><infon key="name_0">surname:Despotovic;given-names:V.</infon><infon key="name_1">surname:Ismael;given-names:M.</infon><infon key="name_2">surname:Cornil;given-names:M.</infon><infon key="name_3">surname:Mc Call;given-names:R.</infon><infon key="name_4">surname:Fagherazzi;given-names:G.</infon><infon key="pub-id_doi">10.1016/j.compbiomed.2021.104944</infon><infon key="pub-id_pmid">34656870</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">138</infon><infon key="year">2021</infon><offset>46816</offset><text>Detection of COVID-19 from voice, cough and breathing patterns: Dataset and preliminary results</text></passage><passage><infon key="fpage">100025</infon><infon key="name_0">surname:Islam;given-names:R.</infon><infon key="name_1">surname:Abdel-Raheem;given-names:E.</infon><infon key="name_2">surname:Tarique;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.bea.2022.100025</infon><infon key="pub-id_pmid">35013733</infon><infon key="section_type">REF</infon><infon key="source">Biomed. Eng. Adv.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2022</infon><offset>46912</offset><text>A study of using cough sounds and deep neural networks for the early detection of COVID-19</text></passage><passage><infon key="file">diagnostics-12-00920-g001.jpg</infon><infon key="id">diagnostics-12-00920-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47003</offset><text>Methodology of the study.</text></passage><passage><infon key="file">diagnostics-12-00920-g002b.jpg</infon><infon key="id">diagnostics-12-00920-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47029</offset><text>Cough and breath sound waveforms and spectrograms for (A) symptomatic and (B) asymptomatic healthy subjects and COVID-19 patients.</text></passage><passage><infon key="file">diagnostics-12-00920-g003.jpg</infon><infon key="id">diagnostics-12-00920-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47160</offset><text>Stacking model architecture.</text></passage><passage><infon key="file">diagnostics-12-00920-g004.jpg</infon><infon key="id">diagnostics-12-00920-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47189</offset><text>ROC curve for healthy and COVID-19 patients’ classification using cough sounds for (A) symptomatic patients and (B) asymptomatic patients, and using breath sounds for (C) symptomatic patients and (D) asymptomatic patients.</text></passage><passage><infon key="file">diagnostics-12-00920-g005.jpg</infon><infon key="id">diagnostics-12-00920-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47414</offset><text>Confusion matrices for healthy and COVID-19 classification using cough sounds for (A) symptomatic patients and (B) asymptomatic patients, and using breath sounds for (C) symptomatic patients and (D) asymptomatic patients using best performing stacking CNN models.</text></passage><passage><infon key="file">diagnostics-12-00920-g006.jpg</infon><infon key="id">diagnostics-12-00920-f006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47678</offset><text>Accuracy vs inference time plot for binary classification using (A) symptomatic cough sound spectrograms, and (B) asymptomatic cough sound spectrograms.</text></passage><passage><infon key="file">diagnostics-12-00920-g007.jpg</infon><infon key="id">diagnostics-12-00920-f007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>47831</offset><text>Illustration of a generic framework for the QUCoughScope application.</text></passage><passage><infon key="file">diagnostics-12-00920-t001.xml</infon><infon key="id">diagnostics-12-00920-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>47901</offset><text>Details of the total Dataset.</text></passage><passage><infon key="file">diagnostics-12-00920-t001.xml</infon><infon key="id">diagnostics-12-00920-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Experiments&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Healthy&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;COVID-19&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cambridge&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QU&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cambridge&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;QU&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Symptomatic (Cough/Breath)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;264&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Asymptomatic (Cough/Breath)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;318&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;213&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;582&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;245&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;141&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>47931</offset><text>Experiments	Healthy	COVID-19	 	Cambridge	QU	Cambridge	QU	 	Symptomatic (Cough/Breath)	264	32	54	18	 	Asymptomatic (Cough/Breath)	318	213	87	78	 	Total	582	245	141	96	 	</text></passage><passage><infon key="file">diagnostics-12-00920-t002.xml</infon><infon key="id">diagnostics-12-00920-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>48100</offset><text>Experimental pipelines for this study.</text></passage><passage><infon key="file">diagnostics-12-00920-t002.xml</infon><infon key="id">diagnostics-12-00920-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pipelines&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Healthy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pipeline I&lt;break/&gt;(Symptomatic)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;list list-type=&quot;simple&quot;&gt;&lt;list-item&gt;&lt;label&gt;a.&lt;/label&gt;&lt;p&gt;Cough&lt;/p&gt;&lt;/list-item&gt;&lt;list-item&gt;&lt;label&gt;b.&lt;/label&gt;&lt;p&gt;Breath&lt;/p&gt;&lt;/list-item&gt;&lt;/list&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;list list-type=&quot;simple&quot;&gt;&lt;list-item&gt;&lt;label&gt;a.&lt;/label&gt;&lt;p&gt;Cough&lt;/p&gt;&lt;/list-item&gt;&lt;list-item&gt;&lt;label&gt;b.&lt;/label&gt;&lt;p&gt;Breath&lt;/p&gt;&lt;/list-item&gt;&lt;/list&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pipeline II&lt;break/&gt;(Asymptomatic)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;list list-type=&quot;simple&quot;&gt;&lt;list-item&gt;&lt;label&gt;a.&lt;/label&gt;&lt;p&gt;Cough&lt;/p&gt;&lt;/list-item&gt;&lt;list-item&gt;&lt;label&gt;b.&lt;/label&gt;&lt;p&gt;Breath&lt;/p&gt;&lt;/list-item&gt;&lt;/list&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;list list-type=&quot;simple&quot;&gt;&lt;list-item&gt;&lt;label&gt;a.&lt;/label&gt;&lt;p&gt;Cough&lt;/p&gt;&lt;/list-item&gt;&lt;list-item&gt;&lt;label&gt;b.&lt;/label&gt;&lt;p&gt;Breath&lt;/p&gt;&lt;/list-item&gt;&lt;/list&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>48139</offset><text>Pipelines	COVID-19	Healthy	 	Pipeline I(Symptomatic)	CoughBreath	CoughBreath	 	Pipeline II(Asymptomatic)	CoughBreath	CoughBreath	 	</text></passage><passage><infon key="file">diagnostics-12-00920-t003.xml</infon><infon key="id">diagnostics-12-00920-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>48271</offset><text>Number of mages per class and per fold used for different pipelines.</text></passage><passage><infon key="file">diagnostics-12-00920-t003.xml</infon><infon key="id">diagnostics-12-00920-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Categories&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classes&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total Samples&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Training Samples&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Validation&lt;break/&gt;Samples&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Test &lt;break/&gt;Samples&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Symptomatic&lt;break/&gt;&lt;bold&gt;(Cough/Breath)&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Healthy&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;296&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;213 × 10 = 2130&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52 × 38 = 1976&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Asymptomatic&lt;break/&gt;&lt;bold&gt;(Cough/Breath)&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Healthy&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;531&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;383 × 5 = 1915&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;106&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;165&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;119 × 17 = 2023&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>48340</offset><text>Categories	Classes	Total Samples	Training Samples	ValidationSamples	Test Samples	 	Symptomatic(Cough/Breath)	Healthy	296	213 × 10 = 2130	24	59	 	COVID-19	72	52 × 38 = 1976	6	14	 	Asymptomatic(Cough/Breath)	Healthy	531	383 × 5 = 1915	42	106	 	COVID-19	165	119 × 17 = 2023	13	33	 	</text></passage><passage><infon key="file">diagnostics-12-00920-t004.xml</infon><infon key="id">diagnostics-12-00920-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>48624</offset><text>Details of training parameters for classification.</text></passage><passage><infon key="file">diagnostics-12-00920-t004.xml</infon><infon key="id">diagnostics-12-00920-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/th&gt;&lt;th colspan=&quot;6&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Training Parameters for Classification&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Batch Size&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Learning Rate&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of Epochs&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Epoch Patience&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Stopping Criteria &lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Optimizer&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Parameters&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.001&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ADAM&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>48675</offset><text>	Training Parameters for Classification	 		Batch Size	Learning Rate	Number of Epochs	Epoch Patience	Stopping Criteria 	Optimizer	 	Parameters	32	0.001	30	15	15	ADAM	 	</text></passage><passage><infon key="file">diagnostics-12-00920-t005.xml</infon><infon key="id">diagnostics-12-00920-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>48843</offset><text>Comparison of different CNN performances for binary classification for symptomatic and asymptomatic patients’ (A) cough and (B) breath sounds.</text></passage><passage><infon key="file">diagnostics-12-00920-t005.xml</infon><infon key="id">diagnostics-12-00920-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;8&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;(A)&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Scheme&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Network&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Overall&lt;/bold&gt;
&lt;/td&gt;&lt;td colspan=&quot;4&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Weighted 95% CI&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Inference Time (Sec)&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Accuracy&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Precision&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Sensitivity&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;&lt;italic toggle=&quot;yes&quot;&gt;F&lt;/italic&gt;1-Score&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Specificity&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;9&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Symptomatic&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.20 ± 2.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.65 ± 2.49&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.21 ± 2.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.35 ± 2.55&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.94 ± 3.07&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0024&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.38 ± 2.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.41 ± 2.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.38 ± 2.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.39 ± 2.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.47 ± 3.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0061&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.29 ± 2.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.41 ± 2.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.29 ± 2.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.53 ± 2.32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.56 ± 1.58&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0108&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception_v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.76 ± 2.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.53 ± 2.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.76 ± 2.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.02 ± 2.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.19 ± 3.52&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0238&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DenseNet201&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.25 ± 2.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.78 ± 2.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.21 ± 2.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.39 ± 2.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.99 ± 2.93&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0258&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobilenetv2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.49 ± 3.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.78 ± 2.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.49 ± 3.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.61 ± 2.98&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.92 ± 3.93&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0055&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.20 ± 2.89&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.15 ± 2.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.30 ± 2.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.20 ± 2.89&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.97 ± 4.16&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0106&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.30 ± 2.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.40 ± 2.86&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.31 ± 2.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.35 ± 2.87&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.12 ± 3.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0428&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Stacking CNN model&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;96.50 ± 1.88&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;96.30 ± 1.93&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;96.42 ± 1.90&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;96.32 ± 1.92&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;95.47 ± 2.12&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.0389&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;9&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Asymptomatic&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.70 ± 1.33&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.68 ± 1.33&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.69 ± 1.33&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.66 ± 1.33&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.29 ± 1.98&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0027&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.97 ± 1.62&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.12 ± 1.60&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.98 ± 1.62&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.80 ± 1.65&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.07 ± 2.65&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0058&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.84 ± 1.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.84 ± 1.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.84 ± 1.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.84 ± 1.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.42 ± 1.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0121&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception_v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.26 ± 1.41&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.30 ± 1.40&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.27 ± 1.41&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.19 ± 1.42&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.65 ± 2.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0235&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DenseNet201&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.28 ± 0.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.27 ± 0.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.28 ± 1.41&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.11 ± 1.24&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.20 ± 0.66&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0260&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobilenetv2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.50 ± 0.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.30 ± 0.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.45 ± 1.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.25 ± 1.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.20 ± 0.66&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0052&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.82 ± 1.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.74 ± 1.80&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.82 ± 1.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.72 ± 1.80&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.96 ± 2.58&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0118&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.40 ± 1.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.40 ± 1.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.40 ± 1.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.31 ± 1.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.13 ± 2.40&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.046&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Stacking CNN model&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;98.85 ± 0.79&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;97.76 ± 1.10&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;97.01 ± 1.27&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;97.41 ± 1.18&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;99.6 ± 0.47&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.0411&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;8&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;(B)&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Scheme&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Network&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Overall&lt;/bold&gt;
&lt;/td&gt;&lt;td colspan=&quot;4&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Weighted 95% CI&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Inference Time (sec)&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Accuracy&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Precision&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Sensitivity&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;&lt;italic toggle=&quot;yes&quot;&gt;F&lt;/italic&gt;1-Score&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Specificity&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;9&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Symptomatic&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.49 ± 3.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;70.27 ± 4.67&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.27 ± 3.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.80 ± 4.38&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.49 ± 3.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0027&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.66 ± 4.04&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;70.83 ± 4.64&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.83 ± 3.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.93 ± 4.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.67 ± 4.03&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0060&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.53 ± 3.69&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;73.01 ± 4.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.01 ± 3.74&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.12 ± 4.22&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.53 ± 3.69&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0098&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception_v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.49 ± 3.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.05 ± 4.63&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.05 ± 3.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.15 ± 4.35&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.49 ± 3.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0254&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DenseNet201&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.98 ± 3.75&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72.43 ± 4.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.43 ± 3.8&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.54 ± 4.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.98 ± 3.75&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.026&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobilenetv2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.57 ± 3.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.50 ± 4.7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.50 ± 3.38&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.47 ± 4.27&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.57 ± 3.37&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0048&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.33 ± 3.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;70.28 ± 4.67&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.28 ± 3.03&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.03 ± 4.16&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.33 ± 3.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.77 ± 3.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;70.99 ± 4.64&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.99 ± 3.93&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.09 ± 4.36&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.77 ± 3.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0434&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Stacking CNN model&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;91.03 ± 2.92&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;71.91 ± 4.59&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;88.9 ± 3.21&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;79.62 ± 4.12&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;91.5 ± 2.85&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.0265&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;9&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Asymptomatic&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.75 ± 3.50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.95 ± 3.7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.66 ± 3.50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.64 ± 3.64&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.54 ± 3.05&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.67 ± 3.50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.45 ± 3.69&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.67 ± 3.50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.54 ± 3.63&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.27 ± 3.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0047&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Resnet101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.72 ± 3.41&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;56.45 ± 3.68&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.71 ± 3.41&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.38 ± 3.60&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;73.52 ± 3.28&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0118&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception_v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.10 ± 3.49&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.10 ± 3.68&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.26 ± 3.46&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.18 ± 3.60&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.25 ± 2.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0243&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DenseNet201&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.97 ± 3.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.91 ± 3.69&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.97 ± 3.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.35 ± 3.62&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.88 ± 2.98&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0271&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MobileNetv2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.40 ± 3.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.22 ± 3.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.10 ± 3.49&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.36 ± 3.65&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.54 ± 3.05&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0048&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.30± 3.46&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.45 ± 3.67&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.62 ± 3.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.54 ± 3.60&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.50 ± 3.15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0128&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;EfficientNet_B7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.60 ± 3.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;54.20 ± 3.70&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72.59 ± 3.31&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.06 ± 3.61&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.20 ± 2.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0511&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Stacking CNN model&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;80.01 ± 2.97&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;56.02 ± 3.69&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;72.04 ± 3.33&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;63.3 ± 3.58&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;82.67 ± 2.81&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.0687&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>48988</offset><text>(A)	 	Scheme	Network	Overall	Weighted 95% CI	Inference Time (Sec)	 	Accuracy	Precision	Sensitivity	F1-Score	Specificity	 	Symptomatic	Resnet18	93.20 ± 2.57	93.65 ± 2.49	93.21 ± 2.57	93.35 ± 2.55	89.94 ± 3.07	0.0024	 	Resnet50	95.38 ± 2.14	95.41 ± 2.14	95.38 ± 2.14	95.39 ± 2.14	90.47 ± 3.00	0.0061	 	Resnet101	94.29 ± 2.37	95.41 ± 2.14	94.29 ± 2.37	94.53 ± 2.32	97.56 ± 1.58	0.0108	 	Inception_v3	90.76 ± 2.96	91.53 ± 2.84	90.76 ± 2.96	91.02 ± 2.92	86.19 ± 3.52	0.0238	 	DenseNet201	93.25 ± 2.56	93.78 ± 2.47	93.21 ± 2.57	93.39 ± 2.54	90.99 ± 2.93	0.0258	 	Mobilenetv2	90.49 ± 3.00	90.78 ± 2.96	90.49 ± 3.00	90.61 ± 2.98	81.92 ± 3.93	0.0055	 	EfficientNet_B0	90.20 ± 2.89	90.15 ± 2.90	91.30 ± 2.88	91.20 ± 2.89	78.97 ± 4.16	0.0106	 	EfficientNet_B7	91.30 ± 2.88	91.40 ± 2.86	91.31 ± 2.88	91.35 ± 2.87	82.12 ± 3.92	0.0428	 	Stacking CNN model	96.50 ± 1.88	96.30 ± 1.93	96.42 ± 1.90	96.32 ± 1.92	95.47 ± 2.12	0.0389	 	Asymptomatic	Resnet18	96.70 ± 1.33	96.68 ± 1.33	96.69 ± 1.33	96.66 ± 1.33	92.29 ± 1.98	0.0027	 	Resnet50	94.97 ± 1.62	95.12 ± 1.60	94.98 ± 1.62	94.80 ± 1.65	85.07 ± 2.65	0.0058	 	Resnet101	96.84 ± 1.30	96.84 ± 1.30	96.84 ± 1.30	96.84 ± 1.30	94.42 ± 1.71	0.0121	 	Inception_v3	96.26 ± 1.41	96.30 ± 1.40	96.27 ± 1.41	96.19 ± 1.42	89.65 ± 2.26	0.0235	 	DenseNet201	98.28 ± 0.97	98.27 ± 0.97	96.28 ± 1.41	97.11 ± 1.24	99.20 ± 0.66	0.0260	 	Mobilenetv2	98.50 ± 0.90	98.30 ± 0.96	96.45 ± 1.37	97.25 ± 1.21	99.20 ± 0.66	0.0052	 	EfficientNet_B0	93.82 ± 1.79	93.74 ± 1.80	93.82 ± 1.79	93.72 ± 1.80	85.96 ± 2.58	0.0118	 	EfficientNet_B7	95.40 ± 1.56	95.40 ± 1.56	95.40 ± 1.56	95.31 ± 1.57	88.13 ± 2.40	0.046	 	Stacking CNN model	98.85 ± 0.79	97.76 ± 1.10	97.01 ± 1.27	97.41 ± 1.18	99.6 ± 0.47	0.0411	 	(B)	 	Scheme	Network	Overall	Weighted 95% CI	Inference Time (sec)	 	Accuracy	Precision	Sensitivity	F1-Score	Specificity	 	Symptomatic	Resnet18	81.49 ± 3.97	70.27 ± 4.67	82.27 ± 3.90	75.80 ± 4.38	81.49 ± 3.97	0.0027	 	Resnet50	80.66 ± 4.04	70.83 ± 4.64	81.83 ± 3.94	75.93 ± 4.37	80.67 ± 4.03	0.0060	 	Resnet101	84.53 ± 3.69	73.01 ± 4.54	84.01 ± 3.74	78.12 ± 4.22	84.53 ± 3.69	0.0098	 	Inception_v3	81.49 ± 3.97	71.05 ± 4.63	82.05 ± 3.92	76.15 ± 4.35	81.49 ± 3.97	0.0254	 	DenseNet201	83.98 ± 3.75	72.43 ± 4.57	83.43 ± 3.8	77.54 ± 4.26	83.98 ± 3.75	0.026	 	Mobilenetv2	87.57 ± 3.37	69.50 ± 4.7	87.50 ± 3.38	77.47 ± 4.27	87.57 ± 3.37	0.0048	 	EfficientNet_B0	90.33 ± 3.02	70.28 ± 4.67	90.28 ± 3.03	79.03 ± 4.16	90.33 ± 3.02	0.0104	 	EfficientNet_B7	81.77 ± 3.94	70.99 ± 4.64	81.99 ± 3.93	76.09 ± 4.36	81.77 ± 3.94	0.0434	 	Stacking CNN model	91.03 ± 2.92	71.91 ± 4.59	88.9 ± 3.21	79.62 ± 4.12	91.5 ± 2.85	0.0265	 	Asymptomatic	Resnet18	66.75 ± 3.50	53.95 ± 3.7	66.66 ± 3.50	59.64 ± 3.64	78.54 ± 3.05	0.0025	 	Resnet50	66.67 ± 3.50	55.45 ± 3.69	66.67 ± 3.50	60.54 ± 3.63	75.27 ± 3.21	0.0047	 	Resnet101	69.72 ± 3.41	56.45 ± 3.68	69.71 ± 3.41	62.38 ± 3.60	73.52 ± 3.28	0.0118	 	Inception_v3	67.10 ± 3.49	57.10 ± 3.68	68.26 ± 3.46	62.18 ± 3.60	81.25 ± 2.90	0.0243	 	DenseNet201	67.97 ± 3.47	55.91 ± 3.69	67.97 ± 3.47	61.35 ± 3.62	79.88 ± 2.98	0.0271	 	MobileNetv2	68.40 ± 3.45	53.22 ± 3.71	67.10 ± 3.49	59.36 ± 3.65	78.54 ± 3.05	0.0048	 	EfficientNet_B0	68.30± 3.46	57.45 ± 3.67	68.62 ± 3.45	62.54 ± 3.60	76.50 ± 3.15	0.0128	 	EfficientNet_B7	75.60 ± 3.19	54.20 ± 3.70	72.59 ± 3.31	62.06 ± 3.61	80.20 ± 2.96	0.0511	 	Stacking CNN model	80.01 ± 2.97	56.02 ± 3.69	72.04 ± 3.33	63.3 ± 3.58	82.67 ± 2.81	0.0687	 	</text></passage><passage><infon key="file">diagnostics-12-00920-t006.xml</infon><infon key="id">diagnostics-12-00920-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>52581</offset><text>Comparison of the proposed work with similar studies.</text></passage><passage><infon key="file">diagnostics-12-00920-t006.xml</infon><infon key="id">diagnostics-12-00920-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Papers&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dataset&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Phenomenon&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reported Method&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Performance&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;N. Sharma (2020)&lt;break/&gt;&lt;xref rid=&quot;B48-diagnostics-12-00920&quot; ref-type=&quot;bibr&quot;&gt;48&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Healthy and COVID-19-positive: 941&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cough, Breathing, Vowel, and Counting (1–20)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Random forest classifier using spectral contrast, MFCC, spectral roll-off, spectral centroid, mean square energy, polynomial fit, zero-crossing rate, spectral bandwidth, and spectral flatness. &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy: 76.74%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;C. Brown et al. (2021)&lt;break/&gt;&lt;xref rid=&quot;B55-diagnostics-12-00920&quot; ref-type=&quot;bibr&quot;&gt;55&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19-positive: 141,&lt;break/&gt;Non-COVID: 298,&lt;break/&gt;COVID-19-positive with Cough: 54,&lt;break/&gt;Non-COVID-19 with Cough: 32, Non-COVID-19 asthma: 20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cough and Breathing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN-based approach using spectrogram, spectral centroid, MFCC. &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy: 80%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V. Espotovic (2021)&lt;break/&gt;&lt;xref rid=&quot;B71-diagnostics-12-00920&quot; ref-type=&quot;bibr&quot;&gt;71&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19-Positive: 84, COVID-19-Negative: 419&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cough and Breathing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ensemble-boosted approach using spectrogram and wavelet. &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy: 88.52%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;R.Islam (2022)&lt;break/&gt;&lt;xref rid=&quot;B72-diagnostics-12-00920&quot; ref-type=&quot;bibr&quot;&gt;72&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19-Positve: 50,&lt;break/&gt;Healthy: 50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN-based approach using zero-crossing rate, energy, energy entropy, spectral centroid, spectral entropy, spectral flux, spectral roll-offs, MFCC. &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy: 88.52%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proposed Study&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;COVID-19-Positve: 237, Healthy: 827&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cough and Breathing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Stacking-based CNN based approach using spectograms&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;For symptomatic, accuracy: 96.5% and for asymptomatic, accuracy: 98.85%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>52635</offset><text>Papers	Dataset	Phenomenon	Reported Method	Performance	 	N. Sharma (2020)	Healthy and COVID-19-positive: 941	Cough, Breathing, Vowel, and Counting (1–20)	Random forest classifier using spectral contrast, MFCC, spectral roll-off, spectral centroid, mean square energy, polynomial fit, zero-crossing rate, spectral bandwidth, and spectral flatness. 	Accuracy: 76.74%	 	C. Brown et al. (2021)	COVID-19-positive: 141,Non-COVID: 298,COVID-19-positive with Cough: 54,Non-COVID-19 with Cough: 32, Non-COVID-19 asthma: 20	Cough and Breathing	CNN-based approach using spectrogram, spectral centroid, MFCC. 	Accuracy: 80%	 	V. Espotovic (2021)	COVID-19-Positive: 84, COVID-19-Negative: 419	Cough and Breathing	Ensemble-boosted approach using spectrogram and wavelet. 	Accuracy: 88.52%	 	R.Islam (2022)	COVID-19-Positve: 50,Healthy: 50	Cough	CNN-based approach using zero-crossing rate, energy, energy entropy, spectral centroid, spectral entropy, spectral flux, spectral roll-offs, MFCC. 	Accuracy: 88.52%	 	Proposed Study	COVID-19-Positve: 237, Healthy: 827	Cough and Breathing	Stacking-based CNN based approach using spectograms	For symptomatic, accuracy: 96.5% and for asymptomatic, accuracy: 98.85%	 	</text></passage></document></collection>
