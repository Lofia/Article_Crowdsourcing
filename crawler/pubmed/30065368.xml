<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220624</date><key>pmc.key</key><document><id>8863499</id><infon key="license">author_manuscript</infon><passage><infon key="article-id_doi">10.1038/s41592-018-0069-0</infon><infon key="article-id_manuscript">NIHMS1773628</infon><infon key="article-id_pmc">8863499</infon><infon key="article-id_pmid">30065368</infon><infon key="fpage">587</infon><infon key="issue">8</infon><infon key="license">
          This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
        </infon><infon key="lpage">590</infon><infon key="name_0">surname:Hughes;given-names:Alex J.</infon><infon key="name_1">surname:Mornin;given-names:Joseph D.</infon><infon key="name_2">surname:Biswas;given-names:Sujoy K.</infon><infon key="name_3">surname:Beck;given-names:Lauren E.</infon><infon key="name_4">surname:Bauer;given-names:David P.</infon><infon key="name_5">surname:Raj;given-names:Arjun</infon><infon key="name_6">surname:Bianco;given-names:Simone</infon><infon key="name_7">surname:Gartner;given-names:Zev J.</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">15</infon><infon key="year">2022</infon><offset>0</offset><text>Quanti.us: a tool for rapid, flexible, crowd-based annotation of images</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>72</offset><text>We describe Quanti.us, a crowd-based image-annotation platform that provides an accurate alternative to computational algorithms for difficult image-analysis problems. We used Quanti.us for a variety of medium-throughput image-analysis tasks and achieved 10–50× savings in analysis time compared with that required for the same task by a single expert annotator. We show equivalent deep learning performance for Quanti.us-derived and expert-derived annotations, which should allow scalable integration with tailored machine learning algorithms.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>620</offset><text>Image analysis is increasingly crucial in quantitative biology and medicine. Features in images can be accurately annotated by humans, but this approach becomes impractical when one is working with hundreds to thousands of images. Therefore, researchers use custom-written scripts to analyze images via methods such as contrast-based segmentation, edge detection, and tracking. These approaches can give excellent performance under specific experimental conditions, but they can respond unpredictably to slight variations in experimental setup. Issues related to image volume and diversity have motivated the development of machine learning algorithms including convolutional neural networks that are trained on extensive sets of human-annotated images. Synthetic datasets offer researchers some ability to circumvent high annotation burdens, but they often do not capture the range of real-world phenotypes that algorithms must discriminate between.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1571</offset><text>Crowdsourcing offers an attractive alternative. Indeed, the scientific community has begun to build annotation pipelines that leverage large groups of human annotators working in parallel. Specific large-scale image-annotation projects have been custom-built in platforms such as EyeWire and Project Discovery. Zooniverse aims to make crowd annotation accessible to scientists across disciplines by allowing researchers to select from a palette of image-annotation tools, and offers a modular interface for use by volunteer annotators. However, all of these platforms rely on a volunteer labor pool, which requires continuous marketing or ‘gamification’ to draw attention to individual projects, and to make up for inconsistent motivation among human annotators and temporal volatility in volunteer numbers. Practically speaking, this means that jobs can suffer from lower annotation collection rates and quality (Supplementary Note 1). Other crowdsourcing approaches, such as Amazon’s “Mechanical Turk,” enable operators to circumvent these problems through the use of micropayments. However, services like Mechanical Turk are not yet used extensively by the life sciences community, perhaps because they lack interfaces for generic image annotation and have not been quantitatively validated.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2876</offset><text>We therefore developed Quanti.us, a flexible portal that helps scientists recruit groups of untrained Mechanical Turk workers (‘Turkers’) to annotate images using a set of interaction tools that can be applied individually to many types of jobs. Annotations can be collected and used in series to refine further rounds of annotation, like pre-segmented input to conventional algorithms, or used as training data for machine learning algorithms (Fig. 1a).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3335</offset><text>The Quanti.us website allows researchers to upload image sets, choose an analysis tool, and provide simple sets of instructions. Turkers are presented with individual images or sets of sequential images as stacks via a ‘slider’ interface. Each image or stack is referred to as a ‘task’ within a larger ‘job’. The website automatically interfaces with Mechanical Turk to set up tasks and return raw data to the researcher. These data include click location, Turker identification numbers, and time stamps. Quanti.us can also provide a link to users for access to a free ‘test mode’ that allows them to bypass Turkers and recruit annotators from other communities such as classrooms, the general public, or research groups (Methods).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4083</offset><text>We first evaluated Quanti.us for a particle-discrimination task involving images of fluorescent cells migrating through a porous Transwell membrane (Fig. 1b). Counting such cells on the basis of contrast-defined segmentation is difficult because the autofluorescent pores of the membrane are hard to distinguish from cells. We tasked Turkers with clicking on cells with a crosshair tool and evaluated their performance relative to a ‘ground truth’ expert dataset. We found that 59% of Turkers who completed at least one image performed better in terms of both precision (the ability to exclude false positives) and recall (the ability to exclude false negatives) than a semi-automated FIJI pipeline consisting of brightness threshold, watershed, and particle-size threshold steps (Fig. 1c).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4878</offset><text>We asked multiple Turkers to analyze each image and then leveraged the ‘wisdom of crowds’ to improve the overall performance by means of two strategies. First, subtractive spatial clustering of the annotations from ten replicate Turkers produced precision and recall metrics of 0.99 and 0.81—0.015 and 0.18 higher, respectively, than the values obtained from application of the performance envelope followed by the FIJI algorithm for different particle-size thresholds. The ability of crowds to mitigate the effects of rare poorly performing workers was accrued for as few as three replicates per image (Fig. 1d). Second, we generated an inherent Turker quality score by comparing annotations from each Turker with clusters generated from annotations made by their peers, which could allow more poorly performing Turkers to be automatically screened out, even without an expert dataset for comparison. When we filtered out the contributions of the bottom third of Turkers, the performance of Quanti.us increased to within the range of the performance of five other experts who were not involved in generation of the ground truth dataset (Fig. 1c, inset). Turkers also improved in performance at cell/pore discrimination over their first 50 annotations, with average false positive rates decreasing from 3.7% to 1.3% per click (Fig. 1e). These data suggest opportunities to further improve Turker performance by, for example, providing an initial training image set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6350</offset><text>In our evaluation of pointing accuracy, we saw that average Turker clicking periods correlated with root-mean-square (r.m.s.) errors, according to Fitts’s law of speed–accuracy tradeoffs in human pointing tasks (Fig. 1f). Median Turker click times were ~2s for r.m.s. errors of ~3 pixels. Overall, the 129 Turkers who made at least one annotation had r.m.s. errors of less than 13 pixels (the average diameter of the cells they annotated). In agreement with other studies of worker contributions in crowds, the number of images attempted by each Turker followed the Pareto ‘80/20’ principle: ~20% of Turkers accounted for ~80% of images completed (Supplementary Fig. 1).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7029</offset><text>Because Quanti.us pays individuals to annotate images through Mechanical Turk, Turkers change their performance and job choice on the basis of the economic tradeoffs inherent in completing a task accurately and quickly. We measured the relationships among Turker performance, task complexity, overall task completion rate, and the amount paid per task. We ran calibration experiments on synthetic ground truth images containing variable numbers of spatially distributed particles. We found that Turkers tolerated around 60 annotations per image task at a pay rate of $0.02 per image (Supplementary Fig. 2a–d). Above this image-complexity threshold, the average number of images attempted by Turkers dropped from 15 to 8. At complexity levels above 110 particles per image, recall dipped from &gt; 0.8 to ~0.6, and the rate of collected annotations dropped from &gt; 25,000h−1 to ~3,000h−1. However, an increase in the amount paid per image broadly reversed these complexity-associated losses in recall and overall annotation rate (Supplementary Fig. 2e–g). We observed an annotation collection rate of ~105 h−1 for images requiring 110 annotations each at $0.06 per image, reflecting a time savings of 10–50× compared with that required for a single expert annotator to achieve similar accuracy (Supplementary Fig. 2e and Supplementary Note 2).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8381</offset><text>We next assessed the performance of this approach on more complex annotation tasks. First, we asked Turkers to draw polylines (piecewise linear curves) over microtubules recorded in a ‘gliding’ motility assay (Fig. 2a). Such images are challenging to segment automatically because microtubules often overlap. We spatially clustered polyline annotations from ten Turkers per image, and used these cluster centers as input to FIJI’s TrackMate plugin. We then compared these data with the output of a semi-automated gliding assay analysis package, FIESTA. Although both Quanti. us and FIESTA velocity distributions approximately matched that recovered from manual microtubule tracking by an expert, the Quanti.us microtubule-length distribution matched the expert distribution more closely than FIESTA’s. This seemed to be because Turkers were better at ignoring overlap junctions between microtubules, whereas FIESTA tended to break microtubule annotations into smaller segments bordered by junctions (Fig. 2a).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9399</offset><text>Pushing Quanti.us toward 3D image analysis, we asked Turkers to draw closed polygons over cell nuclei in frames from a stack of fluorescence microscopy images of epithelial cysts (Fig. 2b). We clustered outlines from individual Turkers by thresholding their degree of overlap, and generated consensus outlines suitable for comparison with outlines from a conventional 3D nuclear segmentation algorithm (MINS) or from a trained expert. The consensus Turker outlines and expert outlines gave similar estimates of the number of nuclei in the stack, resulting in precision and recall metrics greater than 0.9 for the Turker collective compared with the expert’s values. Certain parameter sets used during MINS analyses gave similar performance, although these parameters required optimization to suit a particular frame in each stack. We used a pixel-wise scheme to analyze precision and recall in order to compare estimates of nuclear area, and observed moderate performance of the Turker collective compared with that of the expert. However, in this analysis the Turker collective performed better than MINS across a wide range of parameters. We also saw similar outlining performance for Turker collectives and experts in an epithelial organoid annotation task that required segmentation of bright-field microscopy images against a dynamic background of migrating single cells, which negatively affected the performance of an automated segmentation algorithm (Supplementary Fig. 3).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10883</offset><text>We tasked Turkers with making multiple crosshair annotations to track the nose, digits, and tail of a freely moving mouse in a movie showing the mouse’s ventral aspect (Fig. 2c). Clustered Turker annotations were analyzed by FIJI’s TrackMate, and successfully captured the dynamics recovered through manual gait analysis by a trained expert. These dynamics were missed by conventional contrast-based segmentation consisting of brightness thresholding, particle analysis, and TrackMate because of difficulty in distinguishing the mouse from the background. In the more difficult case of tracking ants in low-contrast images acquired near terrestrial nests (Supplementary Note 3 and Supplementary Fig. 4), we found that an initial deficiency in Turker performance compared with that of an expert for single images could be overcome if Turkers were presented with multiple images via the slider interface, which took advantage of the ants’ movement to make them more easily detectable. This shows that although the Turkers lacked the prior knowledge and experience of experts, this deficit could be compensated for when the task was presented in a more tailored context.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12057</offset><text>Finally, we tested the use of Quanti.us-derived annotations as training data for machine learning. As proof of principle, we studied a movie of fluorescently labeled mammary epithelial cell clusters spreading over an in vitro culture surface, a particularly challenging problem because of frequent cell overlaps and heterogeneous cell morphologies that change over time (Fig. 2d). We trained a deep convolutional regression network on Turker annotations to determine whether it could achieve performance similar to that of a network trained on expert annotations (Supplementary Note 4). After designing a two-stage training procedure, we produced an algorithm trained on the annotations of ten Turkers; this yielded an F-score (the harmonic mean of precision and recall) similar to that of an algorithm trained on the annotations of an expert (0.68 and 0.71, respectively). Both algorithms showed better performance than traditional Bayes-optimal Otsu segmentation (F-score of 0.62). Further, the performance of the algorithm trained on the Turker collective was better than the mean performance of algorithms trained on annotations from individual Turkers, reflecting a ‘wisdom of the crowd’ benefit to the training process (Supplementary Fig. 5).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13310</offset><text>Quantitation of biomedical imaging data remains a major bottleneck. The Quanti.us approach addresses this bottleneck by making crowd analysis of scientific images fast and applicable to many annotation problems. We show here that Quanti.us can enable researchers to gather hand annotations quickly, at significant scale and with high quality, by marshaling paid Turkers to annotate a range of image types. Annotations of difficult segmentation tasks may be used both for rapid pilot-scale analyses and to train convolutional neural networks. Quanti.us is also designed to allow nimble image annotation that better suits the iterative cycles of imaging, analysis, and hypothesis reformulation that characterize life science research. Pools of even higher-quality Turkers could be curated through dynamic performance tracking. Further, Quanti.us tasks could be integrated with machine learning to produce multi-stage annotation pipelines. These efforts would simplify quantitative biology analyses for fundamental, health-related, and diagnostic ends.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>14360</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14368</offset><text>Annotation collection using Quanti.us.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14407</offset><text>Image sets were uploaded to quanti.us, a publicly available website developed for this work. The website enables a researcher to upload an image set (each image is considered a task within the larger job), select an annotation tool, provide instructions to Turkers (Supplementary Fig. 6), and specify the desired number of ‘replicates’ (number of independent Turkers making annotations on each image task). A cost calculator gives the researcher a transparent estimate of the cost of the job before it is submitted. Each image task in the job is created by Quanti.us as a ‘human intelligence task’ on Amazon Mechanical Turk. When all tasks are complete, Quanti.us returns annotations as Cartesian coordinates, along with individual anonymized Turker identification numbers. Time stamps are also returned for each submitted image, and for each annotation relative to the first annotation made by a given Turker on that image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15340</offset><text>Quanti.us can also be used in a free test mode that has two key uses. First, it enables users to test-drive their own job as if they were Turkers, which allows them to check the rendering of their images, how their annotation tool operates, how their instructions appear to Turkers, the format of the raw annotation data they can expect to be returned to them, etc. Second, it can provide a public link to the job that the user can disseminate to any other annotators from other communities such as classrooms, the general public, or their own research group.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15900</offset><text>We encourage users to experiment with small trial batches of five to ten images before submitting larger batches intended for final analysis in order to determine a set of instructions that best informs Turkers of the intended annotation outcome. Users can also ‘stress test’ their annotation experiment by sending a link to the Quanti.us test mode version of the job to non-expert peers. These peers can then provide feedback on image quality, instruction clarity, and overall difficulty of the task, for example.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16419</offset><text>A detailed Matlab pipeline with instructions and example images that covers automated image pre-processing for upload to Quanti.us and many annotation post-processing and overlay options is publicly available (see “Code availability”)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16658</offset><text>Annotation post-processing.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16686</offset><text>Annotations were overlaid onto images and postprocessed with custom scripts in Matlab R2015b (Mathworks, Natick, MA). Descriptions of annotation methods by figure are presented in Supplementary Table 1. Spatial subtractive clustering was performed on individual annotations made with the crosshair tool, or on centroids of sets of annotations made with the polyline tool (subclust.m). In analyses of crosshair annotations made by individual Turkers, false positive annotations (fp) were taken as those more than x pixels from the nearest annotation in the corresponding expert ground truth dataset (true positives (tp)), and false negative annotations (fn) were taken as ground truth annotations more than x pixels from the nearest annotation made by the Turker. The value of x was set to twice the average full width at half-maximum of the objects being annotated. The collective performance of Turkers was evaluated via similar computations for clusters rather than for individual annotations. A simple Turker score was defined as 1 – ((fp’+ k × fn’)/(2 × tp’)), where fp’ and fn’ are the numbers of false positives and false negatives determined for the Turker under consideration relative to Turker annotation clusters (tp’) rather than the expert ground truth annotations. The score can be tailored to specific job types through the arbitrary parameter k (we set k = 0.2 for Fig. 1). More complex inherent worker quality scores have also been defined.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18159</offset><text>Spatial r.m.s. errors for each Turker were computed from the minimal distances between their true positive annotations and corresponding ground truth annotations. Performance metrics were precision (tp/(tp + fp)) and recall (tp/(tp + fn)). Fitts’s law was fit using the Levenberg–Marquardt algorithm (fit.m) as t = a + blog2 ((c + σ)/σ), where a, b, and c are fitting parameters, and σ is the r.m.s. error.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18578</offset><text>The centroids of polyline objects associated with each image were clustered and an arbitrary distance threshold was used to determine the membership of a given polyline in a consensus group describing a putative image structure. Polylines outside this threshold were discarded. For microtubule polylines, centroids were computed from the set of annotations in each group and passed as input to FIJI’s TrackMate (National Institutes of Health, Bethesda, MD, USA) to determine velocity distributions. The annotations in each group could also be fit by Deming regression to extract consensus polylines and their length distributions (deming.m). For polygon and freehand annotations, we used a threshold on the spatial overlap of outlines to assign them to local consensus groups in each image. We converted outlines in each group to a consensus outline by summing their associated masks and performing thresholding, erosion, and dilation. Overlapping consensus outlines within an image were discarded via a similar thresholding step. For 3D image stacks, a threshold on the overlap between consensus outlines in successive z frames was used to ‘connect’ annotations to form 3D segmentations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19773</offset><text>The MINS analysis in Fig. 2b was conducted for 18 parameter sets comprising all combinations of expected nucleus diameter (20, 30, 40 μm), noise level (2, 3), and kernel smoothing (1.0, 1.5, 2.0).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19971</offset><text>Users should undertake their own data quality assessment for each job type to ensure interpretability and accuracy of the raw Quanti.us output. This typically involves, first, overlaying raw annotations onto the input images as a visual check for a rough correspondence between image features and annotations. Second, spatial clustering of Turker annotations should be performed, and the results of the Turker collective should be compared with corresponding expert annotations for a small, representative subset of each batch of images submitted. Qualitative or quantitative evaluation of precision and recall metrics is suited to this. The user can generate these expert annotations with the image analysis software of their choice—for example, FIJI—or through the Quanti.us test mode that provides users with a link to a test area where they can annotate their own image sets (see “Annotation collection using Quanti.us”).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20905</offset><text>Machine learning.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20923</offset><text>We trained a machine learning system to predict the center locations of cell nuclei in 500 × 500 pixel images. Conventional approaches such as the extraction of regional features to develop a region-level detector did not offer viable options owing to the small size of the cellular objects. In contrast, a convolutional neural network (CNN) allowed an end-to-end system design without extraction of separate features to be fed into the learning system. The CNN transforms the image channels by applying a set of ‘learnable’ filters, successively, directly into an output matrix (as big as the input image) containing high scores in the locations of nuclei centers and very small to almost zero scores elsewhere. The objective of a fully convolutional regression network is to regress this Gaussian weighted output matrix from the input image channels. Here, the term “fully convolutional” refers to the fact that the target variable is a matrix of full image size instead of a vector quantity. The output matrix yields the center locations of the nuclei upon local thresholding. The hierarchical (layered) filter structure constitutes a ‘deep’ neural net. The filters encapsulate both linear (convolution) and nonlinear (rectified linear unit (ReLU)) operations. We adapted an elegant regression framework for cellular object detection. Further, we implemented a simple CNN architecture that minimizes an L2 loss with an exponentially decreasing learning rate.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22398</offset><text>For effective stabilization of the network weights (to avoid overfitting), the training process proceeded in two stages: a pre-training stage (with cropped 100 × 100 pixel images and augmented by geometric transformations such as flipping and rotation) followed by final training with full images. The test set had five images annotated by six experts. The images were further augmented (by rotation) to make a final test set of 20 images in total.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22848</offset><text>The deep CNN had layers comprising convolutional kernels and ReLUs. The CNN layers, along with all parameters, were specified in the following order from the input channels to the output: convolution (3×3×2×32 kernels), ReLU, convolution (3×3×32×32 kernels), ReLU, convolution (3×3×32×1 kernel). The convolution kernels were initialized with Xavier weights. The peaks of the Gaussian weights in the target matrix were set at 7, in accordance with prior convention. The learning rate started at 10−3 for pre-training and at 10−2 for final training, and decayed exponentially. The weight decay was set at 10−3 for both cases. The scores were thresholded at 1.0, with a window for non-maximum suppression as large as 25×25 pixels.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23592</offset><text>The center locations of image objects from the thresholded output matrix were scored for false positives and negatives against the expert ground truth as described in the section “Annotation post-processing.”</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23805</offset><text>Reporting Summary.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23824</offset><text>Further information on experimental design is available in the Nature Research Reporting Summary linked to this article.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23945</offset><text>Code availability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23964</offset><text>Source code is available at https://github.com/quantius-science/. This code is published under the open source MIT license. Researchers are free to use it without restriction. This repository includes code for the Quanti.us pre- and post-processing pipelines and machine learning pipeline.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24254</offset><text>Data availability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24273</offset><text>Raw data are available on request from the corresponding author.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>24338</offset><text>Supplementary Material</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24361</offset><text>Competing interests</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24381</offset><text>J.D.M. holds an equity interest in Quanti.us LLC. Quanti.us passes payments from users to Amazon Mechanical Turk, which then distributes these payments to workers.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24545</offset><text>Methods</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24553</offset><text>Methods, including statements of data availability and any associated accession codes and references, are available at https://doi.org/10.1038/s41592-018-0069-0.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24715</offset><text>Additional information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24738</offset><text>Supplementary information is available for this paper at https://doi.org/10.1038/s41592-018-0069-0.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24838</offset><text>Reprints and permissions information is available at www.nature.com/reprints.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>24916</offset><text>Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>25055</offset><text>References</text></passage><passage><infon key="fpage">331</infon><infon key="lpage">336</infon><infon key="name_0">surname:Kim;given-names:JS</infon><infon key="pub-id_pmid">24805243</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">509</infon><infon key="year">2014</infon><offset>25066</offset></passage><passage><infon key="fpage">679</infon><infon key="lpage">684</infon><infon key="name_0">surname:Chen;given-names:F</infon><infon key="pub-id_pmid">27376770</infon><infon key="section_type">REF</infon><infon key="source">Nat. Methods</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2016</infon><offset>25067</offset></passage><passage><infon key="fpage">382</infon><infon key="lpage">397</infon><infon key="name_0">surname:Lou;given-names:X</infon><infon key="name_1">surname:Kang;given-names:M</infon><infon key="name_2">surname:Xenopoulos;given-names:P</infon><infon key="name_3">surname:Muñoz-Descalzo;given-names:S</infon><infon key="name_4">surname:Hadjantonakis;given-names:A-K</infon><infon key="section_type">REF</infon><infon key="source">Stem Cell Rep</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2014</infon><offset>25068</offset></passage><passage><infon key="fpage">2820</infon><infon key="lpage">2828</infon><infon key="name_0">surname:Ruhnow;given-names:F</infon><infon key="name_1">surname:Zwicker;given-names:D</infon><infon key="name_2">surname:Diez;given-names:S</infon><infon key="pub-id_pmid">21641328</infon><infon key="section_type">REF</infon><infon key="source">Biophys. J</infon><infon key="type">ref</infon><infon key="volume">100</infon><infon key="year">2011</infon><offset>25069</offset></passage><passage><infon key="fpage">115</infon><infon key="lpage">118</infon><infon key="name_0">surname:Esteva;given-names:A</infon><infon key="pub-id_pmid">28117445</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">542</infon><infon key="year">2017</infon><offset>25070</offset></passage><passage><infon key="fpage">2672</infon><infon key="lpage">2680</infon><infon key="name_0">surname:Goodfellow;given-names:IJ</infon><infon key="section_type">REF</infon><infon key="source">Proc. Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2014</infon><offset>25071</offset></passage><passage><infon key="comment">https://arxiv.org/abs/1606.03498</infon><infon key="name_0">surname:Salimans;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>25072</offset></passage><passage><infon key="fpage">eaal3321</infon><infon key="name_0">surname:Thul;given-names:PJ</infon><infon key="pub-id_pmid">28495876</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">356</infon><infon key="year">2017</infon><offset>25073</offset></passage><passage><infon key="fpage">1049</infon><infon key="lpage">1054</infon><infon key="name_0">surname:Simpson;given-names:R</infon><infon key="name_1">surname:Page;given-names:KR</infon><infon key="name_2">surname:De Roure;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Proc. 23rd International Conference on World Wide Web</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>25074</offset></passage><passage><infon key="fpage">679</infon><infon key="lpage">684</infon><infon key="name_0">surname:Sauermann;given-names:H</infon><infon key="name_1">surname:Franzoni;given-names:C</infon><infon key="pub-id_pmid">25561529</infon><infon key="section_type">REF</infon><infon key="source">Proc. Natl. Acad. Sci. USA</infon><infon key="type">ref</infon><infon key="volume">112</infon><infon key="year">2015</infon><offset>25075</offset></passage><passage><infon key="name_0">surname:Hitlin;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Research in the Crowdsourcing Age, A Case Study</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>25076</offset></passage><passage><infon key="comment">https://www.biorxiv.org/content/early/2017/11/15/220145</infon><infon key="name_0">surname:Bruggemann;given-names:J</infon><infon key="name_1">surname:Lander;given-names:GC</infon><infon key="name_2">surname:Su;given-names:AI</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>25077</offset></passage><passage><infon key="fpage">450</infon><infon key="lpage">451</infon><infon key="name_0">surname:Galton;given-names:F</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">75</infon><infon key="year">1907</infon><offset>25078</offset></passage><passage><infon key="fpage">64</infon><infon key="lpage">67</infon><infon key="name_0">surname:Ipeirotis;given-names:PG</infon><infon key="name_1">surname:Provost;given-names:F</infon><infon key="name_2">surname:Wang;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM SIGKDD Workshop on Human Computation</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>25079</offset></passage><passage><infon key="fpage">823</infon><infon key="lpage">856</infon><infon key="name_0">surname:Zhai;given-names:S</infon><infon key="name_1">surname:Kong;given-names:J</infon><infon key="name_2">surname:Ren;given-names:X</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Hum. Comput. Stud</infon><infon key="type">ref</infon><infon key="volume">61</infon><infon key="year">2004</infon><offset>25080</offset></passage><passage><infon key="fpage">16</infon><infon key="lpage">21</infon><infon key="name_0">surname:Ipeirotis;given-names:PG</infon><infon key="section_type">REF</infon><infon key="source">XRDS</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2010</infon><offset>25081</offset></passage><passage><infon key="fpage">365</infon><infon key="lpage">372</infon><infon key="name_0">surname:Scharrel;given-names:L</infon><infon key="name_1">surname:Ma;given-names:R</infon><infon key="name_2">surname:Schneider;given-names:R</infon><infon key="name_3">surname:Jülicher;given-names:F</infon><infon key="name_4">surname:Diez;given-names:S</infon><infon key="pub-id_pmid">25028878</infon><infon key="section_type">REF</infon><infon key="source">Biophys. J</infon><infon key="type">ref</infon><infon key="volume">107</infon><infon key="year">2014</infon><offset>25082</offset></passage><passage><infon key="fpage">7860</infon><infon key="name_0">surname:Sadanandan;given-names:SK</infon><infon key="name_1">surname:Ranefall;given-names:P</infon><infon key="name_2">surname:Le Guyader;given-names:S</infon><infon key="name_3">surname:Wählby;given-names:C</infon><infon key="pub-id_pmid">28798336</infon><infon key="section_type">REF</infon><infon key="source">Sci. Rep</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2017</infon><offset>25083</offset></passage><passage><infon key="fpage">283</infon><infon key="lpage">292</infon><infon key="name_0">surname:Xie;given-names:W</infon><infon key="name_1">surname:Noble;given-names:JA</infon><infon key="name_2">surname:Zisserman;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Comput. Methods Biomech. Biomed. Eng. Imaging Vis</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2016</infon><offset>25084</offset></passage><passage><infon key="fpage">689</infon><infon key="lpage">692</infon><infon key="name_0">surname:Vedaldi;given-names:A</infon><infon key="name_1">surname:Lenc;given-names:K</infon><infon key="section_type">REF</infon><infon key="source">Proc. 23rd ACM International Conference on Multimedia</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>25085</offset></passage><passage><infon key="fpage">85</infon><infon key="lpage">88</infon><infon key="name_0">surname:Talpalar;given-names:AE</infon><infon key="pub-id_pmid">23812590</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">500</infon><infon key="year">2013</infon><offset>25086</offset></passage><passage><infon key="fpage">431</infon><infon key="lpage">441</infon><infon key="name_0">surname:Marquardt;given-names:DW</infon><infon key="section_type">REF</infon><infon key="source">J. Soc. Ind. Appl. Math</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1963</infon><offset>25087</offset></passage><passage><infon key="fpage">80</infon><infon key="lpage">90</infon><infon key="name_0">surname:Tinevez;given-names:JY</infon><infon key="pub-id_pmid">27713081</infon><infon key="section_type">REF</infon><infon key="source">Methods</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">2017</infon><offset>25088</offset></passage><passage><infon key="fpage">53</infon><infon key="name_0">surname:Adcock;given-names:RJ</infon><infon key="section_type">REF</infon><infon key="source">Anal. (Lond.)</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">1878</infon><offset>25089</offset></passage><passage><infon key="fpage">3</infon><infon key="lpage">16</infon><infon key="name_0">surname:Arteta;given-names:C</infon><infon key="name_1">surname:Lempitsky;given-names:V</infon><infon key="name_2">surname:Noble;given-names:JA</infon><infon key="name_3">surname:Zisserman;given-names:A</infon><infon key="pub-id_pmid">25980675</infon><infon key="section_type">REF</infon><infon key="source">Med. Image Anal</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2016</infon><offset>25090</offset></passage><passage><infon key="fpage">436</infon><infon key="lpage">444</infon><infon key="name_0">surname:LeCun;given-names:Y</infon><infon key="name_1">surname:Bengio;given-names:Y</infon><infon key="name_2">surname:Hinton;given-names:G</infon><infon key="pub-id_pmid">26017442</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">521</infon><infon key="year">2015</infon><offset>25091</offset></passage><passage><infon key="fpage">1469</infon><infon key="lpage">1472</infon><infon key="name_0">surname:Vedaldi;given-names:A</infon><infon key="name_1">surname:Fulkerson;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Proc. 18th ACM International Conference on Multimedia</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>25092</offset></passage><passage><infon key="fpage">249</infon><infon key="lpage">256</infon><infon key="name_0">surname:Glorot;given-names:X</infon><infon key="name_1">surname:Bengio;given-names:Y</infon><infon key="name_2">surname:Teh;given-names:YW</infon><infon key="name_3">surname:Titterington;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Proc. 13th International Conference on Artificial Intelligence and Statistics</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>25093</offset></passage><passage><infon key="file">nihms-1773628-f0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>25094</offset><text>Leveraging the wisdom of crowds for scientific image analysis with Quanti.us.</text></passage><passage><infon key="file">nihms-1773628-f0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25172</offset><text>a, Scientists designate a tool that human Turkers then use to annotate uploaded images according to a set of brief instructions. The resulting annotations can be interpreted in raw form and used as input to conventional algorithms, or be used as training data for machine learning algorithms. b, Left, raw example image of cell nuclei (true positives) and autofluorescent pores (true negatives). Right, corresponding overlay of expert, Turker, and clustered Turker crosshair annotations. False positive and false negative annotations were scored against those provided by a trained expert for individual Turkers, or for spatially clustered annotations from all Turkers (Methods). Each of 300 images was annotated by ten Turkers (a subset of 20 images was used to determine Turker performance). The scale bar applies to the higher-magnification (bottom) images, which represent the regions outlined by dashed squares in the corresponding images above; high magnification is 3× that in the lower-magnification image. c, Precision and recall metrics for individual Turkers (n = 46), for the clustered annotations from ten Turkers completing each image (“Turker collective”), for other experts not involved in ground truth annotation, and for a conventional FIJI object-detection pipeline over a range of particle-size thresholds. An inherent Turker quality score is shown. The gray dashed box indicates the portion of the graph highlighted in the inset to the right. Inset: the arrow indicates the effect of filtering out the bottom one-third of workers, assessed in terms of their performance, on the basis of this score. d, Annotations from every combination of a representative set of one to six ‘good’ Turkers and one ‘bad’ Turker who completed the same five image tasks were clustered and used to determine the indicated performance metrics. e, False positive errors contributed over the first k annotations submitted by a Turker (in chronological order), fit by a quadratic function (n = 29 Turkers). f, Spatial error of annotations versus the time between annotations, with Fitts’s law tradeoff (n = 129 Turkers). Fit envelopes are 95% confidence intervals. Data are representative of two experimental replicates.</text></passage><passage><infon key="file">nihms-1773628-f0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>27405</offset><text>Case studies and machine learning integration of Quanti.us.</text></passage><passage><infon key="file">nihms-1773628-f0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27465</offset><text>a, Left, raw example image (top) and corresponding overlay of Turker annotations and clustered annotations (bottom) of fluorescent microtubules in a gliding assay, annotated with a polyline tool. Each of 50 images was annotated by ten Turkers. Right, FIESTA output (top). Plots (bottom) show microtubule speed and length distributions. The scale bar applies to the higher-magnification images, which represent the regions outlined by dashed squares in the corresponding images; high magnification is 2.75× that in the larger, lower-magnification images. b, Top, raw image frames of a 3D z-stack spanning an organoid. Middle, raw Turker outlines of nuclei, Turker consensus outlines, expert outlines, and MINS algorithm outlines associated with one frame of the stack (outlined by a dashed rectangle). Ten Turkers annotated 30 frames. The plot in the lower right shows performance metrics (prec., precision; rec., recall) for MINS for 18 runs spanning a range of parameter settings (Methods), and for the Turker collective, relative to results from an expert. c, Left and top, raw example images and corresponding overlays of Turker annotations and clustered annotations of the nose, digits, and tail of a walking mouse (images adapted with permission from ref., Springer Nature). Each of 29 images was annotated by 20 Turkers. We input expert or spatially clustered Turker annotations into FIJI’s TrackMate to construct gait plots (bottom) and also compared them to results of a conventional segmentation (seg.) pipeline in FIJI. “Hind” and “fore” refer to limbs. d, Left, raw example images (top) and corresponding overlays (bottom) of Turker annotations and clustered annotations for 2 of 48 frames from a movie of mammary epithelial cell spreading (ten Turkers per frame). Right, F-score plotted for five experts; the Turker collective; automated Otsu segmentation; and convolutional neural networks trained on annotations from five randomly chosen Turkers, clustered Turker annotations, or expert annotations. Data are shown as mean ± s.d. and are representative of at least two experimental replicates.</text></passage></document></collection>
