<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210104</date><key>pmc.key</key><document><id>6223327</id><infon key="license">author_manuscript</infon><passage><infon key="article-id_doi">10.1007/s10115-017-1053-1</infon><infon key="article-id_manuscript">NIHMS968555</infon><infon key="article-id_pmc">6223327</infon><infon key="article-id_pmid">30416242</infon><infon key="fpage">749</infon><infon key="issue">3</infon><infon key="kwd">Crowd labelling Generative model Bayesian Latent Dirichlet allocation EEG</infon><infon key="license">
          This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
        </infon><infon key="lpage">765</infon><infon key="name_0">surname:Pion-Tonachini;given-names:Luca</infon><infon key="name_1">surname:Makeig;given-names:Scott</infon><infon key="name_2">surname:Kreutz-Delgado;given-names:Ken</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">53</infon><infon key="year">2018</infon><offset>0</offset><text>Crowd labeling latent Dirichlet allocation</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>43</offset><text>Large, unlabeled datasets are abundant nowadays, but getting labels for those datasets can be expensive and time-consuming. Crowd labeling is a crowdsourcing approach for gathering such labels from workers whose suggestions are not always accurate. While a variety of algorithms exist for this purpose, we present crowd labeling latent Dirichlet allocation (CL-LDA), a generalization of latent Dirichlet allocation that can solve a more general set of crowd labeling problems. We show that it performs as well as other methods and at times better on a variety of simulated and actual datasets while treating each label as compositional rather than indicating a discrete class. In addition, prior knowledge of workers‚Äô abilities can be incorporated into the model through a structured Bayesian framework. We then apply CL-LDA to the EEG independent component labeling dataset, using its generalizations to further explore the utility of the algorithm. We discuss prospects for creating classifiers from the generated labels.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1069</offset><text>1 Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1084</offset><text>Crowd labeling (CL), also referred to as crowd consensus, is a form of crowdsourcing with the purpose of labeling or categorizing the elements of a provided set of items. Examples of such problems include identifying types of food from pictures and rating the emotion most representative of a sentence. Generating a label for a single example is typically easy and takes anywhere from a second to a minute depending on the task. However, with the large, unlabeled datasets that are so common nowadays, the number of labels needed is often far larger than any person has the time or inclination to produce. In such cases, crowdsourcing can be an effective solution as it greatly reduces the time required through sharing and parallelization of labor among volunteers or paid workers. Unfortunately, skill levels within the pool of workers are typically unknown beforehand. When monetary incentives are provided on a per-task basis, there may even be malicious workers who assign labels at random. This results in a collection of labels that are not entirely reliable, meaning that some labels would likely not match the opinion of a domain expert. CL algorithms exist specifically to estimate a label for each question, object, or feature (henceforth called an instance) that is more reliable than the worker inputs from which that label is generated.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2435</offset><text>The simplest crowdlabeling strategy, selecting the instance category by majority vote, assigns the most commonly submitted label for each instance. In many situations, this method is good enough. Given sufficient votes, low error rates, and lack of consistent bias among workers, the law of large numbers guarantees the average will be reliable. More complex algorithms learn a set of parameters to describe the skill or biases of each worker. In the simplest case, the problem can be recast as learning a weighted average over worker submissions. From this perspective, the majority vote can be described as an averaging method assigning equal weight to all workers. Even more complex models can also learn parameters to describe the difficulty of each instance to account for disagreement between otherwise reliable workers. These algorithms can then be applied to binary classification, multiclass classification, multiple-choice classification (where classes vary for each question or task), or even to more free from paradigms in which workers are allowed to respond with unique, self-generated responses. Though many such algorithms already exist, they largely share the assumptions that each instance pertains to a single class, that responses relate to a single class, and that workers provide at most one response per instance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3772</offset><text>Instance classes are viewed as compositional rather than categorical.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3842</offset><text>Workers may respond with any number of guessed possibilities if they cannot distinguish an obvious correct answer.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3957</offset><text>Responses that do not directly correspond to a class, and may have a different assumed meaning for each worker, are allowed as response options.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4102</offset><text>Prior information on workers can be incorporated in a structured and Bayesian manner.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4188</offset><text>After first formally posing the problem of CL, we introduce crowd labeling latent Dirichlet allocation (CL-LDA), an algorithm that generalizes the family of multiclass classification CL algorithms in four important ways: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4410</offset><text>While latent Dirichlet allocation (LDA) is a well-known algorithm that has been applied to a wide variety of problems, including something approaching crowd labeling, as far as we are aware it has not been generalized in a way that renders it applicable to general CL problems. We provide a generalization that allows CL-LDA to be used in all the above applications of CL excluding multiple-choice classification. For simplicity, we restrain our analysis to binary and multiclass datasets. We then provide a comparison to prior CL methods. The notation used in the paper is presented in Table 1. Vector variables are distinguished by boldface and matrix variables by underlined boldface. An additional subscript on vector variables indicates indexing over the scalar elements of the vector.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5201</offset><text>2 Problem description</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5223</offset><text>The CL problem begins with the assumption that for each instance in a dataset, there is a true class label. CL algorithms then attempt to estimate the class label for each instance in a dataset. Estimates are made using the responses (henceforth referred to as votes) to those instances provided by a set of workers. A CL algorithm accomplishes this by comparing the votes provided by all workers, considering which workers agree or disagree with other workers and on which instances, thereby determining which label is most probable for each instance. More rigorously, CL requires a set of workers ùí∞ indexed u ‚àà {1, ‚Ä¶, U} who consider a set of instances ùíü indexed d ‚àà {1, ‚Ä¶, D} producing a set of votes ùí± = {œÖdi ‚àà {1, ‚Ä¶, R}| d ‚àà ùíü, i ‚àà {1, ‚Ä¶, Nd }} where U is the number of workers, D is the number of instances, R is the number of possible responses, and Nd is the number of votes on instance d as can be seen in Table 1. For each instance there assumed to exist an unknown, true class vector yd ‚àà ùí¥ that relates the instance to C possible distinct classes. The goal of CL, provided this information, is to generate an estimate Œ∏‚Éód ‚àà Œò as close to y‚Éód as possible for each instance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6452</offset><text>In previous methods, y‚Éód was assumed to be a discrete value indicating the true class. To accommodate generalization 1 for compositional data, it is assumed that y‚Éód ‚àà ùïäC‚àí1 where ùïän is the n-dimensional probability simplex in ‚Ñùn+1. This means that  ; i.e., the elements of y‚Éód sum to one. The assumption of previous methods that y‚Éód is discrete is a special case solution under this generalization, wherein y‚Éód is a binary indicator vector with only the element related to the true class being equal to one and all others zero.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>7002</offset><text>3 Latent Dirichlet allocation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7032</offset><text>Latent Dirichlet allocation (LDA) is a generative hierarchical Bayesian mixture model for unsupervised data clustering based on unobserved similarities or themes common throughout a dataset. It is most well known for topic modeling in documents, but has many other applications such as recommendation systems, object detection in images, and image annotation. In this section we provide a brief review of LDA for context before presenting the generalization to CL-LDA. While the model described here is referred to in the original paper as smoothed-LDA, the smoothed-LDA model is also commonly called LDA as we do here. For simplicity, we describe LDA from the perspective of document topic modeling to maintain a consistent analogy between the intuitions behind LDA and CL-LDA.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7811</offset><text>LDA (Fig. 1) applied to document topic modeling learns a probabilistic generative model for a corpus, ùíü, comprised of D documents. Each document, d, contains Nd words. The probabilistic generation of these documents and words begins with a Dirichlet prior over topics in the corpus with parameter vector Œ±‚Éó, from which each document‚Äôs topic distribution, Œ∏‚Éód, is drawn. For each document, Nd samples are taken from a multinomial distribution with parameter vector Œ∏‚Éód. These samples are word-topics within the document, denoted as zdi, which make explicit the topic of the context in which a word is used. For example, the word ‚Äúrash‚Äù could be a symptom in the context of medical literature, but might also describe a decision in the context of a political commentary. A second Dirichlet prior has parameter vector Œ≤‚Éó over word distributions given topics from which each topic-dependent word distribution, œï‚Éók, is sampled. For each word-topic zdi, a word is drawn from a multinomial distribution with parameter vector œï‚Éózdi. Effectively, œï‚Éók parameterizes the vocabulary used by topic k. In summary:    </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8945</offset><text>The only information provided to the model are the words, ùí±, in the corpus and the priors, Œ±‚Éó and Œ≤‚Éó. In the original derivation Nd is treated as a random variable drawn from a Poisson distribution, but as it is independent from the other data generating variables, Œ∏‚Éód and zdi, it may be treated as deterministic.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>9272</offset><text>4 Crowd labeling latent Dirichlet allocation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9317</offset><text>Though LDA may not immediately appear applicable to the problem of CL, it can be shown to be analogous given two generalizations (Fig. 2). Where before the data were a corpus of documents containing words, for CL we analyze a set of instances on which workers have voted. There is a clear relation between documents in a corpus and instances from CL in which the words in documents are analogous to votes on those instances. The missing component in the topic modeling paradigm is some complement to the relation between workers and their votes. Such a relationship can be easily added by generalizing the topic-dependent word distribution in classical LDA, œï‚Éók, to a class and worker-dependent vote distribution,  . Likewise, the prior over vote distribution, Œ≤‚Éó, is generalized on a per-worker and per-class basis as  . This generalized prior can be thought of as a per worker prior on confusion matrices. While the possibility exists for a unique prior assigned to each worker, a more apt model would use the worker-dependent priors to describe known populations of workers. Worker-dependent priors need not be different, as the worker and class-dependent vote distributions will still be learned from the data. Therefore, that flexibility should only be employed when prior information supports its use.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10631</offset><text>As a third generalization, vote-classes are given weights, mdu, such that each voting worker has equal weight on an instance, independent of how many votes they submit on that instance. This is an adaptation of term weighting schemes using a different formula for the value of weights and serves to allow multiple responses within a single vote, CL generalization 2 from Sect. 1, without biasing the result in favor of users who do so more often.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11078</offset><text>4.1 Meaning of vote-classes</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11106</offset><text>When compared to word-topics in LDA, vote-classes in CL-LDA are less intuitive because when a worker submits a vote, that vote often appears obvious in its intention. A distinction can nevertheless be made between equivalent votes, one of which is made explicit through the vote-class latent variables. Suppose there is an instance from a dataset with two possible classes. One worker submits a vote for the first class, and a second worker submits a vote for the second class. Assuming the first worker is estimated as trustworthy, the vote-class matches the vote by that user. If the second worker is estimated as inaccurate, as they often misidentify the first class as being the second, then the vote-class will likely still be for the first class and the instance will be estimated as strongly first class. If, instead, the second worker is estimated to be accurate, the second vote-class will follow the vote as being of the second class and the instance will be estimated to be evenly split. In the case that the second worker votes both classes, each vote has a unique vote-class and so can vary the interpretation of this instance from either fully class one due to a misunderstanding of class two on the workers behalf, a mixture of both classes as the worker correctly identified similarities to both, or fully class two due to a misunderstanding of class one. Vote-classes estimate the best interpretation of votes, as they might differ from the obvious intention due to a misunderstanding of response options or misinterpretation of instances by the worker.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12677</offset><text>4.2 Inference on CL-LDA</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12701</offset><text>We use collapsed Gibbs sampling to perform inference on the CL-LDA model. For reference, the word-topic probabilities in LDA are calculated as: and the marginalized distribution parameters reconstructed as: where ùíµ‚àídi are all the word-topics excluding zdi and njkl is the number of times word l appears in document j with topic k. A * indicates a summation over the index it occupies; e.g., njk* is the number of words in document j that have topic k.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13158</offset><text>Accounting for workers and weighting votes to equalize the influence of each worker, the instance-class probabilities in CL-LDA are calculated as: and the marginalized distribution parameters reconstructed as: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13369</offset><text>Everything is the same as before except that z, w, and n are additionally indexed by workers and Œ≤‚Éó is additionally indexed by workers and classes. Although not evident from the equation, the counts are also changed to summations of weights, m, such that   rather than simply subtracting one as before. The result is that computational complexity remains unchanged from LDA.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13747</offset><text>4.3 Effect of priors in CL-LDA</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13778</offset><text>Just as in the original derivation of LDA, priors can be interpreted as pseudo-votes that act to smooth the solution toward prior beliefs. Selection of adequate parameters for the prior distributions is essential for CL-LDA to correctly infer instance classes. This is especially true for each  . If   is set to be uniform, then the class-dependent distributions that CL-LDA finds will be associated with an unknown class and the resulting solution would have to be analyzed to determine the meaning of each ‚Äúclass,‚Äù thereby negating the utility and autonomy of the algorithm. By choosing   such that each possible vote is favored by the most associated class, the results are guided to a known distribution of classes. If it is assumed a priori that the workers are highly skilled, and given that each vote has a one-to-one correspondence to a single, unique class, then Œ≤Ã≤u is a scaled identity matrix with scaling equivalent to the strength of the assumption of worker competence. Workers are usually not perfect, so it instead makes sense to set Œ≤Ã≤u to a positive, linear combination of an identity matrix and a matrix of uniform values. If a possible vote does not have a one-to-one correspondence to a class, then that value can be set however best fits prior assumptions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15065</offset><text>The class distribution prior does not require such premeditated structure but is also important. This distribution is entirely analogous to that of LDA and should incorporate any prior knowledge on the distribution of instance classes within the dataset. The smoothing effect of the class distribution prior leads to a concern when choosing the scaling for Œ±‚Éó as there are typically far fewer responses per instance in CL than there are words in a document. Care should therefore be taken that Œ±* is significantly less than the typical number of worker responses on an instance of the dataset or else the smoothing effect of the Œ± will overpower most votes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15734</offset><text>4.4 Bayesian prior estimation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15764</offset><text>Imposing a prior can benefit inference by favoring estimates that are closer to solutions that are believed to be more likely, especially when there are minimal data available. Conversely, if there is a large discrepancy between the belief guiding an imposed prior and the true distribution, that prior can be equally detrimental when there are not enough data to overcome its influence. Bayesian prior estimation (BPE) optimizes the prior distribution so as to maximize the data evidence. Following the analysis by Wallach et. al. of estimating non-symmetric Dirichlet priors in LDA, an extension is possible to CL-LDA with Bayesian prior estimation (CL-LDA-BPE). While Wallach suggests the use of her derived estimator, this is not possible with CL-LDA as Wallach‚Äôs derivation assumes that counts are integer valued, which is not applicable here as a result of vote weighting. Therefore, CL-LDA-BPE uses Minkas fixed-point iteration. It is adapted to CL-LDA-BPE as: and where Œ±ÃÇk is the updated kth element of Œ±‚Éó,   is the updated lth element of  , and Œ®(¬∑) is the Digamma function. BPE is a single example of the extensive literature of LDA modifications that can possibly be adapted to CL-LDA. Other advantageous modifications include parallelizing inference across processors, parallelizing inference on a graphical processing unit, and adaptation for online inference when receiving streaming data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>17178</offset><text>4.5 Similar prior methods</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17204</offset><text>CL-LDA can be interpreted as part of a larger family of CL algorithms based upon the confusion matrix approach of Dawin and Skene (DS). DS models each worker using a confusion matrix across classes and votes with the assumption that each instance is of a particular discrete class. This approach can be, and has been, extended many times. Many extensions make the model Bayesian by incorporating hierarchical prior distributions over the classes, workers, or both. For example, the independent Bayesian classifier combination and its variations extend DS by imposing a Dirichlet prior over the class distribution, Dirichlet priors over the rows of the confusion matrices, and exponential priors on the Dirichlet parameters. Latent confusion analysis (LCA) also extends this method using confusions matrices by imposing normalized gamma priors on the confusion matrices as well as assuming that voting patterns are structured and shared throughout populations of workers. LCA can also be framed from the perspective of LDA but is changed substantially to incorporate shared voting patterns and latent variables for instance difficulty. As previously described, preserving more similarities to LDA, as CL-LDA does, provides many additional benefits, such as access to a rich literature and the use of efficient collapsed Gibbs sampling for inference.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>18553</offset><text>5 Experimental evaluation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18579</offset><text>To quantify the performance of CL-LDA when applied to CL problems, we use SQUARE: a toolbox that applies CL algorithms to publicly available datasets as well as simulated datasets under various conditions ranging from unsupervised to fully supervised. As CL-LDA is an unsupervised method, only the unsupervised methods of SQUARE are utilized.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>18922</offset><text>5.1 Datasets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18935</offset><text>To compare CL-LDA against other CL methods, we are required to use data that are compatible with those other methods. As a result, datasets that require CL-LDA‚Äôs added capabilities cannot be used in these comparisons. An exception is made for multiclass datasets, in which case algorithms that only accept binary classes are excluded. The data in this experiment are therefore in the form of workers voting for a single discrete label per instance.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>19386</offset><text>5.1.1 Found data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19403</offset><text>All non-simulated CL datasets used in these tests are publicly available on the internet. Each varies in response types, overall number of responses, and distributions of responses across workers and instances. Details of these datasets are shown in Table 2. AC2 consists of collected worker ratings on Web sites ranging from child-friendly to pornographic on a four-point scale. In BM, workers rate the sentiment of tweets as positive or negative. CSv3B is a series of judgments on whether statements are true or false and is a binarized version of CSv3 in which there was a very rare ‚Äúskip‚Äù response accounting for less than 0.15% of all responses. In HC, workers rate search results as either not-, somewhat-, or very relevant, while HCB combines the somewhat-relevant and very relevant ratings into a single response. WVSCM has workers discriminate images of genuine smiles from images of forced smiles. All datasets are either binary or multiclass except for AC2 which is ordinal.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>20393</offset><text>5.1.2 Simulated data</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20414</offset><text>To compare the resilience of CL-LDA to spammers as compared to other methods, we generate simulated data. The simulated datasets are again made to conform with the requirements of the other algorithms. Every simulated dataset consists of 16,000 instances which pertain to one of seven classes. The prior probabilities of the ith class are i/28. To explore the effects of low-accuracy workers, we model those workers as random spammers who vote uniformly at random over all classes. Smart spammers are modeled as trying to avoid detection while minimizing effort by always voting with the class having the highest prior probability. Such workers might appear when monetary rewards are offered for every instance completed. Here, smart spammers always vote for the seventh class. Each worker votes on 300 separate instances at random within the dataset. Non-spamming workers are modeled purely as an accuracy so as to not explicitly favor any family of methods over any other. Each dataset contains 16 workers with 98% accuracy and 160 with 70% accuracy. For each type of spammer, five additional datasets are created adding 32‚Äì160 spammers in increments of 32. For each condition, ten datasets are generated with different random seeds to provide a measure the stability on each solution.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>21704</offset><text>5.2 Other CL algorithms</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21728</offset><text>Sheshadri et. al. provide a suitable review of each method compared, which is summarized here. The majority vote (MV) takes the most commonly voted class for instance as the correct answer without any regard for which workers produced those votes. ZenCrowd (ZC) generalizes MV by adding a scalar parameter for each worker‚Äôs ability which can be either positive for helpful workers or negative for adversarial. Dawin and Skene (DS) estimates a confusion matrix for each worker to provide a more detailed model of worker ability. Naive Bayes (NB) also employs a confusion matrix, but with Laplace smoothing. The Generative model of Labels, Abilities, and Difficulties (GLAD) models workers with a scalar parameter and additionally models each instance with a parameter estimating its difficulty. The algorithm for GLAD used in SQUARE is only compatible with datasets containing binary-valued votes. Caltech UCSD Binary Annotation Model (CUBAM) estimates workers with parameters for skill and bias while also estimating instance difficulty. The implementation of CUBAM used here is also only applicable to binary data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22846</offset><text>5.3 Implementation details</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22873</offset><text>For all the experiments in this paper, CL-LDA uses four Gibbs sampling chains with a burn-in of 200 samples and then takes the average over vote-classes associated with each instance for the next 300 samples of each chain combined. CL-LDA-BPE has a longer burn-in of 4000 samples to allow the BPE to converge and then averages the next 1000 samples. Equations 9 and 10 are applied until convergence after every 20 complete Gibbs sampling iterations during the burn-in period, beginning after the first 100 samples.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23388</offset><text>For both methods, Œ≤Ã≤ùí∞ is set to 3 ¬∑ (0.9 ¬∑ IÃ≤W√óW + 0.1 ¬∑ 1Ã≤W√óW/W), which weakly assumes that workers are proficient. Œ±‚Éó is set to a uniform vector with total sum of 0.5 which effectively adds a pseudo-vote with weight of 0.5 to each instance. Each vote-class is initialized to the class with the highest probability to produce the relevant vote given the workers prior, i.e.,  .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23782</offset><text>5.4 Results</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23794</offset><text>Both implementations of CL-LDA generally perform well as other CL methods. Each class of algorithms performs better or worse on any given dataset based on the characteristics specific to that dataset and those variations can be seen in the following results.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24053</offset><text>5.4.1 Found datasets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24074</offset><text>The performance of both versions of CL-LDA on the SQUARE datasets is shown in Tables 3 and 4. All performance metrics are calculated using the mean across class-specific performance. Accuracies among all algorithms are similar on AC2, BM, and CSv3B. Only on HC, HCB, and WVSCM is there a wider range of accuracies, and for two of those three, CL-LDA is the top performer. For precision, both CL-LDA methods display a wider range in performance. When compared to DS, which is heavily cited and uses a very similar model, CL-LDA and CL-LDA-BPE do better on almost all datasets in both accuracy and precision.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24681</offset><text>5.4.2 Simulated datasets</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24706</offset><text>The simulated data experiments indicate that CL-LDA performs better than MV under the influence of many spammers, as shown in Fig. 3. CL-LDA-BPE and DS are also affected much less than MV and also perform better than CL-LDA in this case. ZC strangely appears to perform better with the influence of spammers though suffers from high variability with random spammers. These results should be interpreted with some skepticism as DS and CL-LDA-BPE did not perform in such a superior manner on the found CL datasets as compared to CL-LDA. Still, these experiments provide some insight into each algorithm‚Äôs robustness to poor workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>25339</offset><text>5.4.3 Effects of Bayesian prior estimation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25382</offset><text>CL-LDA-BPE does perform better than CL-LDA in certain cases, but it also does worse in other as demonstrated by some of the non-simulated datasets. In effect, BPE becomes beneficial when the provided priors greatly misalign with the true class distributions. This claim is supported by the increased efficacy of CL-LDA-BPE when applied to the simulated data as the initial priors impose the beliefs that all classes are equally likely and that all workers are somewhat competent. Once an initial estimate has been formed, BPE can relearn priors to support the data. It follows that in cases when some idea of the data distribution and worker capability is already known, CL-LDA is sufficient. Applying BPE to only the class distribution prior or only to the workers, even to just a subset of workers, is not only possible, but very easy as well. A disadvantage to CL-LDA-BPE is that the BPE requires significantly more Gibbs sampling iterations, and therefore more time, to converge than CL-LDA.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>26378</offset><text>6 Application to the independent component labeling dataset</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26438</offset><text>We demonstrate our methodology on the challenging problem of labeling unmixed, independent components (ICs) of multidimensional electroencephalography (EEG) data, making full use of all four generalizations listed in Section 1. We apply CL-LDA to crowdsourced label suggestions to provide training labels for a subset of the independent component labeling (ICL) dataset, a collection of millions of EEG ICs which currently has no such labels. Generating labels for a subset of ICL allows the use of semi-supervised learning algorithms on the entire dataset, enabling the creation of an automated EEG IC classifier to aid neuroscientists in analyzing large collections of datasets, to help and teach those who do not know how to distinguish ICs manually, and for applications which require automation such as certain real-time brain‚Äìcomputer interfaces.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27293</offset><text>ICs are separated into seven classes based on the estimated source of the EEG component signal. The classes are ‚Äúbrain‚Äù for signals originating from a subject‚Äôs cerebral cortex, ‚Äúmuscle‚Äù for signals generated by muscle activity, ‚Äúeye‚Äù for electrical potentials produced by the retina, ‚Äúheart‚Äù for components that account for the electrical activity from the heart, ‚Äúline noise‚Äù for components following external electrical fields produced by nearby power fixtures or electronics, ‚Äúchannel noise‚Äù for artifacts resulting from poor electrode quality or loose electrode contacts, and ‚Äúother‚Äù as an amalgamation of additional rare classes and poorly unmixed or otherwise uninterpretable signals which provide little or no usable information to neuroscientists. Information regarding ICL can be seen in Table 5.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28130</offset><text>The task of labeling ICs is difficult enough that when asking experts to evaluate the same components, it is not uncommon for them to disagree on the correct labels for a significant number of those components. Because of this difficulty and the occasionally imperfect unmixing that may result from the algorithms used, compositional labels on ICs provide a more informative model when ascribing meaning or origin to an IC. By describing ICs as a composition, as CL-LDA allows with generalization 1, labels can express similarity to multiple classes while maintaining the capacity to define an IC as primarily from a single class. Pursuant to the compositional label model, votes are also not limited to a single class. Instead, using generalization 2, workers may select any number of classes they find to be applicable to a given IC. In cases when a worker feels significant doubt in his or her assessment of an IC, the worker can indicate that uncertainty through an additional ‚Äú?‚Äù response which may be used in addition to uncertain guesses or alone as a way to abstain from voting, as provided for by generalization 3. Finally, there is a subset of workers whom we, a priori, deem to be experts. This information is incorporated into the CL model to counteract any biases in the general population of workers. As ‚Äúexpert‚Äù does not mean infallible in this context, their individual votes cannot be treated as ground truth. Information regarding expert skill is instead incorporated into the model using generalization 4, i.e., worker matrix priors  .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>29693</offset><text>6.1 Implementation details</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29720</offset><text>When applying CL-LDA to ICL, the matrix prior for experts and non-experts is set to 30 ¬∑ [IÃ±7√ó7 17√ó1/7] and 30 ¬∑ [(0.4 ¬∑ IÃ≤W√óW + 0.6 ¬∑ 1W√óW/W) 17√ó1/2], respectively. These priors effectively add 30 pseudo-votes to each possible vote value for a total of 240 pseudovotes per worker which strongly assumes a skill gap between experts and non-experts, but not so strong that those assumptions cannot be overcome by workers who cast many votes. While such a strong prior may appear excessive, it is helpful in overcoming many common biases among non-expert workers for this task. The class prior vector is set to [0.15 0.1 0.05 0.025 0025. 0.025 0.15] for ICs of type ‚Äúbrain,‚Äù ‚Äúmuscle,‚Äù ‚Äúeye,‚Äù ‚Äúheart,‚Äù ‚Äúline noise,‚Äù ‚Äúchannel noise,‚Äù and ‚Äúother,‚Äù respectively. As in Sect. 5.3, CL-LDA is run with a 200 sample burning and an average is taken over the following 300 samples.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>30629</offset><text>6.2 Results</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30641</offset><text>Without ground-truth labels, evaluation is difficult on ICL. The occasional ambiguity between EEG component class types adds to the challenge as well. To overcome these difficulties, we inspect ‚Äúheart‚Äù components as they are very clearly defined while still necessitating a level of skill to identify. ‚ÄúHeart‚Äù components are rare, allowing for a full manual analysis of all components with any ‚Äúheart‚Äù votes. For this evaluation, classes are separated by taking the maximum class contribution to each compositional label, solely to simplify manual analysis. The manual classification of ‚Äúheart‚Äù components was done with the help of a cardiologist. Figure 4 shows all components that CL-LDA estimates as primarily ‚Äúheart‚Äù component. Of the 23 instances selected, 16 are actually ‚Äúheart,‚Äù while the other five are not. Of all 91 ICs that have any ‚Äúheart‚Äù votes, 21 are actually ‚Äúheart.‚Äù Combining this information provides an accuracy of 89%. For comparison, MV only accurately labels 12 heart components and achieves an overall accuracy of 82%. More importantly, the results in Fig. 4 are almost linearly separable. As the proportion ‚Äúheart‚Äù component (PHC) decreases and the variance of PHC increases, the less likely a component is to be an actual ‚Äúheart‚Äù component.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>31949</offset><text>6.3 Label variance</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31968</offset><text>Label variances can potentially provide an important benefit to the broader goal of the ICL dataset and generally to other CL datasets as well. CL-LDA separates the estimate into a composition over classes and generates variance measures on fractions of the composition as an analog to confidence. By having an explicit measure of the estimate credibility, heavy voting overlap (which implicitly ensures a level of label confidence) is no longer a requirement and therefore should allow workers to label more unique components, thereby increasing the expected number of low-probability components in the labeled dataset. To fully utilize this approach in CL, the classifier has to incorporate confidence values during training by assuming label heteroscedasticity. An exemplary method that makes this assumption is generalized least squares (GLS) which scales errors according to the noise covariance using Mahalanobis distance. The effect is to diminish the penalty of misclassifying an instance along dimensions with high label variance. Therefore, if a class estimate is the product of a single unreliable worker, the resulting penalty for misclassification will be minuscule and the classifier will not be heavily skewed as a result of the inaccurate label. In fact, CL-LDA can just as easily provide an estimate of the full label covariance which matches the GLS example more closely. This benefit is clearly applicable to the ICL results shown in Fig. 4 as all but one of the incorrectly labeled heart components have high label variance in addition to lower PHC. Figure 4 is indicative of how label variances provides an additional layer of information that can aid the generation of the EEG IC classifier and other CL-dependent applications and comprehensive validation of the claim will be presented in future work.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>33793</offset><text>7 Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>33806</offset><text>In this paper we have presented a new CL algorithm, CL-LDA: a generalization of the well-known latent Dirichlet allocation. Using SQUARE, we show that CL-LDA performs comparable to or better than many other CL algorithms, depending upon the dataset, while allowing for four useful generalizations to the CL problem. These generalizations allow CL-LDA to be used with datasets that require or would benefit from compositional labels, multiresponse votes, class-agnostic responses, and structured Bayesian incorporation of prior knowledge regarding worker abilities. Furthermore, CL-LDA provides variance estimates on each class proportion assigned to an instance as a measure of confidence. We discuss the convenience of using a method based upon LDA as it provides access to an extensive literature with which to easily extend CL-LDA, a fact which we exploit by incorporating Bayesian prior learning of all priors in CL-LDA-BPE. We show CL-LDA-BPE to be better in cases when true class distributions and worker abilities vary strongly from uninformed guesses.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>34866</offset><text>We then apply CL-LDA to the ICL dataset which uses all four stated generalizations. EEG components that capture heart signals demonstrate the utility of variance on class proportions to separate poor class labels estimates from those that are more likely to be true. These class label variances can be incorporated into the error function during classifier training making the classifier robust to unreliable labels rather than discarding or suffering from those labels. In future work, such a classifier can be used to further validate the efficacy of CL-LDA by comparing the performance of a classifier trained on labels generating according to the majority vote against that of a classifier trained with labels from CL-LDA.</text></passage><passage><infon key="fpage">91</infon><infon key="lpage">100</infon><infon key="name_0">surname:Agarwal;given-names:D</infon><infon key="name_1">surname:Chen;given-names:B-C</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the third ACM international conference on web search and data mining</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>35593</offset><text>fLDA: matrix factorization through latent Dirichlet allocation</text></passage><passage><infon key="fpage">993</infon><infon key="lpage">1022</infon><infon key="name_0">surname:Blei;given-names:DM</infon><infon key="name_1">surname:Ng;given-names:AY</infon><infon key="name_2">surname:Jordan;given-names:MI</infon><infon key="section_type">REF</infon><infon key="source">J Mach Learn Res</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2003</infon><offset>35656</offset><text>Latent Dirichlet allocation</text></passage><passage><infon key="name_0">surname:Buckley;given-names:C</infon><infon key="name_1">surname:Lease;given-names:M</infon><infon key="name_2">surname:Smucker;given-names:MD</infon><infon key="section_type">REF</infon><infon key="source">The nineteenth text retrieval conference (TREC) notebook</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>35684</offset><text>Overview of the TREC 2010 Relevance Feedback Track (Notebook)</text></passage><passage><infon key="fpage">65</infon><infon key="lpage">72</infon><infon key="name_0">surname:Canini;given-names:KR</infon><infon key="name_1">surname:Shi;given-names:L</infon><infon key="name_2">surname:Griffiths;given-names:TL</infon><infon key="section_type">REF</infon><infon key="source">International conference on artificial intelligence and statistics (AISTATS)</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2009</infon><offset>35746</offset><text>Online inference of topics with latent Dirichlet allocation</text></passage><passage><infon key="fpage">20</infon><infon key="lpage">28</infon><infon key="name_0">surname:Dawid;given-names:AP</infon><infon key="name_1">surname:Skene;given-names:AM</infon><infon key="section_type">REF</infon><infon key="source">Appl Stat</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">1979</infon><offset>35806</offset><text>Maximum likelihood estimation of observer error-rates using the EM algorithm</text></passage><passage><infon key="name_0">surname:Della Penna;given-names:N</infon><infon key="name_1">surname:Reid;given-names:MD</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv:1204.3511</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>35883</offset><text>Crowd and prejudice: an impossibility theorem for crowd labelling without a gold standard</text></passage><passage><infon key="fpage">469</infon><infon key="lpage">478</infon><infon key="name_0">surname:Demartini;given-names:G</infon><infon key="name_1">surname:Difallah;given-names:DE</infon><infon key="name_2">surname:Cudr√©-Mauroux;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 21st international conference on World Wide Web</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>35973</offset><text>Zencrowd: leveraging probabilistic reasoning and crowd sourcing techniques for large-scale entity linking</text></passage><passage><infon key="fpage">5228</infon><infon key="lpage">5235</infon><infon key="name_0">surname:Griffiths;given-names:TL</infon><infon key="name_1">surname:Steyvers;given-names:M</infon><infon key="pub-id_pmid">14872004</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci</infon><infon key="type">ref</infon><infon key="volume">101</infon><infon key="year">2004</infon><offset>36079</offset><text>Finding scientific topics</text></passage><passage><infon key="comment">
            https://research.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html
          </infon><infon key="name_0">surname:Orr;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">50,000 Lessons on How to Read: a Relation Extraction Corpus</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>36105</offset></passage><passage><infon key="fpage">856</infon><infon key="lpage">864</infon><infon key="name_0">surname:Hoffman;given-names:M</infon><infon key="name_1">surname:Bach;given-names:FR</infon><infon key="name_2">surname:Blei;given-names:DM</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>36106</offset><text>Online learning for latent Dirichlet allocation</text></passage><passage><infon key="fpage">64</infon><infon key="lpage">67</infon><infon key="name_0">surname:Ipeirotis;given-names:PG</infon><infon key="name_1">surname:Provost;given-names:F</infon><infon key="name_2">surname:Wang;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM SIGKDD workshop on human computation</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>36154</offset><text>Quality management on Amazon mechanical turk</text></passage><passage><infon key="fpage">619</infon><infon key="lpage">627</infon><infon key="name_0">surname:Kim;given-names:HC</infon><infon key="name_1">surname:Ghahramani;given-names:Z</infon><infon key="section_type">REF</infon><infon key="source">International conference on artificial intelligence and statistics (AISTATS)</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>36199</offset><text>Bayesian classifier combination</text></passage><passage><infon key="fpage">61</infon><infon key="lpage">68</infon><infon key="name_0">surname:Krestel;given-names:R</infon><infon key="name_1">surname:Fankhauser;given-names:P</infon><infon key="name_2">surname:Nejdl;given-names:W</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the third ACM conference on recommender systems</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>36231</offset><text>Latent Dirichlet allocation for tag recommendation</text></passage><passage><infon key="fpage">28</infon><infon key="issue">1</infon><infon key="lpage">32</infon><infon key="name_0">surname:Lienou;given-names:M</infon><infon key="name_1">surname:Ma√Ætre;given-names:H</infon><infon key="name_2">surname:Datcu;given-names:M</infon><infon key="pub-id_doi">10.1109/LGRS.2009.2023536</infon><infon key="section_type">REF</infon><infon key="source">IEEE Geosci Remote Sens Lett</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2010</infon><offset>36282</offset><text>Semantic annotation of satellite images using latent Dirichlet allocation</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">151</infon><infon key="name_0">surname:Makeig;given-names:S</infon><infon key="name_1">surname:Bell;given-names:AJ</infon><infon key="name_2">surname:Jung;given-names:TP</infon><infon key="name_3">surname:Sejnowski;given-names:TJ</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems</infon><infon key="type">ref</infon><infon key="year">1996</infon><offset>36356</offset><text>Independent component analysis of electroencephalographic data</text></passage><passage><infon key="name_0">surname:Minka;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Tech. rep</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>36419</offset><text>Estimating a Dirichlet distribution</text></passage><passage><infon key="name_0">surname:Moreno;given-names:PG</infon><infon key="name_1">surname:Teh;given-names:YW</infon><infon key="name_2">surname:Perez-Cruz;given-names:F</infon><infon key="name_3">surname:Art√©s-Rodr√≠guez;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv:1407.5017</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>36455</offset><text>Bayesian nonparametric crowdsourcing</text></passage><passage><infon key="name_0">surname:Mozafari;given-names:B</infon><infon key="name_1">surname:Sarkar;given-names:P</infon><infon key="name_2">surname:Franklin;given-names:MJ</infon><infon key="name_3">surname:Jordan;given-names:MI</infon><infon key="name_4">surname:Madden;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv:1209.3686</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>36492</offset><text>Active learning for crowd-sourced databases</text></passage><passage><infon key="name_0">surname:Muhammadi;given-names:J</infon><infon key="name_1">surname:Rabiee;given-names:HR</infon><infon key="name_2">surname:Hosseini;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint arXiv:1301.2774</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>36536</offset><text>Crowd labeling: a survey</text></passage><passage><infon key="fpage">1116</infon><infon key="lpage">1124</infon><infon key="name_0">surname:Sato;given-names:I</infon><infon key="name_1">surname:Kashima;given-names:H</infon><infon key="name_2">surname:Nakagawa;given-names:H</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st international conference on machine learning (ICML-14)</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>36561</offset><text>Latent confusion analysis by normalized gamma construction</text></passage><passage><infon key="name_0">surname:Sheshadri;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Master‚Äôs thesis</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>36620</offset><text>A collaborative approach to IR evaluation</text></passage><passage><infon key="name_0">surname:Sheshadri;given-names:A</infon><infon key="name_1">surname:Lease;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">First AAAI conference on human computation and crowdsourcing</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>36662</offset><text>SQUARE: a benchmark for research on computing crowd consensus</text></passage><passage><infon key="fpage">254</infon><infon key="lpage">263</infon><infon key="name_0">surname:Snow;given-names:R</infon><infon key="name_1">surname:O‚ÄôConnor;given-names:B</infon><infon key="name_2">surname:Jurafsky;given-names:D</infon><infon key="name_3">surname:Ng;given-names:AY</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the conference on empirical methods in natural language processing</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>36724</offset><text>Cheap and fast‚Äîbut is it good? Evaluating non-expert annotations for natural language tasks</text></passage><passage><infon key="fpage">36</infon><infon key="lpage">41</infon><infon key="name_0">surname:Tang;given-names:W</infon><infon key="name_1">surname:Lease;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">ACM SIGIR workshop on crowdsourcing for information retrieval (CIR)</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>36818</offset><text>Semi-supervised consensus labeling for crowdsourcing</text></passage><passage><infon key="name_0">surname:Wallach;given-names:HM</infon><infon key="section_type">REF</infon><infon key="source">PhD thesis</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>36871</offset><text>Structured topic models for language</text></passage><passage><infon key="fpage">1973</infon><infon key="lpage">1981</infon><infon key="name_0">surname:Wallach;given-names:HM</infon><infon key="name_1">surname:Mimno;given-names:DM</infon><infon key="name_2">surname:McCallum;given-names:A</infon><infon key="name_3">surname:Bengio;given-names:Y</infon><infon key="name_4">surname:Schuurmans;given-names:D</infon><infon key="name_5">surname:Lafferty;given-names:JD</infon><infon key="name_6">surname:Williams;given-names:CKI</infon><infon key="name_7">surname:Culotta;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems 22</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>36908</offset><text>Rethinking LDA: why priors matter</text></passage><passage><infon key="fpage">1577</infon><infon key="lpage">1584</infon><infon key="name_0">surname:Wang;given-names:X</infon><infon key="name_1">surname:Grimson;given-names:E</infon><infon key="name_2">surname:Platt;given-names:JC</infon><infon key="name_3">surname:Koller;given-names:D</infon><infon key="name_4">surname:Singer;given-names:Y</infon><infon key="name_5">surname:Roweis;given-names:ST</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems 20</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>36942</offset><text>Spatial latent Dirichlet allocation</text></passage><passage><infon key="fpage">301</infon><infon key="lpage">314</infon><infon key="name_0">surname:Wang;given-names:Y</infon><infon key="name_1">surname:Bai;given-names:H</infon><infon key="name_2">surname:Stanton;given-names:M</infon><infon key="name_3">surname:Chen;given-names:WY</infon><infon key="name_4">surname:Chang;given-names:EY</infon><infon key="name_5">surname:Goldberg;given-names:AV</infon><infon key="name_6">surname:Zhou;given-names:Y</infon><infon key="pub-id_doi">10.1007/978-3-642-02158-9_26</infon><infon key="section_type">REF</infon><infon key="source">Algorithmic aspects in information and management</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>36978</offset><text>PLDA: parallel latent Dirichlet allocation for large-scale applications</text></passage><passage><infon key="fpage">2424</infon><infon key="lpage">2432</infon><infon key="name_0">surname:Welinder;given-names:P</infon><infon key="name_1">surname:Branson;given-names:S</infon><infon key="name_2">surname:Belongie;given-names:S</infon><infon key="name_3">surname:Perona;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd International Conference on Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>37050</offset><text>The multidimensional wisdom of crowds</text></passage><passage><infon key="fpage">2035</infon><infon key="lpage">2043</infon><infon key="name_0">surname:Whitehill;given-names:J</infon><infon key="name_1">surname:fan Wu;given-names:T</infon><infon key="name_2">surname:Bergsma;given-names:J</infon><infon key="name_3">surname:Movellan;given-names:JR</infon><infon key="name_4">surname:Ruvolo;given-names:PL</infon><infon key="name_5">surname:Bengio;given-names:Y</infon><infon key="name_6">surname:Schuurmans;given-names:D</infon><infon key="name_7">surname:Lafferty;given-names:JD</infon><infon key="name_8">surname:Williams;given-names:CKI</infon><infon key="name_9">surname:Culotta;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2009</infon><offset>37088</offset><text>Whose vote should count more: optimal integration of labels from labelers of unknown expertise</text></passage><passage><infon key="fpage">465</infon><infon key="lpage">473</infon><infon key="name_0">surname:Wilson;given-names:AT</infon><infon key="name_1">surname:Chew;given-names:PA</infon><infon key="section_type">REF</infon><infon key="source">Human language technologies: the 2010 annual conference of the North American Chapter of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>37183</offset><text>Term weighting schemes for latent Dirichlet allocation</text></passage><passage><infon key="fpage">2134</infon><infon key="lpage">2142</infon><infon key="name_0">surname:Yan</infon><infon key="name_1">surname:Xu;given-names:N</infon><infon key="name_2">surname:Qi;given-names:Y</infon><infon key="name_3">surname:Bengio;given-names:Y</infon><infon key="name_4">surname:Schuurmans;given-names:D</infon><infon key="name_5">surname:Lafferty;given-names:JD</infon><infon key="name_6">surname:Williams;given-names:CKI</infon><infon key="name_7">surname:Culotta;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems 22</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>37238</offset><text>Parallel inference for latent Dirichlet allocation on graphics processing units</text></passage><passage><infon key="file">nihms968555f1.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37318</offset><text>Graphical model for LDA</text></passage><passage><infon key="file">nihms968555f2.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37342</offset><text>Graphical model for CL-LDA</text></passage><passage><infon key="file">nihms968555f3.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37369</offset><text>Effects of varying numbers and types of spammers on CL-LDA, CL-LDA-BPE, and other CL algorithms. The black dashed line is the expected performance for the proficient workers, while the black dot-dashed line is the expected performance for the medium performance workers. Central lines indicate mean performance over 10 datasets. Colored area around central lines indicates the standard deviation of performances over 10 datasets</text></passage><passage><infon key="file">nihms968555f4.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>37798</offset><text>EEG components from the ICL dataset that CL-LDA estimated as mostly capturing signals generated from a subject‚Äôs heart. The horizontal axis shows how strong the estimate is, while the vertical axis shows how stable that estimate is. The blue parabola represents the maximum estimate variance which would only occur in the case of estimates drawn from multinomial distributions. Green dots represent actual ‚Äúheart‚Äù components, while red dots are false positives. A clear trend can be seen with false positives having lower strength and higher variance estimates. On the right, exemplar EEG components are shown. The cartoon head visualizes the resulting pattern of electrical potentials at the scalp resulting from activity of that EEG component. Red, green, and blue represent positive, neutral, and negative polarity, respectively, with recording electrode positions shown as black dots. The graph to the right of each head shows a segment of time series activity from that component. The only high strength false positive has time series activity that closely resembles the QRS complex that is highly characteristic of heart activity</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>38940</offset><text>Notation</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Symbol&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Meaning&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;underline&gt;
                &lt;bold&gt;
                  &lt;italic&gt;I&lt;/italic&gt;
                &lt;/bold&gt;
              &lt;/underline&gt;
              &lt;sup&gt;&lt;italic&gt;N&lt;/italic&gt; √ó &lt;italic&gt;N&lt;/italic&gt;&lt;/sup&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Identity matrix of size &lt;italic&gt;N&lt;/italic&gt; by &lt;italic&gt;N&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;underline&gt;
                &lt;bold&gt;1&lt;/bold&gt;
              &lt;/underline&gt;
              &lt;sup&gt;&lt;italic&gt;N&lt;/italic&gt; √ó &lt;italic&gt;M&lt;/italic&gt;&lt;/sup&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Matrix of all ones of size &lt;italic&gt;N&lt;/italic&gt; by &lt;italic&gt;M&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;D&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of instances in the dataset&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;C&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of classes&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;R&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of possible responses&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;N&lt;sub&gt;d&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of votes submitted on sample &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;ùí±&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All votes&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;U&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of workers contributing votes&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;ùí∞&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All workers contributing votes&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;Œ±‚Éó&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Prior class distribution in the dataset&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;mml:math id=&quot;M22&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mover&gt;&lt;mml:mi&gt;Œ≤&lt;/mml:mi&gt;&lt;mml:mo&gt;‚Üí&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Prior vote distribution given class &lt;italic&gt;k&lt;/italic&gt; and worker &lt;italic&gt;u&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;
                &lt;underline&gt;
                  &lt;bold&gt;Œ≤&lt;/bold&gt;
                &lt;/underline&gt;
                &lt;sup&gt;u&lt;/sup&gt;
              &lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Prior vote distribution matrix on all classes for worker &lt;italic&gt;u&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;Œ∏‚Éó&lt;sub&gt;d&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Class distribution of instance &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;mml:math id=&quot;M23&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mover&gt;&lt;mml:mi&gt;œï&lt;/mml:mi&gt;&lt;mml:mo&gt;‚Üí&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Vote distribution given class &lt;italic&gt;k&lt;/italic&gt; and worker &lt;italic&gt;u&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;œÖ&lt;sub&gt;di&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Value of vote &lt;italic&gt;i&lt;/italic&gt; in document &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;z&lt;sub&gt;di&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Class of vote &lt;italic&gt;i&lt;/italic&gt; in document &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;ùíµ&lt;/bold&gt;
              &lt;sup&gt;‚àí&lt;italic&gt;di&lt;/italic&gt;&lt;/sup&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;All vote-classes excluding that of vote &lt;italic&gt;i&lt;/italic&gt; in document &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;m&lt;sub&gt;du&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weight of a vote on sample &lt;italic&gt;d&lt;/italic&gt; by worker &lt;italic&gt;u&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;italic&gt;n&lt;sub&gt;jklu&lt;/sub&gt;&lt;/italic&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Combined weight of votes with value &lt;italic&gt;l&lt;/italic&gt; by worker &lt;italic&gt;u&lt;/italic&gt; that are assigned class &lt;italic&gt;k&lt;/italic&gt; on instance &lt;italic&gt;j&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;mml:math id=&quot;M24&quot; overflow=&quot;scroll&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mtext mathvariant=&quot;italic&quot;&gt;jklu&lt;/mml:mtext&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;‚àí&lt;/mml:mo&gt;&lt;mml:mtext mathvariant=&quot;italic&quot;&gt;di&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Same as above excluding the weight of the &lt;italic&gt;i&lt;/italic&gt;th vote on instance &lt;italic&gt;d&lt;/italic&gt;&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>38949</offset><text>Symbol	Meaning	 	IN √ó N	Identity matrix of size N by N	 	1N √ó M	Matrix of all ones of size N by M	 	D	Number of instances in the dataset	 	C	Number of classes	 	R	Number of possible responses	 	Nd	Number of votes submitted on sample d	 	ùí±	All votes	 	U	Number of workers contributing votes	 	ùí∞	All workers contributing votes	 	Œ±‚Éó	Prior class distribution in the dataset	 		Prior vote distribution given class k and worker u	 	Œ≤u	Prior vote distribution matrix on all classes for worker u	 	Œ∏‚Éód	Class distribution of instance d	 		Vote distribution given class k and worker u	 	œÖdi	Value of vote i in document d	 	zdi	Class of vote i in document d	 	ùíµ‚àídi	All vote-classes excluding that of vote i in document d	 	mdu	Weight of a vote on sample d by worker u	 	njklu	Combined weight of votes with value l by worker u that are assigned class k on instance j	 		Same as above excluding the weight of the ith vote on instance d	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>39896</offset><text>Benchmark data</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dataset name&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classes&lt;/th&gt;
            &lt;th align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Instances&lt;/th&gt;
            &lt;th align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Evaluation labels&lt;/th&gt;
            &lt;th align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Workers&lt;/th&gt;
            &lt;th align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Votes&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AC2&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11,040&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;333&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;825&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89,948&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BM&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1000&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1000&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5000&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CSv3B&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42,624&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;550&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;214,665&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HC&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20,232&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4459&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;766&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97,164&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HCB&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20,026&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3277&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;762&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90,564&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WVSCM&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2134&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;159&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64&lt;/td&gt;
            &lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19287&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>39911</offset><text>Dataset name	Classes	Instances	Evaluation labels	Workers	Votes	 	AC2	4	11,040	333	825	89,948	 	BM	2	1000	1000	83	5000	 	CSv3B	2	42,624	550	57	214,665	 	HC	4	20,232	4459	766	97,164	 	HCB	2	20,026	3277	762	90,564	 	WVSCM	3	2134	159	64	19287	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>40153</offset><text>Unsupervised accuracy on found data</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Algorithms&lt;/th&gt;
            &lt;th align=&quot;left&quot; colspan=&quot;6&quot; rowspan=&quot;1&quot;&gt;Datasets&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;th align=&quot;left&quot; colspan=&quot;6&quot; valign=&quot;bottom&quot; rowspan=&quot;1&quot;&gt;
&lt;hr/&gt;&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AC2&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BM&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CSv3B&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HC&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HCB&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WVSCM&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL-LDA&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.876&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.813&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.960&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.947&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.664&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.757&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL-LDA-BPE&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.885&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.812&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.972&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.938&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.631&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.786&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CUBAM&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.804&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.942&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.646&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.671&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DS&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.850&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.812&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.958&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.926&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.643&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.772&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLAD&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.811&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.972&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.325&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.826&lt;/bold&gt;
            &lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MV&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.867&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.812&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.973&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.884&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.506&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.710&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RY&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.818&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.972&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.493&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.809&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ZC&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.833&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.815&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.975&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.684&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.271&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.818&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>40189</offset><text>Algorithms	Datasets	 			 		AC2	BM	CSv3B	HC	HCB	WVSCM	 	CL-LDA	0.876	0.813	0.960	0.947	0.664	0.757	 	CL-LDA-BPE	0.885	0.812	0.972	0.938	0.631	0.786	 	CUBAM	‚Äì	0.804	0.942	‚Äì	0.646	0.671	 	DS	0.850	0.812	0.958	0.926	0.643	0.772	 	GLAD	‚Äì	0.811	0.972	‚Äì	0.325	0.826	 	MV	0.867	0.812	0.973	0.884	0.506	0.710	 	RY	‚Äì	0.818	0.972	‚Äì	0.493	0.809	 	ZC	0.833	0.815	0.975	0.684	0.271	0.818	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>40578</offset><text>Bold values indicate the highest performance across CL algorithms on each dataset</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>40660</offset><text>Unsupervised precision on found data</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Algorithms&lt;/th&gt;
            &lt;th align=&quot;left&quot; colspan=&quot;6&quot; rowspan=&quot;1&quot;&gt;Datasets&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;th align=&quot;left&quot; colspan=&quot;6&quot; valign=&quot;bottom&quot; rowspan=&quot;1&quot;&gt;
&lt;hr/&gt;&lt;/th&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AC2&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BM&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CSv3B&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HC&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HCB&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WVSCM&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL-LDA&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.507&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.636&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.919&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.647&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.722&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.716&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL-LDA-BPE&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.540&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.578&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.943&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.654&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.758&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.717&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CUBAM&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.613&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.882&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.736&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.746&lt;/bold&gt;
            &lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DS&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.491&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.613&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.915&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.664&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.734&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.690&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLAD&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.573&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.943&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.773&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.700&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MV&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.481&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.576&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.944&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.595&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.746&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.729&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RY&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.586&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.943&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.778&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.694&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ZC&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.405&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.530&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.950&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
              &lt;bold&gt;0.666&lt;/bold&gt;
            &lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.771&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.318&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>40697</offset><text>Algorithms	Datasets	 			 		AC2	BM	CSv3B	HC	HCB	WVSCM	 	CL-LDA	0.507	0.636	0.919	0.647	0.722	0.716	 	CL-LDA-BPE	0.540	0.578	0.943	0.654	0.758	0.717	 	CUBAM	‚Äì	0.613	0.882	‚Äì	0.736	0.746	 	DS	0.491	0.613	0.915	0.664	0.734	0.690	 	GLAD	‚Äì	0.573	0.943	‚Äì	0.773	0.700	 	MV	0.481	0.576	0.944	0.595	0.746	0.729	 	RY	‚Äì	0.586	0.943	‚Äì	0.778	0.694	 	ZC	0.405	0.530	0.950	0.666	0.771	0.318	 	</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>41086</offset><text>Bold values indicate the highest performance across CL algorithms on each dataset</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>41168</offset><text>Independent component labeling dataset</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;
        &lt;thead&gt;
          &lt;tr&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classes&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ICs&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Experts&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Votes&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Non-experts&lt;/th&gt;
            &lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Votes&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4375&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2596&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19&lt;/td&gt;
            &lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8754&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
</infon><offset>41207</offset><text>Classes	ICs	Experts	Votes	Non-experts	Votes	 	7	4375	3	2596	19	8754	 	</text></passage></document></collection>
