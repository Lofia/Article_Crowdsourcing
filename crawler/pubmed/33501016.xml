<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20210128</date><key>pmc.key</key><document><id>7805642</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/frobt.2018.00138</infon><infon key="article-id_pmc">7805642</infon><infon key="article-id_pmid">33501016</infon><infon key="elocation-id">138</infon><infon key="kwd">opinion mining social media messages sentiment analysis collective intelligence deep learning crowdsourcing</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Tsapatsoulis;given-names:Nicolas</infon><infon key="name_1">surname:Djouvas;given-names:Constantinos</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">5</infon><infon key="year">2018</infon><offset>0</offset><text>Opinion Mining From Social Media Short Texts: Does Collective Intelligence Beat Deep Learning?</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>95</offset><text>The era of big data has, among others, three characteristics: the huge amounts of data created every day and in every form by everyday people, artificial intelligence tools to mine information from those data and effective algorithms that allow this data mining in real or close to real time. On the other hand, opinion mining in social media is nowadays an important parameter of social media marketing. Digital media giants such as Google and Facebook developed and employed their own tools for that purpose. These tools are based on publicly available software libraries and tools such as Word2Vec (or Doc2Vec) and fasttext, which emphasize topic modeling and extract low-level features using deep learning approaches. So far, researchers have focused their efforts on opinion mining and especially on sentiment analysis of tweets. This trend reflects the availability of the Twitter API that simplifies automatic data (tweet) collection and testing of the proposed algorithms in real situations. However, if we are really interested in realistic opinion mining we should consider mining opinions from social media platforms such as Facebook and Instagram, which are far more popular among everyday people. The basic purpose of this paper is to compare various kinds of low-level features, including those extracted through deep learning, as in fasttext and Doc2Vec, and keywords suggested by the crowd, called crowd lexicon herein, through a crowdsourcing platform. The application target is sentiment analysis of tweets and Facebook comments on commercial products. We also compare several machine learning methods for the creation of sentiment analysis models and conclude that, even in the era of big data, allowing people to annotate (a small portion of) data would allow effective artificial intelligence tools to be developed using the learning by example paradigm.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1971</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1987</offset><text>Big Data does not only refer to dealing with enormous data sets in terms of data capturing, data storage, and data processing (De Mauro et al.,; Hashem et al.,) but it is also strongly related with predictive analytics (Yaqoob et al.,) and data mining (Fan and Bifet,). Artificial intelligence tools, on the one hand, are strongly related with data mining and artificial intelligence is nowadays ranked first among the top-10 technology (Buzzwords,). A great portion of the huge amounts of online data that led us to the era of Big Data is created within or with the aid of social media platforms. Among those data we see customer reviews, comments and opinions about products, people, and events. All this information, if properly processed, is invaluable for businesses, governments, and individuals. As a result, opinion mining in social media became one of the primary pillars of social media marketing (Zafarani et al.,). It is not a surprise that digital media giants such as Google and Facebook developed and employed their own artificial intelligence tools for that purpose. Going one step further they created and made publicly available software libraries and tools such as Word2Vec (Mikolov et al.,) or Doc2Vec (Le and Mikolov,) and fasttext (Joulin et al.,) to show that they are at the front of applied research and to increase their prestige among the academic community. The aforementioned tools basically emphasize topic modeling through word embeddings; the latter being low-level feature representations of digital words extracted with the aid of deep learning approaches (Socher et al.,). Nevertheless, there is an increasing tendency nowadays to develop intelligent data mining applications by combining data crawled from social media sites with crowdtagging (Giannoulakis and Tsapatsoulis,; Ntalianis and Tsapatsoulis,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3829</offset><text>In one of our previous studies (Tsapatsoulis and Djouvas,), we have shown that tokens identified through crowd-tagging of tweets can be used as an index of terms for training effective tweet classification models in a learning by example paradigm. We had concluded that this type of indices, i.e., human-indicated terms, are probably the best feature set one can use for tweet classification. We empirically proved this through extended experimentation, in which we compared the human-created index of terms with many different automatically extracted feature sets in a machine learning scenario using three different classifiers. In this work we extend that study by investigating a more difficult problem: that of sentiment classification of tweets into three challenging categories (anger, hate, neutral). We also examine the problem of sentiment classification of Facebook comments regarding commercial products. The human-created indices are developed by using crowd-tagging through a well-known dedicated crowdsourcing platform to allow full reproduction of the hereby suggested empirical study. However, the real purpose of this study is to empirically compare the power of collective intelligence, as expressed by the crowd-collected keywords from tweets and Facebook comments, with that of deep learning, as expressed through the modeling of those short texts (i.e, tweets and Facebook comments) with character n-grams as in Doc2Vec and the fastText classifier. To the best of our knowledge none of the three research actions briefly mentioned above have been reported before in the corresponding literature.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5447</offset><text>2. Theoretical Framework and Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5489</offset><text>This paper investigates the importance of collective knowledge in creating artificial intelligence systems that operate in a ‚ÄúBig Data‚Äù environment. In this context it is imperative to review practical tools that allow collective knowledge - intelligence to be gathered. Crowdsourcing platforms are the contemporary solution of this demand. The second pillar of the literature review, presented next, focuses on short text classification methods, and techniques. Since the majority of these methods were applied to tweets, it is obvious that our emphasis is also given there. Methods that combine crowdsourcing and tweet classification are also examined extensively.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6160</offset><text>2.1. Crowdsourcing and Crowdtagging</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6196</offset><text>The theoretical roots of crowdsourcing are located in the so-called ‚Äúwisdom of crowds‚Äù theory which was first introduced by Surowiecki. The term ‚Äúcrowdsourcing‚Äù itself, a composite word consisting of the words ‚Äúcrowd‚Äù and ‚Äúoutsourcing,‚Äù was coined by two editors at Wired, namely Jeff Howe and Mark Robinson, to define the process through which businesses were using the Internet to ‚Äúoutsource work to the crowd‚Äù. In particular, Jeff Howe defines crowdsourcing as: ‚Äú‚Ä¶the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. ‚Ä¶The crucial prerequisite is the use of the open call format and the large network of potential laborers.‚Äù Nowadays a variety of crowdsourcing platforms are available in the Web (Doan et al.,) to allow crowdsourcing to take place in a few steps: Amazon Mechanical Turk (MTurk,), TurKit, uTest and Figure-eight are a few of them.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7206</offset><text>Kleemann et al. were among the first that explored the phenomenon of crowdsourcing for research and marketing purposes. In their article they sought a more precise definition of crowdsourcing, cataloged some of its forms, and differentiated them from satellite phenomena. They concluded with a discussion regarding potential consequences, negative and positive, of a wider use of crowdsourcing, especially in marketing. Brabham investigated public involvement for urban planning as a means to collect intellect from a population in ways that person-to-person planning meetings fail. He suggested that crowdsourcing provides a distributed problem-solving paradigm for business that enables seeking creative solutions for public planning projects through citizens' active involvement in that process. As a proof of concept he analyzed crowdsourcing in a hypothetical neighborhood planning scenario. He concluded his work with a discussion on the challenges that effective crowdsourcing implementation (at that time) posed. Vukovic reviewed a variety of crowdsourcing platforms through a crowdsourcing scenario for requirements' collection aiming at the development of generic crowdsourcing services in the cloud. He identified a set of critical features that crowdsourcing platforms should have and he evaluated these platforms against those features. He concluded with a taxonomy proposal for the crowdsourcing platforms while he outlined research challenges for enhancing crowdsourcing capabilities.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8706</offset><text>The Amazon Mechanical Turk (MTurk) was probably the first dedicated crowdsourcing platform that was used in research experimentation and especially for data annotation (crowdtagging). Hsueh et al. compared the quality of annotation data from expert and non-expert annotators, recruited through MTurk, in the context of classifying sentiments from political blog snippets. Noise level in the data, sentiment ambiguity and lexical uncertainty were identified as the three main factors that impede harnessing high-quality annotations from the crowd (non-experts). Finin et al. used both Amazon Mechanical Turk and Figure-eight (previously named CrowdFlower) to gather named entity annotations of Twitter status updates taking into account the informal and abbreviated nature of named entities in tweets. According to the authors, the collected annotations and the proposed annotation approaches provide a first step toward the full study of named entity recognition in social media like Facebook and Twitter that takes advantage of the crowd intelligence.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9759</offset><text>Crowdsourcing in marketing, based on tweets, was examined by Machedon et al., aiming at the development of techniques and tools for automatic topic and sentiment identification in social media communications using supervised machine-learning classifiers. The authors concluded that effective classifiers can be created using the crowdsourced training data. Although this work presents some similarities with our work, we should note here that our emphasis is put on the comparison of keyword selection for lexicon-based classifiers with classifiers that use low-level features extracted through deep learning. Furthermore, we also examine classification of Facebook comments and, as we will see later, these comments generally differ from tweets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10506</offset><text>Borromeo and Toyama used crowdsourcing to account for the huge effort that is required for manual sentiment analysis of written texts. They claim that the performance of automatic systems for this task is poor except for systems that are based on the learning by example paradigm. They compared the results obtained by crowdsourcing, manual sentiment analysis and an automatic sentiment analysis system and they concluded that both paid and volunteer-based crowdsourced sentiment analysis is significantly more effective than automatic sentiment analysis but cannot achieve high accuracy with respect to manual annotation by experts.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11140</offset><text>Crowd-tagging was mostly applied in the specific task of image annotation. For the completeness of our literature review we report here some recent work, but many other reports do exist. Mitry et al. compared the accuracy of crowdsourced image classification with that of experts. They used 100 retinal fundus photograph images selected by two experts. Each annotator was asked to classify 84 retinal images while the ability of annotators to correctly classify those images was first evaluated on 16 practice‚Äîtraining images. The study concluded that the performance of naive individuals to retinal image classifications was comparable to that of experts. Giuffrida et al. measured the inconsistency among experienced and non-experienced users in a leaf-counting task of images of Arabidopsis Thaliana. According to their results, everyday people can provide accurate leaf counts. Maier-Hein et al. investigated the effectiveness of large-scale crowdsourcing on labeling endoscopic images and concluded that non-trained workers perform comparably to medical experts. In a survey by Cabrall et al. on categorizing driving scenarios, they used the crowd to annotate features from driving scenes such as the presence of other road users and bicycles, pedestrians etc. They used the Crowdflower platform (now Figure-eight) to categorize large amounts of videos with diverse driving scene contents. As usual, the Gold Test Questions in Crowdflower were used to verify that the annotators perform well in their job. The results indicated that crowdsourcing through Crowdflower was effective in categorizing naturalistic driving scene contents.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12781</offset><text>2.2. Short Text Classification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12812</offset><text>In Table 1 we show the six categories of features that can be used for text classification according to Layton et al.. Those belonging to the categories of structure, content and semantics are intended for large texts while the ones belonging to the categories of word, character and syntax are well-suited for short texts. In our previous work (Tsapatsoulis and Djouvas,) we performed an evaluation of the features in these three categories in order to identify the ones that are more suitable for the task of sentiment classification of tweets into three broad classes (positive, negative, and neutral). We concluded that numerical features, that is frequencies and ratios, do not carry any discriminative power while methods based on indices of words (unigrams‚Äîtokens) or n-grams of characters perform much better. We also found that bigram-based indices (proposed to account for negation) give only minor accuracy improvements compared to unigram indices in contrast to what Pak and Paroubek suggested.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>13821</offset><text>Frequently used features for text analysis as categorized by Tsapatsoulis and Djouvas.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Word level&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Character level&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Syntax&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean word length&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Character n-grams&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frequency of function words&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of hapax Legomena &lt;xref rid=&quot;B33&quot; ref-type=&quot;bibr&quot;&gt;n.d.&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of alphabetic characters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Frequency of punctuation marks&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of hapax dislegomena&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of character repetition&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Part of speech (POS) tags&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of distinct words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of digit characters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of spelling errors&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of short words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of emoticons&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total number of lines&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Skip grams&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of special characters&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total number of sentences&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total number of unique words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of tab space characters&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total number of words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of upper case letters&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Word distribution per length&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ratio of white space characters&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Word frequencies&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Total number of characters&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Word &lt;italic&gt;n&lt;/italic&gt;-grams&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Vowel combinations&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Structure&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Content&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Semantics&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Characters per paragraph&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of abbreviations&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hyperonyms of words&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of quoted content&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of age based words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hyponyms of words&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of paragraphs&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of gender based words&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Synonyms of words&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of sentences&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of keywords&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sentences per paragraph&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of slang words&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Words per paragraph&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of stopwords&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>13908</offset><text>Word level	Character level	Syntax	 	Mean word length	Character n-grams	Frequency of function words	 	Number of hapax Legomena 	Ratio of alphabetic characters	Frequency of punctuation marks	 	Number of hapax dislegomena	Ratio of character repetition	Part of speech (POS) tags	 	Ratio of distinct words	Ratio of digit characters	Ratio of spelling errors	 	Ratio of short words	Ratio of emoticons	Total number of lines	 	Skip grams	Ratio of special characters	Total number of sentences	 	Total number of unique words	Ratio of tab space characters		 	Total number of words	Ratio of upper case letters		 	Word distribution per length	Ratio of white space characters		 	Word frequencies	Total number of characters		 	Word n-grams	Vowel combinations		 	Structure	Content	Semantics	 	Characters per paragraph	Number of abbreviations	Hyperonyms of words	 	Number of quoted content	Number of age based words	Hyponyms of words	 	Number of paragraphs	Number of gender based words	Synonyms of words	 	Number of sentences	Number of keywords		 	Sentences per paragraph	Number of slang words		 	Words per paragraph	Number of stopwords		 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15031</offset><text>In general, we can identify three different approaches for short text classification. Lexicon-based methods are more common in sentiment analysis. The lexicons consist of emotionally-colored terms showing sentiment polarity. These terms are usually suggested by human experts and are used in a rule-based manner or through learned classifier models to assess the sentiment of short texts such as tweets. The lexicon-based approach is rather effective for classifying short texts into two different classes, e.g., positive and negative sentiment, but as new categories are included the classification performance drops steeply. Machine learning approaches use pairs of texts and relate labels - tags to train classification models. The key in these methods is the features that are extracted from the texts to train/feed the classifier. Given that the proposed method belongs to this type of approach, our literature review emphasizes the feature sets used in these methods. Finally, social network approaches are targeting social media messages and involve techniques from the field of Social Network Analysis (SNA) to extract social content and characteristics. Social network properties are not sufficient for short text classification and, therefore, social network approaches are combined with methods from the other two categories. Nevertheless, not a solid improvement has been reported when social network characteristics are combined with lexicon-based or machine learning approaches.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16524</offset><text>Mac Kim and Calvo evaluated various automatic sentiment polarity detection methods applied on students' responses to unit of study evaluations (USE). They started from the five universal emotion categories (Karpouzis et al.,)‚Äîanger, fear, joy, sadness and surprise‚Äîand they further classified joy and surprise as related to positive polarity while anger, fear and sadness were classified in the negative polarity. The performance of the developed category-based and dimension-based emotion prediction models was assessed on the 2940 textual responses of the students. The WordNet-Affect was utilized as a linguistic lexical resource for the category-based modeling while two dimensionality reduction techniques, namely latent semantic analysis (LSA) and non-negative matrix factorization (NMF), were also applied in the dimension-based modeling. In the latter case, the Affective Norm for English Words (ANEW) normative database, composed of affective terms, was used. Both the NMF-based categorical modeling and the dimensional modeling resulted in performances above the chance level (50% in that case).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17634</offset><text>Barbosa and Feng used pairs of text snippets and noisy labels, obtained from three websites, to develop a sentiment classification model. An additional set of 2000 manually labeled tweets were equally split and utilized for model-tuning on tweets and for testing. Their feature set consists primarily of Part of Speech (POS) tags of tweet words along with other syntax analysis features. They have also used network activity characteristics like retweets, hashtags, links, punctuation and exclamation marks as well as prior polarity of words found in sentiment lexicons. Agarwal et al. compared the POS-based method of Barbosa and Feng with the unigram baseline on both tree kernel and feature-based machine learning models. They showed that the models developed with POS tags and syntax characteristics outperformed the unigram-based ones. Their feature analysis revealed that the most discriminative features are those that combine the prior polarity of words and their parts-of-speech tags. They also concluded that sentiment classification of tweets is similar to sentiment classification of other text types. The authors, however, did extend preprocessing on the tweets, such as emoticons, acronyms and slang word translation to formal language words and, thus, they alleviated the majority of Twitter data's distinct characteristics. In other words, they actually transformed the problem of sentiment analysis of tweets into a typical text-classification task. Finally, we should remind here that, as in all POS-based approaches, language dependency makes the specific method non-generalizable to languages other than English.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19267</offset><text>Narayanan et al. experimented with a Naive Bayes classifier for sentiment analysis of movie reviews, aiming to find the most suitable feature set-data preprocessing combination. They found that effective negation handling along with word n-grams and feature selection through mutual information metrics, results in a clear improvement in sentiment prediction accuracy which reached 88.80% on the IMDB movie reviews dataset. Thus, they concluded that a highly effective and efficient sentiment classifier can be built using a simple Naive Bayes model, which has linear training and testing time complexities. According to the authors, their method can be generalized to several different text classification problems whenever a combination of speed, simplicity and accuracy is required. Compared with the work of Tsapatsoulis and Djouvas this work shows that accounting for negation and using some kind of syntactic analysis could be helpful in other types of short texts, in contrast to what happens with tweets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20280</offset><text>Stavrianou et al. initially experimented with natural language processing (NLP) methods for sentiment analysis of tweets but they soon recognized, soon, that NLP techniques alone do not provide the desired accuracy of sentiment prediction on unseen cases. In order to account for this ineffectiveness, they proposed a hybrid method in which the results of natural language analysis, mainly POS-related features, feed a machine learning classifier. Although they observed a slight improvement on the sentiment classification performance, the most important finding was that NLP features obtained through syntactic analysis do not fit well with machine learning classifiers in the learning by example paradigm. Thus, it would be better to keep NLP as a data preprocessing stage rather than as a dedicated feature extractor. Shirbhate and Deshmukh also tried to incorporate NLP into their system. They trained a Naive Bayes classifier to categorize positive, negative and neutral tweets referring to customer opinions regarding cameras, mobiles and movies. Their feature set consists of unigrams and POS tags. They also applied filtering based on mutual information measures for feature selection. A prediction accuracy of 89% on a dataset consisting of 2, 000 tweets was reported. However, the small test set composed from 250 tweets, which leads to a proportion of training:test set equal to 88:12, along with the experimentation with a single classifier, i.e., Naive Bayes, limits the validity of their conclusions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21796</offset><text>Hamdan et al. experimented extensively on a variety of low-level feature categories for a two-class sentiment classification of tweets. They used an adapted logistic regression classifier fed with combined word n-grams, lexicons, Z score and semantic features, such as topic features and the distribution of tweet tokens into the Brown collection (Brown et al.,). They have also taken into account negation during data preprocessing. They found that the lexicon-based approach, i.e., the use of sentiment lexicons as an index of terms, provided the best results. Their work is, indeed, informative, well-developed and related to the current study in terms of its aims. However, in the real world, the neutral tweets and comments constitute a high percentage of short-texts/messages exchanged every day in social media platforms. Thus, a two-class tweet classification in which the neutral category is excluded would definitely lead to misleading conclusions and, more importantly, it would be difficult to apply in realistic problems related to sentiment analysis of short messages.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22879</offset><text>Prusa et al. approached the problem of tweet classification from a different perspective: They denoted that due to the variability of tweets' contents, word- or character-based indices generate thousands of features while each tweet instance, due to the character length limit, contained only a few features of the entire feature set. Thus, the feature vector representation of each tweet will be sparse. Starting from this observation, they explored the influence of filter-based feature selection techniques on the tweet classification performance, using ten feature subsets of varying lengths and four different machine learning methods. They empirically showed that feature selection significantly improves classification performance and they concluded that both the selection of ranker and the feature subset length affect tweet classification. Deep learning methods for feature extraction do, in fact, carry out the above-mentioned procedure implicitly but in a much more systematic way. Thus, a comparison with methods that are based on feature extraction through deep learning, as we do in the current study, covers fully the method of Prusa et al..</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>24037</offset><text>3. Methodology</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24052</offset><text>The basic assumption in this work is that collective intelligence regarding the appropriate sentiments of short texts, including tweets and Facebook comments, can be obtained with the aid of crowd-tagging within a dedicated crowdsourcing platform. In addition, we argue that the tokens used by humans for the classification of those short texts can be used to represent them as multidimensional points in high informative vector spaces (Salton et al.,). This representation allows for effective models (classifiers) to be learned in the learning by example scenario. Our main hypothesis is that these classifiers surpass, in terms of effectiveness, classifiers learned with low-level features of the short texts, as in Doc2Vec, even in the case where they are combined with deep learning architectures which have embedded and especially designed classifiers, as in fasttext. The methodology we follow to confirm or reject this hypothesis consists of four stages and is described below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>25038</offset><text>3.1. Mathematical Formulation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25068</offset><text>Let us assume a set of N short texts (i.e, facebook comments, tweets, etc.) ùîª = {d1, d2, ‚Ä¶, dN} and a corresponding set of labels ùïÉ = {l1, l2, ‚Ä¶, lN} so that every short text di is assigned a label li from a limited set ‚ÑÇ = {c1, c2, ‚Ä¶, cK}, corresponding to short texts' sentiments.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25365</offset><text>Let us also denote with ùîΩ = {f1, f2, ‚Ä¶, fM} a set of transforming (or feature extraction) functions  so that every short text di is represented as a vector  in a vector space.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25546</offset><text>The purpose of the current study is to identify the function fopt(ùîª) which maximizes short text classification in terms of accuracy of label prediction for a given classifier T, that is:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25736</offset><text>where</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25742</offset><text>and .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>25748</offset><text>3.2. Data Collection and Crowdtagging</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25786</offset><text>In this study we used two datasets crawled from Twitter and Facebook and annotated with the aid of the Figure-eight (previously known as Crowdflower) crowdsourcing platform. The main characteristics of these datasets are shown in Tables 2, 3.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>26029</offset><text>Dataset #1: Facebook comments on commercial products.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;5&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Sentiment&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Total&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Comparative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Ironic&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Negative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Positive&lt;/bold&gt;&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gold&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;613&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;111&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;929&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,509&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,351&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4,513&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crowd&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;362&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,443&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,162&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,496&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4,513&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Agreed&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;282&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;865&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;920&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,173&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3,251&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4600&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0991&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.9311&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6097&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.8682&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7203&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7790&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2200&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5994&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7917&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7841&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7203&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Training set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;153&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;572&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;613&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;765&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2,103&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Test set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;129&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;293&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;307&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;408&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,137&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26083</offset><text>	Sentiment	Total	 		Comparative	Ironic	Negative	Neutral	Positive		 	Gold	613	111	929	1,509	1,351	4,513	 	Crowd	362	50	1,443	1,162	1,496	4,513	 	Agreed	282	11	865	920	1,173	3,251	 	Recall	0.4600	0.0991	0.9311	0.6097	0.8682	0.7203	 	Precision	0.7790	0.2200	0.5994	0.7917	0.7841	0.7203	 	Training set	153	‚Äì	572	613	765	2,103	 	Test set	129	‚Äì	293	307	408	1,137	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>26447</offset><text>Comparison with ‚Äúgold standard‚Äù (students' assessment) is also shown.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>26521</offset><text>Dataset #2: Tweets.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;7&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Sentiment&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Total&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Anger&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Disgust&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Fear&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Hate&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Sarcasm&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Other&lt;/bold&gt;&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Fully agreed&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;202&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;335&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;43&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;539&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,277&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Partially agreed&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;495&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;245&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;456&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;170&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;547&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2,031&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Contradicting&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;692&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PRR&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2898&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2213&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1154&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.3431&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4235&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2019&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Training set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;429&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;247&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;502&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1,178&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Test set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;268&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;126&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;289&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚Äì&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;683&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26541</offset><text>	Sentiment	Total	 		Anger	Disgust	Fear	Hate	Neutral	Sarcasm	Other		 	Fully agreed	202	27	3	128	335	43	539	1,277	 	Partially agreed	495	95	23	245	456	170	547	2,031	 	Contradicting								692	 	PRR	0.2898	0.2213	0.1154	0.3431	0.4235	0.2019			 	Training set	429	‚Äì	‚Äì	247	502	‚Äì		1,178	 	Test set	268	‚Äì	‚Äì	126	289	‚Äì		683	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>26870</offset><text>The degree of agreement between the annotators is also shown.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26932</offset><text>The first dataset consists of Facebook comments about commercial electronic comments of a well-known multinational company. The comments were manually collected and stored in .csv files by students of the Cyprus University of Technology in the framework of the course ‚ÄúCIS 459: Natural Language Processing‚Äù during the period from October 2017 to January 2018. A subset of 4,513 comments was assessed by the students regarding their sentiment category as indicated in Table 2. We denote this initial dataset evaluation in terms of the expressed sentiment as our ‚Äúgold standard.‚Äù</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27518</offset><text>The second dataset consists of 4,000 tweets selected from an original dataset of 32 million tweets acquired through the Twitter streaming API during the period of March 2017 to April 2017 in the framework of the ENCASE project (see also Founta et al.,). The first step of the selection processes was to filter out some undoubted spam entries by discarding tweets with more than four hashtags, tweets with lengths of &lt; 80 characters and native retweets (i.e., tweets that contain the ‚Äúretweeted_status‚Äù field).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28032</offset><text>In addition to spam filtering, two more filtering criteria were applied in order to account for the fact that the great majority of the 32 million tweets were neutral in terms of sentiment. Toward this end, tweets underwent a preprocessing step, during which each tweet was augmented with two arguments: (a) polarity, and (b) number of inappropriate words (counter). The former was calculated using the TextBlob Python library. TextBlob produces a polarity output in the range of [‚àí1.0, 1.0]. The latter parameters were created using two dictionaries containing Hate base and offensive words (Denn et al.,). All words in a tweet were stemmed and lower cased and matched against the two dictionaries; matching entries were counted and the final score was added to the augmented tweet. Using the two injected variables, tweets in this dataset are filtered so that they have a polarity in the range [‚àí1, ‚àí0.7] and at least 1 offensive word. No filtration or processing on the users was applied; thus, more than one tweet from the same user might appear in the dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29104</offset><text>Both datasets were uploaded to Figure-eight for sentiment labeling and crowdtagging as indicated in Figure 1. This figure shows the tweets' project but a similar design was also adopted for the Facebook comments. Every shot text was assessed from at least three annotators. The number of annotators does not really affect the identified sentiment category but it does increase the total number of (different) tokens that annotators used in crowdtagging. This, in turn, allows for different token selection strategies to form the crowd lexicon. Thus, a token can be added to the crowd lexicon in case it is suggested by all annotators during crowdtagging (full agreement), the majority of annotators or at least two annotators (Ntalianis et al.,).</text></passage><passage><infon key="file">frobt-05-00138-g0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>29851</offset><text>A snapshot of the crowd-tagging process that took place through Figure-eight.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29929</offset><text>As we see in Tables 2, 3, sentiment labeling proved to be a difficult task even for humans. Regarding the Facebook comments dataset, agreement of the majority of annotators (indicated as ‚ÄúCrowd‚Äù in Table 2) with the gold standard (‚ÄúGold‚Äù) was achieved on 3,251 comments (72.03%). However, the level of agreement varies significantly across the various sentiments. Identifying ‚ÄúIrony‚Äù in FB comments seems to be impossible, justifying that irony is not one of the universally recognizable sentiments (Cowie et al.,) even in written texts. The comparative comments present also low recall value (0.46) but the precision score is quite high (0.779). Some of the neutral comments are understood by the annotators as negative; this results in low recall value (0.6097) for the neutral comments and low precision value (0.5994) for the negative comments. Thus, we used the agreed comments, excluding the ‚Äúirony‚Äù ones, to train and test our classifiers as explained in section 3.4.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30920</offset><text>The case of tweets was somehow different. For this dataset we did not have a gold standard, thus we decided to keep only those tweets for which the majority of annotators (crowd) agreed on the expressed sentiment. Through this process, 17.6% of the tweets were removed as being contradictory regarding their sentiments (see Table 3 for the absolute numbers). We also removed the tweets categorized as ‚ÄúOther‚Äù since their sentiment could not recognized and, obviously could not form a concrete and well defined category on its own. Given that there was no gold standard we decided to define a pseudo-recall rate (PRR) as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31553</offset><text>where PRR[s] denotes the pseudo-recall rate for sentiment s and NF[s] and NP[s] are, respectively, the numbers of tweets that were fully and partially agreed upon by the crowd that they express sentiment s.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31760</offset><text>As can be seen in Table 3 the PRR for all sentiments was rather low and, in some cases, as for the sentiments ‚ÄúFear‚Äù and ‚ÄúSarcastic,‚Äù was extremely low. This indicates the difficulty of classifying tweets into real sentiment categories and not into broad ones, such as ‚ÄúPositive‚Äù and ‚ÄúNegative,‚Äù as was done in the majority of the previous studies. Another important conclusion we can draw from Table 3 is that the ‚ÄúFear‚Äù sentiment, although it is considered one of the universal sentiments (Cowie et al.,), is neither easily expressed nor easily identified in short written messages. On the contrary contrary, ‚ÄúHate‚Äù is more easily understood in small written texts, although is not of one of the universal sentiments. This finding is in agreement with a recent study by Founta et al..</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32571</offset><text>Since the absolute number of agreed tweets (NF) and the PRR of sentiments ‚ÄúFear,‚Äù ‚ÄúDisgust,‚Äù and ‚ÄúSarcasm‚Äù were both too low, we decided to exclude these sentiments for the experimentation. Thus, we ended up with a total number of 1,861 tweets, which were used for training and testing our models (see further details in section 3.4).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>32918</offset><text>3.3. Vector Space Models and Word Embeddings</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32963</offset><text>In our previous study (Tsapatsoulis and Djouvas,), we showed that, among a variety of low-level features, the ones that are best suited for tweet classification are the unigrams that are either indicated by humans, denoted as Human Index (HI) or Crowd Lexicon (CL), or composed of the Globally Most Frequent Tokens (GMFT) in the training set. These two types of features, along with the classic Bag of Words (BoW) method that makes use of all tokens and their respective TF-IDF values (Maas et al.,), are compared with fasttext and Doc2Vec that are based on character n-gram features extracted through deep learning. Since GMFT, CL and BoW feature sets are all based on tokens, the corresponding trained classification schemes are, in fact, Vector Space Model representations. On the other hand, both fasttext and Doc2Vec are based on the Word Embeddings scheme.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33826</offset><text>The GMFT, CL, and BoW tokens of tweets and Facebook comments are utilized to construct indices of terms {t1, t2, ‚Ä¶, tQ} in order to represent those short texts as high-dimensional points in a vector space model. In the case of GMFT and CL, each short text di is represented as a binary vector  indicating whether the corresponding term is included or not in the short text. In the case of BoW the texts are represented through real value vectors , indicating the TF-IDF value (Aizawa,) of each term in the short text.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34346</offset><text>In the case of the GMFT feature set, the 100 most frequent tokens in each sentiment category were used for the creation of the index of terms. This resulted in an index with a length equal to Q = 203 for the Facebook comments and Q = 178 for the tweets. In both cases, there were common tokens among the various categories, including, of course, Stopwords. In the case of BoW, the indices for both Facebook comments and tweets are quite lengthy (3,166 and 3,151 respectively), and this length increases logarithmically with the number of samples (short texts) in the training set.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34927</offset><text>As already explained earlier, in the current study crowd intelligence regarding the short text classification was collected in the form of crowdtagging. Since every short text message was assessed by at least three annotators, we adopted two different crowd lexicon creation strategies, named CL2V (Crowd Lexicon 2 Votes) and CLMV (Crowd Lexicon Majority Voting). In the first, we included in the index tokens (tags) suggested by at least two annotators while in CLMV a token was included in the index if it was suggested by the majority of the annotators. We should note here, as can also be seen in Figure 1, that the annotators were instructed to select the tags from the body of short text (tweet and Facebook comments), thus crowd intelligence was collected in the form of token filtering and implicit sentiment understanding.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35759</offset><text>By comparing the length of CL2V and CLMV indices in tweets and Facebook comments, we see some interesting variations. Despite the fact that the number of Facebook comments used the training set (2,103) was almost twice the corresponding number of tweets (1,178), the length of the CL2V index for the Facebook comments (427) was significantly smaller than that of the tweets (746). A similar observation can be made for the length of CLMV indices. In addition, it appears, by comparing the lengths of CL2V and CLMV in each one of the two short text types, that the annotators tend to more easily agree on the important keywords in a tweet than in a Facebook comment. The overall conclusion we can draw based on these observations is that sentiment-related keywords can be more easily identified in tweets rather than in Facebook comments. This can be attributed to the presence of hashtags and mentions, but it also indicates that although both tweets and Facebook comments are short texts, they are not so similar as it appears in some studies (Barbosa and Feng,).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36824</offset><text>Word Embeddings is an umbrella term for a set of language modeling approaches and feature extraction techniques in natural language processing (NLP), where tokens (words), phrases or short documents are mapped to vectors of real numbers. The typical size of these vectors is 100 but in fact there is no rule that can help you decide on the optimal vector length. Although pre-trained models, usually learned using the Wikipedia documents, do exist for practically each word (or more correctly token) that appears in the Web, this is obviously not the case for short texts and phrases. In that case, short text embeddings must be learned from scratch as explained by Le and Mikolov. We have experimented extensively regarding the optimal vector length of word embeddings, for both Doc2Vec and fasttext methods, and we concluded that the typical vector size of 100 elements shows a slightly better performance independently of the classifier that is used.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>37778</offset><text>3.4. Learning</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37792</offset><text>In order to assess the appropriateness of each feature set on short text classification, both tweets and Facebook comments were first randomly separated into a training and a test set with an approximate ratio of 65:35. The corresponding distributions per category in these two sets are shown in Tables 2, 3. In both cases, one of the sentiment categories, that is the ‚ÄòHate' category in tweets and the ‚ÄúComparative‚Äù category in Facebook comments, is underrepresented. Thus, constructing good classification models becomes harder. In order to emulate as much as possible real operation situations, the index sets were developed from tokens found in the training sets. This is one of the reasons that the typical k-fold evaluation scheme was not adopted in our experiments. Short texts were represented as binary vectors (CMFT, CL2V, CLMV cases), as TF-IDF vectors (BoW case) or as real value vectors (Doc2Vec and fasttext case) in the related vector space produced by each one of the indices, as described in section 3.3.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>38820</offset><text>Four different classifier types were constructed for each one of the previously-mentioned indices using the training sets. The case of fasttext was different since feature extraction and classifier learning are combined. The training in this case was done with the help of text files similar to the one shown in Figure 2. As usual the test sets, that is the unseen instances, were utilized for testing the effectiveness of the created classification models. A total of 21 classification models were constructed, for each one of the short text types, corresponding to five different vector spaces and four different learning algorithms along with the fasttext case (see Tables 4, 5). We used the learning algorithms' implementations of the Python module Scikit-learn (Pedregosa et al.,) as well as the corresponding Python library for fastText. In order to have a fair comparison of the feature sets, all learning algorithms were used without tuning based on their default settings. Tokenization of both tweets and Facebook comments was achieved through the TweetTokenizer of the NLTK library. Note, however, that the fasttext classifier does not require any type of tokenization of the short text (see a snapshot of the training file in Figure 2) since it makes use of deep learning and the extracted features are based on character n-grams (Joulin et al.,).</text></passage><passage><infon key="file">frobt-05-00138-g0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40179</offset><text>A screenshot of the file used for the training of the fasttext classifier.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>40254</offset><text>Comparison of feature sets and classifiers for sentiment classification of facebook comments.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Feature set&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;4&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Classifier&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Average&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Decision trees&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Linear SVC&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Naive Bayes&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Stoch. gradient descent&lt;/bold&gt;&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT [203]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5928&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7001&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7353&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6711&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6748&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL2V [427]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6500&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7142&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7432&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7071&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7036&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CLMV [202]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5858&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6790&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7361&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6719&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6682&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BoW [3166]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.6640&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7282&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6429&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7573&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6981&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Doc2Vec [100]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4785&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6289&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6429&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6438&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5985&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Deep learning [100]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Fasttext&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7282&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>40348</offset><text>Feature set	Classifier	Average	 		Decision trees	Linear SVC	Naive Bayes	Stoch. gradient descent		 	GMFT [203]	0.5928	0.7001	0.7353	0.6711	0.6748	 	CL2V [427]	0.6500	0.7142	0.7432	0.7071	0.7036	 	CLMV [202]	0.5858	0.6790	0.7361	0.6719	0.6682	 	BoW [3166]	0.6640	0.7282	0.6429	0.7573	0.6981	 	Doc2Vec [100]	0.4785	0.6289	0.6429	0.6438	0.5985	 	Deep learning [100]	Fasttext	0.7282	 	</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>40729</offset><text>Comparison of feature sets and classifiers for sentiment classification of tweets.</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Feature set&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;4&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Classifier&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Average&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Decision trees&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Linear SVC&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Naive Bayes&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Stoch. gradient descent&lt;/bold&gt;&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT [178]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6955&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7072&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7511&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7233&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7193&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CL2V [746]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7628&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7189&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7350&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6852&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7255&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CLMV [536]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7687&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7306&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7452&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7086&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7376&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bag of Words (3151)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7072&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7291&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7013&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7291&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7167&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Doc2Vec [100]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5256&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6750&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6735&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7013&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6439&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Deep learning [100]&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;4&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Fasttext&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7291&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>40812</offset><text>Feature set	Classifier	Average	 		Decision trees	Linear SVC	Naive Bayes	Stoch. gradient descent		 	GMFT [178]	0.6955	0.7072	0.7511	0.7233	0.7193	 	CL2V [746]	0.7628	0.7189	0.7350	0.6852	0.7255	 	CLMV [536]	0.7687	0.7306	0.7452	0.7086	0.7376	 	Bag of Words (3151)	0.7072	0.7291	0.7013	0.7291	0.7167	 	Doc2Vec [100]	0.5256	0.6750	0.6735	0.7013	0.6439	 	Deep learning [100]	Fasttext	0.7291	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>41202</offset><text>4. Experimental Results and Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41241</offset><text>The classification performance, i.e., label prediction accuracy in regards to the gold standard, of the 21 compared classification models for the Facebook comments are summarized in Table 4 while Table 5 shows the corresponding results for the tweets' case. The reported values are average scores on 10 runs. As expected in the Naive Bayes case, the obtained performance does not change during the different runs since it does not include any randomly selected parameters. However, it turned out that the same happens in the case of the fasttext classifier. Since we do no have further information regarding the implementation of this classifier, we only assume that it is probably based on a rule-based approach without any random parameters or random initialization. We have avoided the typical k-fold evaluation because the emphasis of our work was on the feature sets' assessment rather than on the classifier performance. So, in order to avoid a further addition of randomness we kept our training and test sets fixed.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42265</offset><text>The analysis of results, presented herein, follows three axes. First we try to answer the main research question of this study, that is whether crowd intelligence, as expressed through crowdtagging, can create a more effective feature set than those extracted through deep learning as in fasttext and Doc2Vec. A comparison of sentiment analysis of tweets and Facebook comments follows, while a better way to collect crowd intelligence in the form of crowd-tagging is also discussed.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>42748</offset><text>4.1. Statistical Analysis of the Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42789</offset><text>Since the performance scores shown in Tables 4, 5 appear very close to each other, we ran a multi-way ANOVA test in order to identify the impact of each one of the factors, namely data source (tweets, facebook comments), feature type (GMFT, etc.) and classifier (Decision Trees, etc.). The results of the ANOVA test are summarized in Table 6. While the impacts of all three factors are discussed further‚Äîin a qualitative fashion‚Äîin sections 4.2 and 4.3, respectively, we can see in Table 6 an overview of the significance of their influence. According to the obtained p-values, the type of features affects significantly the retrieved accuracy scores since the probability to get, with a random feature set, an F-ratio higher than the computed one (i.e., 15.782) is p = 0.0001. Thus, we conclude that the feature sets that we have included in our study are indeed informative. A similar interpretation holds for the data source (tweets or facebook comments) and the classifier. We clearly see that the performance scores differ depending on the data source, which in simple words means that no similar performance should be expected for the categorization of tweets and Facebook comments. We should mention here, however, that the categories used and the data distribution per category differ in the tweets' and Facebook comments' case (see Tables 2, 3). Thus, the dependency of performance scores on the data source type is more or less expected.</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>44241</offset><text>The results of the multivariate ANOVA test on the combined results of Tables 4, 5.</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;‚àë&lt;italic&gt;x&lt;/italic&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;df&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;F-ratio&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;p-value&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Data source&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0160&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21.618&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.00056&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feature set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0469&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15.782&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.00010&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classifier&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0284&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.760&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.00483&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Data‚äõfeature&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0035&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.173&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.37088&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Data‚äõclassifier&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0111&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.986&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.01792&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feature‚äõclassifier&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0333&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.734&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.01528&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0089&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>44324</offset><text>	‚àëx2	df	F-ratio	p-value	 	Data source	0.0160	1	21.618	0.00056	 	Feature set	0.0469	4	15.782	0.00010	 	Classifier	0.0284	3	12.760	0.00483	 	Data‚äõfeature	0.0035	4	1.173	0.37088	 	Data‚äõclassifier	0.0111	3	4.986	0.01792	 	Feature‚äõclassifier	0.0333	12	3.734	0.01528	 	Residual	0.0089	12			 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44619</offset><text>It also appears that the classifier affects the obtained scores, which means that proper classifiers are needed for the tasks classifying tweets and Facebook comments. However, among the three influencing factors, the least important is the classifier. Thus, the problem itself, for example the type of data, the selected features, the number and kind of categories used and the data distribution per category, are more important than the selection of the classifier. This is further justified through the multiple comparison of means of the Tukey HSD test, which is reported in Table 9.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>45207</offset><text>As far as the pairwise co-influences are concerned, we observed that co-influence of data source and feature set is insignificant while the co-influences of data source and classifier and feature and classifier, respectively, are approximately equally significant. The fact that the co-influence of data source and feature set is insignificant means that there is no evidence that there are feature sets, among the compared ones, that fit better with one or the other data source type. On the other hand, it turns out that there is a rather strong correlation among data source and the selected classifier, which means that there are combinations of classifiers and data sources that are more suitable than others. Similarly, as expected and further discussed in section 4.2, the combinations of feature sets and classifiers are important and, therefore, we conclude that there are feature types which are better suited for specific classifiers.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46153</offset><text>In an effort to make justified conclusions regarding the better combination of the three influencing factors, i.e., data source, feature set and classifier, we ran the pairwise Tukey Honest Significant Difference (HSD) post hoc test in each one of these three cases. The results of the Tukey HSD test are summarized in Tables 7‚Äì9. We first observed that better results are achieved in the tweet classification problem and, as we see in Table 8 the difference is significant and the null hypothesis that the performance scores on both tweets and Facebook comments are identical is rejected (see the last column in Table 8). For better interpretation of the results in Tables 7‚Äì9, we should mention here that the differences reported in columns 3 ‚àí 5 refer to the G2 ‚àí G1.</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>46932</offset><text>The influence of feature set through multiple comparison of means - Tukey HSD, FWER=0.05.</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Meandiff&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Lower&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Upper&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Reject&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BOW&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0862&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.1635&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0089&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BOW&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GL2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0072&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0701&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0845&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BOW&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLMV&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0041&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0814&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0731&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BOW&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0103&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0876&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.067&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GL2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0934&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0161&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1707&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLMV&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0821&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0048&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1593&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0759&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0014&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1532&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GL2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLMV&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0113&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0886&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.066&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GL2V&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0175&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0948&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0598&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GLMV&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GMFT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0062&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0835&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0711&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>47022</offset><text>G1	G2	Meandiff	Lower	Upper	Reject	 	BOW	D2V	‚àí0.0862	‚àí0.1635	‚àí0.0089	True	 	BOW	GL2V	0.0072	‚àí0.0701	0.0845	False	 	BOW	GLMV	‚àí0.0041	‚àí0.0814	0.0731	False	 	BOW	GMFT	‚àí0.0103	‚àí0.0876	0.067	False	 	D2V	GL2V	0.0934	0.0161	0.1707	True	 	D2V	GLMV	0.0821	0.0048	0.1593	True	 	D2V	GMFT	0.0759	‚àí0.0014	0.1532	False	 	GL2V	GLMV	‚àí0.0113	‚àí0.0886	0.066	False	 	GL2V	GMFT	‚àí0.0175	‚àí0.0948	0.0598	False	 	GLMV	GMFT	‚àí0.0062	‚àí0.0835	0.0711	False	 	</text></passage><passage><infon key="file">T8.xml</infon><infon key="id">T8</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>47481</offset><text>The influence of data source through multiple comparison of means - Tukey HSD, FWER=0.05.</text></passage><passage><infon key="file">T8.xml</infon><infon key="id">T8</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Meandiff&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Lower&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Upper&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Reject&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;FC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TW&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0401&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0023&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0778&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;True&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>47571</offset><text>G1	G2	Meandiff	Lower	Upper	Reject	 	FC	TW	0.0401	0.0023	0.0778	True	 	</text></passage><passage><infon key="file">T9.xml</infon><infon key="id">T9</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>47642</offset><text>The influence of classifier through multiple comparison of means - Tukey HSD, FWER=0.05.</text></passage><passage><infon key="file">T9.xml</infon><infon key="id">T9</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;&lt;italic&gt;G&lt;/italic&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Meandiff&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Lower&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Upper&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Reject&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSVC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0580&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0114&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1275&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NB&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0676&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0019&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1370&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SGD&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0568&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0127&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.1262&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSVC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NB&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0095&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0599&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0790&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSVC&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SGD&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0012&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0707&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0682&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NB&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SGD&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0108&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;‚àí0.0802&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0587&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;False&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>47731</offset><text>G1	G2	Meandiff	Lower	Upper	Reject	 	DT	LSVC	0.0580	‚àí0.0114	0.1275	False	 	DT	NB	0.0676	‚àí0.0019	0.1370	False	 	DT	SGD	0.0568	‚àí0.0127	0.1262	False	 	LSVC	NB	0.0095	‚àí0.0599	0.0790	False	 	LSVC	SGD	‚àí0.0012	‚àí0.0707	0.0682	False	 	NB	SGD	‚àí0.0108	‚àí0.0802	0.0587	False	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48011</offset><text>The most obvious conclusion we draw from Table 7 is that the only feature set that is lacking behind is the Doc2Vec (noted as D2V in Table 7). We note here that these types of features are numerical (non-semantic) ones and are computed as combinations of word embeddings. The features of the fasttext approach are based on the same principle, but in this case they are combined with a classifier taking advantage of a deep level architecture. Regarding the rest of the feature sets, we do not observe a significant difference among them, although with closer investigation and by doing the necessary combinations the best results are obtained with the GL2V feature set. As far as the classifiers are concerned, we see in Table 9 that no significant differences are detected. The Decision Trees seem to have a slightly worse performance than the other three classifiers, with the Naive Bayes being marginally better than the others.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>48943</offset><text>4.2. Crowd Intelligence vs. Deep Learning</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48985</offset><text>We see in the sentiment classification of tweets (Table 5) that a combination of the crowd lexicon with a Decision Tree classifier clearly outperforms, by 4%, the fasttext classifier that is based on features extracted through deep learning. This outcome is also qualitatively supported via a close inspection and comparison of Tables 10, 11. The recall value of the smaller of the three categories, i.e., the ‚ÄúHate‚Äù category, is significantly higher in the crowd lexicon (Table 10) than that of deep learning (Table 11). This result shows that the sentiment keywords proposed by the crowd help to create effective classification models even in cases where the dataset is unbalanced in terms of the samples' distribution per category. The fact that in tweets the crowd lexicon and Decision Tree combination achieves the best performance is also in full agreement with the findings of Tsapatsoulis and Djouvas.</text></passage><passage><infon key="file">T10.xml</infon><infon key="id">T10</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>49899</offset><text>Confusion matrix for the tweets in the best combination, i.e., the CLMV feature set - Decision Trees classifier.</text></passage><passage><infon key="file">T10.xml</infon><infon key="id">T10</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Category&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Anger&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Hate&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Recall&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;174&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6770&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hate&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;34&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;79&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5809&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;272&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.9379&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7733&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7453&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7727&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7687&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>50012</offset><text>Category	Anger	Hate	Neutral	Recall	 	Anger	174	26	57	0.6770	 	Hate	34	79	23	0.5809	 	Neutral	17	1	272	0.9379	 	Precision	0.7733	0.7453	0.7727	0.7687	 	</text></passage><passage><infon key="file">T11.xml</infon><infon key="id">T11</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>50164</offset><text>Confusion matrix for the fasttext classifier for the tweets.</text></passage><passage><infon key="file">T11.xml</infon><infon key="id">T11</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Category&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Anger&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Hate&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Recall&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;178&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6926&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hate&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;45&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;64&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4706&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;256&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.8828&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7036&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7711&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7378&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7291&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>50225</offset><text>Category	Anger	Hate	Neutral	Recall	 	Anger	178	15	64	0.6926	 	Hate	45	64	27	0.4706	 	Neutral	30	4	256	0.8828	 	Precision	0.7036	0.7711	0.7378	0.7291	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>50377</offset><text>The case of sentiment classification of Facebook comments (see Table 4) is more complicated. First, the best performance is achieved with the classic bag-of-words representation in combination with the Stochastic Gradient Descent learning algorithm (BoW-SGD), while a combination of crowd lexicon with Naive Bayes follows. Thus, we can conclude that the lengthy indices of terms lead to better classification performance. This, in turn, shows that keyword-based indices lead to effective sentiment classification models for Facebook comments, but the quality of the selected keywords is of primary importance. In the context of crowdtagging systems this can be facilitated by increasing the number of assessments per Facebook comment, combined with an intelligent tag selection scheme as suggested by Giannoulakis et al..</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>51199</offset><text>In Tables 12, 13, we see the confusion matrices of sentiment classification of Facebook comments of the BoW-SGD combination and fasttext. The low recall values of the smaller category, i.e., the ‚ÄúComparative‚Äù category, show an ineffective modeling case. The performance of the BoW-SGD combination, though, is much better than that of fasttext. Thus, better selection of keywords related to the ‚ÄúComparative‚Äù category could help on the overall improvement of sentiment classification of Facebook comments using indices of terms. On the contrary, this is very unlikely to happen with the case of features extracted through deep learning.</text></passage><passage><infon key="file">T12.xml</infon><infon key="id">T12</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>51843</offset><text>Confusion matrix for the facebook comments in the best combination, i.e., BoW feature set - Stochastic Gradient Descent classifier.</text></passage><passage><infon key="file">T12.xml</infon><infon key="id">T12</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Category&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Comparative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Negative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Positive&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Recall&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Comparative&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;56&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;28&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4341&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;252&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.8601&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;35&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;194&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;65&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5668&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;359&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.8799&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6364&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7590&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7608&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7771&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7573&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>51975</offset><text>Category	Comparative	Negative	Neutral	Positive	Recall	 	Comparative	56	28	27	18	0.4341	 	Negative	10	252	11	20	0.8601	 	Neutral	13	35	194	65	0.5668	 	Positive	9	17	23	359	0.8799	 	Precision	0.6364	0.7590	0.7608	0.7771	0.7573	 	</text></passage><passage><infon key="file">T13.xml</infon><infon key="id">T13</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>52203</offset><text>Confusion matrix for the fasttext classifier for the facebook comments.</text></passage><passage><infon key="file">T13.xml</infon><infon key="id">T13</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Category&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Comparative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Negative&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Neutral&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Positive&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Recall&lt;/bold&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Comparative&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;29&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;54&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2248&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;233&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;39&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7952&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;207&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6743&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;359&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.8799&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7073&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7259&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.6993&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7495&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;0.7282&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>52275</offset><text>Category	Comparative	Negative	Neutral	Positive	Recall	 	Comparative	29	54	37	9	0.2248	 	Negative	10	233	11	39	0.7952	 	Neutral	1	27	207	72	0.6743	 	Positive	1	7	41	359	0.8799	 	Precision	0.7073	0.7259	0.6993	0.7495	0.7282	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>52500</offset><text>The Word Embeddings representation itself, that is the case of Doc2Vec, leads to disappointing performance across all classifiers in the case of Facebook comments, while in tweets a decent performance is achieved only with the stochastic gradient descent learning algorithm. If we contrast this performance with the one achieved by the fastext, we conclude that word embeddings do not fit well with typical machine learning algorithms, but they require a specially designed classifier. Unfortunately, details regarding the classifier type used in fasttext are not publicly available.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>53084</offset><text>4.3. Tweets vs. Facebook Comments and the Role of Classifier Type</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>53150</offset><text>It has been already mentioned in section 3.3 that it is more difficult to extract keywords from Facebook comments than from tweets, probably due to the wide use of hashatgs, mentions and emoticons in the latter. This difficulty is reflected on the performance of the Decision Tree classifier as it can be observed in Tables 4, 5. In the case of tweets, the Decision Trees show excellent performance, even when combined with low- to medium-sized indices of terms as in the cases of CMFT, CL2V, and CLMV. On the contrary, for the Facebook comments a large index is required, i.e., the BoW case, to develop a fairly performing Decision Tree classifier. Thus, selection of appropriate keywords has a high impact on the quality of Decision Tree classifier that is learned. We emphasize Decision Trees here because, among all compared machine learning algorithms, this is the one that better fits human logic due to its rule-based nature.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54083</offset><text>Among the other classification models, it appears that those based on Naive Bayes are less affected by the differences between tweets and Facebook comments. The fasttext classifier shows also a stable performance in both short text types, but this is expected since the type of features it uses are basically character n-gram combinations and not tokens.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>54438</offset><text>4.4. How to Collect Crowd Intelligence in Crowd-Tagging Systems</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>54502</offset><text>Tag selection in crowd-tagging systems is typically accomplished through a voting scheme. This scheme makes use of simple rules, such as full agreement among annotators, majority voting or agreement of at least two annotators, or sophisticated weighting schemes, such as the HITS algorithm as suggested by Giannoulakis et al.. As already explained before, the CL2V and CLMV indices, used in the current work, are based on the two annotator agreement rule and on the majority voting rule respectively. The full agreement rule is rarely used in crowd-tagging systems with more than three assessments per object (herein short texts), whereas, the HITS algorithm is beneficial for situations where many assessments (typical more than 10) per object are available.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>55262</offset><text>We see in Table 5 that tweet classification models that are based on the CLMV index show slightly better performance than those constructed using the CL2V index. The difference, however, is not statistically significant. The situation is totally reversed in the case of sentiment classification of Facebook comments, as can be seen in Table 4. In this case, the models that are based on the CL2V index clearly outperform those constructed using the CLMV index. The difference in performance is quite large independently of the learning algorithm used, with the exception of Naive Bayes classifier where the difference is insignificant. Taking into account that the best-performing classification models for Facebook comments are based on the bag-of-words (BoW) method, which makes use of very large indices of terms, we can conclude that a more relaxed tag selection process is beneficial for crowd-tagging systems that are aiming for Facebook comment classification schemes.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>56238</offset><text>5. Conclusion and Further Work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>56269</offset><text>The main research question of the current study was to compare the effectiveness of features indicated by humans (i.e., keywords) with those extracted through deep learning in regard to sentiment classification of short texts and tweets and facebook comments in particular. We have empirically shown that the human-created indices, called crowd lexicon herein, that are based on crowdtagging, can be effectively used for training sentiment classification models for short texts and that those models are at least as effective as the ones that are developed through deep learning or even better. This result is in line with the findings of our previous study (Tsapatsoulis and Djouvas,) in which we showed that the tokens (unigrams), indicated by humans, lead to classification models with the highest performance regarding tweet classification. The models that use this feature set, consistently and independently of the machine learning algorithm adopted, surpass any other model in terms of tweet classification performance.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>57296</offset><text>We have also demonstrated that the peculiarities of tweets classification compared to Facebook comment classification, regarding the feature selection process, are not so small. Identifying sentiment-related keywords in Facebook comments is more difficult than in tweets. The presence of hashtags and mentions probably helps keyword selection in tweets. This, in turn, has a significant impact on the best-performing classification model that can be developed. Good keywords lead to effective Decision Tree classifiers, which in the case of sentiment classification of tweets outperform any other classification model. On the contrary, classification models based on Decision Trees show poor performance in the case of sentiment classification of Facebook comments. As an intermediated case, the deep learning classifiers (i.e., fasttext), which are basically combined character n-grams, perform similarly on tweets and Facebook comments.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>58235</offset><text>The way collective knowledge is gathered in crowd-tagging systems is another important issue. In contemporary crowdsourcing platforms the number of annotators can be as large as we want. This allows for different token selection strategies to form the human-created indices (crowd lexicon). A token can be added to the crowd lexicon in case it is suggested by all annotators, the majority of annotators, at least two annotators or through a more sophisticated approach such as the HITS algorithm (Giannoulakis et al.,). The cases of majority voting and two annotator agreement were investigated in this study. The full agreement case leads to very short indices of terms, especially whenever many annotators are involved in the crowdtagging process. Majority voting and two annotator agreement show similar performance in the case of tweets, whereas the two annotator agreement approach leads clearly to more effective indices of terms in the case of Facebook comments, showing once again that the process of identifying sentiment keywords is much more difficult in Facebook comments than in tweets.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>59335</offset><text>We did not investigate, in the current study, feature set combination since our primary aim was to compare crowd intelligence with deep learning-related features and not to develop the best classifier for sentiment classification of short texts. Some studies (Hamdan et al.,) claim improvement on the classification performance through feature sets' combination via properly selected weighting schemes. A combination of classification models through voting schemes is also another alternative that deserves further investigation. We will experiment with these issues in future research.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>59922</offset><text>As indicated in previous studies (Hamdan et al.,; Shirbhate and Deshmukh,), the existence of hashtags, emoticons and slang words in tweets favors the unigram method. Is this conclusion valid for other short text types such as Facebook comments? Or are bigram (or generally n-gram tokens)-based features that cope also with negation (Pak and Paroubek,) more effective? We are currently working on these research questions using the same approach as the one followed by Tsapatsoulis and Djouvas.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>60416</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>60437</offset><text>All authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_2</infon><offset>60564</offset><text>Conflict of Interest Statement</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>60595</offset><text>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>60768</offset><text>Funding. This work was supported by Cyprus University of Technology Open Access Author Fund.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>60861</offset><text>References</text></passage><passage><infon key="fpage">30</infon><infon key="lpage">38</infon><infon key="name_0">surname:Agarwal;given-names:A.</infon><infon key="name_1">surname:Xie;given-names:B.</infon><infon key="name_2">surname:Vovsha;given-names:I.</infon><infon key="name_3">surname:Rambow;given-names:O.</infon><infon key="name_4">surname:Passonneau;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Workshop on Languages in Social Media, LSM'11, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>60872</offset><text>Sentiment analysis of twitter data</text></passage><passage><infon key="fpage">45</infon><infon key="lpage">65</infon><infon key="name_0">surname:Aizawa;given-names:A.</infon><infon key="pub-id_doi">10.1016/s0306-4573(02)00021-3</infon><infon key="section_type">REF</infon><infon key="source">Inform. Process. Manage.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2003</infon><offset>60907</offset><text>An information-theoretic perspective of tf-idf measures</text></passage><passage><infon key="fpage">36</infon><infon key="lpage">44</infon><infon key="name_0">surname:Barbosa;given-names:L.</infon><infon key="name_1">surname:Feng;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING'10, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>60963</offset><text>Robust sentiment detection on twitter from biased and noisy data</text></passage><passage><infon key="fpage">90</infon><infon key="lpage">95</infon><infon key="name_0">surname:Borromeo;given-names:R. M.</infon><infon key="name_1">surname:Toyama;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 19th International Database Engineering &amp; Applications Symposium, IDEAS '15</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>61028</offset><text>Automatic vs. crowdsourced sentiment analysis</text></passage><passage><infon key="fpage">242</infon><infon key="lpage">262</infon><infon key="name_0">surname:Brabham;given-names:D. C.</infon><infon key="pub-id_doi">10.1177/1473095209104824</infon><infon key="section_type">REF</infon><infon key="source">Plan. Theory</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2009</infon><offset>61074</offset><text>Crowdsourcing the public participation process for planning projects</text></passage><passage><infon key="fpage">467</infon><infon key="lpage">479</infon><infon key="name_0">surname:Brown;given-names:P. F.</infon><infon key="name_1">surname:deSouza;given-names:P. V.</infon><infon key="name_2">surname:Mercer;given-names:R. L.</infon><infon key="name_3">surname:Pietra;given-names:V. J. D.</infon><infon key="name_4">surname:Lai;given-names:J. C.</infon><infon key="section_type">REF</infon><infon key="source">Comput. Linguist.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">1992</infon><offset>61143</offset><text>Class-based n-gram models of natural language</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Top 10 it &amp; Technology Buzzwords You Won't be Able to Avoid in 2019</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>61189</offset></passage><passage><infon key="fpage">25</infon><infon key="lpage">33</infon><infon key="name_0">surname:Cabrall;given-names:C. D.</infon><infon key="name_1">surname:Lu;given-names:Z.</infon><infon key="name_2">surname:Kyriakidis;given-names:M.</infon><infon key="name_3">surname:Manca;given-names:L.</infon><infon key="name_4">surname:Dijksterhuis;given-names:C.</infon><infon key="name_5">surname:Happee;given-names:R.</infon><infon key="pub-id_doi">10.1016/j.aap.2017.08.036</infon><infon key="pub-id_pmid">28911877</infon><infon key="section_type">REF</infon><infon key="source">Accid. Analy. Prevent.</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2018</infon><offset>61190</offset><text>Validity and reliability of naturalistic driving scene categorization judgments from crowdsourcing</text></passage><passage><infon key="fpage">32</infon><infon key="lpage">80</infon><infon key="name_0">surname:Cowie;given-names:R.</infon><infon key="name_1">surname:Douglas-Cowie;given-names:E.</infon><infon key="name_2">surname:Tsapatsoulis;given-names:N.</infon><infon key="name_3">surname:Votsis;given-names:G.</infon><infon key="name_4">surname:Kollias;given-names:S.</infon><infon key="name_5">surname:Fellenz;given-names:W.</infon><infon key="pub-id_doi">10.1109/79.911197</infon><infon key="section_type">REF</infon><infon key="source">IEEE Signal Process. Magaz.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2001</infon><offset>61289</offset><text>Emotion recognition in human-computer interaction</text></passage><passage><infon key="fpage">122</infon><infon key="lpage">135</infon><infon key="name_0">surname:De Mauro;given-names:A.</infon><infon key="name_1">surname:Greco;given-names:M.</infon><infon key="name_2">surname:Grimald;given-names:M.</infon><infon key="pub-id_doi">10.1108/LR-06-2015-0061</infon><infon key="section_type">REF</infon><infon key="source">Lib. Rev.</infon><infon key="type">ref</infon><infon key="volume">65</infon><infon key="year">2015</infon><offset>61339</offset><text>A formal definition of big data based on its essential features</text></passage><passage><infon key="fpage">215</infon><infon key="lpage">230</infon><infon key="name_0">surname:Denn;given-names:N.</infon><infon key="name_1">surname:Zuping;given-names:Z.</infon><infon key="name_2">surname:Damien;given-names:H.</infon><infon key="name_3">surname:Long;given-names:J.</infon><infon key="pub-id_doi">10.14257/ijmue.2015.10.4.21</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Mult. Ubiquit. Eng.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2015</infon><offset>61403</offset><text>A lexicon-based approach for hate speech detection</text></passage><passage><infon key="fpage">86</infon><infon key="lpage">96</infon><infon key="name_0">surname:Doan;given-names:A.</infon><infon key="name_1">surname:Ramakrishnan;given-names:R.</infon><infon key="name_2">surname:Halevy;given-names:A. Y.</infon><infon key="pub-id_doi">10.1145/1924421.1924442</infon><infon key="section_type">REF</infon><infon key="source">Commun. ACM</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2011</infon><offset>61454</offset><text>Crowdsourcing systems on the world-wide web</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Enhancing Security and Privacy in the Social Web</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>61498</offset></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Fan;given-names:W.</infon><infon key="name_1">surname:Bifet;given-names:A.</infon><infon key="pub-id_doi">10.21742/ijpccem.2014.1.1.01</infon><infon key="section_type">REF</infon><infon key="source">SIGKDD Exp. Newslett.</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2013</infon><offset>61499</offset><text>Mining big data: current status, and forecast to the future</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>61559</offset><text>Library for Fast Text Representation and Classification</text></passage><passage><infon key="section_type">REF</infon><infon key="source">We make ai work in the real world</infon><infon key="type">ref</infon><offset>61615</offset></passage><passage><infon key="fpage">80</infon><infon key="lpage">88</infon><infon key="name_0">surname:Finin;given-names:T.</infon><infon key="name_1">surname:Murnane;given-names:W.</infon><infon key="name_2">surname:Karandikar;given-names:A.</infon><infon key="name_3">surname:Keller;given-names:N.</infon><infon key="name_4">surname:Martineau;given-names:J.</infon><infon key="name_5">surname:Dredze;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, CSLDAMT '10, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>61616</offset><text>Annotating named entities in twitter data with crowdsourcing</text></passage><passage><infon key="fpage">491</infon><infon key="lpage">500</infon><infon key="name_0">surname:Founta;given-names:A.</infon><infon key="name_1">surname:Djouvas;given-names:C.</infon><infon key="name_2">surname:Chatzakou;given-names:D.</infon><infon key="name_3">surname:Leontiadis;given-names:I.</infon><infon key="name_4">surname:Blackburn;given-names:J.</infon><infon key="name_5">surname:Stringhini;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 12th International Conference on Web and Social Media, ICWSM 2018.</infon><infon key="type">ref</infon><infon key="volume">CA</infon><infon key="year">2018</infon><offset>61677</offset><text>Large scale crowdsourcing and characterization of twitter abusive behavior</text></passage><passage><infon key="fpage">304</infon><infon key="lpage">313</infon><infon key="name_0">surname:Giannoulakis;given-names:S.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2nd INNS Conference on Big Data 2016, INNS Big Data 2016</infon><infon key="type">ref</infon><infon key="year">2016a</infon><offset>61752</offset><text>Defining and identifying stophashtags in instagram</text></passage><passage><infon key="fpage">114</infon><infon key="lpage">129</infon><infon key="name_0">surname:Giannoulakis;given-names:S.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="pub-id_doi">10.1016/j.jides.2016.10.001</infon><infon key="section_type">REF</infon><infon key="source">J. Innov. Digital Ecosyst.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2016b</infon><offset>61803</offset><text>Evaluating the descriptive power of instagram hashtags</text></passage><passage><infon key="fpage">89</infon><infon key="lpage">94</infon><infon key="name_0">surname:Giannoulakis;given-names:S.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="name_2">surname:Ntalianis;given-names:K. S.</infon><infon key="section_type">REF</infon><infon key="source">DASC/PiCom/DataCom/CyberSciTech</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>61858</offset><text>Identifying image tags from instagram hashtags using the HITS algorithm</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">14</infon><infon key="name_0">surname:Giuffrida;given-names:M.</infon><infon key="name_1">surname:Chen;given-names:F.</infon><infon key="name_2">surname:Scharr;given-names:H.</infon><infon key="name_3">surname:Tsaftaris;given-names:S.</infon><infon key="pub-id_doi">10.1186/s13007-018-0278-7</infon><infon key="pub-id_pmid">29321806</infon><infon key="section_type">REF</infon><infon key="source">Plant Methods</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2018</infon><offset>61930</offset><text>Citizen crowds and experts: observer variability in image-based plant phenotyping</text></passage><passage><infon key="fpage">568</infon><infon key="lpage">573</infon><infon key="name_0">surname:Hamdan;given-names:H.</infon><infon key="name_1">surname:Bellot;given-names:P.</infon><infon key="name_2">surname:Bechet;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 9th International Workshop on Semantic Evaluation, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>62012</offset><text>lsislif: Feature extraction and label weighting for sentiment analysis in twitter</text></passage><passage><infon key="fpage">98</infon><infon key="lpage">115</infon><infon key="name_0">surname:Hashem;given-names:I. A. T.</infon><infon key="name_1">surname:Yaqoob;given-names:I.</infon><infon key="name_2">surname:Anuar;given-names:N. B.</infon><infon key="name_3">surname:Mokhtar;given-names:S.</infon><infon key="name_4">surname:Gani;given-names:A.</infon><infon key="name_5">surname:Ullah Khan;given-names:S.</infon><infon key="pub-id_doi">10.1016/j.is.2014.07.006</infon><infon key="section_type">REF</infon><infon key="source">Inform. Syst.</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2015</infon><offset>62094</offset><text>The rise of ‚Äúbig data‚Äù on cloud computing</text></passage><passage><infon key="section_type">REF</infon><infon key="source">The world's largest structured repository of regionalized, multilingual hate speech</infon><infon key="type">ref</infon><offset>62140</offset></passage><passage><infon key="name_0">surname:Howe;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>62141</offset></passage><passage><infon key="fpage">27</infon><infon key="lpage">35</infon><infon key="name_0">surname:Hsueh;given-names:P.-Y.</infon><infon key="name_1">surname:Melville;given-names:P.</infon><infon key="name_2">surname:Sindhwani;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, HLT '09, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>62142</offset><text>Data quality from crowdsourcing: a study of annotation selection criteria</text></passage><passage><infon key="fpage">427</infon><infon key="lpage">431</infon><infon key="name_0">surname:Joulin;given-names:A.</infon><infon key="name_1">surname:Grave;given-names:E.</infon><infon key="name_2">surname:Bojanowski;given-names:P.</infon><infon key="name_3">surname:Mikolov;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>62216</offset><text>Bag of tricks for efficient text classification</text></passage><passage><infon key="fpage">443</infon><infon key="lpage">450</infon><infon key="name_0">surname:Karpouzis;given-names:K.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="name_2">surname:Kollias;given-names:S. D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of Human Vision and Electronic Imaging, SPIE, Vol. 3959</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>62264</offset><text>Moving to continuous facial expression space using the MPEG-4 facial definition parameter (FDP) set</text></passage><passage><infon key="fpage">5</infon><infon key="lpage">26</infon><infon key="name_0">surname:Kleemann;given-names:F.</infon><infon key="name_1">surname:Voss;given-names:G.</infon><infon key="name_2">surname:Rieder;given-names:K.</infon><infon key="pub-id_doi">10.17877/DE290R-12790</infon><infon key="section_type">REF</infon><infon key="source">Sci. Technol. Innovat. Stud.</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2008</infon><offset>62364</offset><text>Un(der)paid innovators: the commercial utilization of consumer work through crowdsourcing</text></passage><passage><infon key="fpage">293</infon><infon key="lpage">312</infon><infon key="name_0">surname:Layton;given-names:R.</infon><infon key="name_1">surname:Watters;given-names:P.</infon><infon key="name_2">surname:Dazeley;given-names:R.</infon><infon key="pub-id_doi">10.1017/S1351324911000180</infon><infon key="section_type">REF</infon><infon key="source">Nat. Langu. Eng.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2011</infon><offset>62454</offset><text>Recentred local profiles for authorship attribution</text></passage><passage><infon key="name_0">surname:Le;given-names:Q.</infon><infon key="name_1">surname:Mikolov;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st International Conference on International Conference on Machine Learning - Vol. 32, ICML'14, JMLR.org</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>62506</offset><text>Distributed representations of sentences and documents</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Hapax Legomenon</infon><infon key="type">ref</infon><offset>62561</offset></passage><passage><infon key="fpage">142</infon><infon key="lpage">150</infon><infon key="name_0">surname:Maas;given-names:A. L.</infon><infon key="name_1">surname:Daly;given-names:R. E.</infon><infon key="name_2">surname:Pham;given-names:P. T.</infon><infon key="name_3">surname:Huang;given-names:D.</infon><infon key="name_4">surname:Ng;given-names:A. Y.</infon><infon key="name_5">surname:Potts;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Vol. 1, HLT '11, Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>62562</offset><text>Learning word vectors for sentiment analysis</text></passage><passage><infon key="fpage">111</infon><infon key="lpage">120</infon><infon key="name_0">surname:Mac Kim;given-names:S.</infon><infon key="name_1">surname:Calvo;given-names:R. A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 3rd International Conference on Educational Data Mining</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>62607</offset><text>Sentiment analysis in student experiences of learning</text></passage><passage><infon key="fpage">975</infon><infon key="lpage">978</infon><infon key="name_0">surname:Machedon;given-names:R.</infon><infon key="name_1">surname:Rand;given-names:W.</infon><infon key="name_2">surname:Joshi;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2013 International Conference on Social Computing, SOCIALCOM '13</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>62661</offset><text>Automatic crowdsourcing-based classification of marketing messaging on twitter</text></passage><passage><infon key="fpage">438</infon><infon key="lpage">445</infon><infon key="name_0">surname:Maier-Hein;given-names:L.</infon><infon key="name_1">surname:Mersmann;given-names:S.</infon><infon key="name_2">surname:Kondermann;given-names:D.</infon><infon key="name_3">surname:Bodenstedt;given-names:S.</infon><infon key="name_4">surname:Sanchez;given-names:A.</infon><infon key="name_5">surname:Stock;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 17th International Conference on Medical Image Computing and Computer-Assisted Intervention</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>62740</offset><text>Can masses of non-experts train highly accurate image classifiers? a crowdsourcing approach to instrument segmentation in laparoscopic images</text></passage><passage><infon key="fpage">3111</infon><infon key="lpage">3119</infon><infon key="name_0">surname:Mikolov;given-names:T.</infon><infon key="name_1">surname:Sutskever;given-names:I.</infon><infon key="name_2">surname:Chen;given-names:K.</infon><infon key="name_3">surname:Corrado;given-names:G.</infon><infon key="name_4">surname:Dean;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 26th International Conference on Neural Information Processing Systems - Vol. 2, NIPS'13</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>62882</offset><text>Distributed representations of words and phrases and their compositionality</text></passage><passage><infon key="fpage">6</infon><infon key="name_0">surname:Mitry;given-names:D.</infon><infon key="name_1">surname:Zutis;given-names:K.</infon><infon key="name_2">surname:Dhillon;given-names:B.</infon><infon key="name_3">surname:Peto;given-names:T.</infon><infon key="name_4">surname:Hayat;given-names:S.</infon><infon key="name_5">surname:Khaw;given-names:K.-T.</infon><infon key="pub-id_doi">10.1167/tvst.5.5.6</infon><infon key="pub-id_pmid">27668130</infon><infon key="section_type">REF</infon><infon key="source">Trans. Vis. Sci. Technol.</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2016</infon><offset>62958</offset><text>The accuracy and reliability of crowdsource annotations of digital retinal images</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Amazon Mechanical Turk</infon><infon key="type">ref</infon><offset>63040</offset></passage><passage><infon key="fpage">194</infon><infon key="lpage">201</infon><infon key="name_0">surname:Narayanan;given-names:V.</infon><infon key="name_1">surname:Arora;given-names:I.</infon><infon key="name_2">surname:Bhatia;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 14th International Conference on Intelligent Data Engineering and Automated Learning ‚Äî IDEAL 2013 - Vol. 8206</infon><infon key="type">ref</infon><infon key="volume">Inc</infon><infon key="year">2013</infon><offset>63041</offset><text>Fast and accurate sentiment classification using an enhanced naive bayes model</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Natural Language Toolkit</infon><infon key="type">ref</infon><offset>63120</offset></passage><passage><infon key="fpage">534</infon><infon key="lpage">539</infon><infon key="name_0">surname:Ntalianis;given-names:K.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 9th IEEE International Conference on Cyber, Physical, and Social Computing, CPSCom'2016</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>63121</offset><text>Wall-content selection in social media: a relevance feedback scheme based on explicit crowdsourcing</text></passage><passage><infon key="fpage">397</infon><infon key="lpage">421</infon><infon key="name_0">surname:Ntalianis;given-names:K.</infon><infon key="name_1">surname:Tsapatsoulis;given-names:N.</infon><infon key="name_2">surname:Doulamis;given-names:A.</infon><infon key="name_3">surname:Matsatsinis;given-names:N.</infon><infon key="pub-id_doi">10.1007/s11042-012-0995-2</infon><infon key="section_type">REF</infon><infon key="source">Mult. Tools Appl.</infon><infon key="type">ref</infon><infon key="volume">69</infon><infon key="year">2014</infon><offset>63221</offset><text>Automatic annotation of image databases based on implicit crowdsourcing, visual concept modeling and evolution</text></passage><passage><infon key="fpage">1320</infon><infon key="lpage">1326</infon><infon key="name_0">surname:Pak;given-names:A.</infon><infon key="name_1">surname:Paroubek;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC'10</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>63332</offset><text>Twitter as a corpus for sentiment analysis and opinion mining</text></passage><passage><infon key="fpage">2825</infon><infon key="lpage">2830</infon><infon key="name_0">surname:Pedregosa;given-names:F.</infon><infon key="name_1">surname:Varoquaux;given-names:G.</infon><infon key="name_2">surname:Gramfort;given-names:A.</infon><infon key="name_3">surname:Michel;given-names:V.</infon><infon key="name_4">surname:Thirion;given-names:B.</infon><infon key="name_5">surname:Grisel;given-names:O.</infon><infon key="section_type">REF</infon><infon key="source">J. Mach. Learn. Res.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2011</infon><offset>63394</offset><text>Scikit-learn: machine learning in python</text></passage><passage><infon key="fpage">299</infon><infon key="lpage">304</infon><infon key="name_0">surname:Prusa;given-names:J. D.</infon><infon key="name_1">surname:Khoshgoftaar;given-names:T. M.</infon><infon key="name_2">surname:Dittman;given-names:D. J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 28th International Florida Artificial Intelligence Research Society Conference</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>63435</offset><text>Impact of feature selection techniques for tweet sentiment classification</text></passage><passage><infon key="fpage">613</infon><infon key="lpage">620</infon><infon key="name_0">surname:Salton;given-names:G.</infon><infon key="name_1">surname:Wong;given-names:A.</infon><infon key="name_2">surname:Yang;given-names:C. S.</infon><infon key="section_type">REF</infon><infon key="source">Commun. ACM</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">1975</infon><offset>63509</offset><text>A vector space model for automatic indexing</text></passage><passage><infon key="fpage">2183</infon><infon key="lpage">2189</infon><infon key="name_0">surname:Shirbhate;given-names:A. G.</infon><infon key="name_1">surname:Deshmukh;given-names:S. N.</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Sci. Res.</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2016</infon><offset>63553</offset><text>Feature extraction for sentiment classification on twitter data</text></passage><passage><infon key="fpage">1631</infon><infon key="lpage">1642</infon><infon key="name_0">surname:Socher;given-names:R.</infon><infon key="name_1">surname:Perelygin;given-names:A.</infon><infon key="name_2">surname:Wu;given-names:J. Y.</infon><infon key="name_3">surname:Chuang;given-names:J.</infon><infon key="name_4">surname:Manning;given-names:C. D.</infon><infon key="name_5">surname:Ng;given-names:A. Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of 2013 Conference on Empirical Methods in Natural Language Processing</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>63617</offset><text>Recursive deep models for semantic compositionality over a sentiment treebank</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">146</infon><infon key="name_0">surname:Stavrianou;given-names:A.</infon><infon key="name_1">surname:Brun;given-names:C.</infon><infon key="name_2">surname:Silander;given-names:T.</infon><infon key="name_3">surname:Roux;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 1st International Conference on Interactions Between Data Mining and Natural Language Processing - Vol. 1202 DMNLP'14</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>63695</offset><text>Nlp-based feature extraction for automated tweet classification</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Stop Words</infon><infon key="type">ref</infon><offset>63759</offset></passage><passage><infon key="name_0">surname:Surowiecki;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">The Wisdom of Crowds</infon><infon key="type">ref</infon><infon key="year">2005</infon><offset>63760</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Textblob: Simplified Text Processing</infon><infon key="type">ref</infon><offset>63761</offset></passage><passage><infon key="fpage">53</infon><infon key="lpage">58</infon><infon key="name_0">surname:Tsapatsoulis;given-names:N.</infon><infon key="name_1">surname:Djouvas;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Semantic and Social Media Adaptation and Personalization (SMAP), 2017 12th In'l Workshop on</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>63762</offset><text>Feature extraction for tweet classification: Do the humans perform better?</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Iterative Tasks on Mechanical Turk</infon><infon key="type">ref</infon><offset>63837</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">Tweet Tokenize Package</infon><infon key="type">ref</infon><offset>63838</offset></passage><passage><infon key="section_type">REF</infon><infon key="source">utest</infon><infon key="type">ref</infon><offset>63839</offset></passage><passage><infon key="fpage">686</infon><infon key="lpage">692</infon><infon key="name_0">surname:Vukovic;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">2009 Congress on Services - I</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>63840</offset><text>Crowdsourcing for enterprises</text></passage><passage><infon key="fpage">1231</infon><infon key="lpage">1247</infon><infon key="name_0">surname:Yaqoob;given-names:I.</infon><infon key="name_1">surname:Hashem;given-names:I. A. T.</infon><infon key="name_2">surname:Gani;given-names:A.</infon><infon key="name_3">surname:Mokhtar;given-names:S.</infon><infon key="name_4">surname:Ahmed;given-names:E.</infon><infon key="name_5">surname:Anuar;given-names:N. B.</infon><infon key="pub-id_doi">10.1016/j.ijinfomgt.2016.07.009</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Inform. Manage.</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2016</infon><offset>63870</offset><text>Big data: from beginning to future</text></passage><passage><infon key="name_0">surname:Zafarani;given-names:R.</infon><infon key="name_1">surname:Abbasi;given-names:M. A.</infon><infon key="name_2">surname:Liu;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Social Media Mining: An Introduction</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>63905</offset></passage></document></collection>
