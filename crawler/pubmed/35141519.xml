<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220217</date><key>pmc.key</key><document><id>8819179</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/fdata.2021.736709</infon><infon key="article-id_pmc">8819179</infon><infon key="article-id_pmid">35141519</infon><infon key="elocation-id">736709</infon><infon key="kwd">negation multimodal models transformers multimodal encoders visual dialogue analysis</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Testoni;given-names:Alberto</infon><infon key="name_1">surname:Greco;given-names:Claudio</infon><infon key="name_2">surname:Bernardi;given-names:Raffaella</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">4</infon><infon key="year">2021</infon><offset>0</offset><text>Artificial Intelligence Models Do Not Ground Negation, Humans Do. GuessWhat?! Dialogues as a Case Study</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>104</offset><text>Negation is widely present in human communication, yet it is largely neglected in the research on conversational agents based on neural network architectures. Cognitive studies show that a supportive visual context makes the processing of negation easier. We take GuessWhat?!, a referential visually grounded guessing game, as test-bed and evaluate to which extent guessers based on pre-trained language models profit from negatively answered polar questions. Moreover, to get a better grasp of models' results, we select a controlled sample of games and run a crowdsourcing experiment with subjects. We evaluate models and humans against the same settings and use the comparison to better interpret the models' results. We show that while humans profit from negatively answered questions to solve the task, models struggle in grounding negation, and some of them barely use it; however, when the language signal is poorly informative, visual features help encoding the negative information. Finally, the experiments with human subjects put us in the position of comparing humans and models' predictions and get a grasp about which models make errors that are more human-like and as such more plausible.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1308</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1324</offset><text>Negation is often neglected by computational studies of natural language understanding, in particular when using the successful neural network models. Very recently, a series of work have highlighted that negation is under-represented in existing natural language inference benchmarks (Hossain et al.,) and that Pretrained Language Models have difficulty distinguishing a sentence from its negated form in fill-in-the-blank tests (Kassner and Schütze,). This weakness of Language Models could have a strong impact on their success in real-life applications. For instance, Hossain et al. show that the lack of a proper understanding of negation is an important source of error in machine translation and similarly, it would impact the quality of other applications based on natural language understanding, such as text summarization or personal assistants for health care or other uses. A recent contribution of AI to the society is the development of visual dialogue systems built on Pretrained Language Models. Clearly, they are an important tool for instance as personal assistants of visually impaired people (Gurari et al.,), but again their impressive achievements would be vanished if they fail to distinguish negative and affirmative information.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2579</offset><text>Admittedly, modeling negation is an ambitious goal, and even humans have a harder time understanding negative sentences than positive ones (Clark and Chase,; Carpenter and Just,). However, it has been shown that the presence of supportive context mitigates the processing cost of negation. In particular, this happens within dialogues (Dale and Duran,), and when a visual context is given (Nordmeyer and Frank,). Based on these findings, we argue that Visual Dialogues are a good starting point for making progress toward the ambitious but crucial goal of developing neural network models that can understand negation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3198</offset><text>Visual Dialogues have a long tradition (e.g., Anderson et al.,). They can be chit-chat (e.g., Das et al.,) or task-oriented (e.g., de Vries et al.,; Haber et al.,; Ilinykh et al.,). Task-oriented dialogues are easier to evaluate since their performance can be judged in terms of their task-success, hence we focus on this type of dialogues which can be further divided as following: the two agents can have access to the same visual information (de Vries et al.,), share only part of it Haber et al. and Ilinykh et al. or only one agent has access to the image (Chattopadhyay et al.,). Moreover, dialogues can be symmetric (Haber et al.,), or asymetric, with one agent asking questions and the other answering it de Vries et al., Das et al., and Chattopadhyay et al.. Finally, the dialogue turns can contain different speech acts (Haber et al.,; Ilinykh et al.,) or only question anwer pairs (Chattopadhyay et al.,; Das et al.,; de Vries et al.,). The differences between the various type of dialogues are illustrated in Figure 1. As we can see symmetric games with partially observable data (PhotoBook and Meet up! Haber et al.,; Ilinykh et al.,) sollicitate more complex exchanges than symmetric ones (Visual Dialogue, GuessWhich—the referentional game built from it Chattopadhyay et al.,; Das et al.,, and GuessWhat?! de Vries et al., —the latter is illustrated in Figure 2). Given the difficulty negation poses to models, we take the scenario which is less complex from a dialogue perspective and in which questions are always grounded in the image: the one in which agents have access to the same visual information, only one agent can ask questions, and the questions are all of the same type. Hence, we take GuessWhat?! as case-study and focus on the referential grounded guessing task: a Guesser receives an asymmetric dialogue, consisting of Yes/No-questions over an image, a list of candidates and has to guess the target object the dialogue is about. In this setting, negation is heavily present as the answer to a binary question. As such it functions as a pointer to the alternative set of the negated expression; in other words it should be interpreted as pointing to the set of all the candidates objects which do not have the queried property.</text></passage><passage><infon key="file">fdata-04-736709-g0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5462</offset><text>Examples of dialogues from two asymmetric and partially observable visual dialogue data [PhotoBook and Meet Up! (Haber et al.,; Ilinykh et al.,)] and a symmetric visual dialogue in which the answerer sees the image and the questioner does not see it (Chattopadhyay et al.,; Das et al.,). For all datasets, we selected exchanges containing negation, the focus of our study.</text></passage><passage><infon key="file">fdata-04-736709-g0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5835</offset><text>Two samples of GuessWhat?! human dialogues ending with a positive (left) and a negative (right) turn.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5937</offset><text>GuessWhat?! dialogues have been collected by letting two humans play the game. As illustrated in Figure 2, such dialogues are quite simple: a sequence of rather short questions answered by “Yes” or “No” containing on average 30.1 (SD ± 17.6) tokens per dialogue. The dialogue length differs across the games since the questioner decides when he/she can stop asking questions and is ready to guess the target. To evaluate the extent models understand negatively answered questions, we take the human dialogues as input to the guesser. We select successful games, in other words those dialogues in which human players have succeeded in guessing the target object at the end of the game. We conjecture that within these dialogues a crucial role is played by the last turn whose role is to create a singleton alternative set and that this goal is achieved differently when the question is answered positively or negatively. In the former case, the question tends to almost fully describe the target object, whereas in the latter case it conclusively identifies the target object by excluding those candidates which most likely are not the target (Figure 2). To validate this conjecture, we run an online experiment with humans which set the ground for better evaluating the results obtained by models. We let humans and computational models perform the same task on the same controlled sample set. We compare encoders with respect to the architecture (Recurrent Neural Networks vs. Transformers), the input modalities (only language vs. language and vision) and the model background knowledge (trained from scratch vs. pre-trained and then fine-tuned on the downstream task). Our analysis shows that:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7643</offset><text>While humans profit from negatively answered questions to solve the task, models struggle in grounding negation, and some of them barely use it;</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7788</offset><text>In No-turns, when the language signal is poorly informative, visual features help in processing the QA pair.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7897</offset><text>We hope that these results will stimulate more work on the processing of (grounded) negation and that the data we collected through our online experiment and its annotation will be a valuable contribution to such research direction.1</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>8131</offset><text>2. Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>8147</offset><text>2.1. Scrutinizing Visual Dialogue Encoding</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8190</offset><text>Sankar et al. study how neural dialogue models encode the dialogue history when generating the next utterance. They show that neither recurrent nor transformer based architectures are sensitive to perturbations in the dialogue history and that Transformers are less sensitive than recurrent models to perturbations that scramble the conversational structure; furthermore, their findings suggest that models enhanced with attention mechanisms use more information from the dialogue history than their vanilla counterpart. We follow them in the choice of the architectures we compare, but we change the focus of the analysis by studying whether the polarity of the answer (Yes vs. No) affects the encoding of the information provided by the question-answer pair.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8951</offset><text>Kaushik and Lipton show that in many reading comprehension datasets, that presumably require the combination of both questions and passages to predict the correct answer, models can achieve quite a good accuracy by using only part of the information provided. Similarly to this work, we investigate how much models use the questions as well as the answers, provided by the Oracle, to select the target object among the possible candidates.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9391</offset><text>Interesting exploratory analysis has been carried out to understand Visual Question Answering (VQA) systems and highlight their strengths and weaknesses (e.g., Johnson et al.,; Kafle and Kanan,; Shekhar et al.,; Suhr et al.,). Less is known about how well grounded conversational models encode the dialogue history and in particular, negatively answered questions. Greco et al. shows that pre-trained models transformers detect salient information in the dialogue history independently of the position in which it occurs. We build on their study to dive into how encoders represent positively vs. negatively answered questions within a visual dialogue.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>10044</offset><text>2.2. SOTA LSTM-Based Models on GuessWhat?!</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10087</offset><text>After the introduction of the supervised baseline model (de Vries et al.,), several models have been proposed. Zhao and Tresp has used attention mechanisms based on Memory Networks (Sukhbaatar et al.,) and (Shekhar et al.,) has proposed a model that is jointly trained to ask questions and guess the target. Building on the supervised learning step, all these models have been further trained with either some form of reinforcement learning (Zhang et al.,; Zhao and Tresp,; Yang et al.,; Pang and Wang,) or cooperative learning (Shekhar et al.,; Pang and Wang,); this two-step process has been shown to reach higher task success than the supervised approach. Since our focus is on the Guesser and we are evaluating it on human dialogues, we will compare models that have undergone only the supervised training step.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>10903</offset><text>2.3. Transformer-Based Models</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10933</offset><text>The last years have seen the increasing popularity of transformer-based models pre-trained on several tasks to learn task-agnostic multimodal representations (Chen et al.,; Li et al.,; Lu et al.,; Tan and Bansal,; Su et al.,). ViLBERT (Lu et al.,) has been recently extended by means of multi-task training involving 12 datasets which include GuessWhat?! (Lu et al.,) and has been fine-tuned to play the Answerer of VisDial (Murahari et al.,). Greco et al. have adapted the pre-trained transformer, LXMERT (Tan and Bansal,), to the GuessWhat?! guessing task. Given the high accuracy achieved, we choose LXMERT as pre-trained transformer.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11571</offset><text>2.4. Visually Grounded Negation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11603</offset><text>Negation was already listed by Winograd among the linguistic phenomena a Grounded Conversational System should be able to interpret (Winograd,). Significant progress has been obtained in the development of conversational systems based on neural network architecture; however, little is known about how these models interpret negation. Nordmeyer and Frank show that processing negation can be easier for humans if a visual context creates pragmatic expectations that motivate its use. However, it is unknown whether this holds for multimodal models. Suhr et al. show that SOTA models tested on visual reasoning often fail in properly grounding negative utterances. Gokhale et al. show that models have harder time in answering visual questions containing negation. Both studies look at negation as a logical operation, it reverses the truth value of the negated utterance. However, Oaksford show that humans often use negation not as a logical operator but rather as a way to create an alternative set of the negated expressions. This is exactly the role of the negative answer in the GuessWhat?! game. We are not aware of any study on Visual Dialogue that have tackled this issue.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>12784</offset><text>3. Task and Dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12804</offset><text>In this paper, we run an in-depth analysis on how models integrate Yes/No answers into the question to solve the GuessWhat?! guessing task. We run a comparative analysis to evaluate the role of language priors and visual grounding, and we run a crowdsourcing experiment with subjects on a controlled sample of the games. Using a controlled sample set and knowing about humans' performance give us a better way to interpret the results obtained by the models on the full test set. Below we describe the task and training/validation set and the test sets we use through out the paper.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13387</offset><text>3.1. Task</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13397</offset><text>GuessWhat?! (de Vries et al.,) is an asymmetrical game involving two participants who see a real-world image. One of the participants (the Oracle) is assigned a target object in the image and the other participant (the Questioner) has to guess it by asking Yes/No questions to the Oracle. de Vries et al. collected a human dialogue dataset via Amazon Mechanical Turk. Our focus is on multimodal encoding, hence we focus on the guessing task: given a human dialogue, consisting of Yes/No questions and their answers, an image and a list of possible candidate objects, the agent has to select the object the dialogue is about. Greco et al. have shown that human dialogue length is a good proxy of the guessing task difficulty2, where length is measured in terms of number of turns; for instance in Figure 2 the dialogue on the left is of length 5 (it consists of five turns) whereas the one on the right is of length 3. In the following, we use “turn” to refer to the position (of just the question or the answer or of the QA pair) within the dialogue.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>14452</offset><text>3.2. Full Dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14470</offset><text>The GuessWhat?! dataset contains 155K English dialogues about approximately 66K different images from the MS-COCO dataset (Lin et al.,). We evaluate models using human dialogues, selecting only the games on which human players have succeeded finding the target and contain at most 10 turns (total number of dialogues used: 90K in training and around 18K both in validation and testing). Dialogues contain on average 4.5 Question-Answer (QA) pairs, the vocabulary consists of 4,901 words, and games have on average 8 candidates.3 The answer distribution is the following: 52.2% No, 45.6% Yes, and 2.2% N/A (not applicable). We divide the full test set into games whose dialogue ends in a Yes- vs. in a No-turn and obtain the Yes-set and No-set, whose statistics are reported in Table 1. As we can see, the two sets contain dialogues of the same average length, and similar number of candidate objects, hence their games are expected to be of similar difficulty. The last turns in these two subsets are expected to play a rather different role (as illustrated by the example in Figure 2): a Yes-question in the last turn is rather informative on its own, whereas a last turn answered negatively quite often needs the information gathered in the previous turns to be informative. On the other hand, we should note that last turns containing a negative answer are expected to be rather informative together with the dialogue history to guess the target. Hence, they are an interesting test-bed for our research question.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>15987</offset><text>Statistics on the full test set and on the Controlled test set; both divided into the Yes- (resp. No-) subsets obtained by selecting only dialogues with a positively (resp. negatively) answered question in the last turn.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Nr. Games&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Av. Dialogue length&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Av. nr candidates&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Full test set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18,840&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16,366&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2,350&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Controlled sample&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;300&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;150&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;150&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>16208</offset><text>	Nr. Games	Av. Dialogue length	Av. nr candidates	 	Full test set	18,840	4.5	8	 	Yes-set	16,366	4.5	8	 	No-set	2,350	4.5	7.8	 	Controlled sample	300	4.5	6.1	 	Yes-set	150	4.5	6.1	 	No-set	150	4.3	6.1	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>16410</offset><text>3.3. Controlled Sample</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16433</offset><text>To compare models' results against humans' ones, we run an annotation experiment on a sample of games we carefully select. We consider dialogues consisting of 4- and 6-turns, and select those containing an equal number of Yes/No answers. Moreover, to control for the level of difficulty of the game, we select only games which have a maximum of 10 candidates. We obtain a subset with a balanced overall distribution of the two types of polar answers; it contains 1,491 games, of which 1,327 (resp. 164) contain in the last turn a question answered positively (resp. negatively). From these games, we randomly select 300 games (image, target) from the Yes- and No- test sets (150 each). In this way, we obtain a subset balanced also with respect of the polarity of the last question. We believe games in this sample set are equally difficult, considering the criteria discussed above.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>17317</offset><text>4. Models</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17327</offset><text>Following, Greco et al., all the guesser models we evaluate share the skeleton illustrated in Figure 3: an encoder paired with a Guesser module. For the latter, all models use the module proposed in de Vries et al.. Candidate objects are represented by the embeddings obtained via a Multi-Layer Perceptron (MLP) starting from the category and spatial coordinates of each candidate object. The representations so obtained are used to compute dot products with the hidden dialogue state produced by an encoder. The scores of each candidate object are given to a softmax classifier to choose the object with the highest probability. The Guesser is trained in a supervised learning paradigm, receiving the complete human dialogue history at once. The models we compare differ in how the hidden dialogue state is computed. We compare LSTM vs. Transformers when receiving only the language input (Language-only, henceforth, Blind models) or both the language and the visual input (Multimodal, henceforth, MM models).</text></passage><passage><infon key="file">fdata-04-736709-g0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18338</offset><text>Shared Encoder-Guesser skeleton. The Guesser receives the category labels (e.g., “bottle”) and the spatial coordinates (pos) of each candidate object. Multimodal encoders receive both the image and the dialogue history, whereas blind models receive only the latter.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18608</offset><text>4.1. Language-Only Encoders</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>18636</offset><text>4.1.1. LSTM</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18648</offset><text>As in de Vries et al., the representations of the candidates are fused with the last hidden state obtained by an LSTM which processes only the dialogue history.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>18809</offset><text>4.1.2. RoBERTa</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18824</offset><text>In the architecture of the model described above, we replace the LSTM with a robustly-optimized version of BERT (Devlin et al.,), RoBERTa, a SOTA universal transformer-based encoder introduced in Liu et al..4 We use RoBERTaBASE which has been pre-trained on 160GB of English text trained for 500K steps to perform masked language modeling. RoBERTa was pretrained on several text corpora containing rather long utterances: BookCorpus (Zhu et al.,)+ English Wikipedia (as the original BERT model), CC-NEWS (Nagel,), OPENWEBTEXT (Gokaslan and Cohen,), and STORIES (Trinh and Le,). It has 12 self-attention layers with 12 heads each. It uses three special tokens, namely CLS, which is taken to be the representation of the given sequence, SEP, which separates sequences, and EOS, which denotes the end of the input. We give the output corresponding to the CLS token to a linear layer and a tanh activation function to obtain the hidden state which is given to the Guesser. To study the impact of the pre-training phase, we have compared the publicly available pre-trained model, which we fine-tuned on GuessWhat?! (RoBERTa), against its counterpart trained from scratch only on the game (RoBERTa-S).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>20020</offset><text>4.2. Multimodal Encoders</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>20045</offset><text>4.2.1. V-LSTM</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20059</offset><text>We enhance the LSTM model described above with the visual modality by concatenating the linguistic and visual representation and scaling its result with an MLP; the result is passed through a linear layer and a tanh activation function to obtain the hidden state which is used as input for the Guesser module. We use a frozen ResNet-152 pre-trained on ImageNet (He et al.,) to extract the visual vectors.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>20464</offset><text>4.2.2. LXMERT</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20478</offset><text>To evaluate the performance of a universal multimodal encoder, we employ LXMERT (Learning Cross-Modality Encoder Representations from Transformers) (Tan and Bansal,). It represents an image by the set of position-aware object embeddings for the 36 most salient regions detected by a Faster R-CNN and it processes the text input by position-aware randomly-initialized word embeddings. LXMERT is pre-trained on datasets containing rather short utterances: MSCOCO (Lin et al.,), Visual Genome (Krishna et al.,), VQA v2.0 (Antol et al.,), GQA balanced version (Hudson and Manning,), and VG-QA (Zhu et al.,). Both the visual and linguistic representations are processed by a specialized transformer encoder based on self-attention layers; their outputs are then processed by a cross-modality encoder that, through a cross-attention mechanism, generates representations of the single modality (language and visual output) enhanced with the other modality as well as their joint representation (cross-modality output). Like RoBERTa, LXMERT uses the special tokens CLS and SEP. Differently from RoBERTa, LXMERT uses the special token SEP both to separate sequences and to denote the end of the textual input. LXMERT has been pre-trained on five tasks.5 It has 19 attention layers: 9 and 5 self-attention layers in the language and visual encoders, respectively and 5 cross-attention layers. We process the output corresponding to the CLS token as in RoBERTa. Similarly, we consider both the pre-trained version (LXMERT) and the one trained from scratch (LXMERT-S).6</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>22036</offset><text>5. Experiments on the Full Test set</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22072</offset><text>We aim to understand whether models encode Yes/No answers and properly integrate them into the question. If answers play a role in the performance of the models in guessing the target object, removing them from the dialogues should cause a drop in the task accuracy. Following this conjecture, we evaluate models (at test time, without additional training) when receiving only the questions from the dialogues (without the answers). Moreover, as commented above, the last turn in the Yes-set vs. No-set is expected to play a rather different role. In particular, already alone a positively answered question in the last turn is expected to be rather informative whereas a last turn answered negatively is not. On the other hand, last turns containing a negative answer are expected to enrich the dialogue history and help to guess the target. Hence, in the following, we evaluate models aiming to understand the role of the last turn.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23007</offset><text>5.1. Accuracy Results</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>23029</offset><text>5.1.1. Only Questions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23051</offset><text>We evaluate models when receiving dialogues containing only the questions.7 As expected, all models show an important drop as we can see from Table 2. Blind models have higher accuracy than the multimodal counterpart when receiving only the question, maybe because during training they learn to exploit the language surface more. Moreover, the pre-training phase helps to exploit the keywords in the questions as shown by the difference between the pre-trained and from scratch versions of both transformer based models. These results show that all models take the answers into account to some extent, and thus it is important to study their impact on the performance of the models.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>23734</offset><text>Full test set: Task Accuracy obtained by models when receiving: a) only the questions (Only Q); b) the full dialogue in the Yes-set vs. No-set, viz. games ending with a Yes-turn vs. a No-turn.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only Q&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;all games&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;all games&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Yes-set&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;No-set&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Random&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;2&quot; colspan=&quot;1&quot;&gt;B&lt;sc&gt;LIND&lt;/sc&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;43.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;54.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;3&quot; colspan=&quot;1&quot;&gt;
&lt;sc&gt;MM&lt;/sc&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;67.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;44.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>23927</offset><text>		Full dialogue	Only Q	Full dialogue	 			all games	all games	Yes-set	No-set	 		Random	12.5	12.5	16.4	16.4	 	BLIND	LSTM	64.7	47.9	67.0	49.0	 	RoBERTa-S	64.2	43.7	66.6	48.1	 		RoBERTa	67.9	51.7	69.6	54.5	 	MM	V-LSTM	64.5	46.2	67.0	48.3	 	LXMERT-S	64.4	32.0	66.6	49.5	 	LXMERT	69.2	44.8	71.9	50.9	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>24224</offset><text>All differences between RoBERTa and LXMERT are statistically significant.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>24298</offset><text>5.1.2. Dialogues With a Yes- vs. No- Answer in the Last Turn</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24359</offset><text>We now investigate how the polarity of the final answer in the dialogue affects the performance in the guessing task. Models reach a rather lower accuracy on the No-set, suggesting that models have harder time interpreting dialogues ending with a negative answer (Table 2). Differently from what one would expect, it seems the pre-trained transformer that does not have access to the visual representation of the “alternative set” (RoBERTa) performs better than the multimodal model, LXMERT, in the challenging No-set games. It is not clear, however, where the advantage of RoBERTa comes from. Hence, in the next section, we aim to understand these results better by using the controlled sample and comparing models against the humans' performance, with a particular focus on the role of the last dialogue turn.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>25175</offset><text>5.1.3. The Role of the Last Turn</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25208</offset><text>To analyse the role of the last turn, we compute models' accuracy when receiving the dialogues without the last turn or with only the last turn. The drop obtained from the setting in which models have access to the full dialogue quantifies the role of the last turn. First of all, as shown in Table 3, when removing the last turn in the Yes-set, LXMERT has a higher drop in accuracy than RoBERTa: −22.0% (from 71.9 to 49.9) vs. –16.1% (from 69.6 to 53.5); the fact that LXMERT relies on the last turn a lot might be due to LXMERT having harder time than RoBERTa in encoding the dialogue history, as observed in Greco et al.. When only the last turn is provided, LXMERT profits from the pre-training phase more than RoBERTa. Recall that LXMERT has seen shorter text than RoBERTa during training, e.g., MS-COCO captions vs. Wikipedia text. This difference could be behind such results. In the No-set, LXMERT processes the last turn better than RoBERTa (it reaches 26.6 accuracy when receiving only the last turn, +3.3 than RoBERTa), but again it has more difficulty in integrating such information with that gathered through the dialogue history (it scores –3.4% than RoBERTa when receiving the full dialogue). Finally, as expected, when receiving only the last turn, models obtain a high accuracy when the answer is positive (Yes-set) and are near to chance level when it is negative (No-set). Interestingly, in the No-set, RoBERTa and LXMERT have a rather similar accuracy when the last turn is not given and LXMERT does slightly better than the language encoder when receiving only the last turn. These results suggest that the advantage of RoBERTa over LXMERT highlighted in Table 2 is due to a better processing of the whole dialogue history, while LXMERT exploits better shorter sequences such as the last turn taken individually in the No-Set (Table 3).</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27073</offset><text>Full test set: Accuracy comparison when giving to the model the dialogue without the last turn (W/o Last) or with only the last turn (Last).</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Yes-set&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;No-set&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o Last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o Last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Last&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;39.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;39.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42.5&lt;xref rid=&quot;TN1&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41.9&lt;xref rid=&quot;TN1&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26.6&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27214</offset><text>	Yes-set	No-set	 		W/o Last	Last	W/o Last	Last	 	LSTM	48.3	51.8	39.9	24.5	 	RoBERTa-S	49.8	50.7	39.6	21.8	 	RoBERTa	53.5	55.6	42.5*	23.3	 	V-LSTM	48.6	47.3	37.8	20.7	 	LXMERT-S	48.4	51.7	41.0	22.2	 	LXMERT	49.9	61.2	41.9*	26.6	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>27444</offset><text>(The marks RoBERTa's and LXMERT's scores whose differences are statistically not significant).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>27539</offset><text>5.1.4. Tests of Statistical Significance</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27580</offset><text>To validate our findings about the comparison between RoBERTa and LXMERT, we have run the McNemar's test with a significance level of 0.05. We use an asterisk to signal scores whose differences is not significant (Tables 2, 3).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>27808</offset><text>5.2. Guesser's Probability Distribution</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27848</offset><text>We now analyze how the guesser module assigns probabilities to the target object across the turns to understand better the role of positive and negative answers at a more fine-grained level. We compute how the probability assigned by the Guesser to the target object P(o) changes after each turn (P(o)Ti+i−P(o)Ti) and compare turns Ti with a Yes, No or N/A answer. We expect it is easier to use the Yes-turns than the No ones, but we hope models are able to benefit from the questions answered negatively more than those answered by N/A. Moreover, we focus on the games in which the Guesser succeeds to select the target object, and quantify the effect of the last turn on the probability assigned to the target. We expect the change in the last turn of the No-set to be much higher than No-turns in average, whereas this should not happen with last turn in the Yes-set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28721</offset><text>Although the average probability assigned to the target is similar before a Yes-turn and a No-tun for all models,8 questions answered with Yes bring a much higher increase of probability than questions answered with No—which for LSTM have on average the same impact as those answered by N/A (2.9 vs. 2.3) (Table 4).9 Again, RoBERTa is the model that seems to profit of the negative turn more: the probability the guesser assigns to the target object after a No-turn increases of 5.9 vs. 4.1 when using LXMERT as encoder. However, when we focus on the last turn (Table 4-right), LXMERT is the model for which the negative answer brings a higher increase to the target object In the following, by zooming into the controlled sample we aim to get a more accurate comparison of models with respect to the specific issue of how they encode negatively answered questions.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>29589</offset><text>Change across consecutive turns in the probability assigned to the target after Yes- vs. No- vs. N/A-turns, i.e., P(o)Ti+1−P(o)Ti (full dialogue history in the full test set) and before/after the last turn (Last turn in games on which the model has succeeded).</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;All games&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;All successful games&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue history&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Last turn&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;Yes&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;No&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;N&lt;/italic&gt;/&lt;italic&gt;A&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;Yes&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;No&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;italic&gt;
&lt;bold&gt;T&lt;/bold&gt;
&lt;/italic&gt;
&lt;bold&gt;&lt;sub&gt;&lt;italic&gt;i&lt;/italic&gt;&lt;/sub&gt;:&lt;italic&gt;N&lt;/italic&gt;/&lt;italic&gt;A&lt;/italic&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.2&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29852</offset><text>	All games	All successful games	 		Full dialogue history	Last turn	 		Ti:Yes	Ti:No	Ti:N/A	Ti:Yes	Ti:No	Ti:N/A	 	LSTM	14.5	2.9	2.3	26.3	16.2	6.3	 	RoBERTa-S	12.7	3.5	1.9	24.6	16.4	1.1	 	RoBERTa	12.3	5.9	1.4	22.9	18.8	1.1	 	V-LSTM	14.0	3.1	2.9	23.7	13.7	6.7	 	LXMERT-S	12.3	4.4	2.1	24.8	19.3	0.7	 	LXMERT	16.4	4.1	1.4	30.0	24.9	3.2	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>30185</offset><text>5.3. Summary</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30198</offset><text>In short, the experiments run so far show that all models take the answer of the asymmetric GuessWhat?! dialogues into account. The pre-trained encoders are the best models over all games and are on par with one another in processing positively answered questions. But, the results on the Yes-set when removing the last turn or when giving only the last turn shows that LXMERT profits from the last Yes-turn more than RoBERTa. We conjecture this is due to the fact that LXMERT has a harder time encoding the dialogue history. The overall accuracy obtained on the No-set suggests that RoBERTa encodes the negatively answered questions better than LXMERT. However, an in-depth analysis of the Guesser probability distribution shows that the Guesser profits from the last turn in the No-set more when it is based on LXMERT than on RoBERTa. From the analyses presented so far, it emerges that the models we considered have different strengths and weaknesses, depending on many factors. To establish an upper-bound for models' performance and to assess the severity of the errors made by the models, in the following we present an in-depth analysis we carried out with human annotators playing the same guessing task of the models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>31425</offset><text>6. Controlled Sample: Humans and Models</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31465</offset><text>In order to interpret models' performance on encoding Yes/No-turns, we evaluated humans' performance on the controlled games sample described in section 3. These results set an upper-bound for model performance, and give us a powerful tool to better scrutinize our results.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>31739</offset><text>6.1. Experiments and Results With Human Annotators</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31790</offset><text>We asked human annotators to perform the GuessWhat?! guessing task on a controlled sample of test set games. Similarly to what discussed in section 5, we evaluate several settings: we provide annotators with the full dialogue, the dialogue without the last turns, or only the last turn. Moreover, to check the average informativeness of Yes- No-turns, we add the setting in which we remove from the dialogues all turns of the same polarity.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>32231</offset><text>6.1.1. Data Collection</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32254</offset><text>Through Prolific,10 we collected complete annotations from 48 subjects who were paid Euro 8.27/h. Each participant annotated 75 games from one of the four settings. In total, we have collected 3600 human answers. Each setting has received annotation from 3 participants. Participants were asked to be native English speakers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32580</offset><text>Participants were given an image with bounding boxes associated with each candidate object, together with a progressive ID number, as illustrated in Figure 4. They express their guess by pressing on the device's keyboard the number corresponding to the chosen object. Before starting the experiment, they were shown three trial games for which the correct answer was displayed in case the annotator chose the wrong target. We added two control games in each setting, i.e., games with a full dialogue history and few candidate objects. Participants were told there where control games and that they would have been excluded from the data collection in case the wrong answer was given for those games. Only one annotator wrongly guessed the control games and was therefore excluded. We recorded the time taken by each participant to complete the experiment. On average, humans took 12.23 s for each datapoint in the group A (removing turns), 15.55 s for group B (without last turn), 10.52 s for group C (only last turn), and finally 20.26 for group D (full dialogue). We found no statistically significant correlation between the time taken to guess the target and the success in solving the task.</text></passage><passage><infon key="file">fdata-04-736709-g0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>33776</offset><text>Prolific interface: Humans were given a dialogue, an image with colored bounding boxes, and a numbered list of candidates with colors matching those of the bounding boxes. They had to use the keyboard device to choose the target.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>34006</offset><text>6.1.2. Tests of Statistical Significance</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34047</offset><text>As we did in the previous section, we validate the accuracy results by running a McNemar's test with a significance level of 0.05 (Tables 5, 7). Table 6 reports the times taken by humans to play games belonging to the different groups we have analyzed. The differences within groups are not normally distributed—Shapiro–Wilk test. Hence, to check the validity of such comparisons we have run a Wilcoxon rank-sum statistic for two samples using 0.05 as significance level. Again, we use asterisks to signal the results whose difference is not statistically significant.</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>34620</offset><text>Humans' performance on controlled sample: percentage of games guessed correctly by at least two participants (MAJ) vs. by at least one participant (MIN).</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; colspan=&quot;2&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;A) Removing turns&lt;/bold&gt;
&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;sc&gt;MAJ&lt;/sc&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Only Yes&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.00&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Only No&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.00&lt;/td&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;sc&gt;MIN&lt;/sc&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Only Yes&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.67&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Only No&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72.67&lt;/td&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;B) W/o last&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;C) Only last&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;D) Full dialogue&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;sc&gt;MAJ&lt;/sc&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.67&lt;xref rid=&quot;TN2&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.67&lt;xref rid=&quot;TN2&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;sc&gt;MIN&lt;/sc&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Yes-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No-set&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>34774</offset><text>	A) Removing turns			 	MAJ	Only Yes	66.00			 		Only No	46.00			 	MIN	Only Yes	80.67			 		Only No	72.67			 			B) W/o last	C) Only last	D) Full dialogue	 	MAJ	Yes-set	75.33	71.33	86.67*	 		No-set	49.33	30.67	80.67*	 	MIN	Yes-set	92.00	88.00	98.00	 		No-set	64.67	58.00	90.00	 	</text></passage><passage><infon key="file">T5.xml</infon><infon key="id">T5</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>35050</offset><text>Not significant.</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>35067</offset><text>Average time (seconds) taken by humans to solve games belonging to the different groups analyzed.</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Group&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Description&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Average time/token (s)&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;A&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;only yes turns&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.45&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;A&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;only no turns&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.57&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;without last (yes)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.94&lt;xref rid=&quot;TN3&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;without last (no)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.79&lt;xref rid=&quot;TN3&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;C&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;only last (yes)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;C&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;only last (no)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.53&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;full dial ending with yes&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;full dial ending with no&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>35165</offset><text>Group	Description	Average time/token (s)	 	A	only yes turns	0.45	 	A	only no turns	0.57	 	B	without last (yes)	0.94*	 	B	without last (no)	0.79*	 	C	only last (yes)	1.20	 	C	only last (no)	2.53	 	D	full dial ending with yes	0.72	 	D	full dial ending with no	0.85	 	</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>35431</offset><text>Normalized with respect to the number of token in the text; only successful games are considered.</text></passage><passage><infon key="file">T6.xml</infon><infon key="id">T6</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>35529</offset><text>not significant.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>35546</offset><text>6.1.3. Results With Humans</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35573</offset><text>As mentioned above, we focus on games on which human players have been successful in guessing the target object. It has to be noted that during the GuessWhat?! data collection, each game was played only once and the target object was guessed by the same player who asked the questions. Hence we do not know whether the same dialogue-image would be equally informative for another player to succeed in the game neither we know the level of uncertainty behind the choice made by the successful player. With these questions in mind, in Table 5 we report the accuracy obtained by humans in our controlled experiment by considering a game successfully solved if (a) at least one participant correctly identifies the target object among the list of candidates (the typical GuessWhat?! accuracy evaluation setting, modulo the fact that in our case the questions are already asked) and (b) at least two participants guess the target correctly (the most standard and solid evaluation); we refer to these two accuracy metrics as minority (MIN) and majority (MAJ) schema, respectively.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>36648</offset><text>Given that we are working with games on which GuessWhat?! human players succeed guessing the target, the fact we do not obtain 100% accuracy in the group D (complete dialogues) is by itself interesting. The difference between the two schema shows that, also in the games successfully solved by human players in the GuessWhat?! dataset, there is a margin of uncertainty. As we see from the Table 5 (Group D, full dialogue), 98.00% of the games ending with a Yes-turn could be guessed by at least one participant (minority schema) whereas 86.67% of them were guessed correctly by at least two participants (majority schema). Games ending with a No-turn are more difficult: 90.00% (resp. 80.67%) of the games could be guessed based on the minority (resp. majority) schema. However, whereas the difference between the Yes- vs. No-set in the minority schema is significant it is not so in the majority schema. This suggests that, for humans, the level of difficulty of the two subsets is similar. The results on Group A (removing turns) shows that on average Yes-turns are more informative than No-turns. As expected, the last turn in the Yes-set is quite informative: with only the last turn (Group C), humans' accuracy drops of only –10.00% (resp. –15.34) reaching 88.00 (resp. 71.33) accuracy in the minority (resp. majority) schema. Furthermore, the last turn in the Yes-set is quite redundant with the information provided by the previous turns: when receiving the dialogue without the last turns (Group B), humans' accuracy drops of only 6% (resp. 11.34) in the minority (resp. majority) schema. Instead, the last turn in the No-set seems to provide further information that needs to be integrated with those received in the previous turns: without the last turn the accuracy on the No-set drops of 25.33 (resp. 31.34). All in all, these results show that also for humans gathering information from the No-turn is harder than with the Yes-turn, yet the last turn in the No-set is informative and humans manage to profit from it to succeed in the task relatively well. This result highlights the value of negation in visual dialogues, and show why it is an important requirement for computational models to properly process it.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38880</offset><text>To measure the processing cost of negative turns, we have analyzed the average time taken by human to correctly solve games belonging to the four categories we discussed so far. Table 6 shows that interpreting questions answered positively is faster than interpreting the ones answered negatively, and this result holds for all settings. In particular, processing positively-answered questions takes less than processing negatively-answered ones (group A), and a final positive turn is processed much faster than a negative final turn (group C). Interestingly, in the Yes-set guessing the target is faster when receiving the full dialogue than when receiving the dialogue without the last turn (0.72 vs. 0.94 s/token, p &lt; 0.05), this might be due to what observed above, namely the last Yes-turn summarizes the salient information collected till that point and hence speeds up the choice. Whereas the negative answer in the last turn brings a boost in performance, it does not affect significantly the time taken by human annotators to process the dialogue (0.79 vs. 0.85 s/token, p &gt; 0.05). These results show that the time taken by human participants to solve the game mirrors the processing cost of negation, which is also influenced by the context (dialogue) in which it appears.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>40164</offset><text>6.1.4. Results Humans vs. Models</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>40197</offset><text>We now evaluate the models on the same controlled sample of games we used with human annotators. In Table 7, we report the task accuracy obtained by models when removing all the Yes turns (remaining with only No-turns) or all the No-turns (remaining with only Yes-turns). As can be seen from the table, the performance of the two best models is rather similar: both in the full dialogue and in the only Yes-turns the difference between their results is not significant. Similarly to humans, models accuracy drops less when receiving only the Yes-turns than when receiving only the No-turns. However, models' overall accuracy when receiving the full dialogue is far from the human upper-bound even when using the majority vote schema. As we can see in Table 8 this rather big difference between models and humans is due to the No-set: while humans correctly succeed in 80.67% of the games ending in a No-turn, models reach at most the 50%. It is thus clear that if it is true that negation has a higher processing cost for both humans and computational models, the latter struggle to profit from negatively answered questions.</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>41323</offset><text>Controlled sample.</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;2&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Removing turns&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-bottom: thin solid #000000;&quot;&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only No-turns&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only Yes-turns&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;4&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;BLIND&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Random&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;16.5&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;16.5&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;16.5&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;LSTM&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;57.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;30.67&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;48.00&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;RoBERTa-S&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;54.66&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;29.33&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;50.00&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;RoBERTa&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;60.0&lt;xref rid=&quot;TN4&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;35.33&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;52.00&lt;xref rid=&quot;TN5&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;**&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;4&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;MM&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;V-LSTM&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;55.66&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;25.33&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;50.66&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;LXMERT-S&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;54.33&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;32.66&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;48.00&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;LXMERT&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;59.67&lt;xref rid=&quot;TN4&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;25.33&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;56.66&lt;xref rid=&quot;TN5&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;**&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Human (MAJ)&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;83.67&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;46.00&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;66.00&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>41342</offset><text>		Full dialogue	Removing turns	 				Only No-turns	Only Yes-turns	 	BLIND	Random	16.5	16.5	16.5	 	LSTM	57.0	30.67	48.00	 	RoBERTa-S	54.66	29.33	50.00	 	RoBERTa	60.0*	35.33	52.00**	 	MM	V-LSTM	55.66	25.33	50.66	 	LXMERT-S	54.33	32.66	48.00	 	LXMERT	59.67*	25.33	56.66**	 	Human (MAJ)	83.67	46.00	66.00	 	</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>41645</offset><text>Removing turns: comparison of the task accuracy when models receive the full dialogue vs. only the No- vs. only the Yes-turns. Human accuracy computed with the majority vote.</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>41820</offset><text>
,
</text></passage><passage><infon key="file">T7.xml</infon><infon key="id">T7</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>41824</offset><text>not significant.</text></passage><passage><infon key="file">T8.xml</infon><infon key="id">T8</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>41841</offset><text>Controlled sample.</text></passage><passage><infon key="file">T8.xml</infon><infon key="id">T8</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;Yes-set&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; colspan=&quot;3&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot;&gt;
&lt;bold&gt;No-set&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;last&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;2&quot; colspan=&quot;1&quot;&gt;BLIND&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.30&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.30&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;34.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;44.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;39.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;27.33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;middle&quot; align=&quot;center&quot; rowspan=&quot;2&quot; colspan=&quot;1&quot;&gt;MM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;34.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16.67&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;36.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.00&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; style=&quot;border-bottom: thin solid #000000;&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31.33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Humans (MAJ)&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30.67&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>41860</offset><text>		Yes-set	No-set	 			Full	W/o	Only	Full	W/o	Only	 			dialogue	last	last	dialogue	last	last	 	BLIND	LSTM	68.00	55.30	51.30	46.00	34.00	30.00	 	RoBERTa-S	64.67	49.33	48.67	44.67	39.33	27.33	 		RoBERTa	71.33	55.33	63.33	48.67	40.67	22.00	 	MM	V-LSTM	60.67	49.33	49.33	50.67	34.67	16.67	 	LXMERT-S	61.33	50.00	47.33	47.33	36.00	22.00	 		LXMERT	71.33	53.33	60.67	48.00	46.00	31.33	 		Humans (MAJ)	86.67	75.33	71.33	80.67	49.33	30.67	 	</text></passage><passage><infon key="file">T8.xml</infon><infon key="id">T8</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>42291</offset><text>Without the last turn, Only the last turn, Full Dialogue: accuracy comparison to highlight the role of the last turn when it contains a positive (Yes-set) vs. negative (No-set) answer.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>42476</offset><text>6.2. Comparison With Humans' Errors</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42512</offset><text>In the following, we run an error analysis by comparing models and humans on their failures. We expect that a model that properly grounds the dialogues is likely to make human-like mistakes. To this end, among the games failed by a model, we check how many of them have been failed by at least one human annotator (Table 9); moreover, in the games in which a model and at least one participant failed, we check whether the error made by the model and the participant is exactly the same, i.e., if they have chosen the same (wrong) candidate object (Table 10). As we can see from Table 9, LXMERT is the model whose failed games are most similar to the ones failed by human annotators. However, if we look (in a more fine-grained way) at the exact candidate objects they select, we found that RoBERTa is the model whose errors are more human-like for most of the settings (Table 10). This analysis highlights how human annotations help interpret models' results and evaluate the quality of their predictions.</text></passage><passage><infon key="file">T9.xml</infon><infon key="id">T9</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>43519</offset><text>Error Analysis: Percentage of games human failed among those failed by each model.</text></passage><passage><infon key="file">T9.xml</infon><infon key="id">T9</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Removing turns&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.65&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;73.56&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.61&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.12&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.35&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.12&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.05&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.48&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.19&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.87&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;73.65&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.65&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.88&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.43&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72.44&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.56&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>43602</offset><text>	Removing turns	W/o last	Only last	Full dialogue	 	V-LSTM	80.65	73.56	78.61	53.38	 	LXMERT-S	82.12	71.35	81.12	59.12	 	LXMERT	83.05	77.48	85.19	58.68	 	RoBERTa-S	82.87	73.65	80.65	55.88	 	RoBERTa	83.43	72.44	82.56	55.00	 	</text></passage><passage><infon key="file">T10.xml</infon><infon key="id">T10</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>43825</offset><text>Error Analysis: Percentage of games in which each model does the same mistake made by humans (i.e., by selecting the same wrong candidate object as a human annotator).</text></passage><passage><infon key="file">T10.xml</infon><infon key="id">T10</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Removing turns&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;W/o last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Only last&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Full dialogue&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;V-LSTM&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;45.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.44&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41.14&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52.38&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52.46&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42.77&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LXMERT&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.70&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;58.12&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.10&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa-S&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.22&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.67&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;44.74&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RoBERTa&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.99&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;51.33&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.52&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;53.03&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>43993</offset><text>	Removing turns	W/o last	Only last	Full dialogue	 	V-LSTM	45.33	48.44	41.14	49.30	 	LXMERT-S	52.38	52.46	42.77	51.85	 	LXMERT	51.70	58.12	47.10	49.30	 	RoBERTa-S	57.33	51.22	46.67	44.74	 	RoBERTa	60.99	51.33	53.52	53.03	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>44216</offset><text>In Figure 5, we report a game in which both models and humans failed to guess the target when the last turn was not given; interestingly, at that stage, with only the first three turns, the selection made by RoBERTa and humans could be valid. This shows that checking when models and humans make the same mistakes gives a hint about which errors are plausible. From our qualitative analysis, it seems that RoBERTa takes spatial questions into account more than LXMERT, maybe because it exploits the spatial coordinates of the candidate objects whereas LXMERT overrides that information with the one it receives from the visual features. More in-depth analysis is required to assess what factors most influence the outcome of the models.</text></passage><passage><infon key="file">fdata-04-736709-g0005.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>44953</offset><text>Errors made by humans and computational models when receiving dialogues without the last turn.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>45048</offset><text>6.3. Summary</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45061</offset><text>The evaluation of models on the controlled sample confirms that RoBERTa and LXMERT behave rather similarly on the Yes-set across all settings. More interestingly, it shows that in the No-set LXMERT is closer to humans than RoBERTa considering the accuracy in the task. LXMERT seems to be failing in the integration of the last No-turn with the dialogue history: its accuracy is similar to humans in the settings without last and only last turn, but it is far from them when the whole dialogue is given. Moreover, visual features seem to be of more help in the No-set than in the Yes-set: in the Yes-set across the controlled groups, the blind models do better or similar to their multimodal counterpart, whereas on the No-set the opposite holds. Finally, our error analysis reveals that RoBERTa is the model whose predictions are most human-like when it fails to identify the target object.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>45952</offset><text>7. Discussion and Conclusion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>45981</offset><text>In the current AI research, driven by the success of language models and neural network architectures, negation is under-studied. Dialogue history and visual context have been shown to facilitate the processing of negation in humans. Hence, we took negation in visual dialogues as our object of investigation and studied how SOTA multimodal models profit from negatively answered questions (No-turns) in the GuessWhat?! game. Such negative information is informative for humans to succeed in the game, and this holds in particular when the No-turn occurs as the last one of a game in which the human player has been successful in guessing the target. Therefore, we focus attention on the subset of dialogues ending with a No-turn and compare them with those ending with a Yes-turn. Our results show that SOTA models' performances on these two sub-sets is rather different, e.g., LXMERT obtains 71.9 vs. 50.9% accuracy in the Yes- vs. No-set, respectively (Table 2). To better interpret these results, we have run an online experiment with humans: we carefully selected a controlled sample of games and asked subjects to play the role of the guesser. We evaluated models' behavior on such a controlled sample of games and used humans' results to better interpret the success and failures of models. The analysis shows that humans are much faster in processing positively answered questions than negatively answered ones. Yet, they do profit from the latter to succeed in the referential guessing task reaching 80.67% accuracy in the No-set – on which models guess correctly barely the 50% (Table 8). This shows that models are far away from the human ability to ground negation and we believe efforts should be put to reduce this important gap between humans and models' performance.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>47766</offset><text>Our findings can help design models which could ground negation better than current SOTA models. First of all, our comparison between the accuracy obtained by LXMERT and RoBERTa in the various settings (Tables 2, 3, 8) suggests that LXMERT grounding of negation within a dialogue could be improved by pre-training it on longer text. One could consider adding task-oriented dialogues in the pre-training phase Moreover, our comparison of models' and humans' errors leads us to conjecture that LXMERT fails to exploit the spatial information provided in the dialogue, this could be behind the fact that though it grounds negation in short texts better than RoBERTa, the latter's mistakes are more human-like, since humans rely on such information to locate and identify the target object. This limitation of the LXMERT based Guesser could be overcome by building a model that exploits the image regions received as input to perform the task, similarly to what has been recently proposed in Tu et al. for another multimodal model. Finally, Hosseini et al. shows that pre-trained language models can better understand negation if trained with an unlikelihood objective. This is a first important step ahead in modeling negation in the neural network-era, but the model's performance on entailment judgments involving negation is still low. Cognitive sciences findings on human processing of negation show that humans profit from expectations driven by the visual context to process negative information quickly and effectively Nordmeyer and Frank; we believe that models should be trained to exploit more such expecations and that a (multimodal) communicative setting can help bring a boost for learning to encode (grounded) negation.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49497</offset><text>The results we obtain do not always provide conclusive answers, but we believe they convincingly show the weakness of current multimodal encoders in processing negation and represent a starting point toward future research. We started from the observation that dialogue history and visual context makes the processing of negation easier for humans. To fully understand whether this can be the case for models too, a comparison on processing negation in language-only vs. multimodal settings should be carried out. To this end, the study could be extended to other dasests in which the visual input is not shared or only partially shared by the agents, such as VisDial, PhotoBook and Meet up! (Haber et al.,; Ilinykh et al.,) or language-only task-oriented dialogues (e.g., those used in Wu et al.,). Moreover, negative information can be conveyed in different ways, but we have studied only the easiest: a straightforward negative answer to a binary question. It would be interesting to explore the use of negation in declarative sentences and in more complex interactions. Finally, though our study builds on observations about the information gain the guesser accumulates through the dialogue at each turn, we have taken the dialogues as static blocks. A study about how humans and models incrementally gain information through the dialogue should be run to better understand their behavior.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>50891</offset><text>To conclude, our findings have theoretical and also practical implications: for humans, negatively answered questions can be as informative as affirmatively answered ones; a system that is not able to properly handle negation may be detrimental in real-world scenarios. More research should be done on the issue to better understand whether neural network architectures can learn to ground negation on the alternative set it activates. To this end, we might need to single out various issues that are entangled in our analysis. First of all, it would be beneficial to have a multimodal dataset designed for this purpose. Secondly, when evaluating universal encoders the difference in the pre-training data is a confounder that should be avoided. Finally, it would be useful to have a large-scale human behavioral experiment that takes into account the incremental information gain at the core of a task-oriented dialogue exchange. We believe such data to be crucial both for training models to properly ground negation and for evaluating not only their task success but also their inside mechanisms as advocated for instance by Zhang et al.. Once models learn to encode negation in grounded contexts, the next step will be to transfer such skills to language-only settings by exploiting transfer learning methods (e.g., Ruder,).</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>52220</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>52248</offset><text>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>52370</offset><text>Ethics Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>52387</offset><text>Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. The patients/participants provided their written informed consent to participate in this study.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>52637</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>52658</offset><text>AT: design of computational experiments, model implementation and evaluation (Sections 5.1 and 5.2), human data collection and analysis (Section 6), and writing. CG: design of computational experiments, model implementation, and evaluation (Section 5.1), Supplementary Material, and writing. RB: research question, design of computational experiments, human data collection, and analysis (Section 6), writing. All authors contributed to the article and approved the submitted version.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>53143</offset><text>Conflict of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>53164</offset><text>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>53337</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>53354</offset><text>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>53701</offset><text> 1 https://github.com/albertotestoni/annotation_human_gw </text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>53759</offset><text>2In shorter dialogues the area of the target object is bigger than in longer dialogues, and in short dialogues the target object is quite often a “person” – the most common target category in the dataset; moreover, the number of distractors in longer dialogues is much higher.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54042</offset><text>3The dataset of human dialogues is available at https://guesswhat.ai/download.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54121</offset><text>4We have also tried BERT, but we obtained higher accuracy with RoBERTa.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54193</offset><text>5Masked cross-modality language modeling, masked object prediction via RoI-feature regression, masked object prediction via detected-label classification, cross-modality matching, and image question answering.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54403</offset><text>6We use the code available from https://github.com/claudiogreco/aixia2021.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54478</offset><text>7We replaced all the answers with the “unknown” [UNK] token.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54543</offset><text>8The difference is lower than 10%.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54578</offset><text>9In Table 4 we report the results for all the dialogues. Similar patterns have been seen when comparing the models on games with a given number of candidate objects.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>54744</offset><text> 10 https://www.prolific.co/ </text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>54774</offset><text>Supplementary Material</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>54797</offset><text>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fdata.2021.736709/full#supplementary-material</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>54956</offset><text>References</text></passage><passage><infon key="fpage">351</infon><infon key="lpage">366</infon><infon key="name_0">surname:Anderson;given-names:A. H.</infon><infon key="name_1">surname:Bader;given-names:M.</infon><infon key="name_2">surname:Bard;given-names:E. G.</infon><infon key="name_3">surname:Boyle;given-names:E.</infon><infon key="name_4">surname:Doherty;given-names:G.</infon><infon key="name_5">surname:Garrod;given-names:S.</infon><infon key="pub-id_doi">10.1177/002383099103400404</infon><infon key="pub-id_pmid">11827875</infon><infon key="section_type">REF</infon><infon key="source">Lang. Speech</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">1991</infon><offset>54967</offset><text>The HCRC map task corpus</text></passage><passage><infon key="fpage">2425</infon><infon key="lpage">2433</infon><infon key="name_0">surname:Antol;given-names:S.</infon><infon key="name_1">surname:Agrawal;given-names:A.</infon><infon key="name_2">surname:Lu;given-names:J.</infon><infon key="name_3">surname:Mitchell;given-names:M.</infon><infon key="name_4">surname:Batra;given-names:D.</infon><infon key="name_5">surname:Lawrence Zitnick;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE International Conference on Computer Vision</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>54992</offset><text>Vqa: visual question answering,</text></passage><passage><infon key="fpage">45</infon><infon key="lpage">73</infon><infon key="name_0">surname:Carpenter;given-names:P.</infon><infon key="name_1">surname:Just;given-names:M.</infon><infon key="pub-id_doi">10.1037/h0076248</infon><infon key="section_type">REF</infon><infon key="source">Psychol. Rev.</infon><infon key="type">ref</infon><infon key="volume">82</infon><infon key="year">1975</infon><offset>55024</offset><text>Sentence comprehension: a psycholinguistic processing model of verification</text></passage><passage><infon key="name_0">surname:Chattopadhyay;given-names:P.</infon><infon key="name_1">surname:Yadav;given-names:D.</infon><infon key="name_2">surname:Prabhu;given-names:V.</infon><infon key="name_3">surname:Chandrasekaran;given-names:A.</infon><infon key="name_4">surname:Das;given-names:A.</infon><infon key="name_5">surname:Lee;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Fifth AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>55100</offset><text>Evaluating visual conversational agents via cooperative human-ai games,</text></passage><passage><infon key="name_0">surname:Chen;given-names:Y.-C.</infon><infon key="name_1">surname:Li;given-names:L.</infon><infon key="name_2">surname:Yu;given-names:L.</infon><infon key="name_3">surname:Kholy;given-names:A. E.</infon><infon key="name_4">surname:Ahmed;given-names:F.</infon><infon key="name_5">surname:Gan;given-names:Z.</infon><infon key="pub-id_doi">10.1007/978-3-030-58577-8_7</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>55172</offset><text>UNITER: learning universal image-text representations</text></passage><passage><infon key="fpage">472</infon><infon key="lpage">517</infon><infon key="name_0">surname:Clark;given-names:H.</infon><infon key="name_1">surname:Chase;given-names:W.</infon><infon key="pub-id_doi">10.1016/0010-0285(72)90019-9</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Psychol.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">1972</infon><offset>55226</offset><text>On the process of comparing sentences against pictures</text></passage><passage><infon key="fpage">983</infon><infon key="lpage">996</infon><infon key="name_0">surname:Dale;given-names:R.</infon><infon key="name_1">surname:Duran;given-names:N.</infon><infon key="pub-id_doi">10.1111/j.1551-6709.2010.01164.x</infon><infon key="pub-id_pmid">21463359</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Sci.</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">2011</infon><offset>55281</offset><text>The cognitive dynamics of negated sentence verification</text></passage><passage><infon key="fpage">326</infon><infon key="lpage">335</infon><infon key="name_0">surname:Das;given-names:A.</infon><infon key="name_1">surname:Kottur;given-names:S.</infon><infon key="name_2">surname:Gupta;given-names:K.</infon><infon key="name_3">surname:Singh;given-names:A.</infon><infon key="name_4">surname:Yadav;given-names:D.</infon><infon key="name_5">surname:Moura;given-names:J. M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>55337</offset><text>Visual dialog,</text></passage><passage><infon key="fpage">5503</infon><infon key="lpage">5512</infon><infon key="name_0">surname:de Vries;given-names:H.</infon><infon key="name_1">surname:Strub;given-names:F.</infon><infon key="name_2">surname:Chandar;given-names:S.</infon><infon key="name_3">surname:Pietquin;given-names:O.</infon><infon key="name_4">surname:Larochelle;given-names:H.</infon><infon key="name_5">surname:Courville;given-names:A. C.</infon><infon key="section_type">REF</infon><infon key="source">2017 IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>55352</offset><text>GuessWhat?! Visual object discovery through multi-modal dialogue,</text></passage><passage><infon key="fpage">4171</infon><infon key="lpage">4186</infon><infon key="name_0">surname:Devlin;given-names:J.</infon><infon key="name_1">surname:Chang;given-names:M.-W.</infon><infon key="name_2">surname:Lee;given-names:K.</infon><infon key="name_3">surname:Toutanova;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>55418</offset><text>BERT: pre-training of deep bidirectional transformers for language understanding,</text></passage><passage><infon key="name_0">surname:Gokaslan;given-names:A.</infon><infon key="name_1">surname:Cohen;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">Openwebtext Corpus.</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>55500</offset></passage><passage><infon key="name_0">surname:Gokhale;given-names:T.</infon><infon key="name_1">surname:Banerjee;given-names:P.</infon><infon key="name_2">surname:Yang;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of ECCV</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>55501</offset><text>Vqa-lol: visual question answering under the lens of logic,</text></passage><passage><infon key="fpage">263</infon><infon key="lpage">279</infon><infon key="name_0">surname:Greco;given-names:C.</infon><infon key="name_1">surname:Testoni;given-names:A.</infon><infon key="name_2">surname:Bernardi;given-names:R.</infon><infon key="name_3">surname:Baldoni;given-names:M.</infon><infon key="name_4">surname:Bandini;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">AIxIA 2020 - Advances in Artificial Intelligence- XIXth International Conference of the Italian Association for Artificial Intelligence, Virtual Event, November 25-27, 2020, Revised Selected Papers, volume 12414 of Lecture Notes in Computer Science, eds</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>55561</offset><text>Grounding dialogue history: Strengths and weaknesses of pre-trained transformers,</text></passage><passage><infon key="name_0">surname:Gurari;given-names:D.</infon><infon key="name_1">surname:Li;given-names:Q.</infon><infon key="name_2">surname:Stangl;given-names:A. J.</infon><infon key="name_3">surname:Guo;given-names:A.</infon><infon key="name_4">surname:Lin;given-names:C.</infon><infon key="name_5">surname:Grauman;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>55643</offset><text>Vizwiz grand challenge: answering visual questions from blind people,</text></passage><passage><infon key="fpage">1895</infon><infon key="lpage">1910</infon><infon key="name_0">surname:Haber;given-names:J.</infon><infon key="name_1">surname:Baumgärtner;given-names:T.</infon><infon key="name_2">surname:Takmaz;given-names:E.</infon><infon key="name_3">surname:Gelderloos;given-names:L.</infon><infon key="name_4">surname:Bruni;given-names:E.</infon><infon key="name_5">surname:Fernández;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>55713</offset><text>The PhotoBook dataset: building common ground through visually-grounded dialogue,</text></passage><passage><infon key="fpage">770</infon><infon key="lpage">778</infon><infon key="name_0">surname:He;given-names:K.</infon><infon key="name_1">surname:Zhang;given-names:X.</infon><infon key="name_2">surname:Ren;given-names:S.</infon><infon key="name_3">surname:Sun;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>55795</offset><text>Deep residual learning for image recognition,</text></passage><passage><infon key="fpage">3869</infon><infon key="lpage">3885</infon><infon key="name_0">surname:Hossain;given-names:M. M.</infon><infon key="name_1">surname:Anastasopoulos;given-names:A.</infon><infon key="name_2">surname:Blanco;given-names:E.</infon><infon key="name_3">surname:Palmer;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Findings of the Association for Computational Linguistics: EMNLP 2020</infon><infon key="type">ref</infon><infon key="year">2020a</infon><offset>55841</offset><text>It's not a non-issue: negation as a source of error in machine translation,</text></passage><passage><infon key="fpage">9106</infon><infon key="lpage">9118</infon><infon key="name_0">surname:Hossain;given-names:M. M.</infon><infon key="name_1">surname:Kovatchev;given-names:V.</infon><infon key="name_2">surname:Dutta;given-names:P.</infon><infon key="name_3">surname:Kao;given-names:T.</infon><infon key="name_4">surname:Wei;given-names:E.</infon><infon key="name_5">surname:Blanco;given-names:E.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</infon><infon key="type">ref</infon><infon key="year">2020b</infon><offset>55917</offset><text>An analysis of natural language inference benchmarks through the lens of negation,</text></passage><passage><infon key="fpage">1301</infon><infon key="lpage">1312</infon><infon key="name_0">surname:Hosseini;given-names:A.</infon><infon key="name_1">surname:Reddy;given-names:S.</infon><infon key="name_2">surname:Bahdanau;given-names:D.</infon><infon key="name_3">surname:Hjelm;given-names:R. D.</infon><infon key="name_4">surname:Sordoni;given-names:A.</infon><infon key="name_5">surname:Courville;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>56000</offset><text>Understanding by understanding not: Modeling negation in language models,</text></passage><passage><infon key="name_0">surname:Hudson;given-names:D. A.</infon><infon key="name_1">surname:Manning;given-names:C. D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>56074</offset><text>Gqa: a new dataset for compositional question answering over real-world images,</text></passage><passage><infon key="name_0">surname:Ilinykh;given-names:N.</infon><infon key="name_1">surname:Zarrieß;given-names:S.</infon><infon key="name_2">surname:Schlangen;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd Workshop on the Semantics and Pragmatics of Dialogue</infon><infon key="type">ref</infon><infon key="year">2019a</infon><offset>56154</offset><text>Meet up! a corpus of joint activity dialogues in a visual environment,</text></passage><passage><infon key="fpage">152</infon><infon key="lpage">157</infon><infon key="name_0">surname:Ilinykh;given-names:N.</infon><infon key="name_1">surname:Zarrieß;given-names:S.</infon><infon key="name_2">surname:Schlangen;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 12th International Conference on Natural Language Generation</infon><infon key="type">ref</infon><infon key="year">2019b</infon><offset>56225</offset><text>Tell me more: a dataset of visual scene description sequences,</text></passage><passage><infon key="name_0">surname:Johnson;given-names:J.</infon><infon key="name_1">surname:Hariharan;given-names:B.</infon><infon key="name_2">surname:van der Maaten;given-names:L.</infon><infon key="name_3">surname:Fei-Fei;given-names:L.</infon><infon key="name_4">surname:Zitnick;given-names:C. L.</infon><infon key="name_5">surname:Girshick;given-names:R. B.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Conference on Computer Vision and Pattern Recognition, vol. abs/1612.06890</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>56288</offset><text>CLEVR: a diagnostic dataset for compositional language and elementary visual reasoning,</text></passage><passage><infon key="fpage">1965</infon><infon key="lpage">1973</infon><infon key="name_0">surname:Kafle;given-names:K.</infon><infon key="name_1">surname:Kanan;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE International Conference on Computer Vision</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>56376</offset><text>An analysis of visual question answering algorithms,</text></passage><passage><infon key="fpage">7811</infon><infon key="lpage">7818</infon><infon key="name_0">surname:Kassner;given-names:N.</infon><infon key="name_1">surname:Schütze;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>56429</offset><text>Negated and misprimed probes for pretrained language models: birds can talk, but cannot fly,</text></passage><passage><infon key="fpage">5010</infon><infon key="lpage">5015</infon><infon key="name_0">surname:Kaushik;given-names:D.</infon><infon key="name_1">surname:Lipton;given-names:Z. C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>56522</offset><text>How much reading does reading comprehension require? A critical investigation of popular benchmarks,</text></passage><passage><infon key="fpage">32</infon><infon key="lpage">73</infon><infon key="name_0">surname:Krishna;given-names:R.</infon><infon key="name_1">surname:Zhu;given-names:Y.</infon><infon key="name_2">surname:Groth;given-names:O.</infon><infon key="name_3">surname:Johnson;given-names:J.</infon><infon key="name_4">surname:Hata;given-names:K.</infon><infon key="name_5">surname:Kravitz;given-names:J.</infon><infon key="pub-id_doi">10.1007/s11263-016-0981-7</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Comput. Vis.</infon><infon key="type">ref</infon><infon key="volume">123</infon><infon key="year">2017</infon><offset>56623</offset><text>Visual genome: connecting language and vision using crowdsourced dense image annotations</text></passage><passage><infon key="name_0">surname:Li;given-names:G.</infon><infon key="name_1">surname:Duan;given-names:N.</infon><infon key="name_2">surname:Fang;given-names:Y.</infon><infon key="name_3">surname:Gong;given-names:M.</infon><infon key="name_4">surname:Jiang;given-names:D.</infon><infon key="name_5">surname:Zhou;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of AAAI</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>56712</offset><text>Unicoder-vl: a universal encoder for vision and language by cross-modal pre-training,</text></passage><passage><infon key="name_0">surname:Li;given-names:L. H.</infon><infon key="name_1">surname:Yatskar;given-names:M.</infon><infon key="name_2">surname:Yin;given-names:D.</infon><infon key="name_3">surname:Hsieh;given-names:C.-J.</infon><infon key="name_4">surname:Chang;given-names:K.-W.</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>56798</offset><text>VisualBERT: a simple and performant baseline for vision and language</text></passage><passage><infon key="fpage">740</infon><infon key="lpage">755</infon><infon key="name_0">surname:Lin;given-names:T.-Y.</infon><infon key="name_1">surname:Maire;given-names:M.</infon><infon key="name_2">surname:Belongie;given-names:S.</infon><infon key="name_3">surname:Hays;given-names:J.</infon><infon key="name_4">surname:Perona;given-names:P.</infon><infon key="name_5">surname:Ramanan;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of ECCV (European Conference on Computer Vision)</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>56867</offset><text>Microsoft COCO: common objects in context,</text></passage><passage><infon key="name_0">surname:Liu;given-names:Y.</infon><infon key="name_1">surname:Ott;given-names:M.</infon><infon key="name_2">surname:Goyal;given-names:N.</infon><infon key="name_3">surname:Du;given-names:J.</infon><infon key="name_4">surname:Joshi;given-names:M.</infon><infon key="name_5">surname:Chen;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>56910</offset><text>RoERTa: A robustly optimized bert pretraining approach</text></passage><passage><infon key="fpage">13</infon><infon key="lpage">23</infon><infon key="name_0">surname:Lu;given-names:J.</infon><infon key="name_1">surname:Batra;given-names:D.</infon><infon key="name_2">surname:Parikh;given-names:D.</infon><infon key="name_3">surname:Lee;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>56965</offset><text>ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,</text></passage><passage><infon key="name_0">surname:Lu;given-names:J.</infon><infon key="name_1">surname:Goswami;given-names:V.</infon><infon key="name_2">surname:Rohrbach;given-names:M.</infon><infon key="name_3">surname:Parikh;given-names:D.</infon><infon key="name_4">surname:Lee;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of CVPR</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57063</offset><text>12-in-1: multi-task vision and language representation learning,</text></passage><passage><infon key="name_0">surname:Murahari;given-names:V.</infon><infon key="name_1">surname:Batra;given-names:D.</infon><infon key="name_2">surname:Parikh;given-names:D.</infon><infon key="name_3">surname:Das;given-names:A.</infon><infon key="pub-id_doi">10.1007/978-3-030-58523-5_20</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>57128</offset><text>Large-scale pretraining for visual dialog: a simple state-of-the-art baseline</text></passage><passage><infon key="name_0">surname:Nagel;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Cc-news.</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>57206</offset></passage><passage><infon key="fpage">2699</infon><infon key="lpage">2704</infon><infon key="name_0">surname:Nordmeyer;given-names:A.</infon><infon key="name_1">surname:Frank;given-names:M. C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 36th Annual Meeting of the Cognitive Science Society</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>57207</offset><text>A pragmatic account of the processing of negative sentences,</text></passage><passage><infon key="fpage">135</infon><infon key="lpage">151</infon><infon key="name_0">surname:Oaksford;given-names:M.</infon><infon key="pub-id_doi">10.1080/13546780143000170</infon><infon key="section_type">REF</infon><infon key="source">Thinking Reason.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2002</infon><offset>57268</offset><text>Contrast classes and matching bias as explanations of the effects of negation on conditional reasoning</text></passage><passage><infon key="name_0">surname:Pang;given-names:W.</infon><infon key="name_1">surname:Wang;given-names:X.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of 34th AAAI Conference on Artificial Intelligence</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57371</offset><text>Visual dialogue state tracking for question generation,</text></passage><passage><infon key="name_0">surname:Ruder;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Neural Transfer Learning for Natural Language Processing</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>57427</offset></passage><passage><infon key="fpage">32</infon><infon key="lpage">37</infon><infon key="name_0">surname:Sankar;given-names:C.</infon><infon key="name_1">surname:Subramanian;given-names:S.</infon><infon key="name_2">surname:Pal;given-names:C.</infon><infon key="name_3">surname:Chandar;given-names:S.</infon><infon key="name_4">surname:Bengio;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>57428</offset><text>Do neural dialog systems use the conversation history effectively? an empirical study,</text></passage><passage><infon key="fpage">255</infon><infon key="lpage">265</infon><infon key="name_0">surname:Shekhar;given-names:R.</infon><infon key="name_1">surname:Pezzelle;given-names:S.</infon><infon key="name_2">surname:Klimovich;given-names:Y.</infon><infon key="name_3">surname:Herbelot;given-names:A.</infon><infon key="name_4">surname:Nabi;given-names:M.</infon><infon key="name_5">surname:Sangineto;given-names:E.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vol.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2017</infon><offset>57515</offset><text>FOIL it! Find one mismatch between image and language caption,</text></passage><passage><infon key="fpage">2578</infon><infon key="lpage">2587</infon><infon key="name_0">surname:Shekhar;given-names:R.</infon><infon key="name_1">surname:Venkatesh;given-names:A.</infon><infon key="name_2">surname:Baumgärtner;given-names:T.</infon><infon key="name_3">surname:Bruni;given-names:E.</infon><infon key="name_4">surname:Plank;given-names:B.</infon><infon key="name_5">surname:Bernardi;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2019</infon><offset>57578</offset><text>Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat,</text></passage><passage><infon key="name_0">surname:Su;given-names:W.</infon><infon key="name_1">surname:Zhu;given-names:X.</infon><infon key="name_2">surname:Cao;given-names:Y.</infon><infon key="name_3">surname:Li;given-names:B.</infon><infon key="name_4">surname:Lu;given-names:L.</infon><infon key="name_5">surname:Wei;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">ICLR</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57661</offset><text>VL-BERT: Pre-training of generic visual-linguistic representations,</text></passage><passage><infon key="fpage">217</infon><infon key="lpage">223</infon><infon key="name_0">surname:Suhr;given-names:A.</infon><infon key="name_1">surname:Lewis;given-names:M.</infon><infon key="name_2">surname:Yeh;given-names:J.</infon><infon key="name_3">surname:Artzi;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Annual Meeting of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>57729</offset><text>A corpus of natural language for visual reasoning,</text></passage><passage><infon key="fpage">6418</infon><infon key="lpage">6428</infon><infon key="name_0">surname:Suhr;given-names:A.</infon><infon key="name_1">surname:Zhou;given-names:S.</infon><infon key="name_2">surname:Zhang;given-names:A.</infon><infon key="name_3">surname:Zhang;given-names:I.</infon><infon key="name_4">surname:Bai;given-names:H.</infon><infon key="name_5">surname:Artzi;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>57780</offset><text>A corpus for reasoning about natural language grounded in photographs,</text></passage><passage><infon key="fpage">2440</infon><infon key="lpage">2448</infon><infon key="name_0">surname:Sukhbaatar;given-names:S.</infon><infon key="name_1">surname:Weston;given-names:J.</infon><infon key="name_2">surname:Fergus;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>57851</offset><text>End-to-end memory networks,</text></passage><passage><infon key="fpage">5103</infon><infon key="lpage">5114</infon><infon key="name_0">surname:Tan;given-names:H.</infon><infon key="name_1">surname:Bansal;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>57879</offset><text>LXMERT: learning cross-modality encoder representations from transformers,</text></passage><passage><infon key="name_0">surname:Trinh;given-names:T. H.</infon><infon key="name_1">surname:Le;given-names:Q. V.</infon><infon key="pub-id_pmid">9663553</infon><infon key="section_type">REF</infon><infon key="source">arXiv preprint</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>57954</offset><text>A simple method for commonsense reasoning</text></passage><passage><infon key="fpage">5622</infon><infon key="lpage">5631</infon><infon key="name_0">surname:Tu;given-names:T.</infon><infon key="name_1">surname:Ping;given-names:Q.</infon><infon key="name_2">surname:Thattai;given-names:G.</infon><infon key="name_3">surname:Tur;given-names:G.</infon><infon key="name_4">surname:Natarajan;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>57996</offset><text>Learning better visual dialog agents with pretrained visual-linguistic representation,</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">191</infon><infon key="name_0">surname:Winograd;given-names:T.</infon><infon key="pub-id_doi">10.1016/0010-0285(72)90002-3</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Psychol.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">1972</infon><offset>58083</offset><text>Understanding natural language</text></passage><passage><infon key="fpage">917</infon><infon key="lpage">929</infon><infon key="name_0">surname:Wu;given-names:C.-S.</infon><infon key="name_1">surname:Hoi;given-names:S. C.</infon><infon key="name_2">surname:Socher;given-names:R.</infon><infon key="name_3">surname:Xiong;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>58114</offset><text>TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue,</text></passage><passage><infon key="name_0">surname:Yang;given-names:T.</infon><infon key="name_1">surname:Zha;given-names:Z.-J.</infon><infon key="name_2">surname:Zhang;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Computer Vision (ICCV)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>58195</offset><text>Making history matter: history-advantage sequence training for visual dialog,</text></passage><passage><infon key="fpage">186</infon><infon key="lpage">201</infon><infon key="name_0">surname:Zhang;given-names:J.</infon><infon key="name_1">surname:Wu;given-names:Q.</infon><infon key="name_2">surname:Shen;given-names:C.</infon><infon key="name_3">surname:Zhang;given-names:J.</infon><infon key="name_4">surname:Lu;given-names:J.</infon><infon key="name_5">surname:van den Hengel;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference of Computer Vision (ECCV)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>58273</offset><text>Goal-oriented visual question generation via intermediate rewards,</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">23</infon><infon key="name_0">surname:Zhang;given-names:Z.</infon><infon key="name_1">surname:Singh;given-names:J.</infon><infon key="name_2">surname:Gadiraju;given-names:U.</infon><infon key="name_3">surname:Anand;given-names:A.</infon><infon key="pub-id_doi">10.1145/3359158</infon><infon key="pub-id_pmid">34322658</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM Hum. Comput. Interact.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2019</infon><offset>58340</offset><text>Dissonance between human and machine understanding</text></passage><passage><infon key="name_0">surname:Zhao;given-names:R.</infon><infon key="name_1">surname:Tresp;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of IJCAI</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>58391</offset><text>Improving goal-oriented visual dialog agents via advanced recurrent nets with tempered policy gradient,</text></passage><passage><infon key="fpage">4995</infon><infon key="lpage">5004</infon><infon key="name_0">surname:Zhu;given-names:Y.</infon><infon key="name_1">surname:Groth;given-names:O.</infon><infon key="name_2">surname:Bernstein;given-names:M.</infon><infon key="name_3">surname:Fei-Fei;given-names:L.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>58495</offset><text>Visual7w: grounded question answering in images,</text></passage><passage><infon key="fpage">19</infon><infon key="lpage">27</infon><infon key="name_0">surname:Zhu;given-names:Y.</infon><infon key="name_1">surname:Kiros;given-names:R.</infon><infon key="name_2">surname:Zemel;given-names:R. S.</infon><infon key="name_3">surname:Salakhutdinov;given-names:R.</infon><infon key="name_4">surname:Urtasun;given-names:R.</infon><infon key="name_5">surname:Torralba;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, December 7-13, 2015</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>58544</offset><text>Aligning books and movies: towards story-like visual explanations by watching movies and reading books,</text></passage></document></collection>
