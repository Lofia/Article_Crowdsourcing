<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20211217</date><key>pmc.key</key><document><id>8617678</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/nu13114132</infon><infon key="article-id_pmc">8617678</infon><infon key="article-id_pmid">34836387</infon><infon key="article-id_publisher-id">nutrients-13-04132</infon><infon key="elocation-id">4132</infon><infon key="issue">11</infon><infon key="kwd">nutrition assessment food image image recognition restaurant food environment FAFH crowdsourcing deep learning GIS Hartford</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Chen;given-names:Xiang</infon><infon key="name_1">surname:Johnson;given-names:Evelyn</infon><infon key="name_2">surname:Kulkarni;given-names:Aditya</infon><infon key="name_3">surname:Ding;given-names:Caiwen</infon><infon key="name_4">surname:Ranelli;given-names:Natalie</infon><infon key="name_5">surname:Chen;given-names:Yanyan</infon><infon key="name_6">surname:Xu;given-names:Ran</infon><infon key="name_7">surname:Puglisi;given-names:Michael J.</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">13</infon><infon key="year">2021</infon><offset>0</offset><text>An Exploratory Approach to Deriving Nutrition Information of Restaurant Food from Crowdsourced Food Images: Case of Hartford</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>125</offset><text>Deep learning models can recognize the food item in an image and derive their nutrition information, including calories, macronutrients (carbohydrates, fats, and proteins), and micronutrients (vitamins and minerals). This technology has yet to be implemented for the nutrition assessment of restaurant food. In this paper, we crowdsource 15,908 food images of 470 restaurants in the Greater Hartford region on Tripadvisor and Google Place. These food images are loaded into a proprietary deep learning model (Calorie Mama) for nutrition assessment. We employ manual coding to validate the model accuracy based on the Food and Nutrient Database for Dietary Studies. The derived nutrition information is visualized at both the restaurant level and the census tract level. The deep learning model achieves 75.1% accuracy when compared with manual coding. It has more accurate labels for ethnic foods but cannot identify portion sizes, certain food items (e.g., specialty burgers and salads), and multiple food items in an image. The restaurant nutrition (RN) index is further proposed based on the derived nutrition information. By identifying the nutrition information of restaurant food through crowdsourced food images and a deep learning model, the study provides a pilot approach for large-scale nutrition assessment of the community food environment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1479</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1495</offset><text>Americans’ eating habits have been through a drastic change—they are spending more on eating out rather than cooking at home. According to the United States (US) Department of Agriculture (USDA) Economic Research Service Food Expenditure Series, the total sales of food prepared away from home (FAFH) surpassed that prepared at home (FAH) for the first time in 2014. The gap between the two expenditures continued to widen over the last few years. In 2019, the expenditures for FAFH were approximately $389,677 million, while the total expenditure for FAFH exceeded $418,933 million. The largest portion of the FAFH (i.e., 36.8% based on the 2019 Food Expenditure Series) was consumed at a limited-service restaurant, which is generally known as a fast-food restaurant. Compared to FAH, FAFH is relatively calorie-dense and nutrient-poor, as it contains more saturated fat, sodium, and cholesterol but less dietary fiber. Thus, the change in dietary behaviors has posed risks for FAFH consumers to develop obesity and obesity-related chronic diseases (e.g., Type II diabetes and cardiovascular diseases). For example, recent literature identified a strong association between FAFH consumption and calorific intake among children, which strengthened the evidence of the health adversities as a result of FAFH consumption.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2820</offset><text>To evaluate the nutrition of FAFH, it is essential to employ nutrition assessment methods on individual diets. This evaluation normally takes two different approaches. The first approach refers to the dietary assessment using nutritional biomarkers. Nutritional biomarkers are clinical instruments to identify the existence of nutrients in biological samples and thus can be used as a proxy for the human body’s nutrient absorption and metabolic response to food consumption. While nutritional biomarkers can objectively quantify the nutritional status of samples, their employment is equipment-dependent and restricted to clinical settings. Additionally, the evaluation results are subject to an individual’s disease status or homeostatic regulation. The second approach refers to the individual dietary assessment, such as the food frequency questionnaire (FFQ), 24-h dietary recall (24HR), and dietary record (DR). This approach evaluates individuals’ food consumption and dietary patterns by structured surveys or in-depth interviews. Although the individual dietary assessment is a direct observation of food consumption patterns and can be implemented in non-clinical settings, it is subject to systematic biases induced by human subjects, including recall error and reporting bias. Another prevailing issue in both approaches is that they require considerable efforts in data collection and processing, including personnel training, sample testing, interviewing, and data coding. For these reasons, these traditional nutrition assessment methods cannot be easily implemented on a large scale.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4426</offset><text>Advances in food image capturing and recognition technologies provide alternative means to dietary data collection and nutrition assessment. Food image recognition was initially explored in a pilot study by Williamson et al., which employed digital photography and visual estimation for food selections, plate waste, and portion sizes. This method, further coined as the Remote Food Photography Method (RFPM), was reaffirmed by cross-validating the estimated nutrients in food photographs with trained raters and nutritional biomarkers. The development and advancements in mobile devices and internet services further popularized the use of this technology, allowing individuals to record their dietary intake. For example, a prototype mobile device, called Wellnavi, was used in capturing dietary data for clinical assessments and dietary interventions. In another case, the captured food images were cross-validated with individuals’ voices to verbally describe food items. However, mobile devices in these initial attempts served only as instruments for data collection, storage, and transfer. The actual nutrition assessment component was still reliant on traditional measures, such as FFQ, 24HR, and DR. More recently, technological advancements in computer science provide new opportunities for leveraging dietary data for effective nutrition assessment. These computer-aided methods, primarily deep learning models, can identify the actual food item in an image. Meanwhile, complexities in food images, such as portion sizes and the co-existence of multiple food items, were also resolved by deep learning algorithms. In addition, proprietary mobile apps were developed to estimate the nutrition facts of the food in an image through cloud services, where a nutrient composition database is hosted. Examples of these cutting-edge food image recognition apps (where nutrition facts can be simultaneously estimated) include Calorie Mama, Foodzilla, Lose it!, and Mealviser.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6407</offset><text>While food image recognition by deep learning models has the potential to inform dietary decisions and facilitate health promotion, to date, this method has not been applied on a large scale (e.g., all restaurants in a city). In this paper, we investigate the applicability of this technology for the nutrition assessment of restaurant food using crowdsourced food review images in a US metropolitan area, the Greater Hartford region. Then, we validate the deep-learned results with manually coded nutrition information. Lastly, the paper explores the implication of the new method for community food environmental studies through nutrition mapping and food inequality assessment. These endeavors not only exhibit an exploratory case study of the deep learning model for nutrition assessment but also the potential of the new method for assisting health policymaking and health promotion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>7296</offset><text>2. Materials and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7321</offset><text>Our case study was conducted in the Greater Hartford region in the US. As the capital city of Connecticut, Hartford is the fourth most populous city in the state. The total population of Hartford County was estimated to be 891,720 as of 2019, where the demographics (74.8% white, 15.8% African American, 6.1% Asian, 0.6% Native American) were close to the national average. Because of its vibrant economy and diverse populations, the city serves as a cultural destination and food hub for Central and Eastern Connecticut. As Hartford has intensive spatial interaction with surrounding areas in terms of traffic, human movements, and services, we expanded our study area to Hartford and its five satellite cities (Bloomfield, West Hartford, East Hartford, Newington, and Wethersfield) to portray a comprehensive foodscape.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8143</offset><text>2.1. Data Collection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8164</offset><text>As the study was focused on the nutrition assessment of restaurant food, we utilized two datasets: the restaurant directory and food review images. The first dataset, the restaurant directory, included the business information (e.g., name, address, hours of operation, and contact information) of all restaurants in the study area. This dataset was sourced by using Yelp (i.e., a business listing website with foci on restaurant ratings and reviews) and its Fusion application programming interface (API). The data were further processed by Python codes to retrieve additional information, such as restaurant category, rating, and review count. Then, we cleaned the data by manually cross-validating with Google Place (i.e., a business listing website with foci on restaurant locations and reviews), purging restaurants that were unidentified, permanently closed, or mislabeled (e.g., supermarkets, convenience stores, food pantries). This cross-validation process using Yelp and Google Place ensured a high degree of accuracy and timeliness in the restaurant directory. Eventually, 487 restaurants fit the inclusion criteria and composed the initial restaurant directory dataset for further investigation. These restaurants were visualized in ESRI ArcGIS Pro, as shown in Figure 1.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9447</offset><text>Our second dataset consisted of food review images shared by online users. These images were collected from two different sources—Google Place and Tripadvisor (i.e., a travel advisory website with user-generated reviews)—and were combined for the same restaurant listing. Specifically, the image data collection was conducted by using the simple mass downloader extension on Google Chrome. A total of 19,907 images were initially collected and were manually refined against the following exclusion criteria: (1) the image was staged or was part of an advertisement; (2) the image featured beverages; (3) the image was about a non-food item, such as buildings, dining environments, and people; (4) the restaurant had less than five images. The data collection from two sources and the follow-up filtering process ensured a high degree of completeness and accuracy in the food image dataset. Our final food image dataset included 15,908 images from 470 restaurants, where each image was related to the restaurant by a common ID. These food images were further standardized to 544 by 544 pixels by Python scripting for further processing in the deep learning model.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10614</offset><text>It should be noted that as of 2021, Google, Yelp, and Tripadvisor were the top three review platforms for consumers to make decisions about business patronage. Thus, the choice of the three platforms in our study in terms of retrieving the restaurant directory, crowdsourcing food images, and cross-validation ensured the representativeness of the data and avoided the possible selection bias of just focusing on one platform.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11041</offset><text>2.2. Nutrition Assessments by Deep Learning Model and Manual Coding</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11109</offset><text>We employed a proprietary deep learning model, Calorie Mama, for the nutrition assessment of food images. Developed by Azumio Inc. (Palo Alto, US), Calorie Mama is a deep learning-based image recognition model aiming at the nutrition assessment of food images. We chose Calorie Mama, as a recent comparative study showed that Calorie Mama was the most accurate platform with a top 1 accuracy of 63% and a top 5 accuracy of 88%. When a food image is loaded into the deep-learning model, the model returns the most likely food label and its corresponding nutrition information. The derived nutrition information includes calories, macronutrients (carbohydrates, fats, and proteins), and micronutrients (vitamins and minerals) in the International System of Units (SI) (e.g., calories per 1 kg food). In addition, the model can identify not only fresh produce but also prepared dishes, including regional cuisines and ethnic specialty dishes. Figure 2 shows an example of the nutrition assessment for a crowdsourced food image.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12134</offset><text>We employed the Calorie Mama API to batch process all collected food images and then derived their nutrition information. To validate the results, two trained raters performed a manual nutrition assessment on a random sample of 281 images from 20 restaurants. The two raters independently coded the sample, including food type, nutrition information, and portion size, based on the 2017–2018 Food and Nutrient Database for Dietary Studies (FNDDS). Out of the 281 images, 75 images were double-coded to examine the inter-rater reliability and ensure the validity of the assessment. The rated nutrition information was then compared with that identified by the deep learning model.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>12816</offset><text>2.3. Restaurant Nutrition (RN) Index</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12853</offset><text>Lastly, we performed the nutrition assessment of the restaurants based on the average calories of all food images for each restaurant (i.e., average calories per 1 kg food) and visualized the nutrition information in a Geographic Information System (GIS). Furthermore, we proposed the restaurant nutrition (RN) index by aggregating the restaurants’ calorie estimates on the census tract level. We validated this new index by (1) quantitatively comparing it with an established food environment index—the Modified Retail Food Environment Index (mRFEI), which evaluates the ratio of healthy food retailers (e.g., supermarkets) to unhealthy food retailers (e.g., fast-food restaurants) in a census tract, and (2) assessing the Pearson’s correlations between the RN index and key variables derived from the CDC’s 2018 Social Vulnerability Index (SVI). The SVI utilizes American Community Survey’s (ACS) 5-year estimates to determine the relative vulnerability of census tracts under four categories: socioeconomic status (SES), household composition and disability, minority status and language, and housing type and transportation. The flow chart of the study is shown in Figure 3.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>14042</offset><text>3. Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>14053</offset><text>3.1. Deep Learning Model Validation</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>14089</offset><text>Out of the 75 images that were double-coded, the two raters agreed with each other on 71 images in terms of food types and FNDDS codes, reaching inter-rater reliability of 94.7%. Out of the 281 coded images, we found that the deep learning model correctly identified 211 food images, reaching an accuracy level of 75.1%. Four images were incorrectly identified by both the manual coding and the deep learning model due to poor image quality. It is noted that the deep learning model had more specific and accurate food labels for 24 images, which were mostly ethnic food items (i.e., specifically for Korean dishes and less so for Mexican, Italian, and Chinese dishes). These ethnic food labels were not specified in the FNDDS.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>14817</offset><text>The deep learning model is subject to various limitations. First, we found that the model had inaccurate identifications for images containing multiple food items, where it could only identify one of the food items present in the image. Second, the identification of certain food items was less precise and accurate. For example, the model identified most sandwiches and burgers as the “beef burger”; it also labeled many specialty salads as the “Caesar salad”, while there was apparent variability in the salad type by manual coding. Third, the deep learning model was unable to estimate the portion size from a food image.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>15450</offset><text>3.2. Nutrition Mapping</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>15473</offset><text>The nutrition information identified from the 15,908 images for 470 restaurants can be further employed to estimate the nutrition quality of restaurants, which cannot be easily accomplished by traditional nutrition assessment methods. In this study, we estimated the nutrition quality of a restaurant by averaging the calories for all of its food images in the normalized SI unit (i.e., average calories for 1 kg food). Since the nutrition information is standardized on the restaurant level, it can be compared across all restaurants. We then employed ESRI ArcGIS Pro to map the calorific level in five color-coded classes, where the blue dots represent the lowest-calorie level and the red dots the highest. The mapping result is shown in Figure 4.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>16224</offset><text>3.3. Restaurant Nutrition Index—Measuring the Community Food Environment</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>16299</offset><text>The derived nutrition information on the restaurant level can be further leveraged for justifying critical inequality issues in the community food environment. This revelation can contribute to the lack of nutrition component in existing community food environmental studies. Specifically, we have proposed a restaurant nutrition (RN) index by aggregating the restaurants’ calorie estimates for each census tract (mean = 2214.92, standard deviation (SD) = 276.98, min = 1500, max = 3028.88). The result is shown in Figure 5. This result reveals that the high-index census tracts (red-colored tracts in Figure 5), representing areas with the relative concentration of high-calorie restaurants, are mostly found in the northeast quadrant of the study area, which also happens to be the areas with low food access and fewer healthy food retailers as measured by other food environment indices, such as the Food Access Research Atlas and the mRFEI. While the low-index census tracts (green-colored tracts in Figure 5) are more scattered geographically, a moderate consistency is identified between the RN index and the mRFEI in some census tracts, especially in the western and southern quadrants of the study area. Overall, we have identified a moderate consistency and weak Pearson’s correlation between the RN index and the mRFEI (r = −141, p = 0.26).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>17656</offset><text>To further explore the inequality patterns in the nutrition landscape, we correlated the derived RN index with selected socioeconomic and demographic variables in CDC’s 2018 SVI data on the census tract level. The Pearson’s correlation analysis was performed between selected SVI variables with the RN index across census tracts with available data (n = 66). The result is shown in Table 1.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18051</offset><text>Table 1 shows that there are moderate positive correlations between the RN index and three SVI variables representing socioeconomic status, household composition, and housing type, respectively. These correlated variables include % persons (age 25+) with no high school diploma (r = 0.24, p = 0.057), % single-parent household with children (r = 0.29, p = 0.018), and % persons in group quarters (r = 0.37, p = 0.002). The result signifies that the food inequality pattern in terms of restaurant nutrition does not have a perfect one-to-one matching with social vulnerability in the study area, as the RN index only correlates with some social vulnerability indicators but not the others (e.g., poverty rate, income, and vehicle access). The result can be explained by the argument of using fast-food access as a deprivation indicator—a systematic literature review shows that while many studies (i.e., 16 out of 21 studies) identified that fast-food restaurants were more prevalent in SES-deprived areas, other studies did not reveal such a correlation. To this end, our study can shed insights into the justification of the food inequality—although the Greater Hartford region is regarded as one of the most segregated US metropolitan areas, access to nutritional restaurants across different neighborhoods may be less segregated and exhibit a complex geographical pattern.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>19431</offset><text>4. Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>19445</offset><text>While deep learning models have been vigorously developed for food image recognition and nutrition analysis, this study is among the first to leverage this emerging technology for large-scale nutrition assessment of restaurant food. By crowdsourcing food images from food review websites, the study provides a pilot approach to restaurant nutrition assessment and can complement traditional nutrition assessment measures.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>19867</offset><text>First, the study is among the first to bridge a crowdsourcing approach with a deep learning model to improve the efficiency of nutrition assessment. Existing nutrition assessment measures, including individual dietary assessments (e.g., FFQ, 24HR, and DR) and nutrition environment assessments (e.g., Nutrition Environment Measure Survey—Restaurant [NEMS-R]), collect data on the individual or restaurant level. While they standardize the protocol and variables in the assessment, they are subject to considerable efforts in data collection, testing, and coding. The crowdsourcing method applied to all restaurants on a large scale can automate dietary data collection by largely reducing time, labor, and cost. Additionally, we validated the accuracy of the deep learning model at 75.1% based on the FNDDS codes. Although the accuracy level was not as high as those of the traditional measures, the new method can serve to gain an overarching picture of the regional restaurants’ nutrition landscape at a relatively low cost and high efficiency. From a practical perspective, this method is easily scalable and can be implemented for the nutrition assessment of small, individual-owned restaurants or in underdeveloped countries where nutrition labeling is not readily available.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>21152</offset><text>Second, mapping the nutritional information of restaurants can shed insights into health policymaking and health promotion. With advances in geospatial technologies, primarily GIS, it has become viable to reveal the spatial distribution of food sources across communities and develop food indices and tools, such as the Food Access Research Atlas, the Food Environment Atlas, the “food swamp” index, and the mRFEI. These spatial endeavors have been criticized for an overemphasis on the spatial pattern of food establishments (e.g., proximity, density, varieties), while lacking the food quality and nutrition measures to justify the food environment–diet relationship. The quality and nutrition of food sources, coined by Glanz et al. as the consumer nutrition environment, play a pivotal role in dictating community health. Our study takes a major step in filling this gap by empirically measuring food quality and nutrition in this consumer nutrition environment. While our results show that restaurants with higher calories are more likely to locate in socially vulnerable areas to some degree (e.g., population with lower education, more single-parent households, and living in group quarters), the weak correlation between our proposed RN index and the mRFEI is also somewhat expected and points to the potential discrepancies between spatial food provisioning and food nutrition. By revealing the inequality of nutrition information across different communities, stakeholders can go beyond the simple categorization of food sources and be informed of regional pockets where calorie-dense, nutrient-poor restaurants prevail and where efforts for nutrition assistance and improvement should prioritize. Moreover, the nutrition mapping results can be further developed into an interactive tool to facilitate health promotion in communities inundated with calorie-dense, nutrient-poor restaurants.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>23059</offset><text>As a pilot study, this research has limitations. First, using the crowdsourcing approach deviated from a systematic sampling method and might not fully characterize all restaurants’ nutrition information. The primary dataset, the food images, was solicited from two food review websites. This crowdsourcing approach excluded nutrition information from restaurants that lacked an online presence. For example, it was found that restaurant reviews on Google Place were unevenly distributed, where national chain restaurants were less likely to receive a review than independently-operated restaurants. Second, there were considerable uncertainties about the users who uploaded the food images, as the restaurant reviewers did not mirror the demographics of local residents because of the existence of the “digital divide”. Specifically, young adults were overrepresented in the online community and could influence the restaurants and the food items being reviewed. Third, our tests of validity showed that the deep learning model achieved only 75.1% accuracy, as the model was incapable of estimating portion sizes, identifying certain food items, or distinguishing multiple food items in an image. It is expected that the model performance can be improved by incorporating other food image recognition methods and by cross-validating the results with trained raters. Finally, we only focused on the Greater Hartford region on the census tract level, and therefore the correlation results may not be generalized for another study area or on a different analysis scale. However, the study design and implementation are transferrable to other study areas, especially in countries where menu labeling data are missing.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>24780</offset><text>5. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>24795</offset><text>In this paper, we explore a new deep learning approach for the nutrition assessment of restaurant food. We also validate the accuracy of the method by cross-validating with the FNDDS calorie information. We further estimate the nutrition information of restaurants in the study area and further develop the RN index to explore the food inequality issue in the consumer nutrition environment. Our results show that the deep learning model can be empowered by the crowdsourced food images through gathering dietary data at a minimum cost and acceptable data quality. However, the new method is still in the early phase of development due to the compromised accuracy in the model performance and the many uncertainties in the user-generated dietary data. Thus, this new method should only complement, rather than replace, traditional nutrition assessment methods for estimating the nutrition information. We believe that the new method holds promise as a new instrument for large-scale nutrition assessment when the deep learning model is further improved and when additional means are employed for data screening and result validation. Eventually, we expect that the deep-learned nutrition information can serve as evidence for developing an information system for nutrition education and health promotion.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>26100</offset><text>Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>26226</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>26247</offset><text>Conceptualization, X.C. and R.X.; methodology, X.C., A.K., C.D. and R.X.; validation, Y.C. and N.R.; formal analysis, E.J., A.K. and R.X.; data curation, A.K. and E.J.; writing—original draft preparation, E.J. and X.C.; writing—review and editing, A.K., Y.C., N.R., C.D. and R.X.; visualization, X.C.; supervision, R.X.; funding acquisition, X.C., C.D. and R.X. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>26690</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>26698</offset><text>This research was supported by a hatch grant from the College of Agriculture, Health, and Natural Resources, University of Connecticut, funded by the National Institute of Food and Agriculture, United States Department of Agriculture, grant number CONS01031.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>26957</offset><text>Institutional Review Board Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>26994</offset><text>Not applicable.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>27010</offset><text>Informed Consent Statement</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>27037</offset><text>Not applicable.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title</infon><offset>27053</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>27081</offset><text>The data presented in this study are available upon request from the corresponding author. The data are not publicly available due to privacy concerns.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>27233</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>27255</offset><text>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>27485</offset><text>References</text></passage><passage><infon key="fpage">140</infon><infon key="lpage">150</infon><infon key="name_0">surname:Guthrie;given-names:J.F.</infon><infon key="name_1">surname:Lin;given-names:B.-H.</infon><infon key="name_2">surname:Frazao;given-names:E.</infon><infon key="pub-id_doi">10.1016/S1499-4046(06)60083-3</infon><infon key="pub-id_pmid">12047838</infon><infon key="section_type">REF</infon><infon key="source">J. Nutr. Educ. Behav.</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2002</infon><offset>27496</offset><text>Role of food prepared away from home in the American diet, 1977–78 versus 1994–96: Changes and consequences</text></passage><passage><infon key="fpage">370</infon><infon key="lpage">378</infon><infon key="name_0">surname:Nielsen;given-names:S.J.</infon><infon key="name_1">surname:Siega-Riz;given-names:A.M.</infon><infon key="name_2">surname:Popkin;given-names:B.M.</infon><infon key="pub-id_doi">10.1038/oby.2002.51</infon><infon key="pub-id_pmid">12006636</infon><infon key="section_type">REF</infon><infon key="source">Obes. Res.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2002</infon><offset>27608</offset><text>Trends in energy intake in US between 1977 and 1996: Similar shifts seen across age groups</text></passage><passage><infon key="comment">Available online: https://www.ers.usda.gov/data-products/food-expenditure-series/</infon><infon key="section_type">REF</infon><infon key="source">Constant Dollar Food and Alcohol Expenditures, without Taxes and Tips, for all Purchasers</infon><infon key="type">ref</infon><offset>27699</offset><text>Food Expenditure Series</text></passage><passage><infon key="name_0">surname:Lin;given-names:B.-H.</infon><infon key="name_1">surname:Guthrie;given-names:J.F.</infon><infon key="section_type">REF</infon><infon key="source">Nutritional Quality of Food Prepared at Home and Away from Home, 1977–2008</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>27723</offset></passage><passage><infon key="fpage">539</infon><infon key="lpage">545</infon><infon key="name_0">surname:Gillis;given-names:L.J.</infon><infon key="name_1">surname:Bar-Or;given-names:O.</infon><infon key="pub-id_doi">10.1080/07315724.2003.10719333</infon><infon key="pub-id_pmid">14684760</infon><infon key="section_type">REF</infon><infon key="source">J. Am. Coll. Nutr.</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2003</infon><offset>27724</offset><text>Food away from home, sugar-sweetened drink consumption and juvenile obesity</text></passage><passage><infon key="fpage">459</infon><infon key="lpage">469</infon><infon key="name_0">surname:Mancino;given-names:L.</infon><infon key="name_1">surname:Todd;given-names:J.E.</infon><infon key="name_2">surname:Guthrie;given-names:J.</infon><infon key="name_3">surname:Lin;given-names:B.-H.</infon><infon key="pub-id_doi">10.1007/s13679-014-0121-z</infon><infon key="pub-id_pmid">26626922</infon><infon key="section_type">REF</infon><infon key="source">Curr. Obes. Rep.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2014</infon><offset>27800</offset><text>Food away from home and childhood obesity</text></passage><passage><infon key="fpage">875S</infon><infon key="lpage">880S</infon><infon key="name_0">surname:Potischman;given-names:N.</infon><infon key="pub-id_doi">10.1093/jn/133.3.875S</infon><infon key="pub-id_pmid">12612173</infon><infon key="section_type">REF</infon><infon key="source">J. Nutr.</infon><infon key="type">ref</infon><infon key="volume">133</infon><infon key="year">2003</infon><offset>27842</offset><text>Biologic and methodologic issues for nutritional biomarkers</text></passage><passage><infon key="elocation-id">1092</infon><infon key="name_0">surname:Picó;given-names:C.</infon><infon key="name_1">surname:Serra;given-names:F.</infon><infon key="name_2">surname:Rodríguez;given-names:A.M.</infon><infon key="name_3">surname:Keijer;given-names:J.</infon><infon key="name_4">surname:Palou;given-names:A.</infon><infon key="pub-id_doi">10.3390/nu11051092</infon><infon key="section_type">REF</infon><infon key="source">Nutrients</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2019</infon><offset>27902</offset><text>Biomarkers of nutrition and health: New tools for new approaches</text></passage><passage><infon key="fpage">S37</infon><infon key="lpage">S53</infon><infon key="name_0">surname:Wild;given-names:C.</infon><infon key="name_1">surname:Andersson;given-names:C.</infon><infon key="name_2">surname:O’Brien;given-names:N.</infon><infon key="name_3">surname:Wilson;given-names:L.</infon><infon key="name_4">surname:Woods;given-names:J.</infon><infon key="pub-id_doi">10.1079/BJN2001338</infon><infon key="pub-id_pmid">11520423</infon><infon key="section_type">REF</infon><infon key="source">Br. J. Nutr.</infon><infon key="type">ref</infon><infon key="volume">86</infon><infon key="year">2001</infon><offset>27967</offset><text>A critical evaluation of the application of biomarkers in epidemiological studies on diet and health</text></passage><passage><infon key="fpage">e2014009</infon><infon key="name_0">surname:Shim;given-names:J.-S.</infon><infon key="name_1">surname:Oh;given-names:K.</infon><infon key="name_2">surname:Kim;given-names:H.C.</infon><infon key="pub-id_doi">10.4178/epih/e2014009</infon><infon key="pub-id_pmid">25078382</infon><infon key="section_type">REF</infon><infon key="source">Epidemiol. Health</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2014</infon><offset>28068</offset><text>Dietary assessment methods in epidemiologic studies</text></passage><passage><infon key="fpage">24</infon><infon key="lpage">28</infon><infon key="name_0">surname:Williamson;given-names:D.A.</infon><infon key="name_1">surname:Allen;given-names:H.</infon><infon key="name_2">surname:Martin;given-names:P.D.</infon><infon key="name_3">surname:Alfonso;given-names:A.</infon><infon key="name_4">surname:Gerald;given-names:B.</infon><infon key="name_5">surname:Hunt;given-names:A.</infon><infon key="pub-id_doi">10.1007/BF03325041</infon><infon key="pub-id_pmid">15185830</infon><infon key="section_type">REF</infon><infon key="source">Eat. Weight Disord.-Stud. Anorex. Bulim. Obes.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2004</infon><offset>28120</offset><text>Digital photography: A new method for estimating food intake in cafeteria settings</text></passage><passage><infon key="fpage">446</infon><infon key="lpage">456</infon><infon key="name_0">surname:Martin;given-names:C.K.</infon><infon key="name_1">surname:Han;given-names:H.</infon><infon key="name_2">surname:Coulon;given-names:S.M.</infon><infon key="name_3">surname:Allen;given-names:H.R.</infon><infon key="name_4">surname:Champagne;given-names:C.M.</infon><infon key="name_5">surname:Anton;given-names:S.D.</infon><infon key="pub-id_doi">10.1017/S0007114508027438</infon><infon key="pub-id_pmid">18616837</infon><infon key="section_type">REF</infon><infon key="source">Br. J. Nutr.</infon><infon key="type">ref</infon><infon key="volume">101</infon><infon key="year">2008</infon><offset>28203</offset><text>A novel method to remotely measure food intake of free-living individuals in real time: The remote food photography method</text></passage><passage><infon key="fpage">72</infon><infon key="lpage">81</infon><infon key="name_0">surname:Martin;given-names:C.K.</infon><infon key="name_1">surname:Nicklas;given-names:T.</infon><infon key="name_2">surname:Gunturk;given-names:B.</infon><infon key="name_3">surname:Correa;given-names:J.B.</infon><infon key="name_4">surname:Allen;given-names:H.R.</infon><infon key="name_5">surname:Champagne;given-names:C.</infon><infon key="pub-id_doi">10.1111/jhn.12014</infon><infon key="pub-id_pmid">23848588</infon><infon key="section_type">REF</infon><infon key="source">J. Hum. Nutr. Diet.</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2014</infon><offset>28326</offset><text>Measuring food intake with digital photography</text></passage><passage><infon key="fpage">109</infon><infon key="lpage">116</infon><infon key="name_0">surname:Kikunaga;given-names:S.</infon><infon key="name_1">surname:Tin;given-names:T.</infon><infon key="name_2">surname:Ishibashi;given-names:G.</infon><infon key="name_3">surname:Wang;given-names:D.-H.</infon><infon key="name_4">surname:Kira;given-names:S.</infon><infon key="pub-id_doi">10.3177/jnsv.53.109</infon><infon key="pub-id_pmid">17615997</infon><infon key="section_type">REF</infon><infon key="source">J. Nutr. Sci. Vitaminol.</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2007</infon><offset>28373</offset><text>The application of a handheld personal digital assistant with camera and mobile phone card (Wellnavi) to the general population in a dietary survey</text></passage><passage><infon key="fpage">318</infon><infon key="lpage">323</infon><infon key="name_0">surname:Rollo;given-names:M.E.</infon><infon key="name_1">surname:Ash;given-names:S.</infon><infon key="name_2">surname:Lyons-Wall;given-names:P.</infon><infon key="name_3">surname:Russell;given-names:A.</infon><infon key="pub-id_doi">10.1258/jtt.2011.100906</infon><infon key="pub-id_pmid">21844173</infon><infon key="section_type">REF</infon><infon key="source">J. Telemed. Telecare</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2011</infon><offset>28521</offset><text>Trial of a mobile phone method for recording dietary intake in adults with type 2 diabetes: Evaluation and implications for future applications</text></passage><passage><infon key="elocation-id">657</infon><infon key="name_0">surname:Mezgec;given-names:S.</infon><infon key="name_1">surname:Koroušić Seljak;given-names:B.</infon><infon key="pub-id_doi">10.3390/nu9070657</infon><infon key="pub-id_pmid">28653995</infon><infon key="section_type">REF</infon><infon key="source">Nutrients</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2017</infon><offset>28665</offset><text>NutriNet: A deep learning food and drink image recognition system for dietary assessment</text></passage><passage><infon key="fpage">458</infon><infon key="lpage">465</infon><infon key="name_0">surname:Christodoulidis;given-names:S.</infon><infon key="name_1">surname:Anthimopoulos;given-names:M.</infon><infon key="name_2">surname:Mougiakakou;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Image Analysis and Processing</infon><infon key="type">ref</infon><offset>28754</offset><text>Food recognition for dietary assessment using deep convolutional neural networks</text></passage><passage><infon key="fpage">589</infon><infon key="lpage">593</infon><infon key="name_0">surname:Kawano;given-names:Y.</infon><infon key="name_1">surname:Yanai;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication</infon><infon key="type">ref</infon><offset>28835</offset><text>Food image recognition with deep convolutional features</text></passage><passage><infon key="fpage">37</infon><infon key="lpage">48</infon><infon key="name_0">surname:Liu;given-names:C.</infon><infon key="name_1">surname:Cao;given-names:Y.</infon><infon key="name_2">surname:Luo;given-names:Y.</infon><infon key="name_3">surname:Chen;given-names:G.</infon><infon key="name_4">surname:Vokkarane;given-names:V.</infon><infon key="name_5">surname:Ma;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Smart Homes and Health Telematics</infon><infon key="type">ref</infon><offset>28891</offset><text>Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment</text></passage><passage><infon key="fpage">23</infon><infon key="lpage">28</infon><infon key="name_0">surname:Dehais;given-names:J.</infon><infon key="name_1">surname:Anthimopoulos;given-names:M.</infon><infon key="name_2">surname:Mougiakakou;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management</infon><infon key="type">ref</infon><offset>28982</offset><text>Food image segmentation for dietary assessment</text></passage><passage><infon key="comment">Available online: https://datausa.io/profile/geo/hartford-county-ct</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29029</offset><text>Hartford County, CT</text></passage><passage><infon key="comment">Available online: https://www.yelp.com/fusion</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29049</offset><text>Yelp Fusion</text></passage><passage><infon key="comment">Available online: https://chrome.google.com</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29061</offset><text>Simple Mass Downloader</text></passage><passage><infon key="comment">Available online: https://www.reviewtrackers.com/reports/online-reviews-survey/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29084</offset><text>2021 Online Reviews Statistics and Trends: A Report by ReviewTrackers</text></passage><passage><infon key="comment">Available online: https://dev.caloriemama.ai/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29154</offset><text>Calorie Mama API</text></passage><passage><infon key="fpage">e15602</infon><infon key="name_0">surname:Van Asbroeck;given-names:S.</infon><infon key="name_1">surname:Matthys;given-names:C.</infon><infon key="pub-id_doi">10.2196/15602</infon><infon key="pub-id_pmid">33284118</infon><infon key="section_type">REF</infon><infon key="source">JMIR Form. Res.</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2020</infon><offset>29171</offset><text>Use of Different Food Image Recognition Platforms in Dietary Assessment: Comparison Study</text></passage><passage><infon key="comment">Available online: https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/food-surveys-research-group/docs/fndds/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29261</offset><text>Food and Nutrient Database for Dietary Studies</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Census Tract Level State Maps of the Modified Retail Food Environment Index (mRFEI)</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>29308</offset></passage><passage><infon key="comment">Available online: https://www.atsdr.cdc.gov/placeandhealth/svi/index.html</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29309</offset><text>CDC/ATSDR Social Vulnerability Index</text></passage><passage><infon key="fpage">330</infon><infon key="lpage">333</infon><infon key="name_0">surname:Glanz;given-names:K.</infon><infon key="name_1">surname:Sallis;given-names:J.F.</infon><infon key="name_2">surname:Brian;given-names:E.S.</infon><infon key="name_3">surname:Lawrence;given-names:D.F.</infon><infon key="pub-id_doi">10.4278/0890-1171-19.5.330</infon><infon key="pub-id_pmid">15895534</infon><infon key="section_type">REF</infon><infon key="source">Am. J. Health Promot.</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2005</infon><offset>29346</offset><text>Healthy nutrition environments: Concepts and measures</text></passage><passage><infon key="comment">Available online: https://www.ers.usda.gov/data-products/food-access-research-atlas/go-to-the-atlas/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29400</offset><text>Food Access Research Atlas</text></passage><passage><infon key="fpage">e460</infon><infon key="lpage">e471</infon><infon key="name_0">surname:Fleischhacker;given-names:S.E.</infon><infon key="name_1">surname:Evenson;given-names:K.R.</infon><infon key="name_2">surname:Rodriguez;given-names:D.A.</infon><infon key="name_3">surname:Ammerman;given-names:A.S.</infon><infon key="pub-id_doi">10.1111/j.1467-789X.2010.00715.x</infon><infon key="pub-id_pmid">20149118</infon><infon key="section_type">REF</infon><infon key="source">Obes. Rev.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2011</infon><offset>29427</offset><text>A systematic review of fast food access studies</text></passage><passage><infon key="name_0">surname:Florida;given-names:R.</infon><infon key="name_1">surname:Mellander;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Segregated City: The Geography of Economic Segregation in America’s Metros</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>29475</offset></passage><passage><infon key="fpage">273</infon><infon key="lpage">281</infon><infon key="name_0">surname:Saelens;given-names:B.E.</infon><infon key="name_1">surname:Glanz;given-names:K.</infon><infon key="name_2">surname:Sallis;given-names:J.F.</infon><infon key="name_3">surname:Frank;given-names:L.D.</infon><infon key="pub-id_doi">10.1016/j.amepre.2006.12.022</infon><infon key="pub-id_pmid">17383558</infon><infon key="section_type">REF</infon><infon key="source">Am. J. Prev. Med.</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2007</infon><offset>29476</offset><text>Nutrition Environment Measures Study in restaurants (NEMS-R): Development and evaluation</text></passage><passage><infon key="fpage">1773</infon><infon key="lpage">1785</infon><infon key="name_0">surname:Charreire;given-names:H.</infon><infon key="name_1">surname:Casey;given-names:R.</infon><infon key="name_2">surname:Salze;given-names:P.</infon><infon key="name_3">surname:Simon;given-names:C.</infon><infon key="name_4">surname:Chaix;given-names:B.</infon><infon key="name_5">surname:Banos;given-names:A.</infon><infon key="name_6">surname:Badariotti;given-names:D.</infon><infon key="name_7">surname:Weber;given-names:C.</infon><infon key="name_8">surname:Oppert;given-names:J.-M.</infon><infon key="pub-id_doi">10.1017/S1368980010000753</infon><infon key="pub-id_pmid">20409354</infon><infon key="section_type">REF</infon><infon key="source">Public Health Nutr.</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2010</infon><offset>29565</offset><text>Measuring the food environment using geographical information systems: A methodological review</text></passage><passage><infon key="comment">Available online: https://www.ers.usda.gov/data-products/food-environment-atlas/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29660</offset><text>USDA.Food Environment Atlas</text></passage><passage><infon key="elocation-id">1366</infon><infon key="name_0">surname:Cooksey-Stowers;given-names:K.</infon><infon key="name_1">surname:Schwartz;given-names:M.</infon><infon key="name_2">surname:Brownell;given-names:K.</infon><infon key="pub-id_doi">10.3390/ijerph14111366</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Environ. Res. Public Health</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2017</infon><offset>29688</offset><text>Food swamps predict obesity rates better than food deserts in the United States</text></passage><passage><infon key="fpage">112858</infon><infon key="name_0">surname:Phillips;given-names:A.Z.</infon><infon key="name_1">surname:Rodriguez;given-names:H.P.</infon><infon key="pub-id_doi">10.1016/j.socscimed.2020.112858</infon><infon key="pub-id_pmid">32088514</infon><infon key="section_type">REF</infon><infon key="source">Soc. Sci. Med.</infon><infon key="type">ref</infon><infon key="volume">249</infon><infon key="year">2020</infon><offset>29768</offset><text>US county “food swamp” severity and hospitalization rates among adults with diabetes: A nonlinear relationship</text></passage><passage><infon key="fpage">257</infon><infon key="lpage">260</infon><infon key="name_0">surname:Widener;given-names:M.J.</infon><infon key="pub-id_doi">10.1016/j.physbeh.2018.02.032</infon><infon key="pub-id_pmid">29454842</infon><infon key="section_type">REF</infon><infon key="source">Physiol. Behav.</infon><infon key="type">ref</infon><infon key="volume">193</infon><infon key="year">2018</infon><offset>29883</offset><text>Spatial access to food: Retiring the food desert metaphor</text></passage><passage><infon key="fpage">1734</infon><infon key="lpage">1737</infon><infon key="name_0">surname:Chen;given-names:X.</infon><infon key="name_1">surname:Kwan;given-names:M.-P.</infon><infon key="pub-id_doi">10.2105/AJPH.2015.302792</infon><infon key="pub-id_pmid">26180982</infon><infon key="section_type">REF</infon><infon key="source">Am. J. Public Health</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2015</infon><offset>29941</offset><text>Contextual uncertainties, human mobility, and perceived food environment: The uncertain geographic context problem in food access research</text></passage><passage><infon key="fpage">443</infon><infon key="lpage">455</infon><infon key="name_0">surname:Baginski;given-names:J.</infon><infon key="name_1">surname:Sui;given-names:D.</infon><infon key="name_2">surname:Malecki;given-names:E.J.</infon><infon key="pub-id_doi">10.1080/00330124.2013.866431</infon><infon key="section_type">REF</infon><infon key="source">Prof. Geogr.</infon><infon key="type">ref</infon><infon key="volume">66</infon><infon key="year">2014</infon><offset>30080</offset><text>Exploring the intraurban digital divide using online restaurant reviews: A case study in Franklin County, Ohio</text></passage><passage><infon key="fpage">15</infon><infon key="lpage">29</infon><infon key="name_0">surname:Kelley;given-names:M.J.</infon><infon key="pub-id_doi">10.1007/s10708-013-9482-1</infon><infon key="section_type">REF</infon><infon key="source">GeoJournal</infon><infon key="type">ref</infon><infon key="volume">79</infon><infon key="year">2014</infon><offset>30191</offset><text>Urban experience takes an informational turn: Mobile internet usage and the unevenness of geosocial activity</text></passage><passage><infon key="fpage">602</infon><infon key="lpage">621</infon><infon key="name_0">surname:Hargittai;given-names:E.</infon><infon key="name_1">surname:Hinnant;given-names:A.</infon><infon key="pub-id_doi">10.1177/0093650208321782</infon><infon key="section_type">REF</infon><infon key="source">Commun. Res.</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">2008</infon><offset>30300</offset><text>Digital inequality: Differences in young adults’ use of the Internet</text></passage><passage><infon key="file">nutrients-13-04132-g001.jpg</infon><infon key="id">nutrients-13-04132-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30371</offset><text>Spatial distribution of restaurants in the study area.</text></passage><passage><infon key="file">nutrients-13-04132-g002.jpg</infon><infon key="id">nutrients-13-04132-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30426</offset><text>Example of nutrition assessment by the deep learning model.</text></passage><passage><infon key="file">nutrients-13-04132-g003.jpg</infon><infon key="id">nutrients-13-04132-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30486</offset><text>Flowchart of nutrition assessment by the deep learning model.</text></passage><passage><infon key="file">nutrients-13-04132-g004.jpg</infon><infon key="id">nutrients-13-04132-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30548</offset><text>Estimated calorific level of restaurants in the study area.</text></passage><passage><infon key="file">nutrients-13-04132-g005.jpg</infon><infon key="id">nutrients-13-04132-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30608</offset><text>The RN index in terms of average calories of restaurant food by census tract.</text></passage><passage><infon key="file">nutrients-13-04132-t001.xml</infon><infon key="id">nutrients-13-04132-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>30686</offset><text>Pearson’s correlation analysis between selected SVI variables and the RN index on the census tract level (n = 66).</text></passage><passage><infon key="file">nutrients-13-04132-t001.xml</infon><infon key="id">nutrients-13-04132-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVI Variable&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean (SD)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Min/Max&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Correlation Coefficient (R)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%) below poverty&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.44 (13.38)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/49.2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Unemployment rate (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.16 (5.70)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/23.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.04&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Per capita income&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32,691.94 (16646.26)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5509/68,705&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.18&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%, age 25+) with no high school diploma&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17.19 (12.65)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.3/49&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.24 *&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%) aged 65 and older&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14.76 (6.32)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.6/28.7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.06&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%) aged 17 and younger&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21.54 (6.77)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.1/39.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.21 *&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%) with a disability&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13.02 (4.43)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/25.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Single-parent household (%) with children&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13.88 (14.19)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5/100&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.29 **&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Minority (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.95 (30.79)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9/100&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%, age 5+) who speaks english less than well&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.44 (6.80)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/25.1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.04&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Housing structures (%) with 10 or more units&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20.29 (20.56)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/87.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.09&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mobile homes (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.78 (3.60)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/25.5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Occupied housing units (%) with more people than rooms estimate&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.02 (2.96)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/10.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.05&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Households (%) with no vehicle&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18.89 (14.55)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/60.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Persons (%) in group quarters&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.94 (12.38)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0/93.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.37 ***&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>30803</offset><text>SVI Variable	Mean (SD)	Min/Max	Correlation Coefficient (R)	 	Persons (%) below poverty	17.44 (13.38)	0/49.2	0.03	 	Unemployment rate (%)	9.16 (5.70)	0/23.3	−0.04	 	Per capita income	32,691.94 (16646.26)	5509/68,705	−0.18	 	Persons (%, age 25+) with no high school diploma	17.19 (12.65)	0.3/49	0.24 *	 	Persons (%) aged 65 and older	14.76 (6.32)	1.6/28.7	−0.06	 	Persons (%) aged 17 and younger	21.54 (6.77)	2.1/39.3	−0.21 *	 	Persons (%) with a disability	13.02 (4.43)	0/25.4	−0.03	 	Single-parent household (%) with children	13.88 (14.19)	0.5/100	0.29 **	 	Minority (%)	60.95 (30.79)	9/100	−0.01	 	Persons (%, age 5+) who speaks english less than well	7.44 (6.80)	0/25.1	0.04	 	Housing structures (%) with 10 or more units	20.29 (20.56)	0/87.3	−0.09	 	Mobile homes (%)	0.78 (3.60)	0/25.5	0.15	 	Occupied housing units (%) with more people than rooms estimate	3.02 (2.96)	0/10.3	−0.05	 	Households (%) with no vehicle	18.89 (14.55)	0/60.4	−0.03	 	Persons (%) in group quarters	3.94 (12.38)	0/93.4	0.37 ***	 	</text></passage><passage><infon key="file">nutrients-13-04132-t001.xml</infon><infon key="id">nutrients-13-04132-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>31830</offset><text>*** p &lt; 0.01, ** p &lt; 0.05, * p &lt; 0.1. SVI variables with p &lt; 0.05 are in bold.</text></passage></document></collection>
