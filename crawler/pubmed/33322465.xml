<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201229</date><key>pmc.key</key><document><id>7763261</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/s20247115</infon><infon key="article-id_pmc">7763261</infon><infon key="article-id_pmid">33322465</infon><infon key="article-id_publisher-id">sensors-20-07115</infon><infon key="elocation-id">7115</infon><infon key="issue">24</infon><infon key="kwd">deep learning sentiment analysis deep fusion human activity analysis disastrous situations analysis social media</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Sadiq;given-names:Amin Muhammad</infon><infon key="name_1">surname:Ahn;given-names:Huynsik</infon><infon key="name_2">surname:Choi;given-names:Young Bok</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>0</offset><text>Human Sentiment and Activity Recognition in Disaster Situations Using Social Media Images Based on Deep Learning</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>113</offset><text>A rapidly increasing growth of social networks and the propensity of users to communicate their physical activities, thoughts, expressions, and viewpoints in text, visual, and audio material have opened up new possibilities and opportunities in sentiment and activity analysis. Although sentiment and activity analysis of text streams has been extensively studied in the literature, it is relatively recent yet challenging to evaluate sentiment and physical activities together from visuals such as photographs and videos. This paper emphasizes human sentiment in a socially crucial field, namely social media disaster/catastrophe analysis, with associated physical activity analysis. We suggest multi-tagging sentiment and associated activity analyzer fused with a a deep human count tracker, a pragmatic technique for multiple object tracking, and count in occluded circumstances with a reduced number of identity switches in disaster-related videos and images. A crowd-sourcing study has been conducted to analyze and annotate human activity and sentiments towards natural disasters and related images in social networks. The crowdsourcing study outcome into a large-scale benchmark dataset with three annotations sets each resolves distinct tasks. The presented analysis and dataset will anchor a baseline for future research in the domain. We believe that the proposed system will contribute to more viable communities by benefiting different stakeholders, such as news broadcasters, emergency relief organizations, and the public in general.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1661</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1677</offset><text>Human activity analysis aims to analyze, recognize, and classify the physical actions performed by an individual (e.g., standing, walking, running, etc.). Sentiment analysis aims to extract and evaluate an individual’s views, thoughts, and facial expressions response about an entity (e.g., object, service, or activity). Sentiment analysis is extensively adopted by organizations to help them understand the views of customers regarding their commodities and services. The aim of human activity detection is commonly used to evaluate either medical diagnosis or abnormal activity based on data obtained from wearable devices or accelerometer readings. A recent example is reported by Bevilacqua et al. where the raw data obtained from sensors were used to classify human activity. Researchers have been able to expand the reach of sentiment analysis to other fascinating applications through the recent growth and rise of social media. This recent study reported the application of computational sentiment analysis to extract public sentiments from leading social media platforms about the Syrian refugee crisis. A second example is reported by Ema et al. in which the neutrality of social media posts about the Austrian presidential election winner was evaluated and contrasted with the social media content of the rivals. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3005</offset><text>The notion of human sentiment and activity analysis has been commonly used in NLP (Natural Language Processing), where a variety of techniques have been used to derive optimistic, pessimistic, and neutral perception emotions and activities from text streams or sensors raw data. Due to significant development in NLP, in different application fields, such as sports, education, hospitality, and other businesses, an in-depth study of text streams from diverse sources is possible. There have lately been several proposals by researchers to extrapolate information from visual content about human activities and related sentiments as separate problems. An overwhelming amount of visual sentiment analysis research focused on facial close-up image data, where facial features are used as visual markers to infer emotions. Similarly, human activities have been largely deduced from nonparametric representations which are referred to as Part Affinity Fields (PAFs), which learn associated body parts of individuals in images. Efforts are being made to expand the visual approach to a more complicated visual context, for instance, background and context details and multiple objects. In this area of study, the recent developments in deep learning have also greatly enhanced the outcomes. Nevertheless, it is not straightforward to extract sentiments/emotions as well as human activity information together from visual content and many aspects need to be addressed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4468</offset><text>This work aims to discuss and analyze a challenging problem regarding human sentiments with associated physical activities in disaster-related images collected from social media platforms. People are increasingly using social media networks such as Instagram and Twitter to share situational alerts in the case of crises and emergencies, including news of missing or deceased people, damage to infrastructure and landmarks, calls for immediate concerns and assistance, and so forth. There are many forms of information on social networks, such as texts, images, and videos. In this context, we emphasize visual sentiment and associated physical activity response analysis from disaster-related visual content that is a promising area of study that will help consumers and society in a diverse range of applications. To this end, we suggest a deep sentiment and associated activity analyzer fused with a deep human count tracker to track the number of people in disaster-related visual content. We discuss the preprocessing pipeline of human sentiments and associated physical activity response analysis starting from the collection of image data, annotation, and conclude with the development and training of deep models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5690</offset><text>To the best of our knowledge, this is the first effort to develop a benchmark for deep learning-based sentiment and associated human activity analysis fused with a human count tracker in an occluded environment with a minimum number of identity switches in disaster-related videos and images. Images related to catastrophe situations are complicated and contain multiple objects in an occluded environment (e.g., in floods, broken walls, fire, etc.,) with significant contextual details. We believe that such a complicated use-case is vitally important as an opportunity to address the processing pipeline of visual sentiment and associated activity response analysis and provide a framework for future research in the field. Additionally, human sentiment and associated activity analysis in disaster situations can have numerous applications that can contribute towards more welfare to the society. It can aid broadcasting media outlets to cover emergencies where people are at risk from a different point of view. Likewise, such a framework would be used by emergency relief organizations to disseminate information on a larger scale, based on the visual content expressing the actual number of people with their sentiments and associated activity response that best illustrates the facts of a certain incident. Besides that, a large-scale dataset is compiled, annotated, and made freely accessible to promote potential future research in the domain. The dataset, annotation sets, and trained weight files are made public for future research in this challenging domain. A larger group task has been performed with a significant number of participants for the annotation of the dataset.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7378</offset><text>We diversify sentiment analysis to visuals and combined with associated human activity to a more difficult and critical problem of disaster analysis, typically requiring several artifacts and other relevant details in the context of images.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7619</offset><text>We suggest deep learning models fusion architecture for an automated sentiment with associated human activity analysis based on realistic social media disaster-related images.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7795</offset><text>We fused a deep human count tracker with the YOLO model that enables tracking multi persons in the occluded environment with reduced identity switches and provides an exact count of people on risk in visual content in a disastrous situation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8037</offset><text>Presuming that the proposed deep framework exhibits differently to an image by retrieving diverse but harmonize image features, we evaluate various benchmark pre-trained deep models separately or in combination.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8249</offset><text>We conducted a crowd survey to annotate a disaster-related image dataset containing annotations for human sentiments and corresponding activity responses that are collected from social networks, in which 1210 participants annotated 3995 images.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8494</offset><text>The key contribution of this paper can be outlined as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8557</offset><text>The paper is arranged as follows: Section 2 concludes the literature review in three different domains such as sentiment analysis, human activity, and human trackers based on machine learning specifically deep learning. Section 3 describes the proposed methodology of the fusion of deep models for human sentiment and associated activity analyzer with deep human count trackers in disaster-related visual contents. Section 4 provides the statistics of the crowdsourcing study. It further evaluates the experimental results that are obtained from the crowd-sourced study and its implications on the results of deep model fusion. We further compare and evaluate our results with state-of-the-art deep learning benchmarks. Section 5 concludes the proposed methodology and discusses future directions in this research domain.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>9379</offset><text>2. Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9395</offset><text>In this section, we briefly review the previous related studies and split them into three main directions, such as analyzing human sentiment, analyzing human activity, and finally tracking and counting human activities. To avoid repetition, we quickly review the state-of-the-art, taking into account the diversity of the underlined procedure, system, input source, and maximum performance, as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9798</offset><text>The analysis of sentiment relates to the use of machine learning conventionally natural language processing, text analysis, computational linguistics, and biometrics to systematically define, isolate, evaluate, and study affective states and subjective data. Natural language processing has contributed largely to the accurate determination of sentiment in text or spoken voices, regarding consumers’ assessments of products, movies, etc.. The purpose of the study of sentiment is to decide whether the consumer’s text conveys a neutral, optimistic, or negative opinion. Currently, three approaches address the problem related to sentiments: (1) machine learning methods, (2) lexicon based methods, and (3) hybrid approach.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10526</offset><text>Machine learning methods can be split into two subcategories: (1) conventional methods and (2) deep learning-based. Conventional approaches apply to traditional machine learning techniques such as Naive Bayes and support vector machines (SVM). Deep learning techniques infer better performance than classical machine learning techniques. Such techniques contain deep neural networks (DNN), recurrent neural networks (RNN), and convolutional neural networks (CNN) for sentiment analysis. These methods address classification problems on text, speech, and visual content. Lexicon based methods were first applied to sentiment analysis. These methods rely on a statistical analysis of the content based on documents using techniques such as K-nearest neighbors (KNN) and hidden Markov models (HMM). The hybrid approach combines both machine learning and lexicon applied to classify sentiments. The sentiment lexicon exhibits a vital role in these strategies. Figure 1 describes the taxonomy of sentiment analysis. The literature regarding speculation of sentiments from image data is rather limited. Moreover, being the latest and challenging task, there is a paucity of public datasets which makes it harder to construct a benchmark that can lay the foundation of a firm state of the art. Priya et al. proposed a model to lower the effective gap that extracts objects with the high and low level of background features of an image. These extracted features lead to better recognition performance. A study represented in that extracted features based on art theory and psychology for the classification of the emotional response of an image. The extracted features were grouped by content, texture, and color to be classified by using a Naive Bayes classifier. The study achieved promising results at a time; however, the extracted features struggled to recognize the complicated correlation between human sentiments and image contents. Ref. provide a comprehensive survey on sentiments analysis that summarizes the common datasets, the main characteristics of the datasets, the deep learning model applied to them, their precision, and the contrast of different deep learning models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12708</offset><text>Hence, the recent studies showed the criterion to extract adjective–noun pairs (ANPs) like sorrow face or happy girl, which may be utilized to deduce the emotional sentiment of an image. Damian et al. constructed a dataset that contained 3000 ANPs, aiming to contribute to the research community. They also proposed a set of baseline models that are used frequently to benchmark techniques based on APNs. Zaid et al. presented a study containing a technique for the prediction of sentiment emoticons from images. They trained SimleyNet on a largely collected dataset from social media which contained 4 million images and emoticon pairs. Several researchers have suggested the incorporation of visual and textual data for sentiment analysis. For example, in, authors proposed Attention-based Modality-Gated Networks (AMGN) that employed the images associated with text for the analysis of emotions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13609</offset><text>The most common high-level cues used in human action detection in still images are the body, parts of the body (limbs, legs, etc.), the position of limbs, and legs including background details. Most of the human activity recognition published literature consists of supervised learning and semi-supervised learning. In the case of human activity recognition, the deep models require large training data; to tackle this problem, the transfer learning approach has been thoroughly studied. Since many promising outcomes have been obtained, a widely agreed issue is that it is very costly to annotate all the human activities, as human annotators and deep learning experts offer numerous efforts, and the probability of error still remains high. Nevertheless, providing reliable and relevant details is a salient task in human physical activity detection. Table 1 outlines the details of prominent research studies that are focused on supervised learning.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14562</offset><text>In the recent study to inspire consumers in their everyday operations, this work introduces a music context conscious recommender framework. Its primary goal is to prescribe the most suitable music for the customer to maximize the performance of the physical activity at the recommended time.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14855</offset><text>In the event of a disaster, people could find it more helpful to capture images of the situation and publicly share them to warn others about possible threats, harm to human life, or facilities. To this end, visual content will deliver precise information on the severity and extent of the damage, a better understanding of shelter needs, a more precise assessment of current emergency operations, and easier identification of missing and wounded. Early studies explore the significance of analyzing social media visual content in diverse catastrophe/disaster situations, such as flooding, fires, and earthquakes, motivated by this phenomenon.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15499</offset><text>The presented literature studies in this section reveal the trends which are followed for human sentiments as well as activities that are considered individual issues so far. Most of the features related to human activities are extracted from sensor data. However, we believe ‘A picture is worth a thousand words’. In disastrous conditions, visual content does not just contain descriptions of human emotions, but may also express ample clues correlated with human activity. These cues, which reflect the emotions and sentiments of photographers, can evoke similar feelings from the viewer and may help to interpret visual content beyond semantic meanings in various fields of use, such as education, entertainment, advertising, and media.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>16243</offset><text>3. Proposed Methodology</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16267</offset><text>The proposed approach consists of seven steps in the pipeline. The block diagram in Figure 2 provides the architecture of the proposed methodology for visual sentiment and associated human activity analysis fused with a deep human count tracker.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16513</offset><text>The process begins with crawling social media networks for disaster-related images, preceded by the selection of accurate sentiment and associated human activities categorization by crowd resources. We manually analyzed collected images to remove irrelevant image data before proceeding towards the crowded source study phase. In the crowed sourcing phase, a subset of the image dataset is shared with human annotators for sentiment and related activity annotation. Next to the annotation phase, a Yolo based sentiment and human activity analyzer is used for the detection and classification of human activity and associated sentiment. This study aimed to uniquely detect and count human objects in an image as well as their sentiment and associated activity to analyze the exact situation in a disaster-related visual content. To aim this, another deep count CNN is used to count and uniquely identify humans in a particular image. The result of this deep count CNN is fused with Yolo outcomes to label human sentiment, associated activity, and the total number of humans in a visual scene using multi tags. In the subsection, we provide a detailed description of each component.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>17694</offset><text>3.1. Data Crawling and Categorization of Human Activity with Associated Activity</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17775</offset><text>The first phase starts with the collection of images from social network platforms such as Twitter, Google Images, and Flicker, etc. During crawling for images from mentioned social networks, the copyright aspect was under consideration and all those images were selected which were free to share. Furthermore, the images were crawled with specific tags such as floods, earthquakes, tornadoes, tsunamis, etc. These tags were made further detailed for locations such as ‘Nepal’s Earthquake’, ‘floods in Mexico’, ‘Tsunami in Japan’.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18320</offset><text>The selection for labeling the sentiments and associated activity is one of the crucial tasks in this research study. Most of the literature available concentrates on some specific sentiments such as ‘Negative’, ‘Positive’, ‘Neutral’ with no human-associated activity. However, we intend to target sentiments that are more relevant to disaster-related content, taking into account the design and possible implementations of the suggested deep sentiment and associated human activity analysis processing pipeline. For example, labels such as ‘sorrow’, ‘excited’, and ‘anger’ are the sentiments that are more common in disaster-related situations. Moreover, according to a recent study about human psychology, we deduce two relevant sets of human sentiments that are more expected to be exhibited by people surrounded by a disaster. This study reported various types of human sentiments. The first sets of sentiments include conventional human expressions such as ‘negative’, ‘Positive’, ‘Neutral’. The second list includes ‘Happy’, ’Sad’, ’Neutral’, ‘feared’.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19431</offset><text>The third set is the extended form of human expressions that included more detailed sentiments such ‘Anger’, ‘Disgust’, ‘Joy’, ‘Surprised’, ‘Excited’, ‘Sad’, ‘Pained’, ‘Crying’, ‘Fear’, ‘Anxious’, ‘Relieved’. The human activities associated with these sentiments are labeled as ‘Sitting’, ‘Standing’, ‘Running’, ‘Lying’, ‘Jogging’. Table 2 describes a detailed tagging of possible human sentiments and associated human activities in disastrous circumstances.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>19954</offset><text>3.2. Crowdsourcing Study</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19979</offset><text>For the proposed deep sentiment analyzer, the crowd-sourcing experiment aims to establish ground-truth by acquiring human views and thoughts about disasters and related visual information. The selected images were presented to participants conducted by Hireaowl to annotate during the crowdsource study phase. In the process, a total of 3995 images were annotated by the participants. At least six individuals were selected to review the image to validate the consistency of the annotations. A total of 10,000 different answers from 2300 different participants were collected during the analysis. Individuals from multiple ages, gender groups, and 25 different countries were among the participants. The average response time by an individual is 200 s that helped us to filter out careless and inappropriate participants from the survey. Two trial tests were undertaken until the final analysis was done to fine-tune the test, correct mistakes, and improve consistency and readability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20965</offset><text>Figure 3 shows a template of a questioner with a disaster-related image that was provided to participants to annotate human sentiment and associated activity. In the first question, the participants were asked to mark provided images in the range of 1 to 10, where 10 expresses ‘Positive’, 5 stands for ‘Neutral’, and 1 reflects ‘Negative’ sentiment. The objective of this question was to determine the general opinion of participants regarding the image. The second question is comparatively more specific such as ‘Sad’, ‘Happy’, ‘Angry’, and ‘calm’, which can extract the detailed sentiments that are conveyed by image to participants. In the third question, the participants were asked to tag the visual content from 1 to 7 and they can describe the sentiment that was conveyed to them by an image. Furthermore, the participants were asked to express their feelings about images that are provided to them and tag images manually about particular sentiment in case that sentiment tag is not present in the list of given tags. The fourth query attempts to highlight the features of the image that trigger human emotions and activity at the level of the scene or background context. In the fifth question, the participants were asked to express their perception about an associated human activity such as ‘Sitting’, ‘Standing’, ‘Running’, ‘Laying’, ‘Jogging’.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22373</offset><text>3.3. Deep Sentiment and Associated Activity Detector</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22426</offset><text>The architecture of deep sentiment and associated activity detector is shown in Figure 4. We utilized the CSPDarknet53 backbone which is a combination of Darknet 53 and CSPNet. CSPNet was designed to solve the problems that require a lot of computation overhead at the network level. In network optimization, the problem of high inference computing is considered to be influenced by the duplication of gradient information, while CSPNet reduces computation by integrating gradient differences allover feature maps to ensure accuracy. It not only improves CNN’s learning efficiency, but it can also reduce the bottlenecks in computing and memory while retaining robust accuracy. Residual connections are used for the Darknet53 network module by leveraging the ResNet principle of residues which address the network’s deep gradient issues. Two convolutional layers with a single shortcut link are used in each residue node, while the layers have several redundant residual modules.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23410</offset><text>The pooling layer and completely linked layers are not included in the architecture, whereas the network is under-sampled by specifying the convolution value as 2. After advancing over convolutional layers, the size of the image is reduced to half. Every convolutional layer includes convolution, Leaky ReLU, and batch normalization (BN), while, after each residual layer, zero-padding is applied. In addition, CSPDarnet53 Network Bone is connected to the SSP block, which produces a fixed size output independent of the input size by using different image dimensions as input to obtain pooling characteristics that have the length. As SPP is placed behind the last convolutional layer by just removing the existing pooling layer, it has no impact on the network architecture. As the SSP block is used with CSPDarknet53 network bone, the recognition range increases significantly, and it extracts the most important contextual features by affecting network operations even less to slow down. Table 3 describes the detailed insight of CSPDarknet 53 that is used as a component of human sentiment and associated activity analyzer in the article.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24554</offset><text>In addition to SSP, Path Aggregation Network (PAN) is used as a parametric polymerization mechanism for various bone and detector levels instead of Feature Pyramid Network (FPN) in the Yolo fourth version. It is a bottom-up path enhancement that seeks to promote the flow of data that can contribute to the optimization of the pyramid of features. PANET designed adaptive feature pools to connect the feature grid to all feature layers to allow useful information from each feature layer to transfer directly to the suggested sub-network beneath. Yolo3 is utilized as an anchor-based detection model network head. In addition to these, the residual connections in the deeper Yolo3 mold network structure infer multi-scale detection that enhances mAP and greatly increases small object detections.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>25351</offset><text>Loss Function</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25365</offset><text>We have used the loss function Distance-IoU (DIoU) to reduce the normalized distance between the anchor box and the target box. It obtains a greater convergence rate and is more reliable and rapid when overlapping or even when using regression with the target box. It is concentrated on the union intersection (IoU) that takes the center distance of bounding boxes into account. Equation (1) describes the IoU where Bpred represents the prediction bounding box and Bgt represents the ground truth bounding box. Thus, Equation (2) represents the loss function in case the bounding boxes overlap. When the gradient doesn’t vary, the bounding boxes don’t overlap: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26031</offset><text>Therefore, GIoU improves the loss of IoU in the instance where the gradient does not alter without overlapping boundary boxes, which adds a penalty dependent on the IoU loss function. In Equation (3), an extra parameter C is added that reflects the minimal boundary box that can occupy both Bpred and Bgt:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26337</offset><text>However, if, for example, the other box is overridden by either Bpre or Bgt, the penalty does not operate which is then called an IoU loss. DIoU is introduced to solve these limitations which are represented by Equation (4). The central point of the anchor box and the target box are represented by Bpre, Bgt, and C reflects the diagonal distance of the smallest rectangle that can fill the anchor and the target bounding boxes at the same time, while p is the Euclidean distance between two central points. Equation (5) represents the DIoU loss function: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26894</offset><text>DIoU loss function can apply non-maximum Suppression to eliminate the redundancy of the detection box. Not only the intervening area but also the difference between the ground truth detection box and the center point of the target box are taken into account, which would essentially avoid the above loss function flaws.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>27214</offset><text>3.4. Deep Count and Identity Assignment</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27254</offset><text>We adopted a conventional single hypothesis monitoring methodology of repetitive Kalman filtering and data association for human count and tracking. The framework for the count and track handling is mostly close to the original implementation in: where x represents the target’s horizontal center and y depicts vertical pixel position, while s and  represent the scale (area) and the aspect ratio of the target bounding box. It should be noted that the aspect ratio remains constant. The identified bounding box is used to optimize the target state where the obtained values are efficiently resolved while detection is paired with a target using a Kalman filter. If detection doesn’t correspond to the target, its state is predicted without correction using the linear velocity model. By calculating its new location in the current frame when assigning detections to existing targets, the bounding box geometry of each target is determined. The assignment’s cost matrix is then calculated as the distance from the current target between each BBX observed and all predicted BBX as the intersection-over-union. Hungarian algorithms are used to resolve the assignment optimally. In addition, to reject assignments where the target overlap detection is smaller than the minimal intersection-over-union, the least intersection-over-union threshold is used. We find that short term occlusion caused by moving target is indirectly addressed by the BBX IOU distance. Explicitly, only the occluder is detected when an occluding object obscures a target since similar-scale detections are properly preferred by the intersection-over-union range. This means that both the occluder object identification is corrected while the obscured target is untouched, meaning that no assignment is made.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29041</offset><text>As objects join and leave the image, unique identities need to be correctly created. The tracker or counter is initialized using the BBX shape, with a movement value assigned zero. Because the movement at this stage is not observed, the velocity variable covariance is initialized with significant values, representing this ambiguity. In addition, the newest counter then performs a probationary phase in which detections must be correlated with the target to acquire adequate data to stop counting false positives. The numbers of counts or trackers are aborted until not detected for  frames. This avoids the infinite increase in the number of trackers and localization errors caused without detector corrections by long-term predictions. In contrast, productivity is promoted by the premature exclusion of lost targets. If human objects reappear, with a new identity, tracking would effectively resume.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29946</offset><text>To achieve this, a convolutional neural network has been employed in a large-scale person re-identification dataset containing more than 1 million images with 1000 human pedestrians, making it well-matched in a people counting and tracking context for deep count learning. Table 4 depicts the architecture of CNN that is employed for human tracking and counting. We are using a large residual network with six residual blocks accompanied by two convolutional layers. The global dimensionality feature map 128 is extracted in a dense layer.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30486</offset><text>A final batch and l2 normalization, map features on the unit hypersphere to be in line with the cosine appearance parameter. The network has 2,800,864 parameters in total and on Nvidia GeForce (Santa Clara, California, CA, United States) GTX 1080Ti GPU, and one forward pass of 32 BBX requires roughly 31 ms. Provided that there is a stable GPU available, this network is also well suited for online counting and tracking.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30909</offset><text>The results of both networks are overlapped in a manner in which multi-label detection can be ensured. The detection from the Yolo based network generates two labels for sentiment and associated activity, and the results couple with the deep count tracker network that generates an extra detection label for the number of humans in visual content. The object and scene-level features are based on the responses of the participants in the fourth query, where they were asked to highlight the image features that trigger their emotions and feelings; we believe that this approach is helpful in the classification of sentiments and associated human activity. Besides this, we employed state-of-art benchmarks model such as Dense Net, Alex Net, Inception Net, VGGNet, and ResNet to our collected dataset. These models are fine-tuned on our largely collected dataset for sentiment and associated human activity classification tasks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31837</offset><text>In addition, we made some adjustments in the framework to fit the pre-trained models for the task at hand for the multi-label analysis. As a first step, with the corresponding changes in the models, a vector of the ground truth having all the possible labels has been generated. Particularly, the top layer of the model has been changed by replacing the soft-max function with a sigmoid function to facilitate multi-label classification. The sigmoid function is useful since it expresses the outcomes in probabilistic terms for each label, whereas the soft-max function retains the probability rule and smothers all the values of a matrix into a set of [0, 1]. Similar improvements (i.e., the replacement of Softmax with the sigmoid function) are introduced in the formulation of the cross-entropy for the pre-trained models to be better tuned. We divided the dataset into 70%, 10%, 20% train, validation, and test dataset ratio simultaneously.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>32782</offset><text>4. Experiments and Evaluations</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32813</offset><text>In this section, a detailed analysis of the crowded source study and the experimental results are presented.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>32922</offset><text>4.1. Crowed Source Analysis and Dataset</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32962</offset><text>Figure 5 depicts the detailed analysis of the crowded source study. The participants of this study were asked five questions that are described in Section 3. In the crowd-sourcing analysis, for example, the template image as shown in Figure 3 elicited pessimistic emotions by participants. Figure 5a (where tags 1 to 4 lead to negative feelings, 5 to neutral, and tags 6 to 9 represent positive feelings) indicates that most of the feelings are negative. Reaching the remaining answers, we observed that images were identified as positive during the relief and rescue process, but these responses are neglectable. In Figure 5b (where tags 1 to 4 indicate happy/excited, 5 neutral/calm, and 6 to 9 represent angry/sad), the responses ranged in a broad spectrum but almost of the answers resided in the sad indicator range. Figure 5c represents the response to the third question which contained a relatively larger range of options in the sentiment spectrum such as sad, surprised, anger, happy, fear, and neutral disgust. This sentiment spectrum helped participants as well as us to understand the more specific sentiment for a particular image. As expected, the responses were recorded as sad and feared by most of the participants. Figure 5d represents the related human activity as asked in question 5 from participants that can be visualized by a human observer by visual content. In the case of the sample image in Figure 3, a large number of participants responded with standing and a relatively fewer number of participants visualized the activity as walking as some of the image characters seemed to take the step of tending to move. The final question of the analysis, where we asked the participants to highlight the image sentiments with related human activity that affect their emotions, and tag selection for a given image is represented by Figure 6.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34826</offset><text>As expected, the responses by participants provided detailed perception about the visual content based on image context in a methodological manner. It is evident from Figure 7 that the background context information influences the evoking of common human perception about the actual scene to visualize the human sentiment with related physical activity (e.g., 41% human expression).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35209</offset><text>Human physical activity is very crucial (33%) besides human sentiments, and it is considered as correlated, as both influence each other. Other factors such as background context, text in image, image quality (contrast, saturation), and an object in images effectively evoked a human perception ranging from 11%, 2%, 5%, and 8%, respectively. Crowdsourcing study helped us in collecting an effective and meaningful dataset that contained human sentiment and associated activity images in disaster situations. Table 5 describes the detailed statistics of the dataset in terms of general sentiment distribution (e.g., positive, neutral, and negative). The statistics of a dataset that contain related human physical activity that is distributed in basic human physical reactions associated with sentiments (e.g., sitting, standing, walking, running, laying) are provided in Table 6. Table 7 provides detailed statistics of the breaking down of human sentiment into more expressive sentiment expressions. The images were then multi-labeled based on sentiments and associated human activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>36297</offset><text>4.2. Experimental Results</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36323</offset><text>The experimentations were conducted on Intel® Xeon(R) (Santa Clara, CA, United States) that CPU contained 3.30 GHz octa-core processors with GPU GeForce (Santa Clara, CA, United States) GTX 1080Ti having 12 GB RAM. The Ubuntu 16.04 operating system was installed on the system.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36602</offset><text>The model was fed with 608 × 608 × 3 (width, height, channels) image inputs. The batch size was set as 64 with 64 subdivisions due to limitations of GPU resources. The learning rate and momentum values were set as 0.001 and 0.949, respectively, while the value of decay was set as 0.005. We set the max-batch size with the formula (number of class × 2000), which is again a standard for using Yolo darknet version 4. For instance, if we have three classes, we must set the maximum batch as 6000. Next, we take 90% and 80% of the value of the max batch to generate optimal steps. In our case, the max batch value was set as 32,000, and the step value ranged between 25,600 and 28,800. Figure 8 depicts the summary of the training process where an average loss of 0.3 was achieved on our dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37400</offset><text>Many literature papers aim to find the best accuracy that can be offered by a classifier and then present this value as the classifier’s efficiency. Seeking the best accuracy for a classifier though is typically not straightforward. In comparison, since this high precision can only be obtained with very particular data and classifier parameter values, the outcome is likely to be bad for another dataset, since the parameters have been calibrated for the specific data evaluated. Therefore, the classifier should perform well without being overly susceptible to changes in parameters, in addition to having high accuracy. That is, for a relatively wide range of values of its parameters, a good classifier can provide a stable classification. It is challenging to clearly understand the mechanism of the deep neural network; however, we show several visual hints that DCNN could detect some discriminatory features. As can be seen from Figure 9, some filters have learned color characteristics, such as brown, red, green, etc., while some filters learn edge knowledge in multiple ways. To demonstrate the efficacy of the Yolo based sentiment and activity detection model, some of the feature maps obtained from various convolutional layers (80, 85, and 91) can be seen in Figure 9b–e. Some glimpses of feature detection of these layers are shown with different scales. The feature map in Figure 9a reveals that only the areas corresponding to the most important objects (kids) are activated. Although one object in two objects (kids) was obscured, the area of the middle kids was still weakly triggered. Incorporating the outcomes from various scales, the model correctly detects all the kids with face sentiments and associated activity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39145</offset><text>A variety of experiments was conducted to test the efficiency of the proposed Yolo-based human sentiment and associated activity analyzer. The trained model evaluation indices are Recall, Precision, and F1 score.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39358</offset><text>Another evaluation metric that is used in this study is AP (average precision), typically finding an area under the precision–recall curve that can illustrate the model performance with confidence levels. AP is defined as in Equation (7) below: where </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39612</offset><text> is a measured precision–recall curve.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39653</offset><text>Yolo v4 based sentiment and associated activity were evaluated with task 1 initially. Task 1 corresponds to only detecting the general sentiments of human objects such as positive, neutral, and negative in disastrous situations. Table 8 represents the results for task 1.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>39925</offset><text>The second part of task 1 is based on human activity (sitting, standing, walking, running, laying) in disastrous situations. We have conducted separate experiments to detect human activity as single labeled problems like the task 1 sentiment analyzer. Table 9 shows the results for the Yolo based human activity analyzer.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>40247</offset><text>We extended our experiments with multi-label and multi-task detection where we added extended sentiments (happy, excited, fear, anger, neutral, sad, disgust, surprise, relief) with associated human activity. Table 10 depicts the details of task 2. The experimental results that are shown in Table 10 depict every sentiment with associated activity results in a different score. For example, we can see that the given metric scores for activity lying are lower than other physical activities. The possible reasons can be (1) the human object faces are not visible when in the laying position. (2) The activity of human objects is static like most of the objects in the background context. (3) The number of specific sentiment images for training e.g., the number of images for disgust is lower so we found lower values of metrics for this sentiment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>41096</offset><text>Since one of the key reasons for the experiments is to provide a potential foundation in this domain, with many existing deep models, we compared and assessed the proposed multi-label framework with these benchmarked models. The obtained results from these benchmark deep learning models revealed that our chosen framework is a better option as compared to these models. These models only classify the sentiments and associated activities, but the Yolo based model performs one step further which detects human sentiments and associated activities in real time with a proper bounding box and multi-label captioning. This multi-label captioning is a more natural and reliable source to understand the human sentiment and associated activities in disastrous situations.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>41864</offset><text>As shown in Table 8, we obtained the results in terms of accuracy, precision, recall, and F1 score for task 1 (positive, negative, and neutral) sentiments from these benchmark models pre-trained with datasets like Image Net. By comparing the contents of Table 8 and Table 11, it is evident that using the Yolo version 4 based sentiment analyzer yields better results for recognition and detection than other benchmark models. Similar experiments were conducted for human activity analysis in disaster situations using a deep learning benchmark model. It is evident from Table 12 that the Yolo based activity analyzer has performed better than these deep models—in particular, the effect of class labels smoothing, the influence of various data augmentation strategies, bilateral blurring, mix-up, cut mix and mosaic, and the influence of various activations, such as Leaky-ReLU, Swish, and Mish. The performance of the classifier is increased in our experiments by adding features such as cut mix and mosaic data augmentation, class mark smoothing, and triggering of Mish. As a result, the following features are included in our backbone for classifier training. Moreover, the dynamic scale of the mini-batch and the automatic rise in small resolution training, mini-batch size influenced improved detection for small-size items by using random training shapes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>43228</offset><text>MOTA (Multi-object tracking accuracy) is the primary metric that summarizes cumulative detection accuracy in terms of false positives, false negatives, and identity switches. MOTA can be described using Equation (9):  where  represents missed targets and false positives (ghost paths) are , and the number of identity changes at time t is . In case the intersection of union with the ground truth is inferior to a specified threshold, a goal is deemed missing. It is worth noting that the values for MOTA can be negative:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>43750</offset><text>MOTP (Multi-Object Tracking Precision) is a relative difference between all true positives and associated actual targets. This is determined as bounding box overlap, as:  where  represents similarity in frame t and , with its allocated ground truth object, the bounding box overlaps target i. Thus, MOTP presents the overall overlap for all correlative predictions and ground truth targets that scales from  50 percent to 100 percent:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44185</offset><text>MT (Mostly Tracked) is a portion of ground-truth records that have at least 80% of their life cycle under the same tag.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44305</offset><text>ML (Mostly Lost) is a portion of targets that have a minimum 20% life span under the same tag.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44400</offset><text>IDS (Identity Switches) are the number of changes or shifts in a ground-truth track’s recorded identity.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44507</offset><text>We evaluate the performance of deep human count and tracking in disastrous related visual contents based on MOT16 (Moving Objects Tracking) described as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44669</offset><text>Based on these metrics, we evaluated the deep human count tracker framework. In Table 13, we have evaluated and compared the results with a deep human count analyzer with multiple techniques. The results show that the proposed deep human count is better than the mentioned techniques and is most suitable for real-time tracking.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>44998</offset><text>The results of deep sentiment and associated human activity analyzer (multi captioned) are then fused with deep human count (human ID in visual content). The result contains the human objects with unique identification numbers captioned on the detected bounding box with detected human sentiment and associated human activity captions as can be seen in Figure 10.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>45362</offset><text>Human sentiment and associated human activity analysis in disastrous situations attempt to derive the perceptions about images from people; therefore, crowd-sourcing appears to be an effective option for obtaining a data set. Nevertheless, it is not straightforward to select labels/tags to perform an effective crowd-sourcing study.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>45696</offset><text>In applications such as disaster/catastrophe analysis, the three most widely used sentiment tags, namely positive, negative, and neutral combined with associated human activities, are not adequate to completely leverage the ability of visual sentiment and associated human activity analysis. The complexity surges as we broaden the spectrum of sentiment/emotion with associated human activities.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>46092</offset><text>The plurality of social media images associated with disasters reflects negative feelings (i.e., sorrow, terror, discomfort, rage, fear, etc.). Nevertheless, we realized that there are a variety of samples that can elicit optimistic feelings, such as excitement, joy, and relief.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>46372</offset><text>Disaster-related social media images display ample features to elicit emotional responses. In the visual sentiment study of disaster-related images, objects in images (gadgets, clothing, broken buildings, and landmarks), color/contrast, human faces, movements, and poses provide vital signs. This can be a key component in representing the sentiment and associated activities of people.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>46759</offset><text>As can also be observed from the observations of the crowdsourcing analysis, human sentiments and associated tags are linked or correlated, so a multi-label framework is likely to be the most optimistic direction of research.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>46985</offset><text>This preliminary study on the analysis of visual sentiments and associated activity analysis in disasters has uncovered several challenges, showing us all the various aspects of such a dynamic area of research. We have outlined the key points below:</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>47235</offset><text>5. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>47250</offset><text>We concentrated on the new concept of visual sentiment with associated human activity analysis in this article and demonstrated how images related to natural disasters invoke the sentiments and perceptions of people. To achieve this, we proposed a pipeline that starts from data collection, annotation by using a crowdsourcing study, followed by sentiment and an associated activity analyzer that is fused with a deep human count tracker, finally yields multi-tags that represent human sentiments and associated activity with unique identities for humans with fewer identity switches in occluded context and disaster-related visual content. We evaluated and annotated more than 3500 images with three distinct sets of tags in the crowd-sourcing analysis, resulting in four different datasets of different sentiment and associated human activity hierarchies. The three most commonly used sentiment tags, namely positive, negative, and neutral combined with related human activities, are not appropriate for applications such as disaster/catastrophe analysis to fully exploit the capacity to analyze visual sentiment and associated human behavior. When we extend the definition of sentiment/emotion with related human behaviors, the scope grows. Social network images relevant to disasters show enough functionality to evoke emotional reactions. Things in images provide vital signs in the visual emotion analysis of disaster-related images. This may be a central factor in reflecting people’s sentiment and related physical activities.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>48787</offset><text>Based on our study, we conclude that the analysis of visual sentiments with associated human activity in general and the analysis of content relevant to natural disasters, in particular, is an exciting area of research that will support researchers and society in a diverse range of applications. The latest literature reveals a propensity to interpret visual sentiment in general images posted on social media by deploying deep learning techniques to derive visual cues dependent on an object and facial expression. We believe, nevertheless, that visual sentiment and associated human activity analysis can be applied to more complicated images, as also seen in this work, where many sorts of image features and details may be used jointly, such as object and scene-level features, human faces, movements, and poses. This approach contains greater potential as a baseline for numerous humanitarian and relief services and applications.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49724</offset><text>We believe there is still a lot to be explored in this direction, and this study offers a foundation for potential work in the domain. We tend to utilize most recent developments in adversarial training in affective computation and emotion processing, inspired by the continued progress and accomplishments associated with adversarial training in artificial intelligence. Further research initiatives aimed at exploiting the highlighted benefits of adversarial training have also been called to our attention. We would like to apply GAN (generative adversarial networks) techniques to generate deep fake disastrous images mixed real catastrophe visual content. We believe that training with such a dataset can affect the performance of such detection networks. If successfully applied, the new generation of robust affective computing and sentiment analysis techniques that are capable of broad in-the-wild implementation would encourage and facilitate these techniques. We would like to gather a multi-model dataset in the future where the text correlated with images utilizes visual features that contribute to the enhanced interpretation of visual sentiments and human activities.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>50908</offset><text>Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>51034</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>51055</offset><text>A.M.S. participated in (a) conception and design, experimentations, and interpretation of the data; (b) drafting the article and revising it critically for important intellectual content, and (c) approval of the final version. H.A. supervised this research and approved the final version. Y.B.C. supervised this research and approved the final version. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>51485</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>51493</offset><text>This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2018R1D1A1B07048080).</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>51678</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>51700</offset><text>The authors declare no conflict of interest.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>51745</offset><text>References</text></passage><passage><infon key="name_0">surname:Bevilacqua;given-names:A.</infon><infon key="name_1">surname:MacDonald;given-names:K.</infon><infon key="name_2">surname:Rangarej;given-names:A.</infon><infon key="name_3">surname:Widjaya;given-names:V.</infon><infon key="name_4">surname:Caulfield;given-names:B.</infon><infon key="name_5">surname:Kechadi;given-names:T.</infon><infon key="pub-id_arxiv">1906.01935</infon><infon key="pub-id_doi">10.1007/978-3-030-10997-4_33</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>51756</offset><text>Human Activity Recognition with Convolutional Neural Netowrks</text></passage><passage><infon key="fpage">136</infon><infon key="lpage">147</infon><infon key="name_0">surname:Öztürk;given-names:N.</infon><infon key="name_1">surname:Ayvaz;given-names:S.</infon><infon key="pub-id_doi">10.1016/j.tele.2017.10.006</infon><infon key="section_type">REF</infon><infon key="source">Telemat. Inform.</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">2018</infon><offset>51818</offset><text>Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis</text></passage><passage><infon key="name_0">surname:Kušen;given-names:E.</infon><infon key="name_1">surname:Strembeck;given-names:M.</infon><infon key="pub-id_arxiv">1707.09939</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>51901</offset><text>An Analysis of the Twitter Discussion on the 2016 Austrian Presidential Elections</text></passage><passage><infon key="fpage">2745</infon><infon key="lpage">2761</infon><infon key="name_0">surname:Sadr;given-names:H.</infon><infon key="name_1">surname:Pedram;given-names:M.M.</infon><infon key="name_2">surname:Teshnehlab;given-names:M.</infon><infon key="pub-id_doi">10.1007/s11063-019-10049-1</infon><infon key="section_type">REF</infon><infon key="source">Neural. Process. Lett.</infon><infon key="type">ref</infon><infon key="volume">50</infon><infon key="year">2019</infon><offset>51983</offset><text>A Robust Sentiment Analysis Method Based on Sequential Combination of Convolutional and Recursive Neural Networks</text></passage><passage><infon key="name_0">surname:Barrett;given-names:L.F.</infon><infon key="name_1">surname:Adolphs;given-names:R.</infon><infon key="name_2">surname:Marsella;given-names:S.</infon><infon key="name_3">surname:Martinez;given-names:A.M.</infon><infon key="name_4">surname:Pollak;given-names:S.D.</infon><infon key="pub-id_doi">10.1177/1529100619832930</infon><infon key="pub-id_pmid">31313636</infon><infon key="section_type">REF</infon><infon key="source">Psychol. Sci. Public Interest</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>52097</offset><text>Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements</text></passage><passage><infon key="name_0">surname:Cao;given-names:Z.</infon><infon key="name_1">surname:Hidalgo;given-names:G.</infon><infon key="name_2">surname:Simon;given-names:T.</infon><infon key="name_3">surname:Wei;given-names:S.-E.</infon><infon key="name_4">surname:Sheikh;given-names:Y.</infon><infon key="pub-id_arxiv">1812.08008</infon><infon key="pub-id_doi">10.1109/TPAMI.2019.2929257</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>52193</offset><text>OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</text></passage><passage><infon key="fpage">17</infon><infon key="lpage">25</infon><infon key="name_0">surname:Poria;given-names:S.</infon><infon key="name_1">surname:Majumder;given-names:N.</infon><infon key="name_2">surname:Hazarika;given-names:D.</infon><infon key="name_3">surname:Cambria;given-names:E.</infon><infon key="name_4">surname:Gelbukh;given-names:A.</infon><infon key="name_5">surname:Hussain;given-names:A.</infon><infon key="pub-id_doi">10.1109/MIS.2018.2882362</infon><infon key="section_type">REF</infon><infon key="source">IEEE Intell. Syst.</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2018</infon><offset>52271</offset><text>Multimodal Sentiment Analysis: Addressing Key Issues and Setting Up the Baselines</text></passage><passage><infon key="fpage">102261</infon><infon key="name_0">surname:Imran;given-names:M.</infon><infon key="name_1">surname:Ofli;given-names:F.</infon><infon key="name_2">surname:Caragea;given-names:D.</infon><infon key="name_3">surname:Torralba;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.ipm.2020.102261</infon><infon key="section_type">REF</infon><infon key="source">Inf. Process. Manag.</infon><infon key="type">ref</infon><infon key="volume">57</infon><infon key="year">2020</infon><offset>52353</offset><text>Using AI and Social Media Multimodal Content for Disaster Response and Management: Opportunities, Challenges, and Future Directions</text></passage><passage><infon key="comment">Available online: http://tubo.tu.ac.kr/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52485</offset><text>Cognative Robotics Lab-Tongmyong University</text></passage><passage><infon key="fpage">19</infon><infon key="lpage">25</infon><infon key="name_0">surname:Huq;given-names:M.R.</infon><infon key="name_1">surname:Ali;given-names:A.</infon><infon key="name_2">surname:Rahman;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">IJACSA Int. J. Adv. Comput. Sci. Appl.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>52529</offset><text>Sentiment analysis on Twitter data using KNN and SVM</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Soni;given-names:S.</infon><infon key="name_1">surname:Sharaff;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)</infon><infon key="type">ref</infon><offset>52582</offset><text>Sentiment analysis of customer reviews based on hidden markov model</text></passage><passage><infon key="fpage">764</infon><infon key="lpage">779</infon><infon key="name_0">surname:Pandey;given-names:A.C.</infon><infon key="name_1">surname:Rajpoot;given-names:D.S.</infon><infon key="name_2">surname:Saraswat;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.ipm.2017.02.004</infon><infon key="section_type">REF</infon><infon key="source">Inf. Process. Manag.</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2017</infon><offset>52650</offset><text>Twitter sentiment analysis using hybrid cuckoo search method</text></passage><passage><infon key="elocation-id">483</infon><infon key="name_0">surname:Dang;given-names:N.C.</infon><infon key="name_1">surname:Moreno-García;given-names:M.N.</infon><infon key="name_2">surname:De la Prieta;given-names:F.</infon><infon key="pub-id_doi">10.3390/electronics9030483</infon><infon key="section_type">REF</infon><infon key="source">Electronics</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2020</infon><offset>52711</offset><text>Sentiment Analysis Based on Deep Learning: A Comparative Study</text></passage><passage><infon key="fpage">230</infon><infon key="lpage">233</infon><infon key="name_0">surname:Zhang;given-names:X.</infon><infon key="name_1">surname:Zheng;given-names:X.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)</infon><infon key="type">ref</infon><offset>52774</offset><text>Comparison of text sentiment analysis based on machine learning</text></passage><passage><infon key="fpage">120</infon><infon key="lpage">125</infon><infon key="name_0">surname:Malik;given-names:V.</infon><infon key="name_1">surname:Kumar;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Recent Innov. Trends Comput. Commun.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2018</infon><offset>52838</offset><text>Sentiment Analysis of Twitter Data Using Naive Bayes Algorithm</text></passage><passage><infon key="fpage">123</infon><infon key="lpage">130</infon><infon key="name_0">surname:Firmino Alves;given-names:A.L.</infon><infon key="name_1">surname:Baptista;given-names:C.D.S.</infon><infon key="name_2">surname:Firmino;given-names:A.A.</infon><infon key="name_3">surname:de Oliveira;given-names:M.G.</infon><infon key="name_4">surname:de Paiva;given-names:A.C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 20th Brazilian Symposium on Multimedia and the Web</infon><infon key="type">ref</infon><offset>52901</offset><text>A Comparison of SVM versus naive-bayes techniques for sentiment analysis in tweets: A case study with the 2013 FIFA confederations cup</text></passage><passage><infon key="name_0">surname:Ortis;given-names:A.</infon><infon key="name_1">surname:Farinella;given-names:G.M.</infon><infon key="name_2">surname:Battiato;given-names:S.</infon><infon key="pub-id_arxiv">2004.11639</infon><infon key="pub-id_doi">10.1049/iet-ipr.2019.1270</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>53036</offset><text>Survey on Visual Sentiment Analysis</text></passage><passage><infon key="name_0">surname:Priya;given-names:D.T.</infon><infon key="name_1">surname:Udayan;given-names:J.D.</infon><infon key="pub-id_doi">10.1177/0020720920936834</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Electr. Eng. Educ.</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>53072</offset><text>Affective emotion classification using feature vector of image based on visual concepts</text></passage><passage><infon key="fpage">83</infon><infon key="lpage">92</infon><infon key="name_0">surname:Machajdik;given-names:J.</infon><infon key="name_1">surname:Hanbury;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th ACM international conference on Multimedia</infon><infon key="type">ref</infon><offset>53160</offset><text>Affective image classification using features inspired by psychology and art theory</text></passage><passage><infon key="fpage">4335</infon><infon key="lpage">4385</infon><infon key="name_0">surname:Yadav;given-names:A.</infon><infon key="name_1">surname:Vishwakarma;given-names:D.K.</infon><infon key="pub-id_doi">10.1007/s10462-019-09794-5</infon><infon key="section_type">REF</infon><infon key="source">Artif. Intell. Rev.</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2020</infon><offset>53244</offset><text>Sentiment analysis using deep learning architectures: A review</text></passage><passage><infon key="fpage">6861</infon><infon key="lpage">6875</infon><infon key="name_0">surname:Seo;given-names:S.</infon><infon key="name_1">surname:Kim;given-names:C.</infon><infon key="name_2">surname:Kim;given-names:H.</infon><infon key="name_3">surname:Mo;given-names:K.</infon><infon key="name_4">surname:Kang;given-names:P.</infon><infon key="pub-id_doi">10.1109/ACCESS.2019.2963426</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>53307</offset><text>Comparative Study of Deep Learning-Based Sentiment Classification</text></passage><passage><infon key="fpage">223</infon><infon key="lpage">232</infon><infon key="name_0">surname:Borth;given-names:D.</infon><infon key="name_1">surname:Ji;given-names:R.</infon><infon key="name_2">surname:Chen;given-names:T.</infon><infon key="name_3">surname:Breuel;given-names:T.</infon><infon key="name_4">surname:Chang;given-names:S.-F.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 21st ACM international conference on Multimedia</infon><infon key="type">ref</infon><offset>53373</offset><text>Large-scale visual sentiment ontology and detectors using adjective noun pairs</text></passage><passage><infon key="name_0">surname:Chen;given-names:T.</infon><infon key="name_1">surname:Borth;given-names:D.</infon><infon key="name_2">surname:Darrell;given-names:T.</infon><infon key="name_3">surname:Chang;given-names:S.-F.</infon><infon key="pub-id_arxiv">1410.8586</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>53452</offset><text>DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks</text></passage><passage><infon key="name_0">surname:Al-Halah;given-names:Z.</infon><infon key="name_1">surname:Aitken;given-names:A.</infon><infon key="name_2">surname:Shi;given-names:W.</infon><infon key="name_3">surname:Caballero;given-names:J.</infon><infon key="pub-id_arxiv">1907.06160 [cs]</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>53547</offset><text>Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis</text></passage><passage><infon key="elocation-id">79</infon><infon key="name_0">surname:Huang;given-names:F.</infon><infon key="name_1">surname:Wei;given-names:K.</infon><infon key="name_2">surname:Weng;given-names:J.</infon><infon key="name_3">surname:Li;given-names:Z.</infon><infon key="pub-id_doi">10.1145/3388861</infon><infon key="section_type">REF</infon><infon key="source">ACM Trans. Multimedia Comput. Commun. Appl.</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2020</infon><offset>53612</offset><text>Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis</text></passage><passage><infon key="name_0">surname:He;given-names:J.</infon><infon key="name_1">surname:Zhang;given-names:Q.</infon><infon key="name_2">surname:Wang;given-names:L.</infon><infon key="name_3">surname:Pei;given-names:L.</infon><infon key="pub-id_doi">10.1109/JSEN.2018.2885796</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sens. J.</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>53686</offset><text>Weakly Supervised Human Activity Recognition From Wearable Sensors by Recurrent Attention Learning</text></passage><passage><infon key="name_0">surname:Memiş;given-names:G.</infon><infon key="name_1">surname:Sert;given-names:M.</infon><infon key="pub-id_doi">10.1109/JSEN.2019.2916393</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sens. J.</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>53785</offset><text>Detection of Basic Human Physical Activities With Indoor–Outdoor Information Using Sigma-Based Features and Deep Learning</text></passage><passage><infon key="fpage">6429</infon><infon key="lpage">6438</infon><infon key="name_0">surname:Zhou;given-names:X.</infon><infon key="name_1">surname:Liang;given-names:W.</infon><infon key="name_2">surname:Wang;given-names:K.I.-K.</infon><infon key="name_3">surname:Wang;given-names:H.</infon><infon key="name_4">surname:Yang;given-names:L.T.</infon><infon key="name_5">surname:Jin;given-names:Q.</infon><infon key="pub-id_doi">10.1109/JIOT.2020.2985082</infon><infon key="section_type">REF</infon><infon key="source">IEEE Internet Things J.</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2020</infon><offset>53909</offset><text>Deep-Learning-Enhanced Human Activity Recognition for Internet of Healthcare Things</text></passage><passage><infon key="fpage">897</infon><infon key="lpage">904</infon><infon key="name_0">surname:Chen;given-names:W.-H.</infon><infon key="name_1">surname:Cho;given-names:P.-C.</infon><infon key="name_2">surname:Jiang;given-names:Y.-L.</infon><infon key="section_type">REF</infon><infon key="source">Sens. Mater</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2017</infon><offset>53993</offset><text>Activity recognition using transfer learning</text></passage><passage><infon key="name_0">surname:Hu;given-names:N.</infon><infon key="name_1">surname:Lou;given-names:Z.</infon><infon key="name_2">surname:Englebienne;given-names:G.</infon><infon key="name_3">surname:Kröse;given-names:B.J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Robotics: Science and Systems X</infon><infon key="type">ref</infon><offset>54038</offset><text>Learning to Recognize Human Activities from Soft Labeled Data</text></passage><passage><infon key="elocation-id">5884</infon><infon key="name_0">surname:Amin;given-names:M.S.</infon><infon key="name_1">surname:Yasir;given-names:S.M.</infon><infon key="name_2">surname:Ahn;given-names:H.</infon><infon key="pub-id_doi">10.3390/s20205884</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>54100</offset><text>Recognition of Pashto Handwritten Characters Based on Deep Learning</text></passage><passage><infon key="name_0">surname:Alex;given-names:P.M.D.</infon><infon key="name_1">surname:Ravikumar;given-names:A.</infon><infon key="name_2">surname:Selvaraj;given-names:J.</infon><infon key="name_3">surname:Sahayadhas;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Eng. Technol.</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2018</infon><offset>54168</offset><text>Research on Human Activity Identification Based on Image Processing and Artificial Intelligence</text></passage><passage><infon key="fpage">447</infon><infon key="lpage">453</infon><infon key="name_0">surname:Jaouedi;given-names:N.</infon><infon key="name_1">surname:Boujnah;given-names:N.</infon><infon key="name_2">surname:Bouhlel;given-names:M.S.</infon><infon key="pub-id_doi">10.1016/j.jksuci.2019.09.004</infon><infon key="section_type">REF</infon><infon key="source">J. King Saud Univ. Comput. Inf. Sci.</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2020</infon><offset>54264</offset><text>A new hybrid deep learning model for human action recognition</text></passage><passage><infon key="elocation-id">3113</infon><infon key="name_0">surname:Antón;given-names:M.Á.</infon><infon key="name_1">surname:Ordieres-Meré;given-names:J.</infon><infon key="name_2">surname:Saralegui;given-names:U.</infon><infon key="name_3">surname:Sun;given-names:S.</infon><infon key="pub-id_doi">10.3390/s19143113</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2019</infon><offset>54326</offset><text>Non-Invasive Ambient Intelligence in Real Life: Dealing with Noisy Patterns to Help Older People</text></passage><passage><infon key="fpage">321</infon><infon key="lpage">329</infon><infon key="name_0">surname:Shahmohammadi;given-names:F.</infon><infon key="name_1">surname:Hosseini;given-names:A.</infon><infon key="name_2">surname:King;given-names:C.E.</infon><infon key="name_3">surname:Sarrafzadeh;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)</infon><infon key="type">ref</infon><offset>54423</offset><text>Smartwatch based activity recognition using active learning</text></passage><passage><infon key="fpage">54</infon><infon key="lpage">61</infon><infon key="pub-id_doi">10.1016/j.procs.2019.12.086</infon><infon key="section_type">REF</infon><infon key="source">Proc. Comput. Sci.</infon><infon key="type">ref</infon><infon key="volume">163</infon><infon key="year">2019</infon><offset>54483</offset><text>Smartphone-Based Human Activity Recognition Using Bagging and Boosting</text></passage><passage><infon key="name_0">surname:Štulienė;given-names:A.</infon><infon key="name_1">surname:Paulauskaite-Taraseviciene;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Comput. Sci.</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>54554</offset><text>Research on human activity recognition based on image classification methods</text></passage><passage><infon key="name_0">surname:Alsheikh;given-names:M.A.</infon><infon key="name_1">surname:Selim;given-names:A.</infon><infon key="name_2">surname:Niyato;given-names:D.</infon><infon key="name_3">surname:Doyle;given-names:L.</infon><infon key="name_4">surname:Lin;given-names:S.</infon><infon key="name_5">surname:Tan;given-names:H.-P.</infon><infon key="pub-id_arxiv">1511.04664</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>54631</offset><text>Deep activity recognition models with triaxial accelerometers</text></passage><passage><infon key="fpage">235</infon><infon key="lpage">244</infon><infon key="name_0">surname:Ronao;given-names:C.A.</infon><infon key="name_1">surname:Cho;given-names:S.-B.</infon><infon key="pub-id_doi">10.1016/j.eswa.2016.04.032</infon><infon key="section_type">REF</infon><infon key="source">Expert Syst. Appl.</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2016</infon><offset>54693</offset><text>Human activity recognition with smartphone sensors using deep learning neural networks</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:Bhattacharya;given-names:S.</infon><infon key="name_1">surname:Lane;given-names:N.D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>54780</offset><text>From smart to deep: Robust activity recognition on smartwatches using deep learning</text></passage><passage><infon key="fpage">142</infon><infon key="lpage">151</infon><infon key="name_0">surname:Ospina-Bohórquez;given-names:A.</infon><infon key="name_1">surname:Gil-González;given-names:A.B.</infon><infon key="name_2">surname:Moreno-García;given-names:M.N.</infon><infon key="name_3">surname:de Luis-Reboredo;given-names:A.</infon><infon key="name_4">surname:Dong;given-names:Y.</infon><infon key="name_5">surname:Herrera-Viedma;given-names:E.</infon><infon key="name_6">surname:Matsui;given-names:K.</infon><infon key="name_7">surname:Omatsu;given-names:S.</infon><infon key="name_8">surname:González Briones;given-names:A.</infon><infon key="name_9">surname:Rodríguez González;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Distributed Computing and Artificial Intelligence, 17th International Conference</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>54864</offset><text>Context-Aware Music Recommender System Based on Automatic Detection of the User’s Physical Activity</text></passage><passage><infon key="fpage">187</infon><infon key="lpage">211</infon><infon key="name_0">surname:Luo;given-names:J.</infon><infon key="name_1">surname:Joshi;given-names:D.</infon><infon key="name_2">surname:Yu;given-names:J.</infon><infon key="name_3">surname:Gallagher;given-names:A.</infon><infon key="pub-id_doi">10.1007/s11042-010-0623-y</infon><infon key="section_type">REF</infon><infon key="source">Multimed. Tools Appl.</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2011</infon><offset>54966</offset><text>Geotagging in multimedia and computer vision—A survey</text></passage><passage><infon key="fpage">667</infon><infon key="lpage">689</infon><infon key="name_0">surname:de Albuquerque;given-names:J.P.</infon><infon key="name_1">surname:Herfort;given-names:B.</infon><infon key="name_2">surname:Brenning;given-names:A.</infon><infon key="name_3">surname:Zipf;given-names:A.</infon><infon key="pub-id_doi">10.1080/13658816.2014.996567</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Geogr. Inf. Sci.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2015</infon><offset>55022</offset><text>A geographic approach for combining social media and authoritative data towards identifying useful information for disaster management</text></passage><passage><infon key="name_0">surname:Kumar;given-names:A.</infon><infon key="name_1">surname:Singh;given-names:J.P.</infon><infon key="name_2">surname:Dwivedi;given-names:Y.K.</infon><infon key="name_3">surname:Rana;given-names:N.P.</infon><infon key="pub-id_doi">10.1007/s10479-020-03514-x</infon><infon key="section_type">REF</infon><infon key="source">Ann. Oper. Res.</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>55157</offset><text>A deep multi-modal neural network for informative Twitter content classification during emergencies</text></passage><passage><infon key="name_0">surname:Sadiq Amin;given-names:M.</infon><infon key="name_1">surname:Ahn;given-names:H.</infon><infon key="pub-id_doi">10.1016/j.cogsys.2020.11.002</infon><infon key="section_type">REF</infon><infon key="source">Cogn. Syst. Res.</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>55257</offset><text>Earthquake Disaster Avoidance Learning System Using Deep Learning</text></passage><passage><infon key="fpage">3</infon><infon key="lpage">14</infon><infon key="name_0">surname:Soleymani;given-names:M.</infon><infon key="name_1">surname:Garcia;given-names:D.</infon><infon key="name_2">surname:Jou;given-names:B.</infon><infon key="name_3">surname:Schuller;given-names:B.</infon><infon key="name_4">surname:Chang;given-names:S.-F.</infon><infon key="name_5">surname:Pantic;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.imavis.2017.08.003</infon><infon key="section_type">REF</infon><infon key="source">Image Vis. Comput.</infon><infon key="type">ref</infon><infon key="volume">65</infon><infon key="year">2017</infon><offset>55323</offset><text>A survey of multimodal sentiment analysis</text></passage><passage><infon key="fpage">E7900</infon><infon key="lpage">E7909</infon><infon key="name_0">surname:Cowen;given-names:A.S.</infon><infon key="name_1">surname:Keltner;given-names:D.</infon><infon key="pub-id_doi">10.1073/pnas.1702247114</infon><infon key="pub-id_pmid">28874542</infon><infon key="section_type">REF</infon><infon key="source">Proc. Natl. Acad. Sci. USA</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2017</infon><offset>55365</offset><text>Self-report captures 27 distinct categories of emotion bridged by continuous gradients</text></passage><passage><infon key="comment">Available online: https://www.hireowl.com/</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55452</offset><text>HireOwl:Connecting Businesses to University Students</text></passage><passage><infon key="fpage">390</infon><infon key="lpage">391</infon><infon key="name_0">surname:Wang;given-names:C.-Y.</infon><infon key="name_1">surname:Mark Liao;given-names:H.-Y.</infon><infon key="name_2">surname:Wu;given-names:Y.-H.</infon><infon key="name_3">surname:Chen;given-names:P.-Y.</infon><infon key="name_4">surname:Hsieh;given-names:J.-W.</infon><infon key="name_5">surname:Yeh;given-names:I.-H.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</infon><infon key="type">ref</infon><offset>55505</offset><text>CSPNet: A new backbone that can enhance learning capability of cnn</text></passage><passage><infon key="fpage">770</infon><infon key="lpage">778</infon><infon key="name_0">surname:He;given-names:K.</infon><infon key="name_1">surname:Zhang;given-names:X.</infon><infon key="name_2">surname:Ren;given-names:S.</infon><infon key="name_3">surname:Sun;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>55572</offset><text>Deep Residual Learning for Image Recognition</text></passage><passage><infon key="fpage">8759</infon><infon key="lpage">8768</infon><infon key="name_0">surname:Liu;given-names:S.</infon><infon key="name_1">surname:Qi;given-names:L.</infon><infon key="name_2">surname:Qin;given-names:H.</infon><infon key="name_3">surname:Shi;given-names:J.</infon><infon key="name_4">surname:Jia;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><offset>55617</offset><text>Path aggregation network for instance segmentation</text></passage><passage><infon key="name_0">surname:Lin;given-names:T.-Y.</infon><infon key="name_1">surname:Dollár;given-names:P.</infon><infon key="name_2">surname:Girshick;given-names:R.</infon><infon key="name_3">surname:He;given-names:K.</infon><infon key="name_4">surname:Hariharan;given-names:B.</infon><infon key="name_5">surname:Belongie;given-names:S.</infon><infon key="pub-id_arxiv">1612.03144 [cs]</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>55668</offset><text>Feature Pyramid Networks for Object Detection</text></passage><passage><infon key="name_0">surname:Redmon;given-names:J.</infon><infon key="name_1">surname:Farhadi;given-names:A.</infon><infon key="pub-id_arxiv">1804.02767</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>55714</offset><text>Yolov3: An incremental improvement</text></passage><passage><infon key="fpage">3464</infon><infon key="lpage">3468</infon><infon key="name_0">surname:Bewley;given-names:A.</infon><infon key="name_1">surname:Ge;given-names:Z.</infon><infon key="name_2">surname:Ott;given-names:L.</infon><infon key="name_3">surname:Ramos;given-names:F.</infon><infon key="name_4">surname:Upcroft;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)</infon><infon key="type">ref</infon><offset>55749</offset><text>Simple online and realtime tracking</text></passage><passage><infon key="fpage">95</infon><infon key="lpage">108</infon><infon key="name_0">surname:Kalman;given-names:R.E.</infon><infon key="pub-id_doi">10.1115/1.3658902</infon><infon key="section_type">REF</infon><infon key="source">J. Basic Eng.</infon><infon key="type">ref</infon><infon key="volume">83</infon><infon key="year">1961</infon><offset>55785</offset><text>A New Approach to Liner Filtering and Prediction Problems, Transaction of ASME</text></passage><passage><infon key="fpage">932</infon><infon key="lpage">947</infon><infon key="name_0">surname:Chopra;given-names:S.</infon><infon key="name_1">surname:Notarstefano;given-names:G.</infon><infon key="name_2">surname:Rice;given-names:M.</infon><infon key="name_3">surname:Egerstedt;given-names:M.</infon><infon key="pub-id_doi">10.1109/TRO.2017.2693377</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Robot.</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2017</infon><offset>55864</offset><text>A Distributed Version of the Hungarian Method for Multirobot Assignment</text></passage><passage><infon key="fpage">868</infon><infon key="lpage">884</infon><infon key="name_0">surname:Zheng;given-names:L.</infon><infon key="name_1">surname:Bie;given-names:Z.</infon><infon key="name_2">surname:Sun;given-names:Y.</infon><infon key="name_3">surname:Wang;given-names:J.</infon><infon key="name_4">surname:Su;given-names:C.</infon><infon key="name_5">surname:Wang;given-names:S.</infon><infon key="name_6">surname:Tian;given-names:Q.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Computer Vision</infon><infon key="type">ref</infon><offset>55936</offset><text>Mars: A video benchmark for large-scale person re-identification</text></passage><passage><infon key="name_0">surname:Iandola;given-names:F.</infon><infon key="name_1">surname:Moskewicz;given-names:M.</infon><infon key="name_2">surname:Karayev;given-names:S.</infon><infon key="name_3">surname:Girshick;given-names:R.</infon><infon key="name_4">surname:Darrell;given-names:T.</infon><infon key="name_5">surname:Keutzer;given-names:K.</infon><infon key="pub-id_arxiv">1404.1869</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>56001</offset><text>Densenet: Implementing efficient convnet descriptor pyramids</text></passage><passage><infon key="fpage">1097</infon><infon key="lpage">1105</infon><infon key="name_0">surname:Krizhevsky;given-names:A.</infon><infon key="name_1">surname:Sutskever;given-names:I.</infon><infon key="name_2">surname:Hinton;given-names:G.E.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><offset>56062</offset><text>Imagenet classification with deep convolutional neural networks</text></passage><passage><infon key="fpage">2818</infon><infon key="lpage">2826</infon><infon key="name_0">surname:Szegedy;given-names:C.</infon><infon key="name_1">surname:Vanhoucke;given-names:V.</infon><infon key="name_2">surname:Ioffe;given-names:S.</infon><infon key="name_3">surname:Shlens;given-names:J.</infon><infon key="name_4">surname:Wojna;given-names:Z.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</infon><infon key="type">ref</infon><offset>56126</offset><text>Rethinking the inception architecture for computer vision</text></passage><passage><infon key="name_0">surname:Simonyan;given-names:K.</infon><infon key="name_1">surname:Zisserman;given-names:A.</infon><infon key="pub-id_arxiv">1409.1556</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>56184</offset><text>Very deep convolutional networks for large-scale image recognition</text></passage><passage><infon key="elocation-id">e94137</infon><infon key="name_0">surname:Amancio;given-names:D.R.</infon><infon key="name_1">surname:Comin;given-names:C.H.</infon><infon key="name_2">surname:Casanova;given-names:D.</infon><infon key="name_3">surname:Travieso;given-names:G.</infon><infon key="name_4">surname:Bruno;given-names:O.M.</infon><infon key="name_5">surname:Rodrigues;given-names:F.A.</infon><infon key="name_6">surname:da Fontoura Costa;given-names:L.</infon><infon key="pub-id_doi">10.1371/journal.pone.0094137</infon><infon key="pub-id_pmid">24763312</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2014</infon><offset>56251</offset><text>A Systematic Comparison of Supervised Classifiers</text></passage><passage><infon key="name_0">surname:Milan;given-names:A.</infon><infon key="name_1">surname:Leal-Taixé;given-names:L.</infon><infon key="name_2">surname:Reid;given-names:I.</infon><infon key="name_3">surname:Roth;given-names:S.</infon><infon key="name_4">surname:Schindler;given-names:K.</infon><infon key="pub-id_arxiv">1603.00831</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>56301</offset><text>MOT16: A benchmark for multi-object tracking</text></passage><passage><infon key="fpage">36</infon><infon key="lpage">42</infon><infon key="name_0">surname:Yu;given-names:F.</infon><infon key="name_1">surname:Li;given-names:W.</infon><infon key="name_2">surname:Li;given-names:Q.</infon><infon key="name_3">surname:Liu;given-names:Y.</infon><infon key="name_4">surname:Shi;given-names:X.</infon><infon key="name_5">surname:Yan;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Computer Vision</infon><infon key="type">ref</infon><offset>56346</offset><text>Poi: Multiple object tracking with high performance detection and appearance feature</text></passage><passage><infon key="name_0">surname:Keuper;given-names:M.</infon><infon key="name_1">surname:Tang;given-names:S.</infon><infon key="name_2">surname:Zhongjie;given-names:Y.</infon><infon key="name_3">surname:Andres;given-names:B.</infon><infon key="name_4">surname:Brox;given-names:T.</infon><infon key="name_5">surname:Schiele;given-names:B.</infon><infon key="pub-id_arxiv">1607.06317</infon><infon key="section_type">REF</infon><infon key="source">arXiv</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>56431</offset><text>A multi-cut formulation for joint segmentation and tracking of multiple objects</text></passage><passage><infon key="fpage">68</infon><infon key="lpage">83</infon><infon key="name_0">surname:Lee;given-names:B.</infon><infon key="name_1">surname:Erdenee;given-names:E.</infon><infon key="name_2">surname:Jin;given-names:S.</infon><infon key="name_3">surname:Nam;given-names:M.Y.</infon><infon key="name_4">surname:Jung;given-names:Y.G.</infon><infon key="name_5">surname:Rhee;given-names:P.K.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Computer Vision</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>56511</offset><text>Multi-class multi-object tracking using changing point detection</text></passage><passage><infon key="fpage">84</infon><infon key="lpage">99</infon><infon key="name_0">surname:Sanchez-Matilla;given-names:R.</infon><infon key="name_1">surname:Poiesi;given-names:F.</infon><infon key="name_2">surname:Cavallaro;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Computer Vision</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>56576</offset><text>Online multi-target tracking with strong and weak detections</text></passage><passage><infon key="file">sensors-20-07115-g001.jpg</infon><infon key="id">sensors-20-07115-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56637</offset><text>The taxonomy of sentiments analysis.</text></passage><passage><infon key="file">sensors-20-07115-g002.jpg</infon><infon key="id">sensors-20-07115-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56674</offset><text>Block diagram of proposed system pipeline.</text></passage><passage><infon key="file">sensors-20-07115-g003.jpg</infon><infon key="id">sensors-20-07115-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56717</offset><text>An overview of the web application used in the study of crowd-sourcing. The members who are asked to provide tags are presented with a disaster-related image.</text></passage><passage><infon key="file">sensors-20-07115-g004.jpg</infon><infon key="id">sensors-20-07115-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56876</offset><text>The network architecture of human sentiment and associated activity analyzer.</text></passage><passage><infon key="file">sensors-20-07115-g005.jpg</infon><infon key="id">sensors-20-07115-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>56954</offset><text>The statistics of crowdsource study: (a) statistics of the first question answers. Tags 1 to 9 tags 1 to 4 show negative emotions, while tag 5 is neutral positive feelings, and tags 6 to 9 indicate positive feelings; (b) answer figures for the second question. Tags 1 to 9, tags 1 to 4 indicate calm/relaxed emotion, tag 5 indicates natural state, while tags 6 to 9 reflect the state of excitement/stimulated; (c) statistics of question 4 answers; (d) statistics for associated human activity responses.</text></passage><passage><infon key="file">sensors-20-07115-g006.jpg</infon><infon key="id">sensors-20-07115-f006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>57458</offset><text>The statistics of joint sentiment with associated human activity response for Figure 3 by the crowdsource study.</text></passage><passage><infon key="file">sensors-20-07115-g007.jpg</infon><infon key="id">sensors-20-07115-f007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>57571</offset><text>Overall statistics of crowdsource study about influential aspects in an image that evoked a human perception.</text></passage><passage><infon key="file">sensors-20-07115-g008.jpg</infon><infon key="id">sensors-20-07115-f008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>57681</offset><text>The summary of Yolo V4 based sentiment and associated activity training.</text></passage><passage><infon key="file">sensors-20-07115-g009.jpg</infon><infon key="id">sensors-20-07115-f009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>57754</offset><text>A general visualization of features detection of Yolo based sentiment and activity detector. (a) Obtained feature map from initial layer; (b) obtained feature map from layer 80; (c) obtained feature map from layer 85; (d) obtained feature map from layer 91; (e) Final Output.</text></passage><passage><infon key="file">sensors-20-07115-g010.jpg</infon><infon key="id">sensors-20-07115-f010</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>58030</offset><text>Multi-label sentiments and associated activity with unique human identity results from the proposed framework.</text></passage><passage><infon key="file">sensors-20-07115-t001.xml</infon><infon key="id">sensors-20-07115-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>58141</offset><text>Summary of classical machine learning techniques to recognize human activities.</text></passage><passage><infon key="file">sensors-20-07115-t001.xml</infon><infon key="id">sensors-20-07115-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reference&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Techniques&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Approach&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Input Source&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Activity&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Performance&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Alex et al. &lt;xref rid=&quot;B32-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;32&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Naive Bayes, SVM, MLP, RF&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Human activity classification&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Images&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking, Sleeping, holding a phone&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Jaouedi et al. &lt;xref rid=&quot;B33-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;33&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gated Recurrent Unit, KF, GMM&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Deep learning for human activity recognition&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Video frames&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Boxing, walking, running, waving of hands&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.3%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Antón et al. &lt;xref rid=&quot;B34-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;34&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RF&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Deduce high-level non-invasive ambient that helps to predict abnormal behaviors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ambient sensors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Abnormal activities:&lt;break/&gt;Militancy, yelling, vocal violence, physical abuse&lt;break/&gt;Hostility&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.0%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Shahmohammadi et al. &lt;xref rid=&quot;B35-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RF, Extra Trees,&lt;break/&gt;Naive Bayes,&lt;break/&gt;Logistic Regression, SV&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classification of human activities from smartwatches&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Smartwatch sensors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walk, run, sitting&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Abdulhamit et al. &lt;xref rid=&quot;B36-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;36&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ANN, k-NN,&lt;break/&gt;SVM, Quadratic&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classification of human activities using smartphones&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Smartphones sensors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.3%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Štulien˙e et al. &lt;xref rid=&quot;B37-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;37&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AlexNet,&lt;break/&gt;CaffeRef, k-NN,&lt;break/&gt;SVM, BoF&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Activities classification using images&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Images&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Indoor activities: working on a computer, sleeping, walking&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.75%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Alsheikh et al. &lt;xref rid=&quot;B38-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;38&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RBM&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Deep learning-based activity recognition using triaxial accelerometers&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Body sensors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking, running, standing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ronao et al. &lt;xref rid=&quot;B39-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;39&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ConvNet, SVM&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Classification of human activities using smartphones&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Smartphone sensors&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking upstairs, walking downstairs&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.75%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bhattacharya et al. &lt;xref rid=&quot;B40-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;40&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RBM&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;recognition of human activities using smartwatches based on deep learning&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Smartwatch sensors (Ambient sensors)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gesture-based features, walking, running, standing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>58221</offset><text>Reference	Techniques	Approach	Input Source	Activity	Performance	 	Alex et al. 	Naive Bayes, SVM, MLP, RF	Human activity classification	Images	Walking, Sleeping, holding a phone	86%	 	Jaouedi et al. 	Gated Recurrent Unit, KF, GMM	Deep learning for human activity recognition	Video frames	Boxing, walking, running, waving of hands	96.3%	 	Antón et al. 	RF	Deduce high-level non-invasive ambient that helps to predict abnormal behaviors	Ambient sensors	Abnormal activities:Militancy, yelling, vocal violence, physical abuseHostility	98.0%	 	Shahmohammadi et al. 	RF, Extra Trees,Naive Bayes,Logistic Regression, SV	Classification of human activities from smartwatches	Smartwatch sensors	Walk, run, sitting	93%	 	Abdulhamit et al. 	ANN, k-NN,SVM, Quadratic	Classification of human activities using smartphones	Smartphones sensors	Walking	84.3%	 	Štulien˙e et al. 	AlexNet,CaffeRef, k-NN,SVM, BoF	Activities classification using images	Images	Indoor activities: working on a computer, sleeping, walking	90.75%	 	Alsheikh et al. 	RBM	Deep learning-based activity recognition using triaxial accelerometers	Body sensors	Walking, running, standing	98%	 	Ronao et al. 	ConvNet, SVM	Classification of human activities using smartphones	Smartphone sensors	Walking upstairs, walking downstairs	95.75%	 	Bhattacharya et al. 	RBM	recognition of human activities using smartwatches based on deep learning	Smartwatch sensors (Ambient sensors)	Gesture-based features, walking, running, standing	72%	 	</text></passage><passage><infon key="file">sensors-20-07115-t002.xml</infon><infon key="id">sensors-20-07115-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>59708</offset><text>The detailed list of human sentiment and associated activity used in the crowded souring phase.</text></passage><passage><infon key="file">sensors-20-07115-t002.xml</infon><infon key="id">sensors-20-07115-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sets&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sentiment Tags&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Activity Tags&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set 1 &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative, Positive, Neutral&lt;/td&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Sitting, Standing, Running, Lying, Jogging&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set 2 &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happy, Sad, Neutral, Feared&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Set 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger, Disgust, Joy, Surprised, Excited, Sad, Pained, Crying, Feared, Anxious, Relieved&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>59804</offset><text>Sets	Sentiment Tags	Activity Tags	 	Set 1 	Negative, Positive, Neutral	Sitting, Standing, Running, Lying, Jogging	 	Set 2 	Happy, Sad, Neutral, Feared	 	Set 3	Anger, Disgust, Joy, Surprised, Excited, Sad, Pained, Crying, Feared, Anxious, Relieved	 	</text></passage><passage><infon key="file">sensors-20-07115-t003.xml</infon><infon key="id">sensors-20-07115-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>60054</offset><text>The architecture of human sentiment and associated activity analyzer backbone CSPDarknet53.</text></passage><passage><infon key="file">sensors-20-07115-t003.xml</infon><infon key="id">sensors-20-07115-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Layer&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Filters&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Output&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DarknetConv2D&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BN&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;608 × 608&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mish&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResBlock&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;304 × 304&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2 × ResBlock&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;152 × 152&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8 × ResBlock&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;256&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76 × 76&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8 × ResBlock&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;512&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;38 × 38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4 × ResBlock&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1024&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19 × 19 &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>60146</offset><text>Layer	Filters	Output	 	DarknetConv2D			 	BN	32	608 × 608	 	Mish			 	ResBlock	64	304 × 304	 	2 × ResBlock	128	152 × 152	 	8 × ResBlock	256	76 × 76	 	8 × ResBlock	512	38 × 38	 	4 × ResBlock	1024	19 × 19 	 	</text></passage><passage><infon key="file">sensors-20-07115-t004.xml</infon><infon key="id">sensors-20-07115-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>60361</offset><text>The CNN architecture of the deep count tracker.</text></passage><passage><infon key="file">sensors-20-07115-t004.xml</infon><infon key="id">sensors-20-07115-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Layer&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Patch Size&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Stride&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Output&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Conv&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32 × 128 × 64&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Conv &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32 × 128 × 64&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Max Pool&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32 × 64 × 32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32 × 64 × 32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32 × 64 × 32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64 × 32 × 16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;64 × 32 × 16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128 × 16 × 8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Residual Block&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 × 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128 × 16 × 8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dense&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Batch and &lt;italic&gt;l2&lt;/italic&gt; normalization&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>60409</offset><text>Layer	Patch Size	Stride	Output	 	Conv	3 × 3	1	32 × 128 × 64	 	Conv 	3 × 3	1	32 × 128 × 64	 	Max Pool	3 × 3	2	32 × 64 × 32	 	Residual Block	3 × 3	1	32 × 64 × 32	 	Residual Block	3 × 3	1	32 × 64 × 32	 	Residual Block	3 × 3	2	64 × 32 × 16	 	Residual Block	3 × 3	1	64 × 32 × 16	 	Residual Block	3 × 3	2	128 × 16 × 8	 	Residual Block	3 × 3	1	128 × 16 × 8	 	Dense	-	-	128	 	Batch and l2 normalization	-	-	18	 	</text></passage><passage><infon key="file">sensors-20-07115-t005.xml</infon><infon key="id">sensors-20-07115-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>60841</offset><text>Detailed images statistics of used in all tasks.</text></passage><passage><infon key="file">sensors-20-07115-t005.xml</infon><infon key="id">sensors-20-07115-t005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sentiment Tags&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of Images&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;518&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;480&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2002&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>60890</offset><text>Sentiment Tags	Number of Images	 	Positive	518	 	Neutral	480	 	Negative	2002	 	</text></passage><passage><infon key="file">sensors-20-07115-t006.xml</infon><infon key="id">sensors-20-07115-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>60970</offset><text>Detailed dataset statistics used in human activity expressions.</text></passage><passage><infon key="file">sensors-20-07115-t006.xml</infon><infon key="id">sensors-20-07115-t006</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Physical Activity Tags&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of Images&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sitting&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;780&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Standing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;713&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;782&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Running&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;708&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Laying&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>61034</offset><text>Physical Activity Tags	Number of Images	 	Sitting	780	 	Standing	713	 	Walking	782	 	Running	708	 	Laying	17	 	</text></passage><passage><infon key="file">sensors-20-07115-t007.xml</infon><infon key="id">sensors-20-07115-t007</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>61146</offset><text>Detailed statistics of sentiment breakdown for task 2.</text></passage><passage><infon key="file">sensors-20-07115-t007.xml</infon><infon key="id">sensors-20-07115-t007</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sentiment Tags&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of Images&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Happy&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;413&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Excited&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;105&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Feared&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;608&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;480&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1123&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Disgust&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;203&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Surprised&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;180&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Relief&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;200&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>61201</offset><text>Sentiment Tags	Number of Images	 	Happy	413	 	Excited	105	 	Feared	608	 	Anger	92	 	Neutral	480	 	Sad	1123	 	Disgust	203	 	Surprised	180	 	Relief	200	 	</text></passage><passage><infon key="file">sensors-20-07115-t008.xml</infon><infon key="id">sensors-20-07115-t008</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>61354</offset><text>The sentiment analyzers’ average results for task 1.</text></passage><passage><infon key="file">sensors-20-07115-t008.xml</infon><infon key="id">sensors-20-07115-t008</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sentiment&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AP (%)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.75&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.81&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.75&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.98&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.52&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.76&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>61409</offset><text>Sentiment	Precision (%)	Recall (%)	F1 Score (%)	AP (%)	 	Negative	96.75	95.19	95.81	97.75	 	Neutral	94.21	94.02	94.98	97.62	 	Positive	96.52	95.84	95.26	97.76	 	</text></passage><passage><infon key="file">sensors-20-07115-t009.xml</infon><infon key="id">sensors-20-07115-t009</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>61571</offset><text>Human object activity analyzers’ average results for task 2.</text></passage><passage><infon key="file">sensors-20-07115-t009.xml</infon><infon key="id">sensors-20-07115-t009</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Activity&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AP (%)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sitting&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.04&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.91&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.02&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Standing&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.46&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.35&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.93&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.36&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Running&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.09&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.21&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Laying&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.35&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.16&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>61634</offset><text>Activity	Precision (%)	Recall (%)	F1 Score (%)	AP (%)	 	Sitting	95.20	95.04	95.91	97.02	 	Standing	96.21	96.01	96.46	97.62	 	Walking	96.35	95.93	96.02	97.36	 	Running	93.02	92.09	93.10	97.21	 	Laying	96.35	96.21	96.45	97.16	 	</text></passage><passage><infon key="file">sensors-20-07115-t010.xml</infon><infon key="id">sensors-20-07115-t010</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>61861</offset><text>Sentiments and associated human activity analyzers’ average results for task 2.</text></passage><passage><infon key="file">sensors-20-07115-t010.xml</infon><infon key="id">sensors-20-07115-t010</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Sentiment&lt;/th&gt;&lt;th rowspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Metric&lt;/th&gt;&lt;th colspan=&quot;5&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Activity&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sitting&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Standing&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walking&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Running&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Laying&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Happy/Joy&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.53&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.51&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.76&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.13&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Anger&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.53&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.13&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.52&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.07&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.70&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.14&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Fear&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.73&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.13&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.05&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.78&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.89&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.46&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.17&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.52&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.78&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Sad&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.57&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.52&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.02&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.36&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.17&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.38&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.27&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Neutral&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.34&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.62&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.03&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.91&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.43&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.79&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.34&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Disgust&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.78&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.34&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.22&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.34&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.05&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Surprise&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.45&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.98&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.45&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.05&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.89&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.89&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Relief/Relax&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.23&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.29&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.78&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.12&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.13&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.19&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.14&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score (%)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.78&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.04&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.34&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>61943</offset><text>Sentiment	Metric	Activity	 	Sitting	Standing	Walking	Running	Laying	 	Happy/Joy	Precision (%)	98.21	97.02	95.71	90.53	89.01	 	Recall (%)	97.51	96.90	95.10	91.23	89.00	 	F1 Score (%)	97.76	96.95	95.13	90.15	88.15	 	Anger	Precision (%)	95.53	90.20	94.23	89.13	80.52	 	Recall (%)	94.12	89.12	94.12	88.07	80.12	 	F1 Score (%)	94.70	90.01	93.14	89.11	80.32	 	Fear	Precision (%)	97.73	96.23	93.54	91.23	92.13	 	Recall (%)	97.19	96.05	92.78	91.11	91.89	 	F1 Score (%)	96.46	96.12	92.17	90.52	91.78	 	Sad	Precision (%)	98.57	95.79	93.56	90.52	95.02	 	Recall (%)	98.23	95.36	92.19	89.19	94.56	 	F1 Score (%)	98.17	95.11	93.38	90.27	94.12	 	Neutral	Precision (%)	98.11	94.34	89.10	85.12	78.12	 	Recall (%)	96.62	93.03	88.91	84.43	77.79	 	F1 Score (%)	95.15	93.5	88.56	82.45	77.34	 	Disgust	Precision (%)	80.12	80.56	78.78	77.12	90.56	 	Recall (%)	79.34	80.22	78.01	77.34	90.12	 	F1 Score (%)	78.56	79.45	78.12	77.05	90.27	 	Surprise	Precision (%)	94.01	90.12	81.12	79.01	85.45	 	Recall (%)	93.98	89.45	79.05	78.56	85.01	 	F1 Score (%)	93.56	89.01	79.10	78.89	84.89	 	Relief/Relax	Precision (%)	96.12	96.23	95.29	92.78	90.12	 	Recall (%)	96.00	95.13	94.84	92.19	90.14	 	F1 Score (%)	95.78	96.04	95.11	92.25	89.34	 	</text></passage><passage><infon key="file">sensors-20-07115-t011.xml</infon><infon key="id">sensors-20-07115-t011</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>63148</offset><text>Assessment of the deep models’ sentiment analysis on task 1 (i.e., three classes of single-label classification, namely negative, neutral, and positive).</text></passage><passage><infon key="file">sensors-20-07115-t011.xml</infon><infon key="id">sensors-20-07115-t011</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet-50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.61&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.63&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet-101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.01&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.43&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dense Net&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.77&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.39&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.53&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGGNet (Image Net)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.64&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.63&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.89&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGGNet (Places)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.43&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.07&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.59&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.38&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.81&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.60&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Efficient Net&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.31&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.70&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>63304</offset><text>Model	Accuracy	Precision	Recall	F1 Score	 	ResNet-50	89.61	86.32	85.18	85.63	 	ResNet-101	90.01	87.79	86.84	86.43	 	Dense Net	85.77	79.39	78.53	78.20	 	VGGNet (Image Net)	92.12	88.64	87.63	87.89	 	VGGNet (Places)	92.88	89.92	88.43	89.07	 	Inception-v3	82.59	76.38	68.81	71.60	 	Efficient Net	91.31	87.00	86.94	86.70	 	</text></passage><passage><infon key="file">sensors-20-07115-t012.xml</infon><infon key="id">sensors-20-07115-t012</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>63623</offset><text>Assessment of the deep models’ human activity analysis on task 2 (i.e., five classes of single-label classification, namely sitting, standing, walking, running, and laying).</text></passage><passage><infon key="file">sensors-20-07115-t012.xml</infon><infon key="id">sensors-20-07115-t012</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Precision&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Recall&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1 Score&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet-50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.74&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.43&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.61&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.14&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet-101&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.55&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.08&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dense Net&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.53&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGGNet (Image Net)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.51&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGGNet (Places)&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.43&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.07&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;84.18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.60&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Efficient Net&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.83&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.70&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.39&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>63799</offset><text>Model	Accuracy	Precision	Recall	F1 Score	 	ResNet-50	82.74	80.43	85.61	82.14	 	ResNet-101	85.55	79.26	85.08	81.16	 	Dense Net	81.53	78.21	89.30	82.27	 	VGGNet (Image Net)	82.56	80.25	84.51	81.80	 	VGGNet (Places)	89.88	88.92	88.43	89.07	 	Inception-v3	82.30	79.90	84.18	81.60	 	Efficient Net	82.25	80.83	82.70	81.39	 	</text></passage><passage><infon key="file">sensors-20-07115-t013.xml</infon><infon key="id">sensors-20-07115-t013</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>64118</offset><text>Comparison of human count and tracking algorithms.</text></passage><passage><infon key="file">sensors-20-07115-t013.xml</infon><infon key="id">sensors-20-07115-t013</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Technique&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MOTA&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MOTP&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MT&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ML&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Runtime&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B64-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;64&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;41.0%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19.0%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;933&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.7 Hz&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B65-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;65&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;71.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;46.9%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21.9%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;434&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.5 Hz&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B66-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;66&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;31.5%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24.2%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1394&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;35 Hz&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;xref rid=&quot;B67-sensors-20-07115&quot; ref-type=&quot;bibr&quot;&gt;67&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52.5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.8&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19.0%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;34.9%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;910&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12 HZ&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proposed &lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;61.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.1&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32.8%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18.2%&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;781&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;40 Hz&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>64169</offset><text>Technique	MOTA	MOTP	MT	ML	ID	Runtime	 		68.2	79.4	41.0%	19.0%	933	0.7 Hz	 		71.0	80.2	46.9%	21.9%	434	0.5 Hz	 		62.4	78.3	31.5%	24.2%	1394	35 Hz	 		52.5	78.8	19.0%	34.9%	910	12 HZ	 	Proposed 	61.4	79.1	32.8%	18.2%	781	40 Hz	 	</text></passage></document></collection>
