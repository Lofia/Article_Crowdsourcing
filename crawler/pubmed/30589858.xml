<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201215</date><key>pmc.key</key><document><id>6307743</id><infon key="license">CC BY</infon><passage><infon key="alt-title">Imagery, crowdsourcing, and deep learning for snow monitoring</infon><infon key="article-id_doi">10.1371/journal.pone.0209649</infon><infon key="article-id_pmc">6307743</infon><infon key="article-id_pmid">30589858</infon><infon key="article-id_publisher-id">PONE-D-18-08025</infon><infon key="elocation-id">e0209649</infon><infon key="issue">12</infon><infon key="license">This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</infon><infon key="name_0">surname:Kosmala;given-names:Margaret</infon><infon key="name_1">surname:Hufkens;given-names:Koen</infon><infon key="name_2">surname:Richardson;given-names:Andrew D.</infon><infon key="name_3">surname:Mirjalili;given-names:Seyedali</infon><infon key="name_4">surname:Richardson;given-names:Andrew D.</infon><infon key="name_5">surname:Richardson;given-names:Andrew D.</infon><infon key="notes">All relevant data are within the paper and its Supporting Information files.</infon><infon key="section_type">TITLE</infon><infon key="title">Data Availability</infon><infon key="type">front</infon><infon key="volume">13</infon><infon key="year">2018</infon><offset>0</offset><text>Integrating camera imagery, crowdsourcing, and deep learning to improve high-frequency automated monitoring of snow at continental-to-global scales</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>148</offset><text>Snow is important for local to global climate and surface hydrology, but spatial and temporal heterogeneity in the extent of snow cover make accurate, fine-scale mapping and monitoring of snow an enormous challenge. We took 184,453 daily near-surface images acquired by 133 automated cameras and processed them using crowdsourcing and deep learning to determine whether snow was present or absent in each image. We found that the crowdsourced data had an accuracy of 99.1% when compared with expert evaluation of the same imagery. We then used the image classification to train a deep convolutional neural network via transfer learning, with accuracies of 92% to 98%, depending on the image set and training method. The majority of neural network errors were due to snow that was present not being detected. We used the results of the neural networks to validate the presence or absence of snow inferred from the MODIS satellite sensor and obtained similar results to those from other validation studies. This method of using automated sensors, crowdsourcing, and deep learning in combination produced an accurate high temporal dataset of snow presence across a continent. It holds broad potential for real-time large-scale acquisition and processing of ecological and environmental data in support of monitoring, management, and research objectives.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1499</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1512</offset><text>Snow is a crucial component of Earth’s hydrology and affects climate at global scales. It affects the exchange of mass and energy between the land and atmosphere. The magnitude and timing of snow melt have a huge influence on the seasonality of global hydrological cycles and input of freshwater to the world’s oceans. And in the Arctic, the timing of snow melt affects the persistence of permafrost, with consequences for carbon release and global climate change.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1981</offset><text>Snow is also important at regional and local scales. In boreal and mountainous areas, the timing of snow melt affects spring vegetation phenology and related ecosystem functions such as pollination. In many parts of the world, seasonal mountainous snow melt is a major source of surface water and groundwater recharge. It is important for power generation, irrigation, and as a source of drinking water for millions of people.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2408</offset><text>There is enormous spatial and temporal variation in rates of snow fall and snow melt, resulting in fine-scale heterogeneity in snow cover on the land surface, especially during early and late winter transition seasons. This variation is expected to change in complex ways over the next century, with repercussions for global and regional climate, hydrology, ecosystem functioning, and human societies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2810</offset><text>Knowledge of snow cover is important at local, regional, and global scales, but accurate monitoring of snow is a challenge. During winter months, snow covers a large fraction of the global land area, and the cold and often remote locations where it falls make ground observations logistically challenging. Current approaches to snow monitoring include ground stations, airborne sensors, and satellite sensors. Each has its advantages and drawbacks. Automated ground stations can monitor snow at high temporal resolution but lack spatial extent. Airborne sensors can monitor snow across larger regions but are typically limited to specific seasons and have moderate temporal resolution. Satellite sensors can monitor snow globally and at high temporal resolution using measurements of reflected visible and infrared radiation, but are limited by cloud cover, forest cover, and a mismatch between fine-scale terrain heterogeneity and coarser satellite pixel size. Satellite sensors using passive microwave radiation avoid some of these limitations, but necessarily have coarser spatial resolution and the challenge of interpretation for snow packs of different grain size, density, and depth.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4001</offset><text>Networks of ground observations at high temporal and spatial resolution have the potential to increase the accuracy of snow monitoring products. Such measurements could be used as validation data for satellite sensors or as inputs to hybrid approaches that combine data from multiple types of sensors (e.g.). SNOTEL provides an example of a dense snow-observing network. However, SNOTEL’s coverage is limited to high-elevation mountainous areas in the western U.S., due to the expense of the instrumentation. Automated cameras have been used to track snow extent and depth for a couple decades. But until recently, they have been too expensive to deploy widely, resulting in studies that use no more than a handful of cameras to track snow in targeted areas.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4762</offset><text>Today, automated near-surface cameras are inexpensive and have been deployed to automatically record and transmit outdoor images for myriad reasons, including air quality monitoring, traffic monitoring, and vegetation monitoring. Tens of thousands of such cameras are already operational worldwide and present an opportunity to collect high frequency data about snow occurrence. However, automatically detecting snow in camera images is not easy. While the human eye can quickly determine whether snow is present or not in a given digital image, algorithms based on directly using the red, green, and blue color channels in the image (e.g.) fail to consistently indicate snow presence or absence across a network of many cameras. Difficulties in accurate automated detection include differences in light levels and color balance among cameras, heterogeneous fields of view, and white objects in some cameras’ fields of view.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5689</offset><text>Recent advances in machine learning may offer a solution. A neural network technique known as “deep learning” has been shown to reach human-level accuracy in some computer vision tasks, such as facial recognition. Deep learning shows promise in addressing data processing challenges in ecological and environmental fields. In the last few years, deep learning has been applied to the task of land cover classification from both satellite images and crowd-tagged ground-based photography. Specifically for snow, deep learning has been used in conjunction with support vector machines to classify snow in unlabeled Flickr images with 71.7% accuracy and hand-labeled Flickr images with 80.5% accuracy. One advantage of deep learning is that is can accommodate heterogeneous data, allowing for data processing of “found” data that can be analyzed for purposes other than for which it was originally gathered.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6602</offset><text>However, deep learning requires a large labeled dataset on which to train the identification algorithm. This presents a major challenge in applying such methods to real-world problems beyond the computer science discipline. For example, labeling tens or hundreds of thousands of images, even if the classification was simple e.g. “image has snow” or “image has no snow”, would be a time-consuming burden for a small research team. Updating these classifications in real time would add further challenge. Crowdsourcing, in which many human participants each perform a relatively small number of tasks, can make this challenge tractable. While this approach is well-known in computer science (e.g. Mechanical Turk), it is not commonly used in many other disciplines. Integrating crowdsourcing and deep learning to answer ecological and environmental research questions is new within the past couple years.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7514</offset><text>We investigated the use of automated near-surface cameras as “snow detectors”, using crowdsourcing to develop a labeled dataset to which we then applied deep learning techniques. Our overarching question was, “Can deep learning be used to accurately detect snow in digital camera images?” The images we used came from the PhenoCam network, which is currently comprised of nearly 500 cameras (predominantly in North America) for vegetation monitoring. We used crowdsourcing to determine the presence or absence of snow in nearly 185,000 images from 133 camera sites that were available to us at the time. We then trained a deep learning model on the image labels to automate the detection of snow in camera images. We tested the trained model using classic cross-validation methods. Finally, we compared our results with estimates of snow cover derived from satellite remote sensing (MODIS snow fractional cover product) as an example of how these data might be put to practical use.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>8505</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8513</offset><text>We used images collected by the PhenoCam network (http://phenocam.sr.unh.edu), an expanding network of near-surface automatic cameras (Fig 1). At each site, cameras are mounted on towers, buildings, or other structures and take pictures across a landscape every half hour during the daytime, year-round. Sites are divided into Type I, Type II and Type III. Type I sites use a standard camera and follow a protocol as to camera view and fixed white balance. Type II sites do not use the standard camera but use the Type I protocols. Type III sites are those that have imagery publicly available online and have been judged to meet the aims of the PhenoCam network, but do not necessarily use the standard camera or PhenoCam protocols.</text></passage><passage><infon key="file">pone.0209649.g001.jpg</infon><infon key="id">pone.0209649.g001</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>9247</offset><text>Location of PhenoCam camera sites used in the present analysis.</text></passage><passage><infon key="file">pone.0209649.g001.jpg</infon><infon key="id">pone.0209649.g001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>9311</offset><text>Red: Type I sites; orange: Type II sites; yellow: Type III sites. Made with Natural Earth: free vector and raster map data @ naturalearthdata.com.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9458</offset><text>We used imagery from the 133 PhenoCam sites that existed by 2014 (S1 Table). These sites encompass a wide range of North American ecosystem types, including deciduous broadleaf forest, evergreen coniferous forest, deciduous coniferous forest, mixed forest, savanna, shrubland, grassland, tundra, and wetland. Additionally, a wide range of human uses of the land are represented, including agricultural sites, urban and suburban sites, research sites, and protected natural sites. At each site, we used imagery from the start of collection until December 31, 2014. For each day of collection, we selected the image closest to noon local time to be representative of the snow presence or absence for that day. A total of 184,453 such midday images across all sites comprised the total imagery data set.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>10259</offset><text>1. Crowdsourced classification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10290</offset><text>We contracted the company CrowdMobile to provide snow labels for the images. Each image was manually classified by three independent participants on Android-based mobile devices using the CrowdMobile crowdsourcing platform Knowxel. Participants classified images as having snow, not having snow, or having poor quality such that it is not possible to distinguish whether snow is present or absent (‘bad image’). For images with trees and snow, participants also indicated whether snow was visible on the trees or whether it was only visible on the ground. We received from CrowdMobile each individual classification and consider the ‘crowd consensus’ classification for each image to be the classification that was chosen by at least two of the three participants (S2 Table). A crowd consensus could not be calculated for three of the images (&lt;0.002% of the total), and these images were manually classified by the authors. We received no information about the participants from CrowdMobile.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11290</offset><text>To assess the accuracy of the crowd consensus, crowd consensus classifications were compared with a ‘gold standard’ set of classifications. From the total imagery data set, 2013 images (1% of the total) were randomly selected and classified by PhenoCam scientists using the same Knowxel platform employed by the participants. Each image was independently evaluated by three scientists, and images without unanimous consensus among scientists were reviewed and discussed by the authors to reach a definitive gold standard classification (S3 Table).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11842</offset><text>2. Deep learning classification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11874</offset><text>We used a deep convolutional neural network (CNN) to classify the presence or absence of snow in images. For CNN classification, we excluded images from the total dataset with a crowd classification of ‘bad image’; a total of 172,927 images remained for CNN classification. We used the Places365-VGG CNN to classify each image. The Places dataset and corresponding CNNs result from forefront research on automatic scene classification. Our interest was to apply this technology to a pressing environmental research need, and so we chose the best-performing network according to the analyses performed by computer vision experts. Places365-VGG is trained on the Places2 database and consists of a standard VGG architecture of 16 convolutional layers. Its output is a prediction confidence score for each of 365 scene categories. We extracted the highest-scoring five categories (“top 5”) for each image and classified it as having snow if any of the categories of ‘iceberg’, ‘ice_skating_rink/outdoor’, ‘mountain_snowy’, ‘ski_resort’, ‘ski_slope’, or ‘snowfield’ were present, and not having snow otherwise.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13013</offset><text>We then used transfer learning to investigate whether Places365-VGG could achieve a higher accuracy than the simple “top 5” approach with existing categories. Transfer learning is a technique that uses an existing trained neural network to classify images or other data with a new set of labels. Accuracy is higher for the new labels when the old task and the new task are more similar. Because the neural network already has a representation of the features in the images or data that then get mapped to labels, the raw network output can be used to redefine the mapping to labels. As a result, transfer learning is faster and less complicated than training a network from scratch and requires fewer training data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13733</offset><text>We exchanged the 365 labels built into Places365-VGG for a pair of labels: ‘snow’ and ‘no snow’. We modified the source code for the Places2 CNN to extract the 4096 values of the nodes in the last fully-connected layer and used them as input to a support vector machine (SVM) with the output labels of ‘snow’ or ‘no snow’. We trained SVMs using the software LIBLINEAR (version 2.11, http://www.csie.ntu.edu.tw/~cjlin/liblinear), with the solver set to ‘L2-regularized L2-loss support vector classification (primal)’ and with cost parameter = 0.0078 for all models. The cost parameter was chosen using LIBLINEAR’s cross-validation method for finding an optimal value. However, the models were not very sensitive to the cost parameter, with accuracies varying by just a half a percent or less for reasonable values. In general, LIBLINEAR solvers are not very sensitive to the cost parameter. Final SVM weights are available in S4 Table.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14690</offset><text>We conducted two sets of 10-fold cross validation, training the SVM using 90% of the images and testing on the remaining 10% repeatedly until all images had been included exactly once in the testing group. For the first validation set, we split the images into training and testing groups randomly. For the second validation set, we kept images from the same site together when splitting into training and testing groups. The first method provides an accuracy estimate for classifying new images from existing sites. The second method estimates accuracy for classifying images from new sites.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15283</offset><text>There is considerable heterogeneity in the imagery from the PhenoCam network in terms of camera model, camera view, amount of vegetation, and camera configuration. Some camera sites are more similar to one another in terms of camera model, angle, orientation, and configuration, however, and it we did not know whether the CNNs would work better for prediction among a more homogenous subset of the full set of camera sites. To investigate this, we created two subsets of the CNN data set: a set that contains only images from Type I sites (82,698 images) and a set that contains images from Type I and Type II sites, but not Type III sites (126,908 images). We repeated the two sets of 10-fold cross validation on these two subsets of data to determine if camera configuration accounts for errors in CNN classification.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16104</offset><text>We also trained and tested the SVM on the data set and its two subsets in entirety. While these over-fitted models are not useful for prediction, they provide an upper bound for how well we can expect the transfer learning method to work for detecting snow using the Places2 CNN.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>16384</offset><text>3. MODIS validation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16404</offset><text>For each image in the full dataset, we extracted the corresponding fractional snow cover value from MODIS products MOD10A1 and MYD10A1. These products are derived from radiance data from the Terra and Aqua satellites respectively and use a 500-meter resolution. Using values from both satellites increases the temporal coverage of MODIS data. We considered snow to have been detected if either the MOD10A1 or the MYD10A1 product had a fractional snow cover greater than zero. If neither product reported fractional snow cover corresponding to a given PhenoCam image (e.g. due to cloud cover), then we considered that image to have no MODIS information about snow and did not use it for the following analyses. A total of 96,151 images from the set of all images without a crowd consensus of ‘bad image’ (56%) had a MODIS classification of snow or no snow.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17264</offset><text>We calculated the rate of agreement between MODIS classifications and gold standard classifications, between MODIS classifications and crowd consensus classifications, and between MODIS classifications and the predictions of two CNN models. For the two CNN models, we chose a model that had a high accuracy and one that had a lower accuracy. This provided the range of possible agreements between MODIS classifications and those of the CNN models, without having to do comparisons with every CNN model. The higher accuracy model we used was the SVM trained and tested on Type I sites only, with a fully random split of the images (44,510 images with MODIS classifications). For the lower accuracy model, we chose the SVM trained and tested on all images, with images split by site.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18046</offset><text>We also examined the accuracy rates of MODIS classifications, as compared with the crowd consensus, for images containing trees and for those without trees. For those with trees, we further calculated accuracies based on whether snow was present on the trees or was only on the ground.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18332</offset><text>In comparing accuracies, we focused on true positive (snow is there and is classified as snow) and true negative (snow is absent and is classified as absent) rates, rather than overall accuracy because images without snow were much more frequent and accuracy rates for images with and without snow were very different. Only about 15% of the images with MODIS classifications had snow, and overall accuracy was considerably increased by the large number of true negatives.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>18804</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>18812</offset><text>1. Crowdsourced classification</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>18843</offset><text>Of all the images in the full data set, 6.2% were classified as ‘bad image’ by the Knowxel participants and were unusable for snow detection. The causes of these bad images were poor weather conditions, accumulation of water or ice on the lens, and camera malfunction. Of the remaining images, 72.8% were classified as not having snow; 4.0% were treeless images that had snow; 7.8% were images with snow on the trees and the ground; and 9.2% were images with snow on the ground, but not on the trees. We combined the latter three categories into a classification of ‘snow present’ for most analyses (38,727 images; 21.0%). The three classifications for each image agreed unanimously 94.0% of the time in classifying images as ‘snow’ or ‘no snow’, giving us high confidence in the overall quality of the individual Knowxel classifications.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19698</offset><text>When images with a crowd consensus of ‘bad image’ were removed from the gold standard dataset, the crowd consensus of ‘snow’ vs. ‘no snow’ had an accuracy of 99.1% compared to the gold standard (1797 images) for all sites and 99.4% for the datasets consisting of Type I and Type II sites (1352 images) and Type I sites only (924 images), respectively. (See S1 Appendix for confusion matrices.) These results indicate that the crowd consensus classifications are of excellent quality and can be used as reliably as an expert-annotated dataset.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>20253</offset><text>2. Deep learning classification</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>20285</offset><text>Accuracy for the convolutional neural networks (CNNs) compared to the crowd consensus varied from 83.2% to 97.5%, depending on the specification of training and validation sets (Table 1 and S1 Appendix). Using the top-5 categories from Places365-VGG directly, accuracy was 83.2% for the full dataset. Transfer learning increased the accuracy to 97.0% when images were fully randomized between training and testing sets. The apparent upper bound was only slightly higher at 97.1%, suggesting that the CNNs with transfer learning would be excellent at predicting whether snow is present or not for new images from existing sites.</text></passage><passage><infon key="file">pone.0209649.t001.xml</infon><infon key="id">pone.0209649.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>20913</offset><text>Accuracies of convolutional neural networks (CNNs) as compared against crowd consensus classifications of ‘snow’ or ‘no snow’ for three datasets.</text></passage><passage><infon key="file">pone.0209649.t001.xml</infon><infon key="id">pone.0209649.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot; colspan=&quot;1&quot;&gt; &lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;Image dataset&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Types I, II, III&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Types I, II&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Type I&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM trained &amp;amp; validated on whole set&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.1%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.4%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM with 10-fold cross validation, random&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.0%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.2%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.2%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM with 10-fold cross validation, by site&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.6%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.7%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Places365-VGG using top-5 categories&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.2%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.8%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.5%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21067</offset><text> 	Image dataset	 	Types I, II, III	Types I, II	Type I	 	SVM trained &amp; validated on whole set	97.1%	97.4%	97.5%	 	SVM with 10-fold cross validation, random	97.0%	97.2%	97.2%	 	SVM with 10-fold cross validation, by site	91.6%	93.7%	93.5%	 	Places365-VGG using top-5 categories	83.2%	85.8%	85.5%	 	</text></passage><passage><infon key="file">pone.0209649.t001.xml</infon><infon key="id">pone.0209649.t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>21364</offset><text>The three datasets: the full dataset, with images from Type I, II and III sites, and two subsets. Transfer learning was employed by replacing the classification step of Places365-VGG with a support vector machine (SVM) trained to distinguish between ‘snow’ and ‘no snow’.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21644</offset><text>Transfer leaning increased accuracy from 83.2% to 91.6% when images assigned to training and testing sets were split by site. This accuracy is lower than when training and testing sets were fully randomized and reflects the challenge for the CNNs in predicting whether snow is present or not at sites it has not yet seen. These new sites are outside the set of sites on which the CNN trained, and so the CNN is making inferences about images at these new sites based only on what it knows snow looks like at other sites. When a standard camera configuration was used for all the sites in the dataset (i.e. the subset datasets), the CNNs performed better on sites they had not yet seen; the difference in accuracy was 5.4% between the cross-validation models for the whole dataset, but just 3.5% and 3.7% for the two subsets.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22469</offset><text>CNNs trained on different sets of images almost always agreed on the snow classification for individual images. For the CNNs trained to predict new images from existing sites, the agreement rate was 99.4% and for those trained to predict images from new sites, the rate was 97.0%. For the small number of images where there was disagreement, the reason was usually because they were sites with small amounts of patchy snow. These images frequently also had non-unanimous consensus by participants.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22967</offset><text>Accuracy was always lowest for the full image dataset (Types I, II, and III), but by less than a few percentages points compared to the subset datasets. This suggests that a standard configuration across cameras has a small, but measurable impact on the ability of CNNs to learn to identify snow. Accuracies for the two subset datasets (Type I and II, Type I only) were similar, suggesting that choice of camera model does not affect the ability of CNNs to learn to identify snow if the cameras follow a standard configuration.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23495</offset><text>Sensitivity (true positive rate) for the CNNs with transfer learning was lower than specificity (true negative rate; Table 2). Specificity ranged from 94% to near perfect, whereas sensitivity for the most accurate CNN was 90.5% (S1 Appendix). This means that the CNNs fail to detect snow that is present (“missed snow”) much more often than they falsely detect snow that is not there (“phantom snow”). The overall CNN accuracies, therefore, reflect the high specificity, because images without snow are more frequent.</text></passage><passage><infon key="file">pone.0209649.t002.xml</infon><infon key="id">pone.0209649.t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>24021</offset><text>Sensitivity (true positive rate) and specificity (true negative rate) for selected convolutional neural networks (CNNs) and datasets as compared against the crowd consensus.</text></passage><passage><infon key="file">pone.0209649.t002.xml</infon><infon key="id">pone.0209649.t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot; colspan=&quot;1&quot;&gt;&lt;break/&gt;&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;Types I, II, III&lt;/th&gt;&lt;th align=&quot;center&quot; colspan=&quot;3&quot; rowspan=&quot;1&quot;&gt;Type I&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sensitivity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Specificity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sensitivity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Specificity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM trained &amp;amp; validated on whole set&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.3%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.4%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.1%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.5%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;99.6%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM with 10-fold cross validation, by site&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.3%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.0%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.6%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.0%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.8%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.5%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>24195</offset><text>	Types I, II, III	Type I	 	Sensitivity	Specificity	Accuracy	Sensitivity	Specificity	Accuracy	 	SVM trained &amp; validated on whole set	89.3%	99.4%	97.1%	90.5%	99.6%	97.5%	 	SVM with 10-fold cross validation, by site	83.3%	94.0%	91.6%	86.0%	95.8%	93.5%	 	</text></passage><passage><infon key="file">pone.0209649.t002.xml</infon><infon key="id">pone.0209649.t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>24447</offset><text>Accuracy rates are the same as those reported in Table 1.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24505</offset><text>We examined the reasons for misclassification by the CNNs. Snow was missed by the CNNs most often when there was little snow in an image and the snow was patchy (Fig 2). These “missed snow” images also appear to be difficult for humans to consistently classify. Of the “missed snow” images, 43% of them have non-unanimous classification, as compared with 6% for the whole dataset. Some of the errors were also due to human misclassification of images that do not have snow. There were few errors when snow was not present, and classification of images without snow was almost perfect in predicting new images from existing sites. The few errors were caused by misinterpretation of ice on lakes and rivers as snow, glare from wet pavement and metal equipment, fog and mist, and small amounts of precipitation on the camera lens (Fig 2).</text></passage><passage><infon key="file">pone.0209649.g002.jpg</infon><infon key="id">pone.0209649.g002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25349</offset><text>Examples of false positive (A-C) and false negative (D-I) images. Beneath each image are the labels provided by three participants (“Crowd”), convolutional neural networks (“CNN”), and a MODIS snow product. S = Snow, N = No snow, X = not available. False positives are due to (A) ice on lake, (B) fog, (C) precipitation on the lens. False negatives are due to (D) snow on distant mountains and (E-I) patchy snow.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>25770</offset><text>3. MODIS validation</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25790</offset><text>Overall accuracy of the MODIS snow product was high when validated against the gold standard, crowd consensus, and the most accurate CNN, and was only slightly lower when validated against the least accurate CNN (Table 3 and S1 Appendix). The majority of errors in all cases were images which had snow, but which MODIS missed. This is reflected in the sensitivity, which is substantially lower than the total accuracy in all cases (Table 3).</text></passage><passage><infon key="file">pone.0209649.t003.xml</infon><infon key="id">pone.0209649.t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>26232</offset><text>Sensitivity (true positive rate), specificity (true negative rate), and accuracy of the MODIS fractional snow cover product as validated against four datasets based on ground time-lapse imagery from the PhenoCam network.</text></passage><passage><infon key="file">pone.0209649.t003.xml</infon><infon key="id">pone.0209649.t003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MODIS validated against:&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of images&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Sensitivity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Specificity&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Gold standard&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1044&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;81.7%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.3%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Crowd consensus&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96,151&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;76.6%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.1%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.6%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM trained &amp;amp; validated on Type I images only&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;44,510&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.2%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.0%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.8%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM with 10-fold cross validation, by site, on all images&lt;/td&gt;&lt;td align=&quot;right&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96,151&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37.1%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;94.9%&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.7%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>26453</offset><text>MODIS validated against:	Number of images	Sensitivity	Specificity	Accuracy	 	Gold standard	1044	81.7%	98.3%	95.5%	 	Crowd consensus	96,151	76.6%	97.1%	93.6%	 	SVM trained &amp; validated on Type I images only	44,510	69.2%	95.0%	93.8%	 	SVM with 10-fold cross validation, by site, on all images	96,151	37.1%	94.9%	88.7%	 	</text></passage><passage><infon key="file">pone.0209649.t003.xml</infon><infon key="id">pone.0209649.t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>26771</offset><text>The two CNNs are those with the highest and lowest accuracy respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26845</offset><text>One recognized challenge in detecting snow with satellite sensors is when snow is present but lies beneath a tree canopy. We found that sensitivity for sites with trees (75.4%) was somewhat lower than that for sites without trees (81.8%) when validated against the crowd consensus. More striking was that in cases when snow was visible on the tree foliage, sensitivity was much higher (88.9%) than when snow was only visible on the ground beneath the trees (67.9%). This affirms that visual obstruction of snow by vegetation causes the MODIS product to under-report snow.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27417</offset><text>Because the MODIS product and the CNNs both miss snow when it is present more often than they fail to detect snow when it is there, we checked to see if these two methods tend to make corresponding errors and whether a combined approach would increase sensitivity. We found that for all images with snow, the MODIS classification agreed with classifications from the most accurate and least accurate CNN 83.6% and 75.7% of the time respectively. We then created a combined classification by considering an image to have snow if either the MODIS classification or the CNN classification (or both) indicate snow. With this combined classification, we increased MODIS sensitivity from 76.6% (validated against the crowd consensus) to 91.4% for the most accurate CNN and to 89.4% for the least accurate CNN. The MODIS specificity drops somewhat for the combined classification system (to 95.4%/90.4% for the most/least accurate CNN) affecting overall accuracy, which increases for the most accurate CNN to 94.9%. The large increase in sensitivity and overall increase in accuracy indicate that MODIS and surface-based cameras together with deep learning image processing can be useful complementary methods of snow detection.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>28639</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>28650</offset><text>Using crowdsourcing and deep learning in conjunction with a network of automated near surface cameras provides an automated and accurate means by which the distribution of snow can be estimated, at high temporal frequency (daily), fine spatial resolution (10–100 m), and across a broad spatial extent (regional-to-continental). Crowdsourcing was as good as expert labeling for the task of identifying snow presence or absence in digital images and resulted in a high-quality human-labeled dataset. Using transfer learning and this dataset, we were able to train deep neural networks to determine the presence or absence of snow at 133 heterogeneous sites with up to 98% accuracy. This novel processing chain–involving automated sensors, crowdsourcing, and transfer learning of deep convolutional neural networks (CNNs)–has the capacity to accelerate the acquisition and processing of a wide range of data types for environmental monitoring and research.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>29610</offset><text>1. Crowdsourced classification</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>29641</offset><text>For clear images, most of the time it was obvious whether or not snow is present to the human eye; 94% of clear images had unanimous agreement among the three classifying participants. Many of the non-unanimous classifications were cases in which there was very little snow, or when the snow was in the distant background (Fig 2). We had asked participants to count cases in which any snow was visible as ‘snow’. We recognize that if we had asked them to coarsely quantify the amount of snow (e.g. “no snow”, “some snow”, “mostly covered in snow”, “entirely covered in snow”), it may have resulted in a better training set. Images with little snow might then be more likely to be classified as “some snow” rather than “no snow” by the CNNs because the difference in amount of snow between adjacent classes would be smaller. Volunteer classifications were as accurate as expert classifications when combined into a consensus classification, demonstrating that simple tasks such as determining ‘snow’ versus ‘no snow’ can require as few three independent volunteer classifications for high accuracy. More difficult or complex tasks might require more classifications or other means to ensure data quality.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>30881</offset><text>We crowdsourced labels to our entire image data set of 184,000+ images because we wanted a consistent and complete set of labels for a PhenoCam data product. However, we could have been more efficient. Using temperature records from the various sites would have allowed us to quickly classify many images as having no snow because it is too warm, leaving far fewer for participants to classify. If we had only been creating a dataset for training CNNs, we could have been even more efficient and subsampled–by eliminating warm-weather images and using imagery from every-other day, for example, as consecutive days frequently look very similar.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>31528</offset><text>2. Deep learning classification</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>31560</offset><text>While the overall accuracy of the CNNs was very high, the accuracy for images without snow was about ten percentage points higher than the accuracy for images with snow. The main reason that the CNNs missed snow was that for some images, snow was present in small amounts or was patchy (Fig 2). Presumably, this patchy snow did not match the gestalt of what a snow-covered landscape should look like, based on the majority of the training data. In many cases, though, images with patchy snow were correctly classified. These images with patchy snow were also the ones for which there was most likely to be disagreement among CNNs trained on different datasets, suggesting that they lie close to the hypersurface that separates classes within the state space of CNN parameters. Different training sets are enough to shift that hypersurface slightly so that these “in between” images can fall on the “snow” side sometimes and on the “no snow” side other times. It is notable that the images with snow that were incorrectly classified by the CNNs, mostly due to patchy snow, had a much higher rate of disagreement among human participants than the average. This suggests that for some images, there may be a real ambiguity about whether snow is present or not. A more nuanced classification scheme could potentially address this issue.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32905</offset><text>For some applications, misclassifying patchy snow may not matter. PhenoCam images, for example, are used to track vegetation phenology, or “greenness,” over the seasons. Broad swaths of snow that completely cover vegetation interfere with the greenness signal, but small amounts of snow do not. In tracking the emergence and flowering of alpine plants, a blanket of snow might signify a complete absence of these plants, whereas patchy snow may allow some plants to grow and bloom. For other applications, it may be important to know whether small amounts of snow are present. Patchy snow can affect the exchange of energy and matter between land and atmosphere, for instance. Therefore, coupled land-atmosphere models may need to know not just whether snow is present, but also a measure of snow quantity.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33716</offset><text>One error type that occurred for CNNs trained to predict images from new sites, but not for those trained to predict new images from existing sites, was the presence of a sizable white object in the field of view causing the image to be classified as snow when no snow was present. These white objects could be permanent or temporary, and included white vehicles, buildings, and pieces of equipment (Fig 3). CNNs that were trained to predict snow in new images from existing sites were familiar with these sites and did not mistake the white objects for snow. However, the CNNs that were not trained on the sites with white objects were not always able to determine whether or not the white objects constituted snow. In computer vision, it is assumed that the test set is drawn from the same distribution of images as the training set. When the test set constitutes a different distribution, errors are more likely to occur. However, because we used transfer learning, the CNNs should retain an underlying representation of white objects that are not snow from the original Places2 database. In fact, the CNNs correctly classified some images with white objects, but no snow, as not having snow (Fig 3). However, some white objects look a lot like snow and it is easy to see how a CNN might interpret an object as snow if it was unfamiliar with that particular site (Fig 3).</text></passage><passage><infon key="file">pone.0209649.g003.jpg</infon><infon key="id">pone.0209649.g003</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>35091</offset><text>White object confusion.</text></passage><passage><infon key="file">pone.0209649.g003.jpg</infon><infon key="id">pone.0209649.g003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>35115</offset><text>White objects can fool CNNs unfamiliar with a particular site into classifying images as “snow” when no snow is present (A-C). However, these CNNs frequently correctly classify very similar images correctly as having “no snow” (D).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35355</offset><text>There are several possible ways that overall accuracy of CNN snow detection might be improved. One idea is to add environmental data as input to the CNN along with each image. Geographic location, elevation, day of the year, and temperature provide much information about the likelihood of snow in an image. Deep learning has been shown to be effective at integrating different types of data, such as audio combined with video and imagery combined with text. Combining satellite data with near-surface imagery as direct inputs to a CNN is another area for future research.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35928</offset><text>Another possibility is to take advantage of the auto-correlation in snow cover between subsequent days; if there was snow at a site one day, it is likely that there was snow there the next day, too. A prediction for the prior and/or subsequent image(s) could be used as input to a CNN along with a target image, potentially using a second round of CNN classification for images that have a different classification from surrounding images. Alternatively, the use of recurrent neural networks, which take sequential data as input, could be considered. While recurrent neural networks have typically been used for language data like speech and text, insights gained from using them to classify video sequences might be informative for classifying environmental time-lapse images like those from PhenoCam cameras.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36739</offset><text>A computer-human hybrid system is one way to potentially achieve very high accuracy with a small amount of additional human effort. Images would be initially run through the trained CNN and both their predictions (“snow” or “no snow”) and their confidence measures would be calculated. For predictions below a particular confidence threshold, the associated image would be sent for human evaluation (either crowdsourced or expert) for an authoritative classification. Incorporating additional data about place, time, and the classifications of images before and after the target image into the decision about whether to send an image to a human or not could boost accuracy and efficiency even further. Such a hybrid system, called CrowdSearch, was developed to enable mobile phone users to perform image searches. In this system, the goal was to find similar images from the Internet matching a target image taken by the camera of a mobile phone. By using machine learning to identify candidate matches and then human participants for validation of those images, CrowdSearch performed at over 95% precision in near-real time.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>37873</offset><text>3. MODIS validation</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37893</offset><text>The accuracy of the MODIS snow product validated against our crowdsourced classifications (93.6%) and the CNNs (88.7% to 93.8%) fell in the middle of the wide range of reported validation studies (67%-99%). Our sensitivity for the MODIS snow product was 76.6%, somewhat lower than an evaluation of 11 years of the same product conducted for Colorado and Washington (78%-88%). Most of the MODIS errors are in missing small amounts of snow; there are few false positives. Given that MODIS pixels cover an area of 500m x 500m, determining whether snow is present in small amounts is a sub-pixel problem that has received attention for more than a decade and continues to be a challenge. It is recognized that errors may increase in the MODIS product when there is less than 20% snow cover. Trees were not a problem for MODIS detection of snow if snow was present on the trees themselves, but the rate of “missed snow” was high when snow was present on the ground, but not on the trees. Previous studies have noted that the accuracy of the MODIS snow product is typically lower for densely forested areas.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>38999</offset><text>The sensitivity for MODIS snow detection was highest in the winter, with slightly lower sensitivity in the fall with high variation and a precipitous decline in snow detection in the spring (Fig 4). Other studies have noted similar patterns, suggesting that the decreased detection of snow in spring is due to increased snow patchiness, snow being obscured by new vegetation, or the sensitivity of the MODIS snow fractional cover algorithm to land surface temperature. Except in the spring, the sensitivity for MODIS fell in the range of the sensitivity for the CNNs. However, in the spring, even the worst-performing CNN maintained a reasonably high sensitivity, albeit with high variability, whereas the MODIS sensitivity declined. By contrast, the sensitivity for CNNs remains relatively high during the spring, though there is a lot of variation. While images with small amounts of snow are difficult to classify for both CNNs and for MODIS snow algorithms, they two approaches had trouble with different images. As a result, an approach in which we combine MODIS and CNN classifications yields much better results, boosting the MODIS sensitivity from 77% to ~90%. For the remaining ~10% of images, both methods miss the presence of snow.</text></passage><passage><infon key="file">pone.0209649.g004.jpg</infon><infon key="id">pone.0209649.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>40242</offset><text>Sensitivity (true positive rate) by time of year for MODIS data and for two trained deep convolutional neural network (CNNs), as validated by the crowdsourced dataset.</text></passage><passage><infon key="file">pone.0209649.g004.jpg</infon><infon key="id">pone.0209649.g004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40410</offset><text>Data are not shown for days of year for which there were fewer than five images from which to determine the sensitivity.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40531</offset><text>We investigated whether there was a geographic pattern in MODIS accuracy among sites. Sites with a higher fraction of snow days tended to have a higher accuracy and sensitivity than sites with less snow (Fig 5A), but there is a lot of variability and a logarithmic function fit to the data explains only 43% of the variation. Only sites with &lt; 20% snow days have a sensitivity less than 50%, but there are also many sites with &lt; 20% snow days that have high sensitivity. The sites with many snow days (&gt; 30% snow days) have relatively high sensitivity, rarely lower than 70% (Fig 5A). We thought that this pattern might be explained by the greater number of patchy snow days relative to days of complete snow cover at sites with less snow. However, we did not find this to be the case. For sites with little snow, the number of “missed snow” errors was relatively small, as there were few snow days overall, whereas for sites with a lot of snow, the number of “missed snow” errors could be quite large even if the overall true positive rate was low (Fig 5B). Because amount of snow was correlated with latitude, we found that higher latitude sites had higher sensitivity on average (Fig 6).</text></passage><passage><infon key="file">pone.0209649.g005.jpg</infon><infon key="id">pone.0209649.g005</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>41730</offset><text>Sites with more snow have a higher MODIS sensitivity than those with less snow.</text></passage><passage><infon key="file">pone.0209649.g005.jpg</infon><infon key="id">pone.0209649.g005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>41810</offset><text>They also have more images per year in which MODIS misses the presence of snow. (a) Sites with more snow tend to have a higher MODIS sensitivity. The fit logarithmic function explains 43% of variation. The outlier is the site at Port Alsworth, Lake Clark National Park and Preserve, Alaska. (b) Sites with more snow tend to have a greater number of images for which MODIS misses the snow. The Port Alsworth outlier is not shown; MODIS missed snow at this site 14 times out of 21 images, which scales to a “missed snow” rate of 232 days per year. The camera at Port Alsworth has known color balance issues.</text></passage><passage><infon key="file">pone.0209649.g006.jpg</infon><infon key="id">pone.0209649.g006</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>42420</offset><text>MODIS sensitivity for camera sites.</text></passage><passage><infon key="file">pone.0209649.g006.jpg</infon><infon key="id">pone.0209649.g006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>42456</offset><text>Circle color indicates the sensitivity for each site. The size of each circle indicates the number of images containing snow, according to the crowdsourced data. Made with Natural Earth: free vector and raster map data @ naturalearthdata.com.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>42699</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>42710</offset><text>We have demonstrated an automated method for creating location-specific, high temporal frequency, data on the presence or absence of snow using imagery from automated, near-surface camera monitoring sites. Though we use only about a hundred cameras that were originally set up for a different purpose, such a method shows potential for improving snow monitoring. While the individual cameras cover only a small spatial extent, they were selected to sample across a wide range of North American ecosystems and human land use. As a result, we expect the model to perform well generally for images of temperate North American outdoor scenes, and especially for those that were created using the PhenoCam camera protocol. The cameras are also necessarily limited in number, but the strength of their high accuracy and hourly images can be leveraged by combining camera data with data from technologies that cover high spatial extents, such as satellite and aerial imagery, and possibly crowdsourced image databases such as Flickr.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>43737</offset><text>The uses for our dataset of labeled images are numerous. They could be used to validate MODIS snow products and to help refine their algorithmic approaches. Additionally, the dataset could be used as input to snow models that use data streams from satellites or airborne platforms or crowdsourced image databases to create more accurate continuous predictions of snow. For example, MODIS snow data could be used to create a model of snow extent and then data from surface-based cameras could be used to refine model parameters for higher accuracy or to fill in gaps inferentially when clouds prevent direct measurement. Because our data processing system is automated, such models could be run in near real time. Computer vision researchers might also use the variety of high-quality outdoor images in our labeled dataset as a baseline to better capture information from crowdsourced outdoor image datasets that are noisier in nature.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>44672</offset><text>We expect that our general method of automated sensors, crowdsourced labels, and deep learning for data processing will be implementable for a broad range of complex environmental datasets. We have shown that this method can achieve up to 98% accuracy for automated detection of snow from stationary time-lapse cameras. Other types of automated imagery–from animal camera traps, from remote sensors on satellites and airborne platforms, and from deep sea mapping cameras, for example–can also benefit from this approach. Deep learning has also been shown to be effective in processing audio and video streams, and so audio environmental data, such as animal acoustics, as well as video environmental data, such as animal behavior video monitoring, can potentially benefit from this method as well. The potential of our method for real-time large-scale acquisition and processing of ecological and environmental data opens up new research questions and provides increased information for natural resource managers.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>45690</offset><text>Supporting information</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>45713</offset><text>References</text></passage><passage><infon key="issue">4</infon><infon key="name_0">surname:Zhang;given-names:T.</infon><infon key="pub-id_doi">10.1029/2004RG000157</infon><infon key="section_type">REF</infon><infon key="source">Rev Geophys</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2005</infon><offset>45724</offset><text>Influence of the seasonal snow cover on the ground thermal regime: An overview</text></passage><passage><infon key="fpage">235</infon><infon key="lpage">246</infon><infon key="name_0">surname:Barry;given-names:RG</infon><infon key="section_type">REF</infon><infon key="source">Polar Geogr</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2002</infon><offset>45803</offset><text>The role of snow and ice in the global climate system: a review</text></passage><passage><infon key="fpage">150</infon><infon key="lpage">156</infon><infon key="name_0">surname:Cohen;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Weather</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">1994</infon><offset>45867</offset><text>Snow cover and climate</text></passage><passage><infon key="fpage">2008</infon><infon key="lpage">2022</infon><infon key="name_0">surname:Mote;given-names:TL</infon><infon key="pub-id_doi">10.1175/2007JAMC1823.1</infon><infon key="section_type">REF</infon><infon key="source">J Appl Meteorol Climatol</infon><infon key="type">ref</infon><infon key="volume">47</infon><infon key="year">2008</infon><offset>45890</offset><text>On the Role of Snow Cover in Depressing Air Temperature</text></passage><passage><infon key="fpage">113</infon><infon key="name_0">surname:Dyer;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">J Geophys Res Atmospheres</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>45946</offset><text>Snow depth and streamflow relationships in large North American watersheds</text></passage><passage><infon key="fpage">178</infon><infon key="lpage">193</infon><infon key="name_0">surname:Graybeal;given-names:DY</infon><infon key="name_1">surname:Leathers;given-names:DJ</infon><infon key="section_type">REF</infon><infon key="source">J Appl Meteorol Climatol</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2006</infon><offset>46021</offset><text>Snowmelt-related flood risk in Appalachia: first estimates from a historical snow climatology</text></passage><passage><infon key="fpage">1263</infon><infon key="lpage">1278</infon><infon key="name_0">surname:Todhunter;given-names:PE</infon><infon key="section_type">REF</infon><infon key="source">J Am Water Resour Assoc</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2001</infon><offset>46115</offset><text>A hydroclimatological analysis of the Red River of the north snowmelt flood catastrophe of 1997</text></passage><passage><infon key="fpage">106</infon><infon key="lpage">116</infon><infon key="name_0">surname:Romanovsky;given-names:VE</infon><infon key="name_1">surname:Smith;given-names:SL</infon><infon key="name_2">surname:Christiansen;given-names:HH</infon><infon key="pub-id_doi">10.1002/ppp.689</infon><infon key="section_type">REF</infon><infon key="source">Permafr Periglac Process</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2010</infon><offset>46211</offset><text>Permafrost thermal state in the polar Northern Hemisphere during the international polar year 2007–2009: a synthesis</text></passage><passage><infon key="issue">24</infon><infon key="name_0">surname:Lawrence;given-names:DM</infon><infon key="name_1">surname:Slater;given-names:AG</infon><infon key="pub-id_doi">10.1029/2005GL025080</infon><infon key="section_type">REF</infon><infon key="source">Geophys Res Lett</infon><infon key="type">ref</infon><infon key="volume">32</infon><infon key="year">2005</infon><offset>46330</offset><text>A projection of severe near-surface permafrost degradation during the 21st century</text></passage><passage><infon key="name_0">surname:Jones;given-names:HG</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>46413</offset></passage><passage><infon key="fpage">314</infon><infon key="lpage">322</infon><infon key="name_0">surname:Campbell;given-names:JL</infon><infon key="name_1">surname:Mitchell;given-names:MJ</infon><infon key="name_2">surname:Groffman;given-names:PM</infon><infon key="name_3">surname:Christenson;given-names:LM</infon><infon key="name_4">surname:Hardy;given-names:JP</infon><infon key="section_type">REF</infon><infon key="source">Front Ecol Environ</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2005</infon><offset>46414</offset><text>Winter in northeastern North America: a critical period for ecological processes</text></passage><passage><infon key="fpage">303</infon><infon key="lpage">309</infon><infon key="name_0">surname:Barnett;given-names:TP</infon><infon key="name_1">surname:Adam;given-names:JC</infon><infon key="name_2">surname:Lettenmaier;given-names:DP</infon><infon key="pub-id_doi">10.1038/nature04141</infon><infon key="pub-id_pmid">16292301</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">438</infon><infon key="year">2005</infon><offset>46495</offset><text>Potential impacts of a warming climate on water availability in snow-dominated regions</text></passage><passage><infon key="fpage">1007</infon><infon key="lpage">1029</infon><infon key="name_0">surname:Frei;given-names:A</infon><infon key="name_1">surname:Tedesco;given-names:M</infon><infon key="name_2">surname:Lee;given-names:S</infon><infon key="name_3">surname:Foster;given-names:J</infon><infon key="name_4">surname:Hall;given-names:DK</infon><infon key="name_5">surname:Kelly;given-names:R</infon><infon key="pub-id_doi">10.1016/j.asr.2011.12.021</infon><infon key="section_type">REF</infon><infon key="source">Adv Space Res</infon><infon key="type">ref</infon><infon key="volume">50</infon><infon key="year">2012</infon><offset>46582</offset><text>A review of global satellite-derived snow products</text></passage><passage><infon key="name_0">surname:Eamer;given-names:J</infon><infon key="name_1">surname:Ahlenius;given-names:H</infon><infon key="name_2">surname:Prestrud;given-names:P</infon><infon key="section_type">REF</infon><infon key="source">Global outlook for ice &amp; snow</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>46633</offset></passage><passage><infon key="fpage">653</infon><infon key="lpage">671</infon><infon key="name_0">surname:Pedersen;given-names:SH</infon><infon key="name_1">surname:Tamstorf;given-names:MP</infon><infon key="name_2">surname:Abermann;given-names:J</infon><infon key="name_3">surname:Westergaard-Nielsen;given-names:A</infon><infon key="name_4">surname:Lund;given-names:M</infon><infon key="name_5">surname:Skov;given-names:K</infon><infon key="pub-id_doi">10.1657/AAAR0016-028</infon><infon key="section_type">REF</infon><infon key="source">Arct Antarct Alp Res</infon><infon key="type">ref</infon><infon key="volume">48</infon><infon key="year">2016</infon><offset>46634</offset><text>Spatiotemporal Characteristics of Seasonal Snow Cover in Northeast Greenland from in Situ Observations</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46737</offset><text>Barrett AP. National operational hydrologic remote sensing center snow data assimilation system (SNODAS) products at NSIDC. National Snow and Ice Data Center, Cooperative Institute for Research in Environmental Sciences; 2003.</text></passage><passage><infon key="fpage">2145</infon><infon key="lpage">2160</infon><infon key="name_0">surname:Serreze;given-names:MC</infon><infon key="name_1">surname:Clark;given-names:MP</infon><infon key="name_2">surname:Armstrong;given-names:RL</infon><infon key="name_3">surname:McGinnis;given-names:DA</infon><infon key="name_4">surname:Pulwarty;given-names:RS</infon><infon key="section_type">REF</infon><infon key="source">Water Resour Res</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">1999</infon><offset>46964</offset><text>Characteristics of the western United States snowpack from snowpack telemetry (SNOTEL) data</text></passage><passage><infon key="fpage">237</infon><infon key="lpage">247</infon><infon key="name_0">surname:Buus-Hinkler;given-names:J</infon><infon key="name_1">surname:Hansen;given-names:BU</infon><infon key="name_2">surname:Tamstorf;given-names:MP</infon><infon key="name_3">surname:Pedersen;given-names:SB</infon><infon key="pub-id_doi">10.1016/j.rse.2006.06.016</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">2006</infon><offset>47056</offset><text>Snow-vegetation relations in a High Arctic ecosystem: Inter-annual variability inferred from new monitoring and modeling concepts</text></passage><passage><infon key="fpage">137</infon><infon key="lpage">145</infon><infon key="name_0">surname:Salvatori;given-names:R</infon><infon key="name_1">surname:Plini;given-names:P</infon><infon key="name_2">surname:Giusto;given-names:M</infon><infon key="name_3">surname:Valt;given-names:M</infon><infon key="name_4">surname:Salzano;given-names:R</infon><infon key="name_5">surname:Montagnoli;given-names:M</infon><infon key="pub-id_doi">10.5721/ItJRS201143211</infon><infon key="section_type">REF</infon><infon key="source">Ital J Remote Sens</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>47186</offset><text>Snow cover monitoring with images from digital camera systems</text></passage><passage><infon key="fpage">3327</infon><infon key="lpage">3337</infon><infon key="name_0">surname:Parajka;given-names:J</infon><infon key="name_1">surname:Haas;given-names:P</infon><infon key="name_2">surname:Kirnbauer;given-names:R</infon><infon key="name_3">surname:Jansa;given-names:J</infon><infon key="name_4">surname:Blöschl;given-names:G</infon><infon key="pub-id_doi">10.1002/hyp.8389</infon><infon key="section_type">REF</infon><infon key="source">Hydrol Process</infon><infon key="type">ref</infon><infon key="volume">26</infon><infon key="year">2012</infon><offset>47248</offset><text>Potential of time-lapse photography of snow for hydrological purposes at the small catchment scale</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">100</infon><infon key="name_0">surname:Bernard;given-names:É</infon><infon key="name_1">surname:Friedt;given-names:J-M</infon><infon key="name_2">surname:Tolle;given-names:F</infon><infon key="name_3">surname:Griselin;given-names:M</infon><infon key="name_4">surname:Martin;given-names:G</infon><infon key="name_5">surname:Laffly;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">ISPRS J Photogramm Remote Sens</infon><infon key="type">ref</infon><infon key="volume">75</infon><infon key="year">2013</infon><offset>47347</offset><text>Monitoring seasonal snow dynamics using ground based high resolution photography (Austre Lovénbreen, Svalbard, 79 N)</text></passage><passage><infon key="fpage">4669</infon><infon key="lpage">4682</infon><infon key="name_0">surname:Hinkler;given-names:J</infon><infon key="name_1">surname:Pedersen;given-names:SB</infon><infon key="name_2">surname:Rasch;given-names:M</infon><infon key="name_3">surname:Hansen;given-names:BU</infon><infon key="pub-id_doi">10.1080/01431160110113881</infon><infon key="section_type">REF</infon><infon key="source">Int J Remote Sens</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2002</infon><offset>47465</offset><text>Automatic snow cover monitoring at high temporal and spatial resolution, using images taken by a standard digital camera</text></passage><passage><infon key="fpage">399</infon><infon key="lpage">458</infon><infon key="name_0">surname:Zhao;given-names:W</infon><infon key="name_1">surname:Chellappa;given-names:R</infon><infon key="name_2">surname:Phillips;given-names:PJ</infon><infon key="name_3">surname:Rosenfeld;given-names:A</infon><infon key="section_type">REF</infon><infon key="source">ACM Comput Surv CSUR</infon><infon key="type">ref</infon><infon key="volume">35</infon><infon key="year">2003</infon><offset>47586</offset><text>Face recognition: A literature survey</text></passage><passage><infon key="fpage">436</infon><infon key="lpage">444</infon><infon key="name_0">surname:LeCun;given-names:Y</infon><infon key="name_1">surname:Bengio;given-names:Y</infon><infon key="name_2">surname:Hinton;given-names:G</infon><infon key="pub-id_doi">10.1038/nature14539</infon><infon key="pub-id_pmid">26017442</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">521</infon><infon key="year">2015</infon><offset>47624</offset><text>Deep learning</text></passage><passage><infon key="fpage">1349</infon><infon key="lpage">1362</infon><infon key="name_0">surname:Romero;given-names:A</infon><infon key="name_1">surname:Gatta;given-names:C</infon><infon key="name_2">surname:Camps-Valls;given-names:G</infon><infon key="pub-id_doi">10.1109/TGRS.2015.2478379</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Geosci Remote Sens</infon><infon key="type">ref</infon><infon key="volume">54</infon><infon key="year">2016</infon><offset>47638</offset><text>Unsupervised Deep Feature Extraction for Remote Sensing Image Classification</text></passage><passage><infon key="fpage">549</infon><infon key="lpage">553</infon><infon key="name_0">surname:Scott;given-names:GJ</infon><infon key="name_1">surname:England;given-names:MR</infon><infon key="name_2">surname:Starms;given-names:WA</infon><infon key="name_3">surname:Marcum;given-names:RA</infon><infon key="name_4">surname:Davis;given-names:CH</infon><infon key="pub-id_doi">10.1109/LGRS.2017.2657778</infon><infon key="section_type">REF</infon><infon key="source">IEEE Geosci Remote Sens Lett</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2017</infon><offset>47715</offset><text>Training Deep Convolutional Neural Networks for Land Cover Classification of High-Resolution Imagery</text></passage><passage><infon key="fpage">778</infon><infon key="lpage">782</infon><infon key="name_0">surname:Kussul;given-names:N</infon><infon key="name_1">surname:Lavreniuk;given-names:M</infon><infon key="name_2">surname:Skakun;given-names:S</infon><infon key="name_3">surname:Shelestov;given-names:A</infon><infon key="pub-id_doi">10.1109/LGRS.2017.2681128</infon><infon key="section_type">REF</infon><infon key="source">IEEE Geosci Remote Sens Lett</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2017</infon><offset>47816</offset><text>Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data</text></passage><passage><infon key="fpage">237</infon><infon key="lpage">251</infon><infon key="name_0">surname:Xing;given-names:H</infon><infon key="name_1">surname:Meng;given-names:Y</infon><infon key="name_2">surname:Wang;given-names:Z</infon><infon key="name_3">surname:Fan;given-names:K</infon><infon key="name_4">surname:Hou;given-names:D</infon><infon key="pub-id_doi">10.1016/j.isprsjprs.2018.04.025</infon><infon key="section_type">REF</infon><infon key="source">ISPRS J Photogramm Remote Sens</infon><infon key="type">ref</infon><infon key="volume">141</infon><infon key="year">2018</infon><offset>47900</offset><text>Exploring geo-tagged photos for land cover validation with deep learning</text></passage><passage><infon key="fpage">127</infon><infon key="lpage">134</infon><infon key="name_0">surname:Xu;given-names:G</infon><infon key="name_1">surname:Zhu;given-names:X</infon><infon key="name_2">surname:Fu;given-names:D</infon><infon key="name_3">surname:Dong;given-names:J</infon><infon key="name_4">surname:Xiao;given-names:X</infon><infon key="pub-id_doi">10.1016/j.envsoft.2017.02.004</infon><infon key="section_type">REF</infon><infon key="source">Environ Model Softw</infon><infon key="type">ref</infon><infon key="volume">91</infon><infon key="year">2017</infon><offset>47973</offset><text>Automatic land cover classification of geo-tagged field photos by deep learning</text></passage><passage><infon key="fpage">1097</infon><infon key="lpage">1101</infon><infon key="name_0">surname:Wang;given-names:J</infon><infon key="name_1">surname:Korayem;given-names:M</infon><infon key="name_2">surname:Blanco;given-names:S</infon><infon key="name_3">surname:Crandall;given-names:DJ</infon><infon key="pub-id_doi">10.1145/2964284.2984067</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM on Multimedia Conference—MM ‘16</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>48053</offset></passage><passage><infon key="fpage">452</infon><infon key="lpage">459</infon><infon key="name_0">surname:Wang;given-names:J</infon><infon key="name_1">surname:Korayem;given-names:M</infon><infon key="name_2">surname:Crandall;given-names:DJ</infon><infon key="pub-id_doi">10.1109/ICCVW.2013.66</infon><infon key="section_type">REF</infon><infon key="source">2013 IEEE International Conference on Computer Vision Workshops</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>48054</offset></passage><passage><infon key="fpage">3</infon><infon key="lpage">5</infon><infon key="name_0">surname:Buhrmester;given-names:M</infon><infon key="name_1">surname:Kwang;given-names:T</infon><infon key="name_2">surname:Gosling;given-names:SD</infon><infon key="pub-id_doi">10.1177/1745691610393980</infon><infon key="pub-id_pmid">26162106</infon><infon key="section_type">REF</infon><infon key="source">Perspect Psychol Sci.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2011</infon><offset>48055</offset><text>Amazon’s Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?</text></passage><passage><infon key="fpage">180028</infon><infon key="name_0">surname:Richardson;given-names:AD</infon><infon key="name_1">surname:Hufkens;given-names:K</infon><infon key="name_2">surname:Milliman;given-names:T</infon><infon key="name_3">surname:Aubrecht;given-names:DM</infon><infon key="name_4">surname:Chen;given-names:M</infon><infon key="name_5">surname:Gray;given-names:JM</infon><infon key="pub-id_doi">10.1038/sdata.2018.28</infon><infon key="pub-id_pmid">29533393</infon><infon key="section_type">REF</infon><infon key="source">Sci Data</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2018</infon><offset>48136</offset><text>Tracking vegetation phenology across diverse North American biomes using PhenoCam imagery</text></passage><passage><infon key="fpage">1478</infon><infon key="lpage">1489</infon><infon key="name_0">surname:Keenan;given-names:TF</infon><infon key="name_1">surname:Darby;given-names:B</infon><infon key="name_2">surname:Felts;given-names:E</infon><infon key="name_3">surname:Sonnentag;given-names:O</infon><infon key="name_4">surname:Friedl;given-names:MA</infon><infon key="name_5">surname:Hufkens;given-names:K</infon><infon key="pub-id_doi">10.1890/13-0652.1</infon><infon key="pub-id_pmid">29160668</infon><infon key="section_type">REF</infon><infon key="source">Ecol Appl</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2014</infon><offset>48226</offset><text>Tracking forest phenology and seasonal physiology using digital repeat photography: a critical assessment</text></passage><passage><infon key="fpage">21</infon><infon key="lpage">2</infon><infon key="name_0">surname:Amato;given-names:A</infon><infon key="name_1">surname:Sappa;given-names:AD</infon><infon key="name_2">surname:Fornés;given-names:A</infon><infon key="name_3">surname:Lumbreras;given-names:F</infon><infon key="name_4">surname:Lladós;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>48332</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48333</offset><text>Amato A, Lumbreras F, Sappa AD. A general-purpose crowdsourcing platform for mobile devices. Computer Vision Theory and Applications (VISAPP), 2014 International Conference on. IEEE; 2014. pp. 211–215.</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">1</infon><infon key="name_0">surname:Zhou;given-names:B</infon><infon key="name_1">surname:Lapedriza;given-names:A</infon><infon key="name_2">surname:Khosla;given-names:A</infon><infon key="name_3">surname:Oliva;given-names:A</infon><infon key="name_4">surname:Torralba;given-names:A</infon><infon key="pub-id_doi">10.1109/TPAMI.2017.2723009</infon><infon key="pub-id_pmid">28692961</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Pattern Anal Mach Intell</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>48537</offset><text>Places: A 10 million Image Database for Scene Recognition</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48595</offset><text>Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Proceedings of International Conference on Learning Representations. 2015. Available from: arxiv:1409.1556</text></passage><passage><infon key="fpage">1345</infon><infon key="lpage">1359</infon><infon key="name_0">surname:Pan;given-names:SJ</infon><infon key="name_1">surname:Yang;given-names:Q</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Knowl Data Eng</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2010</infon><offset>48799</offset><text>A survey on transfer learning</text></passage><passage><infon key="fpage">1871</infon><infon key="lpage">1874</infon><infon key="name_0">surname:Fan;given-names:R-E</infon><infon key="name_1">surname:Chang;given-names:K-W</infon><infon key="name_2">surname:Hsieh;given-names:C-J</infon><infon key="name_3">surname:Wang;given-names:X-R</infon><infon key="name_4">surname:Lin;given-names:C-J</infon><infon key="section_type">REF</infon><infon key="source">J Mach Learn Res</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2008</infon><offset>48829</offset><text>LIBLINEAR: A library for large linear classification</text></passage><passage><infon key="pub-id_doi">10.5067/MODIS/MOD10A1.006</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>48882</offset></passage><passage><infon key="fpage">432</infon><infon key="lpage">438</infon><infon key="name_0">surname:Hall;given-names:DK</infon><infon key="name_1">surname:Foster;given-names:JL</infon><infon key="name_2">surname:Salomonson;given-names:VV</infon><infon key="name_3">surname:Klein;given-names:AG</infon><infon key="name_4">surname:Chien;given-names:JYL</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Geosci Remote Sens</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2001</infon><offset>48883</offset><text>Development of a technique to assess snow-cover mapping errors from space</text></passage><passage><infon key="fpage">1534</infon><infon key="lpage">1547</infon><infon key="name_0">surname:Hall;given-names:DK</infon><infon key="name_1">surname:Riggs;given-names:GA</infon><infon key="pub-id_doi">10.1002/hyp.6715</infon><infon key="section_type">REF</infon><infon key="source">Hydrol Process</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2007</infon><offset>48957</offset><text>Accuracy assessment of the MODIS snow products</text></passage><passage><infon key="fpage">980</infon><infon key="lpage">998</infon><infon key="name_0">surname:Arsenault;given-names:KR</infon><infon key="name_1">surname:Houser;given-names:PR</infon><infon key="name_2">surname:De Lannoy;given-names:GJM</infon><infon key="pub-id_doi">10.1002/hyp.9636</infon><infon key="section_type">REF</infon><infon key="source">Hydrol Process</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2014</infon><offset>49004</offset><text>Evaluation of the MODIS snow cover fraction product</text></passage><passage><infon key="fpage">2337</infon><infon key="lpage">2351</infon><infon key="name_0">surname:Gascoin;given-names:S</infon><infon key="name_1">surname:Hagolle;given-names:O</infon><infon key="name_2">surname:Huc;given-names:M</infon><infon key="name_3">surname:Jarlan;given-names:L</infon><infon key="name_4">surname:Dejoux;given-names:J-F</infon><infon key="name_5">surname:Szczypta;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Hydrol Earth Syst Sci</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2015</infon><offset>49056</offset><text>A snow cover climatology for the Pyrenees from MODIS snow products</text></passage><passage><infon key="fpage">44</infon><infon key="lpage">57</infon><infon key="name_0">surname:Raleigh;given-names:MS</infon><infon key="name_1">surname:Rittger;given-names:K</infon><infon key="name_2">surname:Moore;given-names:CE</infon><infon key="name_3">surname:Henn;given-names:B</infon><infon key="name_4">surname:Lutz;given-names:JA</infon><infon key="name_5">surname:Lundquist;given-names:JD</infon><infon key="pub-id_doi">10.1016/j.rse.2012.09.016</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">128</infon><infon key="year">2013</infon><offset>49123</offset><text>Ground-based testing of MODIS fractional snow cover in subalpine meadows and forests of the Sierra Nevada</text></passage><passage><infon key="fpage">551</infon><infon key="lpage">560</infon><infon key="name_0">surname:Kosmala;given-names:M</infon><infon key="name_1">surname:Wiggins;given-names:A</infon><infon key="name_2">surname:Swanson;given-names:A</infon><infon key="name_3">surname:Simmons;given-names:B</infon><infon key="pub-id_doi">10.1002/fee.1436</infon><infon key="section_type">REF</infon><infon key="source">Front Ecol Environ</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2016</infon><offset>49229</offset><text>Assessing data quality in citizen science</text></passage><passage><infon key="fpage">726</infon><infon key="name_0">surname:Kosmala;given-names:M</infon><infon key="name_1">surname:Crall;given-names:A</infon><infon key="name_2">surname:Cheng;given-names:R</infon><infon key="name_3">surname:Hufkens;given-names:K</infon><infon key="name_4">surname:Henderson;given-names:S</infon><infon key="name_5">surname:Richardson;given-names:A</infon><infon key="pub-id_doi">10.3390/rs8090726</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2016</infon><offset>49271</offset><text>Season Spotter: Using Citizen Science to Validate and Scale Plant Phenology from Near-Surface Remote Sensing</text></passage><passage><infon key="fpage">2148</infon><infon key="lpage">2156</infon><infon key="name_0">surname:Denil;given-names:M</infon><infon key="name_1">surname:Shakibi;given-names:B</infon><infon key="name_2">surname:Dinh;given-names:L</infon><infon key="name_3">surname:de Freitas;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>49380</offset><text>Predicting parameters in deep learning</text></passage><passage><infon key="fpage">436</infon><infon key="lpage">443</infon><infon key="name_0">surname:Kudo;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Arct Alp Res</infon><infon key="type">ref</infon><infon key="year">1991</infon><offset>49419</offset><text>Effects of snow-free period on the phenology of alpine plants inhabiting snow patches</text></passage><passage><infon key="fpage">1705</infon><infon key="lpage">1715</infon><infon key="name_0">surname:Liston;given-names:GE</infon><infon key="section_type">REF</infon><infon key="source">J Appl Meteorol</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">1995</infon><offset>49505</offset><text>Local advection of momentum, heat, and moisture during the melt of patchy snow covers</text></passage><passage><infon key="fpage">17</infon><infon key="lpage">36</infon><infon key="name_0">surname:Bengio;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">De Proceedings of ICML Workshop on Unsupervised and Transfer Learning</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>49591</offset><text>ep learning of representations for unsupervised and transfer learning</text></passage><passage><infon key="fpage">514</infon><infon key="lpage">525</infon><infon key="name_0">surname:Chen;given-names:Xue-Wen</infon><infon key="name_1">surname:Lin;given-names:Xiaotong</infon><infon key="pub-id_doi">10.1109/ACCESS.2014.2325029</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2014</infon><offset>49661</offset><text>Big Data Deep Learning: Challenges and Perspectives.</text></passage><passage><infon key="fpage">2625</infon><infon key="lpage">2634</infon><infon key="name_0">surname:Donahue;given-names:J</infon><infon key="name_1">surname:Anne Hendricks;given-names:L</infon><infon key="name_2">surname:Guadarrama;given-names:S</infon><infon key="name_3">surname:Rohrbach;given-names:M</infon><infon key="name_4">surname:Venugopalan;given-names:S</infon><infon key="name_5">surname:Saenko;given-names:K</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE conference on computer vision and pattern recognition</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>49714</offset><text>Long-term recurrent convolutional networks for visual recognition and description</text></passage><passage><infon key="fpage">77</infon><infon key="lpage">90</infon><infon key="name_0">surname:Yan;given-names:T</infon><infon key="name_1">surname:Kumar;given-names:V</infon><infon key="name_2">surname:Ganesan;given-names:D</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 8th international conference on Mobile systems, applications, and services</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>49796</offset></passage><passage><infon key="fpage">160</infon><infon key="lpage">181</infon><infon key="name_0">surname:Sirguey;given-names:P</infon><infon key="name_1">surname:Mathieu;given-names:R</infon><infon key="name_2">surname:Arnaud;given-names:Y</infon><infon key="pub-id_doi">10.1016/j.rse.2008.09.008</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">113</infon><infon key="year">2009</infon><offset>49797</offset><text>Subpixel monitoring of the seasonal snow cover with MODIS at 250 m spatial resolution in the Southern Alps of New Zealand: Methodology and accuracy assessment</text></passage><passage><infon key="fpage">868</infon><infon key="lpage">879</infon><infon key="name_0">surname:Painter;given-names:TH</infon><infon key="name_1">surname:Rittger;given-names:K</infon><infon key="name_2">surname:McKenzie;given-names:C</infon><infon key="name_3">surname:Slaughter;given-names:P</infon><infon key="name_4">surname:Davis;given-names:RE</infon><infon key="name_5">surname:Dozier;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">113</infon><infon key="year">2009</infon><offset>49956</offset><text>Retrieval of subpixel snow covered area, grain size, and albedo from MODIS</text></passage><passage><infon key="fpage">351</infon><infon key="lpage">360</infon><infon key="name_0">surname:Salomonson;given-names:V.</infon><infon key="name_1">surname:Appel;given-names:I.</infon><infon key="pub-id_doi">10.1016/j.rse.2003.10.016</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">89</infon><infon key="year">2004</infon><offset>50031</offset><text>Estimating fractional snow cover from MODIS using the normalized difference snow index</text></passage><passage><infon key="fpage">236</infon><infon key="lpage">252</infon><infon key="name_0">surname:Kuter;given-names:S</infon><infon key="name_1">surname:Akyurek;given-names:Z</infon><infon key="name_2">surname:Weber;given-names:G-W</infon><infon key="pub-id_doi">10.1016/j.rse.2017.11.021</infon><infon key="section_type">REF</infon><infon key="source">Remote Sens Environ</infon><infon key="type">ref</infon><infon key="volume">205</infon><infon key="year">2018</infon><offset>50118</offset><text>Retrieval of fractional snow covered area from MODIS data by multivariate adaptive regression splines</text></passage><passage><infon key="fpage">367</infon><infon key="lpage">380</infon><infon key="name_0">surname:Rittger;given-names:K</infon><infon key="name_1">surname:Painter;given-names:TH</infon><infon key="name_2">surname:Dozier;given-names:J</infon><infon key="pub-id_doi">10.1016/j.advwatres.2012.03.002</infon><infon key="section_type">REF</infon><infon key="source">Adv Water Resour</infon><infon key="type">ref</infon><infon key="volume">51</infon><infon key="year">2013</infon><offset>50220</offset><text>Assessment of methods for mapping snow cover from MODIS</text></passage><passage><infon key="fpage">1096</infon><infon key="lpage">1104</infon><infon key="name_0">surname:Lee;given-names:H</infon><infon key="name_1">surname:Pham;given-names:P</infon><infon key="name_2">surname:Largman;given-names:Y</infon><infon key="name_3">surname:Ng;given-names:AY</infon><infon key="section_type">REF</infon><infon key="source">Advances in neural information processing systems</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>50276</offset><text>Unsupervised feature learning for audio classification using convolutional deep belief networks</text></passage><passage><infon key="fpage">461</infon><infon key="lpage">470</infon><infon key="name_0">surname:Wu;given-names:Z</infon><infon key="name_1">surname:Wang;given-names:X</infon><infon key="name_2">surname:Jiang;given-names:Y-G</infon><infon key="name_3">surname:Ye;given-names:H</infon><infon key="name_4">surname:Xue;given-names:X</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23rd ACM international conference on Multimedia.</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>50372</offset><text>Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</text></passage></document></collection>
