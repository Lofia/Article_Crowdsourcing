<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201216</date><key>pmc.key</key><document><id>5961774</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_pmc">5961774</infon><infon key="article-id_pmid">29888085</infon><infon key="article-id_publisher-id">2840819</infon><infon key="fpage">273</infon><infon key="license">This is an Open Access article: verbatim copying and redistribution of this article are permitted in all media for any purpose</infon><infon key="lpage">280</infon><infon key="name_0">surname:Ye;given-names:Cheng</infon><infon key="name_1">surname:Coco;given-names:Joseph</infon><infon key="name_10">surname:Fabbri;given-names:Daniel</infon><infon key="name_2">surname:Epishova;given-names:Anna</infon><infon key="name_3">surname:Hajaj;given-names:Chen</infon><infon key="name_4">surname:Bogardus;given-names:Henry</infon><infon key="name_5">surname:Novak;given-names:Laurie</infon><infon key="name_6">surname:Denny;given-names:Joshua</infon><infon key="name_7">surname:Vorobeychik;given-names:Yevgeniy</infon><infon key="name_8">surname:Lasko;given-names:Thomas</infon><infon key="name_9">surname:Malin;given-names:Bradley</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">2018</infon><infon key="year">2018</infon><offset>0</offset><text>A Crowdsourcing Framework for Medical Data Sets</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>48</offset><text>Crowdsourcing services like Amazon Mechanical Turk allow researchers to ask questions to crowds of workers and quickly receive high quality labeled responses. However, crowds drawn from the general public are not suitable for labeling sensitive and complex data sets, such as medical records, due to various concerns. Major challenges in building and deploying a crowdsourcing system for medical data include, but are not limited to: managing access rights to sensitive data and ensuring data privacy controls are enforced; identifying workers with the necessary expertise to analyze complex information; and efficiently retrieving relevant information in massive data sets. In this paper, we introduce a crowdsourcing framework to support the annotation of medical data sets. We further demonstrate a workflow for crowdsourcing clinical chart reviews including (1) the design and decomposition of research questions; (2) the architecture for storing and displaying sensitive data; and (3) the development of tools to support crowd workers in quickly analyzing information from complex data sets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1145</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1158</offset><text>Crowdsourcing has gained notoriety as services like Amazon Mechanical Turk (AMT) have enabled researchers to ask questions to crowds of workers and quickly receive labeled responses. These human labeled data sets are increasingly important for training supervised machine models, as labels do not exist for many important research questions and cannot be produced with automated methods. Unfortunately, crowds composed of individuals from the general public are inappropriate for numerous types of data sets that require crowdsourcing, such as clinical data, due to legislation (e.g., the Health Insurance Portability and Accountability Act of 1996) and organizational policies. In particular, privacy concerns prevent arbitrary users from accessing these data. Moreover, the subject matter being analyzed requires highly specialized training and expertise to accurately produce a label, which is often not available in a public crowd.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2094</offset><text>This paper outlines a crowdsourcing framework for medical data sets and one current deployment of the system. There are many components necessary for building such an environment to allow for scalable human computation on medical data sets. Broadly, the main components of the system include: (i) a crowdsourcing system that can be deployed within an organization that has the ability to specify workers’ attributes, roles and access controls, (ii) deidentification routines to perturb identifiers and meet ethical and legal requirements, (iii) graphical user interfaces to display sensitive data, (iv) and machine learning tools to assist workers to produce labels quickly. Moreover, beyond the technical components, this paper describes organizational processes that are needed to train researchers about crowdsourcing so they can construct well-defined questions for the crowd, and approaches to recruit skilled workers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3020</offset><text>To demonstrate the challenges and complexities of developing and deploying a crowdsourcing system for sensitive data, this paper focuses on the important use case of clinical chart reviews. Chart reviews are a common component of medical research in which medical students, staff or nurses comb through semi-structured electronic medical records (EMR) systems (which are designed for clinical treatment rather than research) for specific data. Unfortunately, scrolling through vast amounts of clinical text to produce labels is time-consuming and expensive. For example, at Vanderbilt University Medical Center, it currently costs $109 per hour for a service which pays a nurse to review patient charts and produce labels, where a large part of this fee goes to project management and other overhead. While some researchers have employed software scripts to infer labels from text data automatically, the messiness and complexity of EMR systems’ semi-structured data make verifying the accuracy of the results difficult. More problematic is that the clinical text is filled with misspellings, medical acronyms, and abbreviations which make disambiguation difficult with natural language processing techniques.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4232</offset><text>One major challenge for crowdsourcing workers is uncovering relevant information quickly from complex data sets. For example, in healthcare, patient charts are managed as a collection hundreds, if not thousands, of clinical documents, each of which may include tens of pages of information. Finding the specific paragraph related to a patient’s diabetes care history or cancer medication adherence is nontrivial. While keyword search can help find some content, variations in terminology and other clinical semantics make finding all relevant data challenging. Moreover, identifying all relevant text in a single note related to, say, seizures remains time-consuming and requires extensive skimming. For these reasons, the crowdsourcing framework requires additional tools to assist workers in finding relevant content quickly, such as text highlighting and data visualization for summarization.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5130</offset><text>An effective crowdsourcing system for medical data sets can change how medical research is done and allow researchers to solve important problems. In our experience, the chart review process is often a key rate limiting step for modern studies; crowdsourcing has the ability to substantially lower the time to complete clinical studies. Additionally, the resulting labels are invaluable resources for supervised machine learning researchers that otherwise would be limited by smaller training data sets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5634</offset><text>Consequently, as displayed in Figure 1, our goal is to build a lightweight, customizable pipeline that significantly reduces the cost and time to complete medical research while increasing reproducibility and accuracy, and maintaining privacy and security standards.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>5901</offset><text>Crowdsourcing Framework</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5925</offset><text>This section introduces the main components of the crowdsourcing system, defines the workflow in which researchers develop crowdsourcing questions, and describes the process workers follow to produce labels.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6133</offset><text>Questions Design: After recruiting and identifying a researcher with an amenable research project, we conducted a design workshop. The workshop included the researcher, medical personnel, computer science researchers and anthropologists. The workshop began by introducing the researcher to crowdsourcing preliminaries and non-healthcare crowdsourcing examples. Next, the team worked to clarify and decompose the research objective into atomic questions by refining the structure of the crowdsourcing project as in Figure 2. We discussed data needs (e.g., all notes or specific note types), question format (e.g., boolean, multiple choice or text snipping), scope of tasks (e.g., multiple questions per patient or a single one), and worker skills requirements for the tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6907</offset><text>Crowdsourcing questions were constructed as narrowly as possible. For example:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6986</offset><text>Does a clinical note document patient conversations regarding diabetic diet alternatives? (Y/N)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7082</offset><text>Which of the following dietary alternatives were discussed with the patient? (Healthy oil choices, Sugar free sweets, Unsweetened tea, None)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7223</offset><text>Of the patient’s current diet choices listed in this note, rank them in terms of most problematic to their long-term health: (Soda, French fries, Dark chocolate, Broccoli)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7397</offset><text>In addition to true/false questions, multiple choice questions and ranking questions, researchers also asked that workers snip (or extract) text from notes that support the answer. We found these snippets were extremely helpful when experts were needed to adjudicate disagreements.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7679</offset><text>Data Extraction and De-Identification: APIs are needed to extract data from the underlying data store and load them for analysis. In an academic setting, these APIs are configured to take an Institutional Review Board (IRB) number and return the set of medical record numbers in the study. For each medical record number (MRN), the associated charts are pulled and loaded into the system. Moreover, upon querying the charts, the APIs apply open-source, de-identification tools (e.g., the MITRE Identification Scrubber Toolkit), to remove or scrub HIPAA-designated identifiers, such as patient name and residential addresses.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8304</offset><text>Crowdsourcing System: At its core, a crowdsourcing system matches workers to questions and stores the answers (or labels) in a database. Instead of developing a crowdsourcing system from scratch, we leveraged the open-source Pybossa system. Pybossa provides many basic crowdsourcing features, such as: loading and styling questions (known as a presenter), registering workers, assigning workers to tasks, collecting answers, timing tasks and extracting aggregate statistics and labels.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8790</offset><text>Unfortunately, the default version of Pybossa lacks many of the privacy controls that are needed to manage sensitive data. Therefore, fine-grained access controls with two-factor authentication were added to limit access for each worker. Moreover, worker attributes (or properties) were added to the underlying worker data models so each worker could be categorized by his or her skill level and specialty. These attributes allow for fine-grained question assignment and weighting.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9272</offset><text>The resulting crowdsourcing system was deployed on an internal server within the Vanderbilt University Medical Center firewall. The site was not open to the public. All worker registration, task assignment, question answering and data extraction were managed through a web interface over HTTPS and the activity is logged.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9594</offset><text>The Pybossa system allows researchers to customize how questions are ‘presented’ to workers via basic HTML and JavaScript coding. These presenters are simple templated HTML forms that read from an API and populate question text and candidate answers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9849</offset><text>Customizable Display Interface: We specifically decided to separate the crowdsourcing system from the data display system. This abstraction separated the logic of presenting questions to workers from the task of effectively displaying data for the specific research project. Instead we used HTML IFrames as a means for Pybossa to point to data for analysis. These IFrames can load content from a given URL and provides the developer control over the data input, method of display and tools used to parse the data. The IFrame URL is another parameter specified when configuring a Pybossa project.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10445</offset><text>The IFrame design has proven to be extremely versatile as we have completed projects displaying different data types including clinical text and medical images.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10606</offset><text>Helper Libraries: Perhaps the most important component of the system is a set of helper libraries that assist workers to produce labels. Example helper libraries include text highlighting tools, text search and document ranking.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10835</offset><text>Worker Recruitment: When working with sensitive data, only certain individuals have the necessary credentials to access the data. For instance, in healthcare, only hospital employees (which includes faculty, staff, and trainees) can access medical records. While the pool of workers is limited (in contrast to the aforementioned public crowd on Amazon), there are often groups of highly motivated workers, such as medical students, who are willing to work given incentives.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11309</offset><text>Our worker pool consists of mostly medical students and nursing students, with a small number of faculty. These workers were recruited through Grand Round presentations and IRB-approved email communications. For a worker to participate, he or she signed a data use agreement and, in some cases, was added as key personnel to the researcher’s IRB. We also recorded skill-level of each worker (e.g., medical student, intern, resident, fellow, attending, and nurse) and their specialization (if any), as these answers can impact which questions they are qualified to answer.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11883</offset><text>Supervised Machine Learning: After the crowdsourcing task is complete, researchers can use the labels to train supervised machine learning models. In healthcare, popular prediction systems include clinical decision support and order recommendation, among many others.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>12151</offset><text>Example Use Case</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12168</offset><text>In this section, we present an example use case of the crowdsourcing framework.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12248</offset><text>Suppose a researcher needs to label notes from a diabetes cohort (e.g., patients with ICD code 250.*). For each note, a worker selects one of the following labels: not relevant, relevant, or partially relevant to diabetes care. Moreover, for a note with a relevant or partially relevant label, the researcher also wants to extract supportive snippets from the note.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12614</offset><text>After clarifying the research question, task scope, task corpus and worker action, a presenter is designed and loaded into Pybossa. Similarly, notes are extracted from the EMR system, de-identified and loaded into a chart review data system.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12856</offset><text>Workers are recruited and assigned to the project. A pre-test determines if each candidate worker has sufficient knowledge about diabetes to participate. Only candidates who pass the test are admitted into the worker pool and assigned tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13098</offset><text>Admitted workers then begin reading notes assigned to them by Pybossa and producing labels. One note is shown to each worker at a time. The worker reads the content of the note, chooses a label, and selects relevant snippets from the note. This process continues until all notes are labeled. Depending on the coverage requirements, multiple workers might answer the same question.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13479</offset><text>To reduce the cognitive load of workers, the Pybossa presenter (as shown in Figure 3) only consists of the question, answer options and the input box of the search engine. Moreover, we only select necessary helper tools to assist workers find relevant information.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13744</offset><text>As shown in Figure 4, the interface highlights “diabetes” and semantically similar terms of “diabetes”, such as “hyperlipidemia” and “obesity.” On the left side of the note, a heat map displays the number of terms related to diabetes in each section of the note. When multiple notes are displayed to a worker in a single task, an EMR search engine automatically finds notes that contain diabetes or similar terms, and ranks the notes using an information retrieval metric.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14233</offset><text>After all tasks are completed, the researcher receives the labels and snippets. The researcher then utilizes the data in a supervised machine learning task, such as document classification.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>14423</offset><text>Helper Libraries</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14440</offset><text>Clinical notes contain vast amounts of unstructured information describing a patient’s past medical history and diagnoses. Ideally, crowd workers would be able to quickly parse through these data to answer their crowdsourcing questions. Unfortunately, searching for relevant data remains difficult. While workers deploy basic routines, such as starting with the most recent note or a specific note type (e.g., discharge summary), these basic approaches can miss relevant data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14919</offset><text>The objective of the helper libraries is to make common crowdsourcing operations more efficient. In the case of chart reviews, this means more efficiently finding relevant clinical text. To that end, we have deployed a set of search tools to find and rank documents for review.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15197</offset><text>We have implemented and tested three types of search systems:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15259</offset><text>Keyword search: Return documents that contain the search term. Rank documents by the frequency of the search term in the document.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15390</offset><text>Expanded keyword search (Figure 5): Given a search term, expand the search to include terms that are semantically similar to the term (semantic similarity is defined by a word2vec model trained on the medical records), and return documents with any of the similar terms. Rank documents by the number and extent of similar terms in the document.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15735</offset><text>Learning-to-rank keyword search: A learning-to-rank system takes a few labeled examples, and then adjusts the similarity weights to prioritize certain terms over others. Documents are ranked like the expanded keyword search system, but with updated similarity weights. Clinicians with different specialties read notes with varying intentions. Learning-to-rank assists researchers identify content that is relevant to their specific problem.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16176</offset><text>In addition to search, we have found that highlighting relevant text within notes can help workers more quickly focus on important information. As Figure 4 shows, similar words to the search term are highlighted in dark green, and moderately similar terms are highlighted in yellow. The similarity value is determined by a word2vec model. Because clinical notes often contain many pages of information, highlighting allows workers to scroll to relevant text quickly.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>16643</offset><text>Discussion and Next Steps</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16669</offset><text>We have completed more than six crowdsourcing projects including projects that:</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16749</offset><text>Analyze how well barriers to diabetes care are documented;</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16808</offset><text>Analyze if a patient had dialysis two weeks prior to surgery;</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16870</offset><text>Analyze if a note was relevant to a patient’s diabetes treatment.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>16938</offset><text>For each project, we conducted workshops, and recruited medical students and nursing students to participate in the crowd (over a dozen have participated). We paid the workers a flat fee to complete each project, which was determined by multiplying an hourly rate times the expected number of hours of work.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>17246</offset><text>For many projects, researchers have asked that workers snip the text used to make their decision. These snippets are then provided to an expert for validation. Even though this process requires an expert to review all answers, we find it is useful as the workers complete the time consuming task of scanning the entire document, while the expert simply reviews and approves snippets. If an expert’s time is limited and much more costly than workers, then this design can be effective.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>17733</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>17744</offset><text>In this paper, we presented a crowdsourcing framework for sensitive data sets, such as medical records. We developed a crowdsourcing platform that protects patient privacy and a set of helper libraries to assist workers complete tasks efficiently. Our aim is to help medical researchers attain high quality labels faster and more cheaply than previously possible. Future extensions of the framework include level-of-expertise weighted answers, quorum-detection, and machine learning prediction label assistance.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title_1</infon><offset>18256</offset><text>Funding:</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>18265</offset><text>Crowd Sourcing Labels from Electronic Medical Records to Enable Biomedical Research Award Number: 1 UH2 CA203708-01</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>18381</offset><text>References</text></passage><passage><infon key="fpage">3</infon><infon key="name_0">surname:Jenny;given-names:J Chen</infon><infon key="name_1">surname:Natala;given-names:J Menezes</infon><infon key="name_2">surname:Adam;given-names:D Bradley</infon><infon key="section_type">REF</infon><infon key="source">Interfaces</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2011</infon><offset>18392</offset><text>Opportunities for crowdsourcing research on amazon mechanical turk</text></passage><passage><infon key="fpage">411</infon><infon key="issue">5</infon><infon key="lpage">419</infon><infon key="name_0">surname:Gabriele;given-names:Paolacci</infon><infon key="name_1">surname:Jesse;given-names:Chandler</infon><infon key="name_2">surname:Pg;given-names:Ipeirotis</infon><infon key="section_type">REF</infon><infon key="source">Judgm. Decis. Mak</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2010</infon><offset>18459</offset><text>Running experiments on amazon mechanical turk</text></passage><passage><infon key="fpage">3</infon><infon key="issue">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:M.;given-names:Buhrmester</infon><infon key="name_1">surname:T.;given-names:Kwang</infon><infon key="name_2">surname:S. D.;given-names:Gosling</infon><infon key="pub-id_pmid">26162106</infon><infon key="section_type">REF</infon><infon key="source">Perspect. Psychol. Sci</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2011</infon><offset>18505</offset><text>Amazon’s mechanical turk: a new source of inexpensive, yet high-quality, data?</text></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">23</infon><infon key="name_0">surname:Winter;given-names:Mason</infon><infon key="name_1">surname:Siddharth;given-names:Suri</infon><infon key="pub-id_pmid">21717266</infon><infon key="section_type">REF</infon><infon key="source">Behav. Res. Methods</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2012</infon><offset>18586</offset><text>Conducting behavioral research on amazon’s mechanical turk</text></passage><passage><infon key="fpage">292</infon><infon key="issue">3</infon><infon key="lpage">298</infon><infon key="name_0">surname:Amy;given-names:H Kaji</infon><infon key="name_1">surname:David;given-names:Schriger</infon><infon key="name_2">surname:Steven;given-names:Green</infon><infon key="pub-id_pmid">24746846</infon><infon key="section_type">REF</infon><infon key="source">Ann. Emerg. Med</infon><infon key="type">ref</infon><infon key="volume">64</infon><infon key="year">2014</infon><offset>18647</offset><text>Looking through the retrospectoscope: reducing bias in emergency medicine chart review studies</text></passage><passage><infon key="fpage">162</infon><infon key="issue">1</infon><infon key="lpage">171</infon><infon key="name_0">surname:Teixeira,;given-names:Pedro L</infon><infon key="name_1">surname:Wei,;given-names:Wei-Qi</infon><infon key="name_2">surname:Cronin,;given-names:Robert M</infon><infon key="name_3">surname:Mo,;given-names:Huan</infon><infon key="name_4">surname:VanHouten,;given-names:Jacob P</infon><infon key="pub-id_pmid">27497800</infon><infon key="section_type">REF</infon><infon key="source">J. Am. Med. Inform. Assoc.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2017</infon><offset>18742</offset><text>Evaluating electronic health record data sources and algorithmic approaches to identify hypertensive individuals</text></passage><passage><infon key="fpage">181</infon><infon key="issue">2</infon><infon key="lpage">6</infon><infon key="name_0">surname:S Trent;given-names:Rosenbloom</infon><infon key="name_1">surname:Joshua;given-names:C Denny</infon><infon key="name_2">surname:Hua;given-names:Xu</infon><infon key="name_3">surname:Nancy;given-names:Lorenzi</infon><infon key="name_4">surname:William;given-names:W Stead</infon><infon key="name_5">surname:Kevin;given-names:B Johnson</infon><infon key="pub-id_pmid">21233086</infon><infon key="section_type">REF</infon><infon key="source">J. Am. Med. Inform. Assoc.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2011</infon><offset>18855</offset><text>Data from clinical notes: a perspective on the tension between structure and flexible documentation</text></passage><passage><infon key="fpage">21</infon><infon key="lpage">26</infon><infon key="name_0">surname:Sujan;given-names:Perera</infon><infon key="name_1">surname:Amit;given-names:Sheth</infon><infon key="name_2">surname:Krishnaprasad;given-names:Thirunarayan</infon><infon key="name_3">surname:Suhas;given-names:Nair</infon><infon key="name_4">surname:Neil;given-names:Shah</infon><infon key="section_type">REF</infon><infon key="source">In Proc. 2013 Int. Work. Data Manag. Anal. Healthc. - DARE '13</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>18955</offset><text>Challenges in understanding clinical notes</text></passage><passage><infon key="fpage">70</infon><infon key="issue">2</infon><infon key="lpage">84</infon><infon key="name_0">surname:John;given-names:B Smelcer</infon><infon key="name_1">surname:Hal;given-names:Miller-Jacobs</infon><infon key="name_2">surname:Lyle;given-names:Kantrovich</infon><infon key="section_type">REF</infon><infon key="source">J. Usability Stud.</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2009</infon><offset>18998</offset><text>Usability of electronic medical records</text></passage><passage><infon key="fpage">115</infon><infon key="issue">7639</infon><infon key="lpage">118</infon><infon key="name_0">surname:Andre;given-names:Esteva</infon><infon key="name_1">surname:Brett;given-names:Kuprel</infon><infon key="name_2">surname:Roberto;given-names:A Novoa</infon><infon key="name_3">surname:Justin;given-names:Ko</infon><infon key="name_4">surname:Susan;given-names:M Swetter</infon><infon key="name_5">surname:Helen;given-names:M Blau</infon><infon key="name_6">surname:Sebastian;given-names:Thrun</infon><infon key="pub-id_pmid">28117445</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">542</infon><infon key="year">2017</infon><offset>19038</offset><text>Dermatologist-level classification of skin cancer with deep neural networks</text></passage><passage><infon key="name_0">surname:Yunzhu;given-names:Li</infon><infon key="name_1">surname:Andre;given-names:Esteva</infon><infon key="name_2">surname:Brett;given-names:Kuprel</infon><infon key="name_3">surname:Rob;given-names:Novoa</infon><infon key="name_4">surname:Justin;given-names:Ko</infon><infon key="name_5">surname:Sebastian;given-names:Thrun</infon><infon key="section_type">REF</infon><infon key="source">arXivpreprint arXiv:1612.01074</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>19114</offset><text>Skin cancer detection and tracking using data synthesis and deep learning</text></passage><passage><infon key="name_0">surname:Fresne;given-names:J</infon><infon key="name_1">surname:Youngclaus;given-names:J</infon><infon key="name_2">surname:Shick;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Assoc. Am. Med. Coll.</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>19188</offset><text>Medical student education: debt, costs, and loan repayment fact card</text></passage><passage><infon key="fpage">849</infon><infon key="issue">12</infon><infon key="lpage">859</infon><infon key="name_0">surname:John;given-names:Aberdeen</infon><infon key="name_1">surname:Samuel;given-names:Bayer</infon><infon key="name_2">surname:Reyyan;given-names:Yeniterzi</infon><infon key="name_3">surname:Ben;given-names:Wellner</infon><infon key="name_4">surname:Cheryl;given-names:Clark</infon><infon key="name_5">surname:David;given-names:Hanauer</infon><infon key="name_6">surname:Bradley;given-names:Malin</infon><infon key="name_7">surname:Lynette;given-names:Hirschman</infon><infon key="pub-id_pmid">20951082</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Med. Inform.</infon><infon key="type">ref</infon><infon key="volume">79</infon><infon key="year">2010</infon><offset>19257</offset><text>The MITRE identification scrubber toolkit: design, training, and assessment</text></passage><passage><infon key="name_0">surname:Daniel;given-names:Lombraa Gonzlez</infon><infon key="name_1">surname:alejandro;given-names:dob</infon><infon key="name_2">surname:Marvin;given-names:R.</infon><infon key="name_3">surname:Martin;given-names:Keegan</infon><infon key="name_4">surname:Rufus;given-names:Pollock</infon><infon key="name_5">surname:Nigel;given-names:Babu</infon><infon key="section_type">REF</infon><infon key="source">Zenodo</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>19333</offset><text>Scifabric/pybossa: v2.8.0</text></passage><passage><infon key="fpage">8</infon><infon key="lpage">17</infon><infon key="name_0">surname:Konstantina;given-names:Kourou</infon><infon key="name_1">surname:Themis;given-names:P. Exarchos</infon><infon key="name_2">surname:Konstantinos;given-names:P. Exarchos</infon><infon key="name_3">surname:Michalis;given-names:V. Karamouzis</infon><infon key="name_4">surname:Dimitrios;given-names:I. Fotiadis</infon><infon key="pub-id_pmid">25750696</infon><infon key="section_type">REF</infon><infon key="source">Comput. Struct. Biotechnol. J.</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2015</infon><offset>19359</offset><text>Machine learning applications in cancer prognosis and prediction</text></passage><passage><infon key="fpage">2580</infon><infon key="issue">3</infon><infon key="lpage">2607</infon><infon key="name_0">surname:Martin;given-names:Wiesner</infon><infon key="name_1">surname:Daniel;given-names:Pfeifer</infon><infon key="pub-id_pmid">24595212</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Environ. Res. Public Health</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2014</infon><offset>19424</offset><text>Health recommender systems: concepts, requirements, technical basics and challenges</text></passage><passage><infon key="fpage">3111</infon><infon key="lpage">3119</infon><infon key="name_0">surname:Tomas;given-names:Mikolov</infon><infon key="name_1">surname:Kai;given-names:Chen</infon><infon key="name_2">surname:Greg;given-names:Corrado</infon><infon key="name_3">surname:Jeffrey;given-names:Dean</infon><infon key="section_type">REF</infon><infon key="source">Adv. Neural Inf. Process. Syst.</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>19508</offset><text>Distributed representations of words and phrases and their compositionality</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:Tomas;given-names:Mikolov</infon><infon key="name_1">surname:Greg;given-names:Corrado</infon><infon key="name_2">surname:Kai;given-names:Chen</infon><infon key="name_3">surname:Jeffrey;given-names:Dean</infon><infon key="section_type">REF</infon><infon key="source">Proc. Int. Conf. Learn. Represent. (ICLR 2013)</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>19584</offset><text>Efficient estimation of word representations in vector space</text></passage><passage><infon key="name_0">surname:Cao,;given-names:Zhe</infon><infon key="name_1">surname:Qin,;given-names:Tao</infon><infon key="name_2">surname:Liu,;given-names:Tie-Yan</infon><infon key="name_3">surname:Tsai,;given-names:Ming-Feng</infon><infon key="name_4">surname:Li,;given-names:Hang</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 24th International Conference on Machine Learning</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>19645</offset><text>Learning to Rank: From Pairwise Approach to Listwise Approach</text></passage><passage><infon key="file">2840819f1.jpg</infon><infon key="id">f1-2840819</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19707</offset><text>Overview of the crowdsourcing system.</text></passage><passage><infon key="file">2840819f2.jpg</infon><infon key="id">f2-2840819</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19745</offset><text>Agenda for crowdsourcing workshop with researchers.</text></passage><passage><infon key="file">2840819f3.jpg</infon><infon key="id">f3-2840819</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19797</offset><text>Example of Pybossa presenter with a text search engine.</text></passage><passage><infon key="file">2840819f4.jpg</infon><infon key="id">f4-2840819</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19853</offset><text>An example helper library: highlighting similar words in a note</text></passage><passage><infon key="file">2840819f5.jpg</infon><infon key="id">f5-2840819</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19917</offset><text>An example of expanded search terms for “diabetes”</text></passage></document></collection>
