<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220104</date><key>pmc.key</key><document><id>8610834</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1016/j.csl.2021.101320</infon><infon key="article-id_pii">S0885-2308(21)00115-7</infon><infon key="article-id_pmc">8610834</infon><infon key="article-id_pmid">34840419</infon><infon key="article-id_publisher-id">101320</infon><infon key="fpage">101320</infon><infon key="kwd">COVID-19 Acoustics Machine learning Respiratory diagnosis Healthcare</infon><infon key="license">Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</infon><infon key="lpage">101320</infon><infon key="name_0">surname:Sharma;given-names:Neeraj Kumar</infon><infon key="name_1">surname:Muguli;given-names:Ananya</infon><infon key="name_2">surname:Krishnan;given-names:Prashant</infon><infon key="name_3">surname:Kumar;given-names:Rohit</infon><infon key="name_4">surname:Chetupalli;given-names:Srikanth Raj</infon><infon key="name_5">surname:Ganapathy;given-names:Sriram</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">73</infon><infon key="year">2021</infon><offset>0</offset><text>Towards sound based testing of COVID-19—Summary of the first Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>122</offset><text>The technology development for point-of-care tests (POCTs) targeting respiratory diseases has witnessed a growing demand in the recent past. Investigating the presence of acoustic biomarkers in modalities such as cough, breathing and speech sounds, and using them for building POCTs can offer fast, contactless and inexpensive testing. In view of this, over the past year, we launched the “Coswara” project to collect cough, breathing and speech sound recordings via worldwide crowdsourcing. With this data, a call for development of diagnostic tools was announced in the Interspeech 2021 as a special session titled “Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge”. The goal was to bring together researchers and practitioners interested in developing acoustics-based COVID-19 POCTs by enabling them to work on the same set of development and test datasets. As part of the challenge, datasets with breathing, cough, and speech sound samples from COVID-19 and non-COVID-19 individuals were released to the participants. The challenge consisted of two tracks. The Track-1 focused only on cough sounds, and participants competed in a leaderboard setting. In Track-2, breathing and speech samples were provided for the participants, without a competitive leaderboard. The challenge attracted 85 plus registrations with 29 final submissions for Track-1. This paper describes the challenge (datasets, tasks, baseline system), and presents a focused summary of the various systems submitted by the participating teams. An analysis of the results from the top four teams showed that a fusion of the scores from these teams yields an area-under-the-receiver operating curve (AUC-ROC) of 95.1% on the blind test data. By summarizing the lessons learned, we foresee the challenge overview in this paper to help accelerate technological development of acoustic-based POCTs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2005</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2018</offset><text>The viral respiratory infection caused by the novel coronavirus, SARS-CoV-2, termed as the coronavirus disease 2019 (COVID-19), was declared a pandemic by the World Health Organization (WHO) in March 2020. The current understanding of COVID-19 prognosis suggests that the virus infects the nasopharynx and then spreads to the lower respiratory tract . One of the key strategies to combat the rapid spread of infection across populations is to perform rapid and large-scale testing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2501</offset><text>Currently, the prominent COVID-19 testing methodologies take a molecular sensing approach. The gold-standard technique, termed as reverse transcription polymerase chain reaction (RT-PCR) , relies on using nasopharyngeal or throat swab samples. The swab sample is treated with chemical reagents enabling isolation of the ribonucleic acid (RNA), followed by deoxyribonucleic acid (DNA) formation, amplification and analysis, facilitating the detection of COVID-19 genome in the sample. However, this approach has several limitations. The swab sample collection procedure violates physical distancing . The processing of these samples requires a well equipped laboratory, with readily available chemical reagents and expert analysts. Further, the turnaround time for test results can vary from several hours to a few days. The protein based rapid antigen testing (RAT)  improves over the speed of detection while being inferior to the RT-PCR in detection performance. The RAT test also involves the need for chemical reagents.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>3528</offset><text>A list of publicly accessible COVID-19 audio datasets.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Ref&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Dataset&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Sound categories&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Access&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;COVID/non-COVID samples&lt;xref rid=&quot;tblfn1a&quot; ref-type=&quot;table-fn&quot;&gt;a&lt;/xref&gt;&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Method&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;xref rid=&quot;b37&quot; ref-type=&quot;bibr&quot;&gt;Orlandic et al. (2021)&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;COUGHVID&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Public&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;1155/27 550&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;xref rid=&quot;b46&quot; ref-type=&quot;bibr&quot;&gt;Sharma et al. (2020)&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Coswara&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Cough, speech, breathing&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Public&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;345/1785&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;xref rid=&quot;b52&quot; ref-type=&quot;bibr&quot;&gt;Virufy COVID-19 Open Cough Dataset (2021)&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Virufy&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Public&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;7/9&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Hospital&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;xref rid=&quot;b12&quot; ref-type=&quot;bibr&quot;&gt;Cohen-McFarlane et al. (2020)&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;NoCoCoDa&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;On request&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;13/NA&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;YouTube&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;xref rid=&quot;b9&quot; ref-type=&quot;bibr&quot;&gt;Brown et al. (2020)&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;COVID-19 sounds&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Cough, breathing&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;On request&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;141/318&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>3583</offset><text>Ref		Dataset		Sound categories		Access		COVID/non-COVID samplesa		Method	 			COUGHVID		Cough		Public		1155/27 550		Crowdsourced	 			Coswara		Cough, speech, breathing		Public		345/1785		Crowdsourced	 			Virufy		Cough		Public		7/9		Hospital	 			NoCoCoDa		Cough		On request		13/NA		YouTube	 			COVID-19 sounds		Cough, breathing		On request		141/318		Crowdsourced	 	</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>3947</offset><text>Samples refers to count of distinct audio records from human subjects. Each audio record is composed of a set of sound recordings corresponding to the stated sound categories.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4123</offset><text>In view of the above mentioned limitations in molecular testing approaches (namely, RT-PCR and RAT), there is a need to design highly specific, rapid and easy-to-use point-of-care tests (POCTs) that could identify the infected individuals in a decentralized manner. Using acoustics for developing such a POCT would overcome various limitations in terms of speed, and cost, and also allow scalable remote testing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>4536</offset><text>Exploring acoustics based testing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4570</offset><text>The use of acoustics for diagnosis of pertussis , tuberculosis , childhood pneumonia , and asthma  has been explored using cough sounds recorded with portable devices. As COVID-19 is an infection affecting the respiratory pathways , recently, researchers have made efforts towards COVID-19 acoustic data collection. A list of acoustic datasets is provided in Table 1. Building on these datasets, few studies have evaluated the possibility of COVID-19 detection using acoustics.  used cough and breathing sounds jointly and attempted a binary classification task of separating COVID-19 infected individuals from healthy. The dataset was collected through crowd-sourcing, and the analysis was done on  COVID-19 infected individuals. The authors reported a performance between  AUC-ROC (area-under-the-receiver operating characteristic curve).  demonstrated 81% specificity (at 43% sensitivity) on a subset of the COUGHVID dataset .  studied cough sound samples from four groups of individuals, namely, healthy, and those with bronchitis, pertussis, and COVID-19 infection. They report an accuracy of 92.6%.  used a large sample set of COVID-19 infected individuals and report an AUC-ROC performance of 97.0%.  create a controlled dataset by collecting cough sound samples from patients visiting hospitals, and they report 98.8% AUC-ROC.</text></passage><passage><infon key="file">gr1_lrg.jpg</infon><infon key="id">fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5917</offset><text>The DiCOVA challenge timeline.</text></passage><passage><infon key="file">gr2_lrg.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5948</offset><text>An illustration of Track-1 and Track-2 development datasets. Here, (a,d) show the COVID and non-COVID pool size in terms of number of individuals; (b,e) show the breakdown of non-COVID individuals into categories of no symptoms, symptoms (cold, cough), and pre-existing respiratory ailment (asthma, chronic lung disease, pneumonia); (c,f) depicts the age group distribution in the development dataset.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6350</offset><text>Although these studies are encouraging, they suffer from some limitations. They do not use a common dataset, and a few are based on privately collected datasets. The ratio of COVID-19 patients to healthy (or non-COVID) is different in every study. The performance metrics are also different across studies. Some of the studies report performance per-cough bout, and others report per-patient. Further, most of the studies have not bench-marked on other open source datasets, making it difficult to compare among the various propositions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6888</offset><text>Contribution</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6901</offset><text>We launched the “Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge”  with two primary goals. Firstly, to encourage the speech and audio researchers to analyze acoustics of cough and speech sounds for a problem of immediate societal relevance. The challenge was launched under the umbrella of Interspeech 2021, and participants were given an option to submit their findings to a special session in this flagship conference. Secondly, and more importantly, to provide a benchmark for monitoring the progress in acoustic based diagnostics of COVID-19. The development and (blind) test datasets were provided to the participants to facilitate design and evaluation of classifier systems. A leaderboard was created allowing participants to rank order their performance against others. This paper describes the details of the challenge including the dataset, the baseline system (Section 2), and provides a summary of the various submitted systems (Section 3). An analysis of the scores submitted by the top teams (Section 4), and the insights gained from the challenge are also presented (Section 6).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>8015</offset><text>DiCOVA Challenge</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8032</offset><text>The DiCOVA challenge2 was launched on Feb, 2021 and the challenge lasted till Mar, . The participation was through a registration process. A development set, a baseline system, and a blind test set was provided to all registered participants. A timeline of the challenge is shown in Fig. 1. A remote server based scoring system with a leaderboard setting was created. This provided near real-time ranking and monitoring progress of each team on the blind test set.3 The call for participation in the challenge attracted  plus registrations. Out of this,  teams made final submissions on the blind test set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>8640</offset><text>Dataset</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8648</offset><text>The challenge dataset is derived from the Coswara dataset , a crowd-sourced dataset of sound recordings. The Coswara data is collected using a website.4 The volunteers from across the globe, age groups and health conditions were requested to record their sound data in a quiet environment using an internet connected device (like, mobile phone or computer).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9007</offset><text>The participants initially provide demographic information like age and gender. An account of their current health status in the form of a questionnaire of symptoms as well as pre-existing conditions like respiratory ailments and co-morbidity are recorded. The web based tool also records the result of the COVID-19 test conducted and the possibility of exposure to the virus through primary contacts.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9409</offset><text>The acoustic data from each subject contains  audio categories, namely,  shallow and deep breathing ( types),  shallow and heavy cough ( types),  sustained phonation of vowels [æ] (as in bat), [i] (as in beet), and [u] (as in boot) ( types), and  fast and normal pace number counting ( types). The dataset collection protocol was approved by the Human Ethics Committee of the Indian Institute of Science, Bangalore, and P. D. Hinduja National Hospital and Medical Research Center, Mumbai, India.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9906</offset><text>The DiCOVA Challenge used a subset of the Coswara dataset, sampled from the data collected between April- and Feb-. The sampling included only age group of  years. The subjects with health status of “recovered” (who were COVID positive however fully recovered from the infection) and “exposed” (suspecting exposure to the virus) were not included in the dataset. Further, subjects with audio recordings of duration less than  ms were discarded. The resulting curated subject pool was divided into the following two groups.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10438</offset><text>non-COVID: Subjects self reported as healthy, having symptoms such as cold/cough or having pre-existing respiratory ailments (like asthma, pneumonia, chronic lung disease) but were not tested positive for COVID-19.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10653</offset><text>COVID: Subjects self-declared as COVID-19 positive (asymptomatic or symptomatic with mild/moderate infection)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10763</offset><text>The DiCOVA 2021 challenge featured two tracks. The Track-1 development dataset composed of (heavy) cough sound recordings from  subjects. The Track-2 development dataset composed of deep breathing, vowel [i], and number counting (normal pace) speech recordings from  subjects. An illustration of the important metadata details in the development set is provided in Fig. 2. About % of the subjects were male. The majority of the participants lie in the age group of  years. Also, the dataset is highly imbalanced with less than % of the participants belonging to the COVID category. We retained this class imbalance in the challenge as this reflects the typical real-world scenario.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11446</offset><text>In the data release, the development dataset was further divided into train and validation splits. The splits are illustrated in Fig. 3. A meta-analysis study of COVID-19 symptoms by  found cough (53.9%) as a common symptoms in 281,641 COVID-19 infected individuals. In addition, prior efforts on data collection and modeling largely focused on the cough samples (see Table 1) . Owing to this, the challenge emphasized progress in Track-1. A leaderboard was created and the participants competed by uploading their scores for a blind test dataset and monitoring the performance. The Track-2 featured the test dataset, without any leaderboard-style competition and encouraged the participants to carry out an exploratory analysis.</text></passage><passage><infon key="file">gr3_lrg.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12180</offset><text>Illustration of dataset splits for Track-1 (cough) and Track-2 (breathing and speech).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12267</offset><text>Audio specifications</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12288</offset><text>The crowd-sourced dataset reflects a good representation of real-world data with sensor variability arising from diverse recording devices. For the challenge, we re-sampled all audio recordings to 44.1 kHz and compressed them to FLAC (Free Lossless Audio Codec) format for ease of distribution. The average duration of Track-1 development set cough recordings is standard deviation  s. The average duration of Track-2 development set audio recordings is -breathing  s, vowel [i]  s, and number counting speech  s.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12807</offset><text>Task</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12812</offset><text>Track 1: The task focused on cough audio samples only. This was the primary track of the challenge with most teams participating only in this track. A leaderboard website was hosted for the challenge enabling teams to evaluate their system performance (validation set and blind test set). The participating teams were required to submit the COVID probability score for each audio file in the validation and test sets. The leaderboard website computed the ROC-AUC and the specificity/sensitivity. Every team was provided a maximum of  tickets for submitting scores to the leaderboard.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13396</offset><text>Track 2: Track-2 explored the use of recordings other than cough for the task of COVID diagnostics. The audio recordings released in this track composed of breathing, speech related to sustained phonation of vowel [i] and number counting (). The development and (non-blind) test sets were released concurrently, without any formal leaderboard style evaluation and competition.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13773</offset><text>The data and the baseline system setup were provided to the registered teams after signing a terms and conditions document. As per the document, the teams were not allowed to use the publicly available Coswara dataset.5 </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13994</offset><text>Evaluation metrics</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14013</offset><text>The focus of the challenge was binary classification, that is, detecting COVID or non-COVID using acoustics. As the dataset was imbalanced, we choose not to use accuracy as an evaluation metric. Each team submitted COVID probability scores (, a higher value indicating a higher likelihood of COVID infection) for the list of validation/test audio recordings. For performance evaluation, we used the scores with the ground truth labels to compute the receiver operating characteristics (ROC) curve. The curve was obtained by varying the decision threshold between  with a step size of 0.0001. The area under the resulting ROC curve, AUC-ROC, was used as a performance measure for the classifier, where the area was computed using the trapezoidal method. The AUC-ROC formed the primary evaluation metric. Further, specificity (true negative rate), at a sensitivity (true positive rate) greater than or equal to 80% was used as a secondary evaluation metric. For brevity, we will refer to AUC-ROC by AUC in the rest of the paper.</text></passage><passage><infon key="file">gr4_lrg.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>15040</offset><text>(a) A scatter plot of the average five-fold validation AUC versus test AUC performance for every submission on the leaderboard. (b) Test set AUC performance in rank ordered manner for each of the system submissions. Here, AUC refers to AUC-ROC.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15285</offset><text>Baseline system</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15301</offset><text>The baseline system was implemented using tools from the scikit-learn Python library .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15389</offset><text>Pre-processing: For every audio file, the signal was normalized in amplitude. Using a sound activity detection threshold of 0.01 and a buffer size of ms on either side of a sample, any region of the audio signal with amplitude lower than threshold was discarded. Also, initial and final  snippets of the audio were removed to avoid abrupt start and end activity in the recordings.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15770</offset><text>Feature extraction: The baseline system used the  dimensional mel-frequency cepstral coefficients (MFCCs), its delta and delta–delta coefficients, computed over  samples (23.2 ms), with a hop of  samples ( ms). The resulting feature dimension was 39 × 1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16032</offset><text>Classifiers: The following three classifiers were designed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16092</offset><text>Logistic Regression (LR): The LR classifier was trained for  epochs. The binary cross entropy (BCE) loss with a  regularization strength of 0.01 was used for optimizing the model.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16272</offset><text>Multi layer perceptron (MLP): A single-layer perceptron model with  hidden units and tanh() activation was used. Similar to the LR model, the BCE loss with a  regularization of strength 0.001 was optimized for parameter estimation. The loss was optimized using Adam optimizer with an initial learning rate of 0.001. The COVID samples were over-sampled to compensate for the data imbalance (weighted BCE loss).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16682</offset><text>Random Forests (RF): A random forest classifier was trained with  trees using Gini impurity criterion for tree growing.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16802</offset><text>In the weighted BCE loss used in LR and MLP, the class errors are weighted. That is,  </text></passage><passage><infon key="file">gr5_lrg.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>16889</offset><text>Variation in AUC as a function of number of nodes in the hidden layer of an MLP classifier. The AUC-ROC% is the average over the five validation folds.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17041</offset><text>where  is the loss,  is the number of training samples,  and  are the true and predicted labels, and  and  are the weights associated with non-COVID and COVID classes. We choose  and  as the inverse of fraction of samples associated with the corresponding class in the training set. In RF, class weights are used to weigh the Gini impurity criterion for finding splits for tree growing. In the terminal nodes of each tree, class weights are again taken into consideration for class prediction via “weighted majority” vote .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17570</offset><text>In the MLP design, the goal was to develop a shallow architecture for the baseline system and encourage participants to build deep networks with pre-training from other datasets. Our internal analysis had shown over-fitting issues for deep architectures when trained only with the challenge data. Hence, a single hidden layer architecture with tanh() was chosen. Ablation experiments were carried out to decide on the number of nodes in the hidden layer of this MLP. A grid search in the range of  nodes in steps of  showed average AUC-ROC (over the five validation folds) in the range of . The AUC-ROC improved with addition of nodes from  to  and after this the increase did not so a monotonic improvement. This is shown in Fig. 5. Also, shown is the AUC-ROC obtained using a two hidden layer MLP. The performance is in the similar range as that of single layer MLP. For the baseline system we opted for a single layer MLP with  hidden units.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18516</offset><text>Inference and performance: To obtain a classification score for an audio recording:  the file was pre-processed,  frame-level MFCC features were extracted,  frame-level probability scores were computed using the trained model(s), and  all the frame scores were averaged to obtain a single COVID probability score for the audio recording. For evaluation on the test set files, the probability scores from five validation models (for a classifier type) were averaged to obtain the final score.</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>19008</offset><text>The baseline system performance on Track-1 and Track-2 on development set (5-fold val) and test set.</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Track&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Sound&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;left&quot;&gt;Performance (AUC%)&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Val. (std. dev)&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Test&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;LR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;66.95 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e333&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;3.89)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;61.97&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;68.54 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e350&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;3.69)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;69.85&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;RF&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;70.69 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e366&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;3.10)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;67.59&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;5&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;LR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;60.95 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e381&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;4.85)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;60.94&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Breathing&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;72.47 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e397&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;4.38)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;71.52&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RF&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;75.17 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e412&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;2.75)&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;76.85&lt;/bold&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;LR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;71.48 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e428&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;1.23)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;67.71&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel [i]&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;70.39 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e445&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;4.11)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;73.19&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RF&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;69.73 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e460&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;4.31)&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;75.47&lt;/bold&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;LR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;68.93 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e476&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;2.44)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;61.22&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;Speech&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;73.57 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e492&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;1.59)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;61.13&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;RF&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;69.61 (&lt;inline-formula&gt;&lt;mml:math id=&quot;d1e508&quot; display=&quot;inline&quot; altimg=&quot;si65.svg&quot;&gt;&lt;mml:mo&gt;±&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;3.49)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;65.27&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>19109</offset><text>Track	Sound	Model	Performance (AUC%)	 				Val. (std. dev)	Test	 			LR	66.95 (3.89)	61.97	 	1	Cough	MLP	68.54 (3.69)	69.85	 			RF	70.69 (3.10)	67.59	 		 			LR	60.95 (4.85)	60.94	 		Breathing	MLP	72.47 (4.38)	71.52	 			RF	75.17 (2.75)	76.85	 			LR	71.48 (1.23)	67.71	 	2	Vowel [i]	MLP	70.39 (4.11)	73.19	 			RF	69.73 (4.31)	75.47	 			LR	68.93 (2.44)	61.22	 		Speech	MLP	73.57 (1.59)	61.13	 			RF	69.61 (3.49)	65.27	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19525</offset><text>Table 2, depicts the performance of the three classifiers on the validation folds and the test sets. All classifiers performed better than chance. For Track-1, the AUC for the test set was better for the MLP classifier (69.85% AUC). For Track-2, RF gave the best AUC in all sound categories ( AUC).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19824</offset><text>Further, among the category of acoustic sounds, the breathing samples provided the best AUC (76.85%) performance followed by vowel sound [i] (75.47%). The baseline system code6 was provided to the participants as a reference for setting up a classifier training and scoring pipeline.</text></passage><passage><infon key="file"></infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>20108</offset><text>Summary of submitted systems in terms of feature and model configurations. The specificity (%) is reported at a sensitivity of 80%.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>20240</offset><text>Track-1: Submitted systems overview</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20276</offset><text>A total of  teams (plus the baseline system) participated in the Track-1 leaderboard. Out of these,  teams submitted their system reports describing the explored approaches.7 In this section, we provide a brief overview of the submissions, emphasizing on the obtained performances and explored classifiers, features, model ensembling and data augmentation techniques.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>20644</offset><text>Performance</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20656</offset><text>In total,  out of the  teams reported a performance better than the baseline system. We refer to the teams with Team IDs corresponding to their rank on the leaderboard, that is, best AUC performance as T-1 and so on. A performance summary of all the submitted systems on the validation and the blind test data is given in Fig. 4. Fig. 4(a) depicts a comparison of the validation and test results. Interestingly, there is a slight positive correlation between test and validation performance. For some teams, the validation performances exceed % AUC. Deducing from the system reports, these performances are primarily due to training on the whole development dataset without removing the validation data. Fig. 4(b) depicts the best AUC posted by  participating teams (including baseline) on the blind test data. The best AUC performance on the test data was 87.07%, a significant improvement over the baseline AUC (that is, 69.85%).</text></passage><passage><infon key="file">gr6_lrg.jpg</infon><infon key="id">fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>21591</offset><text>Illustration of AUC performance on full test set, and test set split by gender, and age. For male set:  subjects ( COVID), for female set:  subjects ( COVID), for age  set:  subjects ( COVID), and for age  40 set:  subjects ( COVID).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21825</offset><text>Features</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21834</offset><text>The teams designed and experimented with a wide spectrum of features. A concise highlight is provided in Table 3. A majority of the teams used mel-spectrograms, mel-frequency cepstral coefficients , or equivalent rectangular bandwidth (ERB)  spectrograms ( submissions out ). Further, the openSMILE features , which consist of statistical measures extracted on low-level acoustic feature descriptors, were explored by  teams. Few teams explored features derived using Teager energy based cepstral coefficients (TECC ; T-15), and pool of short-term features such as short-term energy, zero-crossing rate, and voicing (T-5, T-14, T-27). Other teams resorted to using embeddings derived from pre-trained neural networks as features. These included VGGish , DeepSpectrum , OpenL3 , YAMNet  embeddings (T-7, T-12), and x-vectors  (T-15).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22677</offset><text>Classifiers</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22689</offset><text>The teams explored various classifier models (see Table 3). These included classical machine learning models, such as decision trees, random forests (RFs), and support vector machines (SVMs), and modern deep learning models, such as convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and residual networks (ResNet). Several teams also attempted an ensemble of models to improve the final system performance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23125</offset><text>The CNNs were explored by teams (T-1, T-5, T-10, T-17). Variants of CNNs with residual connections and recording level average pooling to deal with the variable length input were developed by teams (T-2, T-5, T-8, T-9, T-10, T-13, T-16, T-21). Citing the improved ability of LSTMs to handle variable length inputs, (T-3, T-5, T-12, T-27) explored these models. The classical ML approaches of random forest, logistic regression and SVMs were used by (T-4, T-6, T-12, T-18). LightGBM (Gradient Boosting Machine)  model was explored by (T-15), and extra trees classifiers were studied by (T-7). Pre-training was also studied in several systems (T-3, T-17). Autoencoder style pre-training was used by (T-17). Several teams had also experimented with transfer learning from architectures pre-trained on image based models (T-2, T-8, T-13) and audio based models (T-10, T-13).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23997</offset><text>Model ensembling</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24014</offset><text>The fusion of scores from different classifier architectures was explored by multiple teams (T-3, T-4, T-6 T-10, T-11, T-12, T-13). The fusion of multiple features was explored by (T-13). Further, (T-2, T-3) investigated score fusion of outputs obtained from the model tuned on the five validation folds.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>24319</offset><text>Data augmentation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24337</offset><text>Data augmentation is a popular strategy in which external or synthetic audio data is used in training of deep network models. Five teams reported using this strategy by including COUGHVID cough dataset  (publicly available), adding Gaussian noise at varying SNRs, or doing audio manipulations (pitch shifting, time-scaling, etc., via tools such as audiomentations8 ). Few teams also used data augmentation approaches to circumvent the problem of class imbalance. These included T-1 using mixup , (T-3, T-9, T-11) using SpecAugment , (T-2, T-5, T-9) using additive noise, T-21 using sample replication, and T-5 using Vocal-Tract Length Perturbation (VTLP) , to increase the sample counts of the minority class.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25051</offset><text>Besides these, other strategies for training included gender aware training (T-21), using focal loss  objective function (T-2, T-8, T-11), and hyper-parameter tuning using model searching algorithm TPOT (T-7) .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25264</offset><text>In the next section, we discuss in detail the approaches used by the Track-1 four top performing teams.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>25368</offset><text>Track-1: Top performers</text></passage><passage><infon key="file">tbl4.xml</infon><infon key="id">tbl4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>25392</offset><text>A comparison of AUC and sensitivity of top four teams, their score fusion and the baseline system.</text></passage><passage><infon key="file">tbl4.xml</infon><infon key="id">tbl4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Performance measures&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Team T-1&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Team T-2&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Team T-3&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Team T-4&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Fusion&lt;/th&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;Baseline&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;AUC %&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;87.07&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;85.43&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;85.35&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;85.21&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;95.07&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;69.85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Sensitivity (at 95% Specificity)&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;46.34&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;39.02&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;60.97&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;29.27&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;70.73&lt;/td&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;17.07&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>25491</offset><text>Performance measures		Team T-1		Team T-2		Team T-3		Team T-4		Fusion		Baseline	 	AUC %		87.07		85.43		85.35		85.21		95.07		69.85	 	Sensitivity (at 95% Specificity)		46.34		39.02		60.97		29.27		70.73		17.07	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>25700</offset><text>T-1: The brogrammers</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25721</offset><text>The team  focused on using a multi-layered CNN network architecture. Special emphasis was laid on having a small number of learnable parameters. Every audio segment was trimmed or zero padded to  s. For feature extraction, this segment was represented using  dimensional MFCC features per frame, and a matrix of 15 × 302 frames was obtained. A cascade of a CNN and fully connected layers, with max-pooling and ReLU non-linearities, was used in the neural network architecture. For data augmentation, the team used the audiomentations tool. The classifier was trained using binary cross entropy (BCE) loss to output COVID probability score. The team did not report performing any system combination unlike several other participating teams.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>26466</offset><text>T-2: NUS-Mornin system</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26489</offset><text>The team focused  on using the residual network (ResNet) model with spectrogram images as features. To overcome the limitations of data scarcity and imbalance, the team resorted to three key strategies. Firstly, data augmentation was done by adding Gaussian noise to spectrograms. Secondly, focal loss function was used instead of binary cross entropy loss. Thirdly, the ResNet50 was pre-trained on ImageNet followed by fine-tuning on DiCOVA development set and an ensemble of four models was used to generate final COVID probability scores.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>27032</offset><text>T-3: UIUC SST system</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27053</offset><text>The team  used long short term memory (LSTM) models. With the motivation of generative modeling of mel-spectrogram for capturing informative features of cough, the team proposed using the auto-regressive predictive coding (APC) . The APC is used to pre-train the initial LSTM layers operating on the input mel-spectrogram. The additional layers of the full network, which was composed of BLSTM and fully connected layers, was trained using the DiCOVA development set. As the number of model parameters was high, the team also used data augmentation using COUGHVID dataset  and SpecAugment  tool. The binary cross entropy was chosen as the loss function. The final COVID-19 probability score was obtained as an average of several similar models, trained on development data subsets or sampled at different checkpoints during training.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>27891</offset><text>T-4: The North system</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>27913</offset><text>The team  explored classical machine learning models like random forests (RF), support vector machines (SVM), and multi-layer perceptron (MLP) rather than deep learning models. The features used were the  dimensional openSMILE functional features . The openSMILE features were z-score normalized to prevent feature domination. The hyper-parameters of the models were tuned to obtain the best results. The SVM models alone provided an AUC of 85.1% on the test data. The RF and the MLP scored an AUC of 82.15 and 75.65, respectively. The final scores were obtained by a weighted average of the probability scores from the RF and SVM models, with weights of 0.25 and 0.75, respectively.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>28599</offset><text>Top 4 teams: Fairness</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28621</offset><text>Here, we present a fairness analysis of the scores generated by the top 4 teams. We particularly focus on gender-wise and age-wise performance on the test set. Fig. 6 depicts this performance. Interestingly, all the four teams gave a better performance for female subjects. Similarly, the test dataset was divided into two groups based on subjects with age  and age  40. Here, the top two teams had a considerably higher AUC for age  40 subjects, while T-3 had a lower AUC for this age group and T-4 had the highest. In summary, the performance of top four teams did not reflect the bias in the development data (70% male subjects, and largely in age  40 group; see Fig. 2).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>29298</offset><text>Top 4 teams: Score fusion</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29324</offset><text>The systems from the top four teams differ in terms of features, model architectures, and data augmentation strategies. We consider a simple arithmetic mean fusion of the scores from top  teams. Let ,  and , be the COVID probability score predicted by the th team submission for the th subject in the test data. Here,  denotes the number of subjects in the test set, and , denoting the number of top teams, is four. The scores are first calibrated by correcting for the range as follows. where  and . The fused scores are obtained as, The ROC obtained using these prediction scores is denoted by Fusion in Fig. 7. The Fusion system ROC gives an AUC of 95.10%, a significant improvement over each of the individual system results. Table 4 depicts the sensitivity of the top four systems, the fusion, and baseline (MLP) at % specificity. The fused model surpasses all the other models and achieves a sensitivity of 70.7%.</text></passage><passage><infon key="file">gr7_lrg.jpg</infon><infon key="id">fig7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30247</offset><text>Illustration of ROCs obtained on the test set for the top four teams. The ROC associated with the hypothetical score fusion system obtained using the top four teams is also shown.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>30427</offset><text>Track-2: Systems overview</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30453</offset><text>This track was an exploratory track. It did not feature a leaderboard and did not require system report submission to the organizers. Hence, we have a summary of explorations carried by two teams only (only two reports were available with the details on Track-2 submission).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30728</offset><text> performed a detailed analysis on the COVID detection performance obtained with different spectral features for each of the three sound categories, namely, breathing, vowel [i] and counting. The authors explored acoustical features included MFCCs, Gaussian mixture model (GMM) based super vectors, formant frequency features, fundamental frequency values and its harmonics. The study suggests formant frequency features as the best performing feature for the binary task. Further, complimentary information is present in the three sound categories. A fusion of probability scores from each sound category, for each subject, gave an AUC 73.4% on validation folds and 71.7% AUC on the test (same as eval) dataset.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>31440</offset><text> explored the estimates of breathing patterns, obtained from different sound categories, for COVID-19 detection. Towards this, an encoder which predicts breathing pattern from speech signals was designed using a subset of UCL Speech Breath Monitoring (UCL-SBM) database . This pre-trained encoder is then used to predict the breathing patterns from breathing, vowel-[i], and counting sound categories, separately. The estimated breathing patterns are then used as feature vectors to train a decoder model to predict COVID-19 status. Interestingly, the breathing features performed superior to MFCCs for vowel-e and counting sound categories. Further, a combination of breathing and MFCCs features performed better than either one of these features. Across the three sound categories provided in Track-2, the average validation AUC ranged between  and the test AUC ranged between .</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>32322</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>32333</offset><text>Challenge accomplishments</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32359</offset><text>The challenge problem statement for Track-1 required the design of a binary classifier. A clear problem statement, with a well-defined evaluation metric (AUC), encouraged a significant number of registrations. This included  plus teams from around the globe, with a good representation from both industry and academia. The  teams which completed the challenge came from  different countries. Additionally,  teams associated themselves with industry. Among the submissions,  out of the  teams exhibited a performance well above the baseline system AUC (see Fig. 4(b)).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>32928</offset><text>Altogether, the challenge provided a platform for researchers to explore a healthcare problem of immense and timely societal impact. The results indicate potential in using acoustics for COVID-19 POCT development. The challenge turnaround time was  days, and the progress made by different teams in this short time span highlighted their efforts. Eleven studies pursued in this challenge , after going through the peer review process, were presented at the DiCOVA Special Session, Interspeech 2021 Conference (on  Aug 2021).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>33456</offset><text>The World Health Organization (WHO) has stated that a sensitivity  (at a specificity ) is necessary for an acceptable POCT tool . The top four teams fell short of this benchmark (see Table 4), indicating that there is scope for further development in future. Interestingly, a simple combination of the scores from the systems of these teams achieves a performance more closer to this benchmark. This suggests some ways to reap advantage via collaboration between multiple teams for improved tool development. The development of such an acoustic based diagnostic tool for COVID-19 diagnosis would offer multiple advantages in terms of speed, cost, portability, and accuracy.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>34132</offset><text>Limitations and future scope</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34161</offset><text>The challenge, being first of its kind, also had its own limitations. We discuss some of these below.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34263</offset><text>The development dataset had class imbalance, with a majority of the samples belonging to the non-COVID class. Although the imbalance reflects the prevalence of the infection in the population, it will be ideal to improve the balance in future challenges. The Coswara dataset  developed by our team is being regularly updated with more samples. As of August 2021, it contains data from close to  COVID-19 positive individuals and  non-COVID individuals.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>34717</offset><text>A majority of the DiCOVA dataset samples came from India. While the cultural dependence of cough and breathing is not well established, it will be ideal to evaluate the performance on datasets collected from multiple geographical sites. Towards this, future challenges can include demographically balanced datasets, with close collaborations between multiple sites involved in the data collection efforts.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35123</offset><text>The task involved in the challenge simplified to a binary classification setting. However, in a practical scenario, there are multiple respiratory ailments resulting from bacterial, fungal, or viral infections, with each condition potentially leaving a unique bio-marker. The future challenges can target multi-class categorization. This will also widen the usability of the tool.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35504</offset><text>The data did not contain information regarding the progression of the disease (or the time elapsed since the positive COVID-19 test). Also, the subjects in the “recovered from COVID-19” and “exposed to COVID-19 patient” categories were excluded in the challenge dataset. The leaderboard and system highlights reported were limited to the cough recordings only. As seen in Table 2, analysis using breathing and speech signals can also yield performance results comparable to those observed in cough recordings. In addition, the Coswara tool  also records the symptom data from participants. Using a combination of various sound categories and symptoms in developing a COVID-19 detection tool might further push the detection performance.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36251</offset><text>In the DiCOVA challenge, the performance ranking of the teams was based on AUC-ROC metric. This conveyed the model’s ability to perform binary classification of COVID and non-COVID subjects. However, the challenge did not emphasize model interpretability and explainability as key requirements. In a healthcare scenario, the interpretability of the model decisions may be as important as the accuracy. Hence, future challenges should encourage this aspect. For example, a recent work by  proposes an ensemble framework for quantifying decision uncertainty. Multiple classification models are developed and a disagreement across the learned model during testing phase is used as a measure of uncertainty in decision. In future, it is also important to focus on reproducibility of the models, and lower memory and computational foot-prints as these will benefit design and deployment of tool in mobile devices.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>37163</offset><text>Recently, on 12 Aug 2021, we launched the Second DiCOVA Challenge9 which attempts to circumvent some of the above limitations. It features three sound categories, namely, breathing, cough and speech, and a leaderboard for each category. In a separate track, the participants are also encouraged to fuse scores or decisions from classifiers designed on multiple sound categories. Further, in comparison to the first DiCOVA Challenge, the dataset is larger in size.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>37627</offset><text>Declaration of Competing Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>37661</offset><text>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>37832</offset><text>References</text></passage><passage><infon key="fpage">2448</infon><infon key="issue">11</infon><infon key="lpage">2462</infon><infon key="name_0">surname:Abeyratne;given-names:U.R.</infon><infon key="name_1">surname:Swarnkar;given-names:V.</infon><infon key="name_2">surname:Setyati;given-names:A.</infon><infon key="name_3">surname:Triasih;given-names:R.</infon><infon key="pub-id_pmid">23743558</infon><infon key="section_type">REF</infon><infon key="source">Ann. Biomed. Eng.</infon><infon key="type">ref</infon><infon key="volume">41</infon><infon key="year">2013</infon><offset>37843</offset><text>Cough sound analysis can rapidly diagnose childhood pneumonia</text></passage><passage><infon key="fpage">5</infon><infon key="lpage">9</infon><infon key="name_0">surname:Agbley;given-names:B.L.Y.</infon><infon key="name_1">surname:Li;given-names:J.</infon><infon key="name_2">surname:Haq;given-names:A.</infon><infon key="name_3">surname:Cobbinah;given-names:B.</infon><infon key="name_4">surname:Kulevome;given-names:D.</infon><infon key="name_5">surname:Agbefu;given-names:P.A.</infon><infon key="name_6">surname:Eleeza;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">17th Intl. Computer Conference on Wavelet Active Media Technology and Information Processing)</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>37905</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>37906</offset><text>Amiriparian, S., Gerczuk, M., Ottl, S., Cummins, N., Freitag, M., Pugachevskiy, S., Baird, A., Schuller, B., 2017. Snore sound classification using image-based deep spectrum features. In: Proc. Interspeech, pp. 3512–3516.</text></passage><passage><infon key="fpage">1</infon><infon key="name_0">surname:Andreu-Perez;given-names:J.</infon><infon key="name_1">surname:Perez-Espinosa;given-names:H.</infon><infon key="name_10">surname:Torres;given-names:A.</infon><infon key="name_11">surname:Alberto Reyes-Garcia;given-names:C.</infon><infon key="name_12">surname:Ali;given-names:Z.</infon><infon key="name_13">surname:Rivas;given-names:F.</infon><infon key="name_2">surname:Timonet;given-names:E.</infon><infon key="name_3">surname:Kiani;given-names:M.</infon><infon key="name_4">surname:Giron-Perez;given-names:M.I.</infon><infon key="name_5">surname:Benitez-Trinidad;given-names:A.B.</infon><infon key="name_6">surname:Jarchi;given-names:D.</infon><infon key="name_7">surname:Rosales;given-names:A.</infon><infon key="name_8">surname:Gkatzoulis;given-names:N.</infon><infon key="name_9">surname:Reyes-Galaviz;given-names:O.F.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Serv. Comput.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2021</infon><offset>38130</offset><text>A generic deep learning based cough analysis system from clinically validated samples for point-of-need COVID-19 test and severity levels</text></passage><passage><infon key="fpage">951</infon><infon key="lpage">955</infon><infon key="name_0">surname:Avila;given-names:F.</infon><infon key="name_1">surname:Poorjam;given-names:A.H.</infon><infon key="name_2">surname:Mittal;given-names:D.</infon><infon key="name_3">surname:Dognin;given-names:C.</infon><infon key="name_4">surname:Muguli;given-names:A.</infon><infon key="name_5">surname:Kumar;given-names:R.</infon><infon key="name_6">surname:Chetupalli;given-names:S.R.</infon><infon key="name_7">surname:Ganapathy;given-names:S.</infon><infon key="name_8">surname:Singh;given-names:M.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-2197</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38268</offset></passage><passage><infon key="comment">arXiv:2106.02348</infon><infon key="name_0">surname:Banerjee;given-names:A.</infon><infon key="name_1">surname:Nilhani;given-names:A.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38269</offset></passage><passage><infon key="fpage">946</infon><infon key="lpage">950</infon><infon key="name_0">surname:Bhosale;given-names:S.</infon><infon key="name_1">surname:Tiwari;given-names:U.</infon><infon key="name_2">surname:Chakraborty;given-names:R.</infon><infon key="name_3">surname:Kopparapu;given-names:S.K.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1249</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38270</offset></passage><passage><infon key="issue">4</infon><infon key="name_0">surname:Botha;given-names:G.</infon><infon key="name_1">surname:Theron;given-names:G.</infon><infon key="name_2">surname:Warren;given-names:R.</infon><infon key="name_3">surname:Klopper;given-names:M.</infon><infon key="name_4">surname:Dheda;given-names:K.</infon><infon key="name_5">surname:Van Helden;given-names:P.</infon><infon key="name_6">surname:Niesler;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">Physiol. Meas.</infon><infon key="type">ref</infon><infon key="volume">39</infon><infon key="year">2018</infon><offset>38271</offset><text>Detection of tuberculosis by automatic cough sound analysis</text></passage><passage><infon key="fpage">3474</infon><infon key="lpage">3484</infon><infon key="name_0">surname:Brown;given-names:C.</infon><infon key="name_1">surname:Chauhan;given-names:J.</infon><infon key="name_2">surname:Grammenos;given-names:A.</infon><infon key="name_3">surname:Han;given-names:J.</infon><infon key="name_4">surname:Hasthanasombat;given-names:A.</infon><infon key="name_5">surname:Spathis;given-names:D.</infon><infon key="name_6">surname:Xia;given-names:T.</infon><infon key="name_7">surname:Cicuta;given-names:P.</infon><infon key="name_8">surname:Mascolo;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proc. 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>38331</offset></passage><passage><infon key="comment">URL https://dicova2021.github.io/docs/reports/team_Jemery_DiCOVA_2021_Challenge_System_Report.pdf</infon><infon key="name_0">surname:Chang;given-names:J.</infon><infon key="name_1">surname:Cui;given-names:S.</infon><infon key="name_2">surname:Feng;given-names:M.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38332</offset></passage><passage><infon key="fpage">1</infon><infon key="lpage">24</infon><infon key="name_0">surname:Chen;given-names:C.</infon><infon key="name_1">surname:Liaw;given-names:A.</infon><infon key="name_2">surname:Breiman;given-names:L.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2004</infon><offset>38333</offset></passage><passage><infon key="fpage">154087</infon><infon key="lpage">154094</infon><infon key="name_0">surname:Cohen-McFarlane;given-names:M.</infon><infon key="name_1">surname:Goubran;given-names:R.</infon><infon key="name_2">surname:Knoefel;given-names:F.</infon><infon key="pub-id_pmid">34786285</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>38334</offset><text>Novel coronavirus cough database: NoCoCoDa</text></passage><passage><infon key="issue">3</infon><infon key="name_0">surname:Corman;given-names:V.M.</infon><infon key="name_1">surname:Landt;given-names:O.</infon><infon key="name_2">surname:Kaiser;given-names:M.</infon><infon key="name_3">surname:Molenkamp;given-names:R.</infon><infon key="name_4">surname:Meijer;given-names:A.</infon><infon key="name_5">surname:Chu;given-names:D.K.</infon><infon key="name_6">surname:Bleicker;given-names:T.</infon><infon key="name_7">surname:Brünink;given-names:S.</infon><infon key="name_8">surname:Schneider;given-names:J.</infon><infon key="name_9">surname:Schmidt;given-names:M.L.</infon><infon key="section_type">REF</infon><infon key="source">Eurosurveillance</infon><infon key="type">ref</infon><infon key="volume">25.2000045</infon><infon key="year">2020</infon><offset>38377</offset><text>Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR</text></passage><passage><infon key="fpage">3852</infon><infon key="lpage">3856</infon><infon key="name_0">surname:Cramer;given-names:J.</infon><infon key="name_1">surname:Wu;given-names:H.-H.</infon><infon key="name_2">surname:Salamon;given-names:J.</infon><infon key="name_3">surname:Bello;given-names:J.P.</infon><infon key="section_type">REF</infon><infon key="source">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>38445</offset></passage><passage><infon key="fpage">921</infon><infon key="lpage">925</infon><infon key="name_0">surname:Das;given-names:R.K.</infon><infon key="name_1">surname:Madhavi;given-names:M.</infon><infon key="name_2">surname:Li;given-names:H.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-497</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38446</offset></passage><passage><infon key="fpage">357</infon><infon key="issue">4</infon><infon key="lpage">366</infon><infon key="name_0">surname:Davis;given-names:S.</infon><infon key="name_1">surname:Mermelstein;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Acoust. Speech Signal Process.</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">1980</infon><offset>38447</offset><text>Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</text></passage><passage><infon key="fpage">931</infon><infon key="lpage">935</infon><infon key="name_0">surname:Deshpande;given-names:G.</infon><infon key="name_1">surname:Schuller;given-names:B.W.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-811</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38555</offset></passage><passage><infon key="comment">arXiv:2105.10619</infon><infon key="name_0">surname:Elizalde;given-names:B.</infon><infon key="name_1">surname:Tompkins;given-names:D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38556</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>38557</offset><text>Eyben, F., Wöllmer, M., Schuller, B., 2010. Opensmile: The munich versatile and fast open-source audio feature extractor. In: Proc. 18th ACM Intl. Conf. Multimedia, pp. 1459–1462.</text></passage><passage><infon key="fpage">926</infon><infon key="lpage">930</infon><infon key="name_0">surname:Harvill;given-names:J.</infon><infon key="name_1">surname:Wani;given-names:Y.R.</infon><infon key="name_2">surname:Hasegawa-Johnson;given-names:M.</infon><infon key="name_3">surname:Ahuja;given-names:N.</infon><infon key="name_4">surname:Beiser;given-names:D.</infon><infon key="name_5">surname:Chestek;given-names:D.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-799</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38740</offset></passage><passage><infon key="fpage">2833</infon><infon key="issue">14</infon><infon key="name_0">surname:Hee;given-names:H.I.</infon><infon key="name_1">surname:Balamurali;given-names:B.</infon><infon key="name_2">surname:Karunakaran;given-names:A.</infon><infon key="name_3">surname:Herremans;given-names:D.</infon><infon key="name_4">surname:Teoh;given-names:O.H.</infon><infon key="name_5">surname:Lee;given-names:K.P.</infon><infon key="name_6">surname:Teng;given-names:S.S.</infon><infon key="name_7">surname:Lui;given-names:S.</infon><infon key="name_8">surname:Chen;given-names:J.M.</infon><infon key="section_type">REF</infon><infon key="source">Appl. Sci.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2019</infon><offset>38741</offset><text>Development of machine learning for asthmatic and healthy voluntary cough sounds: a proof of concept study</text></passage><passage><infon key="fpage">131</infon><infon key="lpage">135</infon><infon key="name_0">surname:Hershey;given-names:S.</infon><infon key="name_1">surname:Chaudhuri;given-names:S.</infon><infon key="name_10">surname:Slaney;given-names:M.</infon><infon key="name_11">surname:Weiss;given-names:R.J.</infon><infon key="name_12">surname:Wilson;given-names:K.</infon><infon key="name_2">surname:Ellis;given-names:D.P.W.</infon><infon key="name_3">surname:Gemmeke;given-names:J.F.</infon><infon key="name_4">surname:Jansen;given-names:A.</infon><infon key="name_5">surname:Moore;given-names:R.C.</infon><infon key="name_6">surname:Plakal;given-names:M.</infon><infon key="name_7">surname:Platt;given-names:D.</infon><infon key="name_8">surname:Saurous;given-names:R.A.</infon><infon key="name_9">surname:Seybold;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>38848</offset></passage><passage><infon key="name_0">surname:Imran;given-names:A.</infon><infon key="name_1">surname:Posokhova;given-names:I.</infon><infon key="name_2">surname:Qureshi;given-names:H.N.</infon><infon key="name_3">surname:Masood;given-names:U.</infon><infon key="name_4">surname:Riaz;given-names:M.S.</infon><infon key="name_5">surname:Ali;given-names:K.</infon><infon key="name_6">surname:John;given-names:C.N.</infon><infon key="name_7">surname:Hussain;given-names:M.I.</infon><infon key="name_8">surname:Nabeel;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Inform. Med. Unlocked</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>38849</offset><text>AI4Covid-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app</text></passage><passage><infon key="name_0">surname:Jaitly;given-names:N.</infon><infon key="name_1">surname:Hinton;given-names:G.E.</infon><infon key="section_type">REF</infon><infon key="source">International Conference on Machine Learning ICML, Vol. 117</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>38938</offset></passage><passage><infon key="fpage">906</infon><infon key="lpage">910</infon><infon key="name_0">surname:Kamble;given-names:M.R.</infon><infon key="name_1">surname:Gonzalez-Lopez;given-names:J.A.</infon><infon key="name_10">surname:Gomez;given-names:A.M.</infon><infon key="name_11">surname:Evans;given-names:N.</infon><infon key="name_12">surname:Zuluaga;given-names:M.A.</infon><infon key="name_13">surname:Todisco;given-names:M.</infon><infon key="name_2">surname:Grau;given-names:T.</infon><infon key="name_3">surname:Espin;given-names:J.M.</infon><infon key="name_4">surname:Cascioli;given-names:L.</infon><infon key="name_5">surname:Huang;given-names:Y.</infon><infon key="name_6">surname:Gomez-Alanis;given-names:A.</infon><infon key="name_7">surname:Patino;given-names:J.</infon><infon key="name_8">surname:Font;given-names:R.</infon><infon key="name_9">surname:Peinado;given-names:A.M.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1062</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38939</offset></passage><passage><infon key="fpage">2607</infon><infon key="lpage">2611</infon><infon key="name_0">surname:Kamble;given-names:M.R.</infon><infon key="name_1">surname:Patil;given-names:H.A.</infon><infon key="section_type">REF</infon><infon key="source">In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>38940</offset></passage><passage><infon key="fpage">911</infon><infon key="lpage">915</infon><infon key="name_0">surname:Karas;given-names:V.</infon><infon key="name_1">surname:Schuller;given-names:B.W.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1267</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>38941</offset></passage><passage><infon key="name_0">surname:Ke;given-names:G.</infon><infon key="name_1">surname:Meng;given-names:Q.</infon><infon key="name_10">surname:Bengio;given-names:S.</infon><infon key="name_11">surname:Wallach;given-names:H.</infon><infon key="name_12">surname:Fergus;given-names:R.</infon><infon key="name_13">surname:Vishwanathan;given-names:S.</infon><infon key="name_14">surname:Garnett;given-names:R.</infon><infon key="name_2">surname:Finley;given-names:T.</infon><infon key="name_3">surname:Wang;given-names:T.</infon><infon key="name_4">surname:Chen;given-names:W.</infon><infon key="name_5">surname:Ma;given-names:W.</infon><infon key="name_6">surname:Ye;given-names:Q.</infon><infon key="name_7">surname:Liu;given-names:T.-Y.</infon><infon key="name_8">surname:Guyon;given-names:I.</infon><infon key="name_9">surname:Luxburg;given-names:U.V.</infon><infon key="section_type">REF</infon><infon key="source">Advances in Neural Information Processing Systems, Vol. 30</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>38942</offset></passage><passage><infon key="fpage">275</infon><infon key="lpage">281</infon><infon key="name_0">surname:Laguarta;given-names:J.</infon><infon key="name_1">surname:Hueto;given-names:F.</infon><infon key="name_2">surname:Subirana;given-names:B.</infon><infon key="pub-id_pmid">34812418</infon><infon key="section_type">REF</infon><infon key="source">IEEE Open J. Eng. Med. Biol.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2020</infon><offset>38943</offset><text>COVID-19 artificial intelligence diagnosis using only cough recordings</text></passage><passage><infon key="fpage">250</infon><infon key="issue">1</infon><infon key="lpage">256</infon><infon key="name_0">surname:Le;given-names:T.T.</infon><infon key="name_1">surname:Fu;given-names:W.</infon><infon key="name_2">surname:Moore;given-names:J.H.</infon><infon key="section_type">REF</infon><infon key="source">Bioinformatics</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2019</infon><offset>39014</offset><text>Scaling tree-based automated machine learning to biomedical big data with a feature set selector</text></passage><passage><infon key="fpage">1449</infon><infon key="issue">3</infon><infon key="lpage">1458</infon><infon key="name_0">surname:Li;given-names:J.</infon><infon key="name_1">surname:Huang;given-names:D.Q.</infon><infon key="name_2">surname:Zou;given-names:B.</infon><infon key="name_3">surname:Yang;given-names:H.</infon><infon key="name_4">surname:Hui;given-names:W.Z.</infon><infon key="name_5">surname:Rui;given-names:F.</infon><infon key="name_6">surname:Yee;given-names:N.T.S.</infon><infon key="name_7">surname:Liu;given-names:C.</infon><infon key="name_8">surname:Nerurkar;given-names:S.N.</infon><infon key="name_9">surname:Kai;given-names:J.C.Y.</infon><infon key="pub-id_pmid">32790106</infon><infon key="section_type">REF</infon><infon key="source">J. Med. Virol.</infon><infon key="type">ref</infon><infon key="volume">93</infon><infon key="year">2021</infon><offset>39111</offset><text>Epidemiology of COVID-19: A systematic review and meta-analysis of clinical characteristics, risk factors, and outcomes</text></passage><passage><infon key="fpage">2999</infon><infon key="lpage">3007</infon><infon key="name_0">surname:Lin;given-names:T.-Y.</infon><infon key="name_1">surname:Goyal;given-names:P.</infon><infon key="name_2">surname:Girshick;given-names:R.</infon><infon key="name_3">surname:He;given-names:K.</infon><infon key="name_4">surname:Dollár;given-names:P.</infon><infon key="section_type">REF</infon><infon key="source">2017 IEEE International Conference on Computer Vision (ICCV)</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>39231</offset></passage><passage><infon key="comment">URL https://dicova2021.github.io/docs/reports/team_Brogrammers_DiCOVA_2021_Challenge_System_Report.pdf</infon><infon key="name_0">surname:Mahanta;given-names:S.K.</infon><infon key="name_1">surname:Jain;given-names:S.</infon><infon key="name_2">surname:Kaushik;given-names:D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39232</offset></passage><passage><infon key="fpage">941</infon><infon key="lpage">945</infon><infon key="name_0">surname:Mallol-Ragolta;given-names:A.</infon><infon key="name_1">surname:Cuesta;given-names:H.</infon><infon key="name_2">surname:Gómez;given-names:E.</infon><infon key="name_3">surname:Schuller;given-names:B.W.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1052</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39233</offset></passage><passage><infon key="fpage">901</infon><infon key="lpage">905</infon><infon key="name_0">surname:Muguli;given-names:A.</infon><infon key="name_1">surname:Pinto;given-names:L.</infon><infon key="name_10">surname:Ramoji;given-names:S.</infon><infon key="name_11">surname:Nanda;given-names:V.</infon><infon key="name_2">surname:Nirmala;given-names:R.</infon><infon key="name_3">surname:Sharma;given-names:N.</infon><infon key="name_4">surname:Krishnan;given-names:P.</infon><infon key="name_5">surname:Ghosh;given-names:P.K.</infon><infon key="name_6">surname:Kumar;given-names:R.</infon><infon key="name_7">surname:Bhat;given-names:S.</infon><infon key="name_8">surname:Chetupalli;given-names:S.R.</infon><infon key="name_9">surname:Ganapathy;given-names:S.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-74</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39234</offset></passage><passage><infon key="comment">arXiv preprint arXiv:1807.03748</infon><infon key="name_0">surname:Oord;given-names:A.v.d.</infon><infon key="name_1">surname:Li;given-names:Y.</infon><infon key="name_2">surname:Vinyals;given-names:O.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39235</offset></passage><passage><infon key="fpage">1</infon><infon key="issue">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Orlandic;given-names:L.</infon><infon key="name_1">surname:Teijeiro;given-names:T.</infon><infon key="name_2">surname:Atienza;given-names:D.</infon><infon key="pub-id_pmid">33414438</infon><infon key="section_type">REF</infon><infon key="source">Sci. Data</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2021</infon><offset>39236</offset><text>The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms</text></passage><passage><infon key="comment">arXiv preprint arXiv:1904.08779</infon><infon key="name_0">surname:Park;given-names:D.S.</infon><infon key="name_1">surname:Chan;given-names:W.</infon><infon key="name_2">surname:Zhang;given-names:Y.</infon><infon key="name_3">surname:Chiu;given-names:C.-C.</infon><infon key="name_4">surname:Zoph;given-names:B.</infon><infon key="name_5">surname:Cubuk;given-names:E.D.</infon><infon key="name_6">surname:Le;given-names:Q.V.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>39336</offset></passage><passage><infon key="fpage">2825</infon><infon key="lpage">2830</infon><infon key="name_0">surname:Pedregosa;given-names:F.</infon><infon key="name_1">surname:Varoquaux;given-names:G.</infon><infon key="name_10">surname:Vanderplas;given-names:J.</infon><infon key="name_11">surname:Passos;given-names:A.</infon><infon key="name_12">surname:Cournapeau;given-names:D.</infon><infon key="name_13">surname:Brucher;given-names:M.</infon><infon key="name_14">surname:Perrot;given-names:M.</infon><infon key="name_15">surname:Duchesnay;given-names:E.</infon><infon key="name_2">surname:Gramfort;given-names:A.</infon><infon key="name_3">surname:Michel;given-names:V.</infon><infon key="name_4">surname:Thirion;given-names:B.</infon><infon key="name_5">surname:Grisel;given-names:O.</infon><infon key="name_6">surname:Blondel;given-names:M.</infon><infon key="name_7">surname:Prettenhofer;given-names:P.</infon><infon key="name_8">surname:Weiss;given-names:R.</infon><infon key="name_9">surname:Dubourg;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">J. Mach. Learn. Res.</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2011</infon><offset>39337</offset><text>Scikit-learn: Machine learning in python</text></passage><passage><infon key="name_0">surname:Peeling;given-names:R.W.</infon><infon key="name_1">surname:Olliaro;given-names:P.L.</infon><infon key="name_2">surname:Boeras;given-names:D.I.</infon><infon key="name_3">surname:Fongwen;given-names:N.</infon><infon key="section_type">REF</infon><infon key="source">Lancet Infect. Dis.</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39378</offset><text>Scaling up COVID-19 rapid antigen tests: promises and challenges</text></passage><passage><infon key="comment">https://github.com/tensorflow/models/tree/master/research/audioset/yamnet [Online; accessed on 15-June-2021]</infon><infon key="name_0">surname:Plakal;given-names:M.</infon><infon key="name_1">surname:Ellis;given-names:D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>39443</offset></passage><passage><infon key="issue">9</infon><infon key="name_0">surname:Pramono;given-names:R.X.A.</infon><infon key="name_1">surname:Imtiaz;given-names:S.A.</infon><infon key="name_2">surname:Rodriguez-Villegas;given-names:E.</infon><infon key="section_type">REF</infon><infon key="source">PLoS One</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2016</infon><offset>39444</offset><text>A cough-based algorithm for automatic diagnosis of pertussis</text></passage><passage><infon key="fpage">936</infon><infon key="lpage">940</infon><infon key="name_0">surname:Ritwik;given-names:K.V.S.</infon><infon key="name_1">surname:Kalluri;given-names:S.B.</infon><infon key="name_2">surname:Vijayasenan;given-names:D.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1031</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39505</offset></passage><passage><infon key="fpage">2104</infon><infon key="issue">11</infon><infon key="lpage">2114</infon><infon key="name_0">surname:Schaefer;given-names:I.-M.</infon><infon key="name_1">surname:Padera;given-names:R.F.</infon><infon key="name_2">surname:Solomon;given-names:I.H.</infon><infon key="name_3">surname:Kanjilal;given-names:S.</infon><infon key="name_4">surname:Hammer;given-names:M.M.</infon><infon key="name_5">surname:Hornick;given-names:J.L.</infon><infon key="name_6">surname:Sholl;given-names:L.M.</infon><infon key="pub-id_pmid">32561849</infon><infon key="section_type">REF</infon><infon key="source">Mod. Pathol.</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2020</infon><offset>39506</offset><text>In situ detection of SARS-CoV-2 in lungs and airways of patients with COVID-19</text></passage><passage><infon key="fpage">2042</infon><infon key="lpage">2046</infon><infon key="name_0">surname:Schuller;given-names:B.W.</infon><infon key="name_1">surname:Batliner;given-names:A.</infon><infon key="name_10">surname:Baumeister;given-names:H.</infon><infon key="name_11">surname:MacIntyre;given-names:A.D.</infon><infon key="name_12">surname:Hantke;given-names:S.</infon><infon key="name_2">surname:Bergler;given-names:C.</infon><infon key="name_3">surname:Messner;given-names:E.-M.</infon><infon key="name_4">surname:Hamilton;given-names:A.</infon><infon key="name_5">surname:Amiriparian;given-names:S.</infon><infon key="name_6">surname:Baird;given-names:A.</infon><infon key="name_7">surname:Rizos;given-names:G.</infon><infon key="name_8">surname:Schmitt;given-names:M.</infon><infon key="name_9">surname:Stappen;given-names:L.</infon><infon key="pub-id_doi">10.21437/Interspeech.2020-32</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2020</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>39585</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>39586</offset><text>Sharma, N., Krishnan, P., Kumar, R., Ramoji, S., Chetupalli, S.R., Nirmala, R., Ghosh, P.K., Ganapathy, S., 2020. Coswara – A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis. In: Proc. Interspeech, pp. 4811–4815.</text></passage><passage><infon key="comment">URL https://dicova2021.github.io/docs/reports/team_samsung_DiCOVA_Technical_Report_IS2021.pdf</infon><infon key="name_0">surname:Singh;given-names:V.P.</infon><infon key="name_1">surname:Kumar;given-names:S.</infon><infon key="name_2">surname:Jha;given-names:R.S.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39825</offset></passage><passage><infon key="fpage">697</infon><infon key="issue">6</infon><infon key="lpage">708</infon><infon key="name_0">surname:Smith;given-names:J.O.</infon><infon key="name_1">surname:Abel;given-names:J.S.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Speech Audio Process.</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">1999</infon><offset>39826</offset><text>Bark and ERB bilinear transforms</text></passage><passage><infon key="fpage">5329</infon><infon key="lpage">5333</infon><infon key="name_0">surname:Snyder;given-names:D.</infon><infon key="name_1">surname:Garcia-Romero;given-names:D.</infon><infon key="name_2">surname:Sell;given-names:G.</infon><infon key="name_3">surname:Povey;given-names:D.</infon><infon key="name_4">surname:Khudanpur;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>39859</offset></passage><passage><infon key="fpage">916</infon><infon key="lpage">920</infon><infon key="name_0">surname:Södergren;given-names:I.</infon><infon key="name_1">surname:Nodeh;given-names:M.P.</infon><infon key="name_2">surname:Chhipa;given-names:P.C.</infon><infon key="name_3">surname:Nikolaidou;given-names:K.</infon><infon key="name_4">surname:Kovács;given-names:G.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-2191</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39860</offset></passage><passage><infon key="comment">https://www.who.int/docs/default-source/blue-print/who-rd-blueprint-diagnostics-tpp-final-v1-0-28-09-jc-ppc-final-cmp92616a80172344e4be0edf315b582021.pdf?sfvrsn=e3747f20_1&amp;download=true [Online; accessed 20-May-2021]</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>39861</offset></passage><passage><infon key="comment">https://github.com/virufy/virufy-data [Online; accessed 04-Jun-2021]</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39862</offset></passage><passage><infon key="fpage">2951</infon><infon key="lpage">2955</infon><infon key="name_0">surname:Xia;given-names:T.</infon><infon key="name_1">surname:Han;given-names:J.</infon><infon key="name_2">surname:Qendro;given-names:L.</infon><infon key="name_3">surname:Dang;given-names:T.</infon><infon key="name_4">surname:Mascolo;given-names:C.</infon><infon key="pub-id_doi">10.21437/Interspeech.2021-1320</infon><infon key="section_type">REF</infon><infon key="source">Proc. Interspeech 2021</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>39863</offset></passage><passage><infon key="comment">arXiv preprint arXiv:1710.09412</infon><infon key="name_0">surname:Zhang;given-names:H.</infon><infon key="name_1">surname:Cisse;given-names:M.</infon><infon key="name_2">surname:Dauphin;given-names:Y.N.</infon><infon key="name_3">surname:Lopez-Paz;given-names:D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>39864</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>39865</offset><text>https://dicova2021.github.io/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>39896</offset><text>https://competitions.codalab.org/competitions/29640.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>39949</offset><text>https://coswara.iisc.ac.in/.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>39978</offset><text>https://github.com/iiscleap/Coswara-Data.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>40020</offset><text>https://github.com/dicova2021/Track-1-baseline.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>40068</offset><text>The system reports are available at https://dicova2021.github.io/#reports.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>40143</offset><text>https://github.com/iver56/audiomentations.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">footnote</infon><offset>40186</offset><text>https://dicovachallenge.github.io/.</text></passage></document></collection>
