<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20211005</date><key>pmc.key</key><document><id>8444080</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.7717/peerj-cs.704</infon><infon key="article-id_pmc">8444080</infon><infon key="article-id_pmid">34604526</infon><infon key="article-id_publisher-id">cs-704</infon><infon key="elocation-id">e704</infon><infon key="kwd">Mobile augmented reality BLE and PDR fusion Indoor map Indoor localization Geo-visualization</infon><infon key="license">This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</infon><infon key="name_0">surname:Ma;given-names:Wei</infon><infon key="name_1">surname:Zhang;given-names:Shuai</infon><infon key="name_2">surname:Huang;given-names:Jincai</infon><infon key="name_3">surname:Garg;given-names:Lalit</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">7</infon><infon key="year">2021</infon><offset>0</offset><text>Mobile augmented reality based indoor map for improving geo-visualization</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>74</offset><text>Unlike traditional visualization methods, augmented reality (AR) inserts virtual objects and information directly into digital representations of the real world, which makes these objects and data more easily understood and interactive. The integration of AR and GIS is a promising way to display spatial information in context. However, most existing AR-GIS applications only provide local spatial information in a fixed location, which is exposed to a set of problems, limited legibility, information clutter and the incomplete spatial relationships. In addition, the indoor space structure is complex and GPS is unavailable, so that indoor AR systems are further impeded by the limited capacity of these systems to detect and display location and semantic information. To address this problem, the localization technique for tracking the camera positions was fused by Bluetooth low energy (BLE) and pedestrian dead reckoning (PDR). The multi-sensor fusion-based algorithm employs a particle filter. Based on the direction and position of the phone, the spatial information is automatically registered onto a live camera view. The proposed algorithm extracts and matches a bounding box of the indoor map to a real world scene. Finally, the indoor map and semantic information were rendered into the real world, based on the real-time computed spatial relationship between the indoor map and live camera view. Experimental results demonstrate that the average positioning error of our approach is 1.47 m, and 80% of proposed method error is within approximately 1.8 m. The positioning result can effectively support that AR and indoor map fusion technique links rich indoor spatial information to real world scenes. The method is not only suitable for traditional tasks related to indoor navigation, but it is also promising method for crowdsourcing data collection and indoor map reconstruction.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1972</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>1985</offset><text>Research background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2005</offset><text>Consumer applications, like smart phones integrated with GIS, could provide users with useful information. This geospatial information however, might be difficult to understand. Data pertaining to complex indoor environments is especially challenging for users with limited geographic knowledge, but visualization enhances situational awareness among users for a more satisfying experience. Thus, in either 2D to 3D representation, flexibility and realism requirements for spatial information visualization are becoming ever more demanding. In contrast to traditional visualization methods, AR combines virtual objects with a real scene, as an intuitive way to convey information that is otherwise difficult to transmit. The integration of AR and GIS promotes a more expressive way to display GIS data in context, closely related to information about the surrounding environment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2885</offset><text>Researchers have made some useful explorations in AR-GIS. proposed an AR system for description of Location-Based Services (LBSs) and Points of Interest (POIs), which supports users with disabilities. presented an AR system for infrared thermographic façade inspection. The system employed a third person perspective augmented view displayed and camera tracking based on image registration. Sun et al. offered an interactive method based on the linear characteristics to combine 3D GIS with outdoor AR. Data management between GIS databases and AR visualizations maintains an interactive data round-trip. Liu et al. integrated AR and Location-based social networks for enriching the GIS-science research agenda in data conflation and multimedia storytelling. However, there are still several open challenges for spatial information AR visualization, especially in indoor space where the structure layout complex and mobile phone sensors limited.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3832</offset><text>AR and GIS techniques have received increasing attention. Thus there is a need to better understand the particulars of AR interfaces in indoor environments beyond location-awareness affecting GIS data visualization and interaction performance in AR-GIS applications. Camera pose tracking is the keystone for accurate understanding of spatial relationships in AR visualization. To overcome the limitations of GPS indoors, many researchers have focused on the vision-based tracking methods, such as Structure from Motion (SFM) or Simultaneous Localization and Mapping (SLAM). These camera pose tracking methods rely heavily on the reference points. proposed a Walkability network-based Augmented Reality (WaNAR). But the method needs a man-made 3D drawing of indoor walkable space. Indoor environments often contain scenes without visual features such as bare walls or windows, thus the indoor accuracy is seriously limited. Thus, a light weight and accurate indoor camera pose tracking method suitable for AR is still an elusive goal.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4866</offset><text>The main challenge is that most existing AR applications are usually used for short periods of time and for specific purposes, they do not allow for a continuous, universal, context-aware AR experience, except that there is still a lack of effective organization in the dynamic spatial information visualization in AR-GIS system. It leads to spatial information clutter and limited visibility due to fixed location. Visualizing spatial data in an indoor AR environment requires special consideration because a disproportionate mixture of AR content or redundant information will mislead and confuse users. Therefore, organizing these dynamic spatial information and visual elements is another challenge.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>5570</offset><text>Aim of the study</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5587</offset><text>In this paper, the objective is optimizing and enriching the spatial information visualization in indoor environments. We bridge the indoor map and situational visualization with real world scene based on mobile augmented reality technique. Firstly, we divide an indoor map by a regular grid and build the index of map data to diminish redundant data and limit AR visualization within a certain range of the current location. At the camera tracking stage, we propose a novel multi-sensor fusion-based algorithm, which employs a particle filter to fuse BLE and PDR. Based on the direction and position of the phone, the extent of an indoor map visible in the camera view is extracted and the spatial entities calculated just for this area. Considering different poses of the mobile phone in users’ hand, we also designed changed forms of the AR view. After applying a coordinate transformation to the spatial entities, the fused AR-GIS system renders spatial information in relation to the area visible in the dynamic camera view.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6619</offset><text>Contributions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6633</offset><text>The contributions can be summarized as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6681</offset><text>We propose a novel AR-GIS method for indoor environments that fuses an indoor map with dynamic situated visualizations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6801</offset><text>We designed an online indoor positioning method that fuses the BLE and PDR for AR camera tracking.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6900</offset><text>We designed a flexible AR system that accessibly visualizes a variety of types of spatial information regardless of the pose of a mobile phone.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>7044</offset><text>Organization of the paper</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7070</offset><text>The organization of this paper is as follows. The related works are briefly reviewed in Section 2. Section 3 discussed the main methods. The experimental results and analysis are described in Section 4. The conclusions are then presented in Section 5. The Abbreviation table is shown in Table 1.</text></passage><passage><infon key="file">table-1.xml</infon><infon key="id">table-1</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>7366</offset><text>Abbreviation table.</text></passage><passage><infon key="file">table-1.xml</infon><infon key="id">table-1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Abbreviation&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Whole words&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;AR&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Augmented Reality&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BLE&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Bluetooth low energy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PDR&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Pedestrian dead reckoning&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GIS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Geographic Information System&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;LBSs&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Location-Based Services&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3D GIS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3-Dimensional Geographical Information System&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SFM&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Structure from Motion&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SLAM&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Simultaneous Localization and Mapping&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SIFT&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Scale-Invariant Feature Transform&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WaNAR&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Walkability network-based Augmented Reality&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DoF&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Degrees of freedom&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;GPS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Global Positioning System&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMU&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inertial Measurement Unit&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;WKNN&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Weight k-nearest neighbour&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RSS&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Received signal strength&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PF&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Particle filter&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CDF&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cumulative Distribution Function&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>7386</offset><text>Abbreviation	Whole words	 	AR	Augmented Reality	 	BLE	Bluetooth low energy	 	PDR	Pedestrian dead reckoning	 	GIS	Geographic Information System	 	LBSs	Location-Based Services	 	3D GIS	3-Dimensional Geographical Information System	 	SFM	Structure from Motion	 	SLAM	Simultaneous Localization and Mapping	 	SIFT	Scale-Invariant Feature Transform	 	WaNAR	Walkability network-based Augmented Reality	 	DoF	Degrees of freedom	 	GPS	Global Positioning System	 	IMU	Inertial Measurement Unit	 	WKNN	Weight k-nearest neighbour	 	RSS	Received signal strength	 	PF	Particle filter	 	CDF	Cumulative Distribution Function	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>7998</offset><text>Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8011</offset><text>AR has gradually emerged as a popular way to display LBSs. Hence, researchers are focused on AR techniques for GIS visualization in two major areas; camera tracking techniques for estimating camera poses and tracking target objects, and AR visualization to render virtual data onto a live camera view.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>8313</offset><text>Camera tracking techniques</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8340</offset><text>Indoor localization enables AR camera tracking. The method based on visual features from images was representative way for AR. established 3D model by multi-view correspondences and localized object for AR based on the Scale-Invariant Feature Transform (SIFT) descriptor. However, vision-based method is difficult to remain accurate and robust in an AR camera-tracking environment. These methods tend to be failed in texture-less environments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8784</offset><text>In order to solve this problem, designed and utilized visible/invisible markers to determine the indoor position. extracts the activity landmarks from crowdsourcing data and clusters the activity landmarks into different clusters. But, those localization methods need manual intervention, which is can’t be used in AR. aligned the floor plan and collected fingerprints on reference points based on the Microsoft HoloLens. But these methods either involve cost of installing and managing wireless access points or positioning error accumulated with time. proposed a Walkability network-based Augmented Reality (WaNAR) method to positioning and navigation. But this method needs a man-made 3D drawing of indoor walkable space before being used.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9529</offset><text>SLAM is a widely solution for AR tracking. improve indoor camera localization by optimized 3D prior map. They integrated RGBD SLAM with a deep learning routine whose training dataset is sequential video frames with labelled camera poses. developed an AR application for annotation in unknown environments based on an extended SLAM that tracks and estimates high-level features automatically. MARINS is an indoor navigation system developed using the Apple ARKit SDK and an associated SLAM system. However, this method is computational expensiveness, which is hard to apply to a wide range of scenarios.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10132</offset><text>In addition, some experts consider using sensors embedded in mobile phones for locating, such as WiFi or accelerometer. Instead of single tracking method, multiple sources combination is a promising way to improve camera tracking. proposed a multi-sensor fusion-based algorithm to improve the precision of indoor localization. The multi-source data include activity detection, PDR, and 3D vision-based localization. However, this method cannot meet the requirement of positioning at real-time. combined visual natural markers and an IMU to support AR indoor navigation. They used the IMU data to estimate the camera position and orientation when a natural marker-based method is limited. localized the six DoF pose of a mobile phone by using GPS data and a panoramic view of the environment. integrated monocular camera and IMU measurements to estimate metric distance and localize the mobile device. Hybrid tracking methods improve mobile AR, but there is still a gap to low-latency, high precision AR camera tracking for AR visualization in indoor environments. The camera tracking techniques are summarized in Table 2.</text></passage><passage><infon key="file">table-2.xml</infon><infon key="id">table-2</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>11254</offset><text>Camera tracking techniques.</text></passage><passage><infon key="file">table-2.xml</infon><infon key="id">table-2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Advantage&lt;/bold&gt;
&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Disadvantage&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Visual feature-based&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;low-cost, no need for multiple sensors&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;tend to be failed in texture-less environments&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Landmark-based&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;low-cost, no need for extra equipment, high precision&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;manual intervention required&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SLAM&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;automatic, no need for pre-identify the scene&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;computational expensiveness, multi-sensor required,&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Multiple sensors-based&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;high precision&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;algorithm complicated, pre-establish device required&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>11282</offset><text>	Advantage	Disadvantage	 	Visual feature-based	low-cost, no need for multiple sensors	tend to be failed in texture-less environments	 	Landmark-based	low-cost, no need for extra equipment, high precision	manual intervention required	 	SLAM	automatic, no need for pre-identify the scene	computational expensiveness, multi-sensor required,	 	Multiple sensors-based	high precision	algorithm complicated, pre-establish device required	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11716</offset><text>AR visualization</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11733</offset><text>AR visualization is not only about rendering computer graphics, but also can help make virtual 2D/3D data easier to understand as an aid to navigation in the real world. For helping designers in managing the aspect of layout and the representation of reasonable AR view, compared the different visualization effect between two types of AR content—image and text and image by using Unity 3D and the Vuforia library. considered the impact of the AR visualization style on user perception and designed three types of directional arrows to improve the AR presentation in large distances. They rendered directional arrows according to different speeds. To manage the amount of information in AR view, proposed two types of zooming interfaces to reduce the user reading load, the egocentric panoramic 360° view and an exocentric top-down view. investigated the AR content presentation and human interaction problems based on the smart glasses. This research provides some interesting references for AR visualization, but the spatial information visualization contains more content and needs more complex visual element.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12851</offset><text>To solve the problem of limited information visibility, proposed a set of Situated Visualization techniques for a “street-view” perspective. They implemented the dynamic annotation placement, label alignment and occlusion culling for scene information extracted from a GIS database. To resolve the similar problem, presented a 3D representation algorithm for virtual or mixed environments with virtual objects. However, the structural distortions in representations of space are influenced peoples’ spatial perception. projected a holographic grid into 3D space to explore whether the structural distortions can be reduced. However, their method was based on the additional holograms equipment. proposed a method for providing a non-3D display based on pseudo motion parallax. Considering the user’s point of view, they superimposed computer graphics (CG) images behind the scene display. proposed an image-based label placed method that combined a visual saliency algorithm with edge analysis to find the image regions and geometric constraints. developed an AR-GIS system for mapping and capturing underground utilities. However, these AR-GIS applications are used for short periods of time and for specific purposes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14079</offset><text>The existing AR-GIS applications only provide local spatial information in fixed location for users. The applications with comprehensive spatial information are rarely reported in the literature. These systems do not allow for a continuous and context-aware AR experience. Moreover, the dynamic spatial information visualization in AR-GIS system is tend to clutter and limited visibility due to fixed location.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14490</offset><text>In this paper, we focus on how to localize and organize the spatial information into an AR view. The goal is to construct a mobile augmented reality system that visualizes spatial information in indoor environment. The proposed AR-GIS system is fused an indoor map and situational visualization based on a mobile phone, which can provide dynamic and comprehensive spatial information for users.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>14885</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>14893</offset><text>Overview</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14902</offset><text>The proposed workflow is illustrated in Fig. 1. Given an indoor map from the GIS database, we first divide the indoor map by a regular grid and build an index of map data, to support AR visualization within a certain range at the current user location. At the indoor positioning stage, our novel multi-sensor fusion-based algorithm employs a particle filter to fuse BLE and PDR. We extract the extent of the current location from the indoor map and calculate the spatial entities in front of the camera view according to the direction and position of the phone. The algorithm is proposed to determine screen coordinates for semantic information. The fused mobile augmented reality system renders the semantic information for parts of the indoor model corresponding to the AR camera view.</text></passage><passage><infon key="file">peerj-cs-07-704-g001.jpg</infon><infon key="id">fig-1</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>15690</offset><text>The workflow of augmented reality and indoor map fusion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15747</offset><text>Indoor positioning</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15766</offset><text>Since we set up a configuration to display the indoor spatial information, the first step in our implementation is tracking the camera pose based on the mobile phone. We propose a novel multi-sensor fusion-based algorithm, which utilizes a particle filter to fuse BLE and PDR. The indoor localization algorithm is presented in Fig. 2.</text></passage><passage><infon key="file">peerj-cs-07-704-g002.jpg</infon><infon key="id">fig-2</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>16101</offset><text>Indoor localization algorithm.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16132</offset><text> The input of the algorithm includes BLE and inertial sensors data. Based on the BLE data, we obtain the BLE-based localization result. Meanwhile, based on the inertial sensors data, we get the displacement estimation result. Then, we use particle filter to fuse these two results. The displacement is used to update the particles for the particle filter.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16488</offset><text>BLE-based indoor localization</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16518</offset><text>BLE is a wireless personal area network technology designed for applications in the healthcare, fitness, beacons, security, and home entertainment industries. Proximity sensing is one of its applications, which also provides a new method for indoor localization. BLE realizes indoor localization by measure the distance between a mobile device and several beacons. The locations of the beacons are known. Specifically, based on the known location of the beacon, the received signal strength (RSS) of the beacon can be used to estimate the distance from the mobile device to the beacon, which can be expressed as Eq. (1):  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17141</offset><text>where λ is the distance from the beacon to device,  is RSS of a beacon, λ0 is reference distance (normally λ0 equal to 1m), η is the path loss exponent, and X is a zero-mean Gaussian distribution variable with variance .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17379</offset><text>The localization method is called triangulation. If the location of the mobile device is , the locations of the beacons are , i = 1, 2, …, N. N is usually more than 3. The measured distances are di. Then, we can get N equations: . By solving the N equations, we can get the location of the mobile device.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17691</offset><text>Because of the complexity of indoor environments, the measured distance usually contains error, which makes localization results unreliable. We propose a novel algorithm that improves the localization accuracy of BLE. By the beacon, we can obtain the signal quality index (SQI), which reflects the confidence of estimated distance.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18023</offset><text>If the SQI is greater than the threshold, we use a triangulation method to calculate the position of the device; otherwise, we use fingerprinting-based method to estimate the location. A weight k-nearest neighbour (WKNN) is adopted for location calculation. The location is calculated by the following equation: p = (wi∗pi)(i = 1, 2, …, k),, where p is the estimated location, wi is the weight, pi is the location of the ith beacon, k is a parameter, which is determined by experiment. wi is calculated based on the measured distance between the device and beacon:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18599</offset><text>1) wi = 1/di,</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18615</offset><text>2) normalize wi.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>18632</offset><text>PDR</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18636</offset><text>PDR algorithm is utilized to estimate the displacement. If the previous location is , the next location is calculated as:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18760</offset><text>where l is the step length, n the step number, and the heading. Step number is obtained by the peak detection algorithm. The step detection result is shown in Fig. 3. The step length is estimated using the step frequency-based model: l = a∗f + b, where f is the step frequency, and (a, b) are the parameters that can be trained offline.</text></passage><passage><infon key="file">peerj-cs-07-704-g003.jpg</infon><infon key="id">fig-3</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>19104</offset><text>Step detection result.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>19127</offset><text>Particle filter-based data fusion</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19161</offset><text>The PF algorithm is based on the sequential Monte Carlo framework, applied to nonlinear and non-Gaussian estimation problems. A typical particle filter comprises of the following steps:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19347</offset><text>Initialization: Sampling N particles based on the initial localization result.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19426</offset><text>Prediction Sampling: Predict a new particle  for each particle  based on the prediction function. In this paper, we use PDR to model the user’s movement for the prediction of multiple particles. The new location () of ith particle is updated by  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19675</offset><text>where Δl and Δh are the distance and heading change obtained by PDR, and hk is the heading at time k.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19785</offset><text>Importance Sampling: Calculate weights  for each new particle .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19849</offset><text>Normalization and Resampling: The weights are normalized and resampled. In the resampling process, particles with low weight are deleted and particles with high weight are duplicated.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20033</offset><text>In our proposed method, the initial localization is obtained by on BLE-based localization. The particle prediction is realized by PDR algorithm. The weights are calculated based on the distances between the new particles and BLE-based localization results. Based on this online indoor positioning method for AR camera tracking, we fused AR and indoor map for GIS data visualization.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20416</offset><text>Augmented reality and indoor map fusion</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20456</offset><text>The workflow of AR-GIS visualization was shown in Fig. 4. We first set up a pre-processing procedure onto indoor map since the GIS information is not adjusted to the visual perspective of the user. During the AR visualization, we extract the extent of the current location from the indoor map and calculate the spatial entities in front of the camera view according to the direction and position of the phone. Then, the spatial entities in front of the camera view are detected and their coordinates transformed from real world to AR view.</text></passage><passage><infon key="file">peerj-cs-07-704-g004.jpg</infon><infon key="id">fig-4</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>20996</offset><text>The workflow of AR-GIS visualization fusion.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>21041</offset><text>Indoor map pre-processing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21067</offset><text>A map constructed of planar features will block the real scene; the indoor map is only composed of line feature that looks transparent in AR view. In order to display attribute information on the indoor map, we keep the text information as a texture and map it onto indoor map. Thus, textual information will align with indoor map whenever an application requires the user to zoom in, zoom out or rotate the map.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21480</offset><text>The user may be distracted by showing entire map of a large space in an AR view. The displayed part of indoor map should be limited within a certain range of the current location. Thus, we divide indoor map by a regular grid and build an index for map data. Each regular grid cell records the minimum bounding box, transverse step, and longitudinal step. During the AR visualization, we get the current position grid as center, and acquired its surrounding eight grids for displaying. This data structure based on a spatial index of regular grid needs low storage and little communication.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>22070</offset><text>AR-GIS visualization</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22091</offset><text>We designed the interface of AR-GIS system that fused the situation and indoor map visualization. As described in Fig. 5, we divided the screen into two parts, which the top one third is displayed the situation information, and the lower two third is rendered the indoor map. Moreover, the indoor positioning result is rendered into the indoor map to show the current user position.</text></passage><passage><infon key="file">peerj-cs-07-704-g005.jpg</infon><infon key="id">fig-5</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>22474</offset><text>The interface of AR view.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22500</offset><text>We transformed all these visual elements into the AR coordinate system. As shown in Fig. 6, Fig. 6A is indoor map coordinate system and Fig. 6B is AR coordinate system.</text></passage><passage><infon key="file">peerj-cs-07-704-g006.jpg</infon><infon key="id">fig-6</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>22669</offset><text>The coordinate transformation.</text></passage><passage><infon key="file">peerj-cs-07-704-g006.jpg</infon><infon key="id">fig-6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>22700</offset><text>(A) The indoor map coordinate system. (B) The AR view coordinate system.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22773</offset><text>We acquired the rendered part of indoor map according to the current position, and calculated the bounding box of indoor map ABCO and the center point M(xm, ym). We put the point M as a new original point, and reduce the size of indoor map in accordance with a certain proportion to keep it within the AR view. Considering the indoor map coordinates in Euclidean space [O; x, y], we define the AR coordinate system as [O; x′, y′]. The coordinate transformation can be described as Eq. (4):  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23274</offset><text>Furthermore, we optimized the AR-GIS visualization system from three aspects.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23352</offset><text>The existing map visualization methods do not consider the direction relationship between map and the real world. As shown in Fig. 7B, the orientation of indoor map always changes with mobile phone, not the real scene. Users commonly identify the north by a compass in the map. However, this approach is not suitable for AR visualization. AR explains the spatial context by overlaying digital data onto the users’ view of the real world. The preferable way to make user understand spatial data clearly in an AR view is not by providing more visual elements like a compass, but rendering virtual data that directly matches the real scene. In order to keep a same orientation between indoor map and the real scene, we first obtained the angle between the mobile phone’s orientation and the north. We obtained the orientation and rotation of mobile phone from multiple sensors. Considering clockwise direction as positive direction, when the phone is tilted by an angle θ away from the north, we rotate the indoor map by the same angle in the opposite direction. The calculation of angle θ in indoor environment needs to account for the deflection angle of the indoor map. As shown in Fig. 7C, the direction of indoor map always matches the real scene whenever the mobile phone rotated, so that users can better understand the map information in AR view.</text></passage><passage><infon key="file">peerj-cs-07-704-g007.jpg</infon><infon key="id">fig-7</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>24712</offset><text>The orientation of indoor map.</text></passage><passage><infon key="file">peerj-cs-07-704-g007.jpg</infon><infon key="id">fig-7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24743</offset><text>(A) Mobile phone orientation. (B) The orientation of indoor map changes with mobile phone. (C) The orientation of indoor map changes with the real scene.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24897</offset><text>The foreshortening effects are basic visual rules in the real world. To make the AR visualization more realistic, we rendered the indoor map with foreshortening effects when the phone’s pitch angle changes. We obtain the pitch angle from the sensors in mobile phone, and rotate the indoor map around the X-axis along with the angle change.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25239</offset><text>As show in Fig. 8A, when the phone is placed horizontally that the pitch angle is approximately equal to 0°, the user can only see the ground through their phone, so there is no foreshortening effects. The pitch angle is getting bigger along with user lifts the mobile phone, the indoor scene appears in the camera view, and AR map shows the effect of “near big far small” as well (Fig. 8B).</text></passage><passage><infon key="file">peerj-cs-07-704-g008.jpg</infon><infon key="id">fig-8</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>25636</offset><text>The foreshortening effects of indoor map.</text></passage><passage><infon key="file">peerj-cs-07-704-g008.jpg</infon><infon key="id">fig-8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25678</offset><text>(A) The mobile phone is placed horizontally. (B) The mobile phone is placed vertically.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25766</offset><text>We provide the spatial information onto AR view based on the spatial relationship between GIS data and the actual indoor environment. As shown in Fig. 9, O is the current position, V is the camera direction, and θ is the angle to the north. We set two threshold values d1 and β, which identify distance and the angle of vision. Thus, the quadrangle Op1p2p3 is the field of AR view. When an object appears in the quadrangle, its information will be rendered onto AR view. In addition, if the object fall in the triangle Op1p2, its information will rendered on the left of the AR view. Otherwise, the information will be rendered on the right of the AR view.</text></passage><passage><infon key="file">peerj-cs-07-704-g009.jpg</infon><infon key="id">fig-9</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>26430</offset><text>The field of AR view.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26452</offset><text>We build the virtual and real camera alignment by coordinate transformation. For example, a point p in the Op1p2, whose screen coordinates of information can be described as:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26629</offset><text>where the width and height are the width and height of screen, d2 is the distance between p1 to p3, d3 is the perpendicular distance between p to the camera direction v, and the d4 is the perpendicular distance between p2 to the perpendicular line segment from p and camera direction v, where a perpendicular line from d3 intersects the line from p and camera direction v.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27002</offset><text>If the point p in the Op2p3, the screen coordinates of information can be described as:  </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27092</offset><text>Finally, the indoor map and situational information are rendered on to camera AR view by the coordinate transformation.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>27212</offset><text>Experimental Results and Analysis</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27246</offset><text>Our experiment was operated on an android system mobile phone of Samsung SM-G9500. The AR visualization system was implemented based on OpenES as single-threaded programs. As shown in Fig. 10, the experiment environment is a 52.5 m * 52.5 m floor plan, which includes office area, lift well and public area.</text></passage><passage><infon key="file">peerj-cs-07-704-g010.jpg</infon><infon key="id">fig-10</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>27554</offset><text>Experiment environment.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>27578</offset><text>Indoor positioning</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27597</offset><text>To evaluate the performance of the proposed indoor positioning method, the participants were asked to walk along four routes with the smartphone in the hand. We set some markers with known coordinates along the routes to collect the ground truth data. When participants walk over a marker, they push the button to record the time. The ground truth between the markers is obtained by interpolating the step count. We calculated the positioning error based on the Euclidean distance between the estimated position and the ground truth, and also gave the the Cumulative Distribution Function (CDF) of positioning resulte.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28216</offset><text>We evaluated the effect of the number of particles in PF algorithm on the performance of the proposed positioning method. The performance was evaluated in two metrics: average positioning error and computing time. The computing time is a key factor since the proposed positioning system is implemented online on a smartphone. Figure 11 shows the effect of number of the particles on the performance of the positioning system. We can see that with increasing particle number, the average positioning error decreased until the number increased to 750. The computing time increased gradually with the increasing particle number. Therefore, we set the number of particles to 750 in our PF algorithm.</text></passage><passage><infon key="file">peerj-cs-07-704-g011.jpg</infon><infon key="id">fig-11</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>28912</offset><text>The effect of particles number on the performance of the positioning system.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28989</offset><text>After determine number of particles, we the collected the statistical positioning information of each route. Take route 1 as an example, we captured 143 BLE location, and detected 203 steps. Based on these collected data (143+203), we obtained 210 localization results by using PF algorithm. The statistical positioning information of each route was given in Table 3.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29357</offset><text>As shown in Fig. 12, we evaluated the quality of the positioning resulte using the Cumulative Distribution Function (CDF).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29480</offset><text>Figure 12 shows the CDF of the PDR, Beacon, and PF positioning methods for each route. For route 1, 80% of PF error is within approximately 1.9 m (80% of beacon error is within approximately 2.3 m, and 80% of beacon error is within approximately 6.4 m). For route 2, 80% of PF error is within approximately 2.5 m. For route 3, 80% of PF error is within approximately 2.3 m. For route 4, 80% of PF error is within approximately 1.8 m.</text></passage><passage><infon key="file">table-3.xml</infon><infon key="id">table-3</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>29914</offset><text>The statistical positioning information of each route.</text></passage><passage><infon key="file">table-3.xml</infon><infon key="id">table-3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Step number of PDR&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of BLE location&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Number of PF location&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;203&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;143&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;210&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;63&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;148&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;105&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;196&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;131&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;204&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29969</offset><text>	Step number of PDR	Number of BLE location	Number of PF location	 	Route 1	203	143	210	 	Route 2	91	63	97	 	Route 3	148	96	105	 	Route 4	196	131	204	 	</text></passage><passage><infon key="file">peerj-cs-07-704-g012.jpg</infon><infon key="id">fig-12</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>30121</offset><text>The CDF of different routes.</text></passage><passage><infon key="file">peerj-cs-07-704-g012.jpg</infon><infon key="id">fig-12</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>30150</offset><text>The blue line represents PDR. The red line represents BLE. The black line is the result of proposed method. (A) Route 1, (B) Route 2, (C) Route 3, (D) Route 4.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30310</offset><text>We also compared the proposed method to PDR and BLE by using average positioning errors. As shown in Table 4, we can see that the average positioning errors of the proposed method for all the routes were less than that of PDR and BLE. For route 1, the average positioning errors of PDR, BLE and proposed were 7.23 m, 1.55 m, and 1.29 m, respectively. The mean errors of the three methods for all the routes were 5.26 m, 1.84 m, and 1.47 m, respectively. In PF-based data fusion, the average positioning error was reduced by 72.04% and 19.89% as compared to PDR and BLE. The positioning results show that the proposed PF-based data fusion algorithm can effectively improve the accuracy of indoor positioning system.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31025</offset><text>The online positioning results of different positioning methods are shown in Fig. 13 showing Online positioning result. The red line is Ground truth. The green line represents PDR only used. The blue line represents is BLE only used. The purple line is the result of proposed method.</text></passage><passage><infon key="file">table-4.xml</infon><infon key="id">table-4</infon><infon key="section_type">TABLE</infon><infon key="type">table_title_caption</infon><offset>31309</offset><text>The average positioning errors (m).</text></passage><passage><infon key="file">table-4.xml</infon><infon key="id">table-4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;col span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;PDR&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;BLE&lt;/th&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proposed&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 1&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.23&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.55&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.6&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.01&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.67&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 3&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6.08&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.08&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Route 4&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.12&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.7&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.26&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.84&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.47&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31345</offset><text>	PDR	BLE	Proposed	 	Route 1	7.23	1.55	1.29	 	Route 2	2.6	2.01	1.67	 	Route 3	6.08	2.08	1.62	 	Route 4	5.12	1.7	1.3	 	Mean	5.26	1.84	1.47	 	</text></passage><passage><infon key="file">peerj-cs-07-704-g013.jpg</infon><infon key="id">fig-13</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>31485</offset><text>Online positioning result.</text></passage><passage><infon key="file">peerj-cs-07-704-g013.jpg</infon><infon key="id">fig-13</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>31512</offset><text>The red line is ground truth. The green line represents PDR only used. The blue line represents is BLE only used. The purple line is the result of proposed method.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31676</offset><text>We can see that the PDR positioning error is large; the estimated trajectories deviate sharply from the ground truth. The BLE positioning results were more accurate that of PDR due to our proposed algorithm. However, in some areas, BLE has occasional large errors. For example, the bottom right of Route 1 and bottom left of Route 2, show significant deviations. Our proposed PDR and BLE fusion method is more accurate than either PDR or BLE alone. The estimated trajectories are consistent with the ground truth.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>32190</offset><text>AR-GIS visualization</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32211</offset><text>We walked with the smartphone to the test the AR-GIS visualization, and recorded the spatial information visualization results. The video of mobile augmented reality system was uploaded on the website: https://youtu.be/XNhpaIWk1IQ.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32443</offset><text>The AR visualization effects are illustrated in Fig. 14, where each column represents a different position. The first row presents the AR visualization effect when the mobile phone is upright. It consists of situational information, the indoor map and localization results. In Fig. 14A, the lobby elevator is on the left side of the current position, so the text was rendered on the left of AR view. In Fig. 14B three targets were detected, the texts were displayed from top to bottom according to the distance from far to near. In Fig. 14C, the office of Prof.Tu is on the right of the current position, so the label was rendered on the right of the AR view.</text></passage><passage><infon key="file">peerj-cs-07-704-g014.jpg</infon><infon key="id">fig-14</infon><infon key="section_type">FIG</infon><infon key="type">fig_title_caption</infon><offset>33103</offset><text>The results of AR-GIS visualization.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33140</offset><text>The second row (Figs. 14D, 14E, 14F) shows the AR visualization results when the mobile phone is placed horizontally. User can clearly perceive the indoor space and current position from this perspective.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33345</offset><text>We discuss our approach as compared with two state-of-the-art AR systems:. proposed an AR system for indoor/outdoor navigation. The AR positioning signals are corrected continuously by the ground-truth 3D indoor/outdoor walkability network in a 3D model. However, it is necessity for their tool to implement approaches through is a manual 3D drawing of indoor walkable space. presented an AR system based on the correspondences between images. However, failure becomes more likely due to frequent changes in indoor environment and because reliable reference points are unavailable. Moreover, the AR application only offer the position spot onto the AR view, and the AR system only labels the facility’s name along the pathway. These two methods can only provide local spatial in-formation in fixed location for users. Compare with them, our AR and indoor map fusion technique links rich indoor spatial information to real world scenes. Figure 14 presents a high-quality AR-GIS system that integrates rich spatial information with the real world tightly and rationally. It is not only technical but also aesthetical for conveying spatial information.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>34497</offset><text>Conclusions and Future Work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>34525</offset><text>In this paper, we designed a method for fusing AR view and indoor map in changing environments during AR-GIS visualization that also considers the changes of the AR view when the pose of the mobile phone shifts. We presented an innovative indoor positioning approach that fuses BLE and PDR to enhance the accuracy of AR camera tracking. The experiments in an office building of Shenzhen University demonstrate that dynamic AR-GIS visualization techniques can display rich spatial information in real time on mobile phones, while preserving a high accuracy in the AR-GIS fusion. The AR content includes current position, indoor map and spatial information, which change as the real scene changes. The layout of spatial information is rendered reasonable in AR view. This AR-GIS visualization will be clear and easy to understand, which effectively optimizes and enriches the spatial information in the visualization of indoor environment.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>35463</offset><text>Though the AR-GIS system is shown indoor map onto the AR view, it can’t adapt to the change of the scene size. Thus, in future work, we will try to process indoor map data and improve the visualization technique method to realize the adaptive AR visualization. Besides, the current application only labels the name of indoor scene, which can be further advanced with more details about the spatial information. Furthermore, the smoothing algorithm can be applied to enhance the accuracy of the device’s position.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>35980</offset><text>Supplemental Information</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>36005</offset><text>Additional Information and Declarations</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>36045</offset><text>Competing Interests</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>36065</offset><text>The authors declare there are no competing interests.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>36119</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>36140</offset><text>Wei Ma conceived and designed the experiments, performed the experiments, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>36350</offset><text>Shuai Zhang analyzed the data, performed the computation work, prepared figures and/or tables, and approved the final draft.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">footnote</infon><offset>36475</offset><text>Jincai Huang conceived and designed the experiments, authored or reviewed drafts of the paper, and approved the final draft.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>36600</offset><text>Data Availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>36618</offset><text>The following information was supplied regarding data availability:</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>36686</offset><text>The indoor positioning code is available in the Supplemental Files.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>36754</offset><text>References</text></passage><passage><infon key="fpage">57</infon><infon key="lpage">64</infon><infon key="name_0">surname:Ahmad;given-names:A</infon><infon key="name_1">surname:Claudio;given-names:P</infon><infon key="name_2">surname:Alizadeh Naeini;given-names:A</infon><infon key="name_3">surname:Sohn;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">ISPRS Annals of Photogrammetry, Remote Sensing &amp; Spatial Information Sciences</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2020</infon><offset>36765</offset><text>WI-FI RSS fingerprinting for indoor localization using augmented reality</text></passage><passage><infon key="fpage">37</infon><infon key="lpage">46</infon><infon key="name_0">surname:Arth;given-names:C</infon><infon key="name_1">surname:Klopschitz;given-names:M</infon><infon key="name_2">surname:Reitmayr;given-names:G</infon><infon key="name_3">surname:Schmalstieg;given-names:D</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>36838</offset><text>Real-time self-localization from panoramic images on mobile devices</text></passage><passage><infon key="fpage">174</infon><infon key="lpage">188</infon><infon key="name_0">surname:Arulampalam;given-names:S</infon><infon key="name_1">surname:Maskel;given-names:S</infon><infon key="name_2">surname:Gordon;given-names:N</infon><infon key="name_3">surname:Clapp;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Entific Programming</infon><infon key="type">ref</infon><infon key="volume">50</infon><infon key="year">2002</infon><offset>36906</offset><text>A tutorial on PFs for on-line non-linear/non-gaussian bayesian tracking</text></passage><passage><infon key="name_0">surname:Bell;given-names:B</infon><infon key="name_1">surname:Feiner;given-names:S</infon><infon key="name_2">surname:Hllerer;given-names:T</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>36978</offset><text>View management for virtual and augmented reality</text></passage><passage><infon key="fpage">259</infon><infon key="lpage">270</infon><infon key="name_0">surname:Carrera;given-names:C</infon><infon key="name_1">surname:Carlos</infon><infon key="name_2">surname:Asensio;given-names:B</infon><infon key="name_3">surname:Alberto;given-names:L</infon><infon key="section_type">REF</infon><infon key="source">Cartography &amp; Geographic Information Science</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2017</infon><offset>37028</offset><text>Augmented reality as a digital teaching environment to develop spatial thinking</text></passage><passage><infon key="fpage">775</infon><infon key="lpage">788</infon><infon key="name_0">surname:Chengbi;given-names:L</infon><infon key="name_1">surname:Sven;given-names:F</infon><infon key="section_type">REF</infon><infon key="source">Transactions in Gis</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2018</infon><offset>37108</offset><text>Enriching the GIScience research agenda: fusing augmented reality and location-based social networks</text></passage><passage><infon key="name_0">surname:Cho;given-names:DK</infon><infon key="name_1">surname:Mun;given-names:M</infon><infon key="name_2">surname:Lee;given-names:U</infon><infon key="name_3">surname:Kaiser;given-names:WJ</infon><infon key="name_4">surname:Gerla;given-names:M</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>37209</offset><text>AutoGait: a mobile platform that accurately estimates the distance walked</text></passage><passage><infon key="fpage">3442</infon><infon key="name_0">surname:Diao;given-names:PH</infon><infon key="name_1">surname:Shih;given-names:NJ</infon><infon key="pub-id_doi">10.3390/s18103442</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>37283</offset><text>MARINS: a mobile smartphone AR system for pathfinding in a dark environment</text></passage><passage><infon key="fpage">60</infon><infon key="name_0">surname:Fenais;given-names:A</infon><infon key="name_1">surname:Ariaratnam;given-names:ST</infon><infon key="name_2">surname:Ayer;given-names:SK</infon><infon key="name_3">surname:Smilovsky;given-names:N</infon><infon key="pub-id_doi">10.3390/infrastructures4040060</infon><infon key="section_type">REF</infon><infon key="source">Infrastructures</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2019</infon><offset>37359</offset><text>Integrating geographic information systems and augmented reality for mapping underground utilities</text></passage><passage><infon key="name_0">surname:Grasset;given-names:R</infon><infon key="name_1">surname:Langlotz;given-names:T</infon><infon key="name_2">surname:Kalkofen;given-names:D</infon><infon key="name_3">surname:Tatzgern;given-names:M</infon><infon key="name_4">surname:Schmalstieg;given-names:D</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>37458</offset><text>Image-driven view management for augmented reality browsers</text></passage><passage><infon key="fpage">48</infon><infon key="lpage">58</infon><infon key="name_0">surname:Huang;given-names:W</infon><infon key="name_1">surname:Sun;given-names:M</infon><infon key="name_2">surname:Li;given-names:S</infon><infon key="pub-id_doi">10.1016/j.eswa.2016.01.037</infon><infon key="section_type">REF</infon><infon key="source">Expert Systems with Applications</infon><infon key="type">ref</infon><infon key="volume">55</infon><infon key="year">2016</infon><offset>37518</offset><text>A 3D GIS-based interactive registration mechanism for outdoor augmented reality system</text></passage><passage><infon key="fpage">9</infon><infon key="lpage">27</infon><infon key="name_0">surname:Javornik;given-names:A</infon><infon key="name_1">surname:Kostopoulou;given-names:E</infon><infon key="name_2">surname:Rogers;given-names:Y</infon><infon key="name_3">surname:Schieck;given-names:AFG</infon><infon key="name_4">surname:Koutsolampros;given-names:P</infon><infon key="name_5">surname:Moutinho;given-names:AM</infon><infon key="name_6">surname:Julier;given-names:S</infon><infon key="pub-id_doi">10.1080/0144929X.2018.1505950</infon><infon key="section_type">REF</infon><infon key="source">Behaviour &amp; Information Technology</infon><infon key="type">ref</infon><infon key="volume">38</infon><infon key="year">2019</infon><offset>37605</offset><text>An experimental study on the role of augmented reality content type in an outdoor site exploration</text></passage><passage><infon key="fpage">165</infon><infon key="lpage">172</infon><infon key="name_0">surname:Keil;given-names:J</infon><infon key="name_1">surname:Korte;given-names:A</infon><infon key="name_2">surname:Ratmer;given-names:A</infon><infon key="name_3">surname:Edler;given-names:D</infon><infon key="name_4">surname:Dickmann;given-names:F</infon><infon key="pub-id_doi">10.1007/s41064-020-00104-1</infon><infon key="section_type">REF</infon><infon key="source">PFG–Journal of Photogrammetry, Remote Sensing and Geoinformation Science</infon><infon key="type">ref</infon><infon key="volume">88</infon><infon key="year">2020</infon><offset>37704</offset><text>Augmented reality (AR) and spatial cognition: effects of holographic grids on distance estimation and location memory in a 3D indoor scenario</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">14</infon><infon key="name_0">surname:Khan;given-names:RU</infon><infon key="name_1">surname:Oon;given-names:Y</infon><infon key="name_2">surname:Madihiec;given-names:A</infon><infon key="name_3">surname:Chia;given-names:S</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Innovation, Creativity and Change</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2019</infon><offset>37846</offset><text>Indoor navigation systems using annotated maps in mobile augmented reality</text></passage><passage><infon key="fpage">727</infon><infon key="lpage">741</infon><infon key="name_0">surname:Li;given-names:J</infon><infon key="name_1">surname:Wang;given-names:C</infon><infon key="name_2">surname:Kang;given-names:X</infon><infon key="name_3">surname:Zhao;given-names:Q</infon><infon key="section_type">REF</infon><infon key="source">International Journal of Digital Earth</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2019</infon><offset>37921</offset><text>Camera localization for augmented reality and indoor positioning: a vision-based 3D feature database approach</text></passage><passage><infon key="name_0">surname:Li;given-names:P</infon><infon key="name_1">surname:Qin;given-names:T</infon><infon key="name_2">surname:Hu;given-names:B</infon><infon key="name_3">surname:Zhu;given-names:F</infon><infon key="name_4">surname:Shen;given-names:S</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>38031</offset><text>Monocular visual-inertial state estimation for mobile augmented reality</text></passage><passage><infon key="fpage">7173</infon><infon key="lpage">7182</infon><infon key="name_0">surname:Li;given-names:Y</infon><infon key="name_1">surname:Zhuang;given-names:Y</infon><infon key="name_2">surname:Lan;given-names:H</infon><infon key="name_3">surname:Zhang;given-names:P</infon><infon key="name_4">surname:Niu;given-names:X</infon><infon key="name_5">surname:El-Sheimy;given-names:N</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sensors Journal</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>38103</offset><text>Self-contained indoor pedestrian navigation using smartphone sensors and magnetic features</text></passage><passage><infon key="fpage">53</infon><infon key="name_0">surname:Liu;given-names:F</infon><infon key="name_1">surname:Jonsson;given-names:T</infon><infon key="name_2">surname:Seipel;given-names:S</infon><infon key="pub-id_doi">10.3390/ijgi9010053</infon><infon key="section_type">REF</infon><infon key="source">ISPRS International Journal of Geo-Information</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2020</infon><offset>38194</offset><text>Evaluation of augmented reality-based building diagnostics using third person perspective</text></passage><passage><infon key="name_0">surname:Liu;given-names:H</infon><infon key="name_1">surname:Zhang;given-names:G</infon><infon key="name_2">surname:Bao;given-names:H</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>38284</offset><text>Robust keyframe-based monocular SLAM for augmented reality</text></passage><passage><infon key="fpage">8479</infon><infon key="lpage">8490</infon><infon key="name_0">surname:Liu;given-names:X</infon><infon key="name_1">surname:Zhou;given-names:B</infon><infon key="name_2">surname:Huang;given-names:P</infon><infon key="name_3">surname:Xue;given-names:W</infon><infon key="name_4">surname:Li;given-names:Q</infon><infon key="name_5">surname:Zhu;given-names:J</infon><infon key="name_6">surname:Qiu;given-names:L</infon><infon key="pub-id_doi">10.1109/JSEN.2021.3050456</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sensors Journal</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2021</infon><offset>38343</offset><text>Kalman filter-based data fusion of wi-fi rtt and pdr for indoor localization</text></passage><passage><infon key="fpage">7612</infon><infon key="lpage">7624</infon><infon key="name_0">surname:Ma;given-names:W</infon><infon key="name_1">surname:Li;given-names:Q</infon><infon key="name_2">surname:Zhou;given-names:B</infon><infon key="name_3">surname:Xue;given-names:W</infon><infon key="name_4">surname:Huang;given-names:Z</infon><infon key="section_type">REF</infon><infon key="source">IEEE Internet of Things Journal</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2020</infon><offset>38420</offset><text>Location and 3D visual awareness-based dynamic texture updating for indoor 3D model</text></passage><passage><infon key="name_0">surname:Matsuda;given-names:Y</infon><infon key="name_1">surname:Shibata;given-names:F</infon><infon key="name_2">surname:Kimura;given-names:A</infon><infon key="name_3">surname:Tamura;given-names:H</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>38504</offset><text>Poster: creating a user-specific perspective view for mobile mixed reality systems on smartphones</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Mladenov;given-names:M</infon><infon key="name_1">surname:Mock;given-names:M</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>38602</offset><text>A step counter service for Java-enabled devices using a built-in accelerometer</text></passage><passage><infon key="name_0">surname:Mulloni;given-names:A</infon><infon key="name_1">surname:Dünser;given-names:A</infon><infon key="name_2">surname:Schmalstieg;given-names:D</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>38681</offset><text>Zooming interfaces for augmented reality browsers</text></passage><passage><infon key="fpage">18</infon><infon key="lpage">31</infon><infon key="name_0">surname:Neges;given-names:M</infon><infon key="name_1">surname:Koch;given-names:C</infon><infon key="name_2">surname:Koenig;given-names:M</infon><infon key="name_3">surname:Abramovici;given-names:M</infon><infon key="pub-id_doi">10.1016/j.aei.2015.10.005</infon><infon key="section_type">REF</infon><infon key="source">Advanced Engineering Informatics</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2017</infon><offset>38731</offset><text>Combining visual natural markers and IMU for improved AR based indoor navigation</text></passage><passage><infon key="name_0">surname:Reitmayr;given-names:G</infon><infon key="name_1">surname:Eade;given-names:E</infon><infon key="name_2">surname:Drummond;given-names:TW</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>38812</offset><text>Semi-automatic annotations in unknown environments</text></passage><passage><infon key="name_0">surname:Ruta;given-names:M</infon><infon key="name_1">surname:Scioscia;given-names:F</infon><infon key="name_2">surname:Ieva;given-names:S</infon><infon key="name_3">surname:Filippis;given-names:DD</infon><infon key="name_4">surname:Sciascio;given-names:ED</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>38863</offset><text>Indoor/outdoor mobile navigation via knowledge-based POI discovery in augmented reality</text></passage><passage><infon key="fpage">2997</infon><infon key="name_0">surname:Sadeghi-Niaraki;given-names:A</infon><infon key="name_1">surname:Choi;given-names:S-M</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>38951</offset><text>A survey of marker-less tracking and registration techniques for health &amp; environmental applications to augmented reality and ubiquitous geospatial information Systems</text></passage><passage><infon key="name_0">surname:Shaughnessy;given-names:M</infon><infon key="name_1">surname:Resnick;given-names:BM</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>39119</offset><text>Interactive registration for Augmented Reality GIS</text></passage><passage><infon key="name_0">surname:Skrypnyk;given-names:I</infon><infon key="name_1">surname:Lowe;given-names:DG</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2004</infon><offset>39170</offset><text>Scene modelling, recognition and tracking with invariant image features</text></passage><passage><infon key="name_0">surname:Subakti;given-names:H</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>39242</offset><text>A marker-based cyber-physical augmented-reality indoor guidance system for smart campuses</text></passage><passage><infon key="name_0">surname:Tönnis;given-names:M</infon><infon key="name_1">surname:Klein;given-names:L</infon><infon key="name_2">surname:Klinker;given-names:G</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>39332</offset><text>Perception thresholds for augmented reality navigation schemes in large distances</text></passage><passage><infon key="fpage">171</infon><infon key="lpage">181</infon><infon key="name_0">surname:Tsai;given-names:CH</infon><infon key="name_1">surname:Huang;given-names:JY</infon><infon key="section_type">REF</infon><infon key="source">Computer Standards &amp; Interfaces</infon><infon key="type">ref</infon><infon key="volume">55</infon><infon key="year">2017</infon><offset>39414</offset><text>Augmented reality display based on user Behavior</text></passage><passage><infon key="name_0">surname:Wang;given-names:K</infon><infon key="name_1">surname:Zhou;given-names:X</infon><infon key="name_2">surname:Chen;given-names:H</infon><infon key="name_3">surname:Lang;given-names:M</infon><infon key="name_4">surname:Raicu;given-names:I</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>39463</offset><text>Next generation job management systems for extreme-scale ensemble computing</text></passage><passage><infon key="fpage">4</infon><infon key="lpage">9</infon><infon key="name_0">surname:White;given-names:S</infon><infon key="name_1">surname:Feiner;given-names:S</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>39539</offset><text>SiteLens: situated visualization techniques for urban site visits</text></passage><passage><infon key="fpage">444</infon><infon key="lpage">452</infon><infon key="name_0">surname:Xu;given-names:J</infon><infon key="name_1">surname:Xue;given-names:F</infon><infon key="name_2">surname:Chiaradia;given-names:A</infon><infon key="name_3">surname:Lu;given-names:W</infon><infon key="name_4">surname:Cao;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>39605</offset><text>Indoor-outdoor navigation without beacons: compensating smartphone AR positioning errors with 3D pedestrian network</text></passage><passage><infon key="fpage">2774</infon><infon key="issue">5</infon><infon key="lpage">2785</infon><infon key="name_0">surname:Zhou;given-names:B</infon><infon key="name_1">surname:Li;given-names:Q</infon><infon key="name_2">surname:Mao;given-names:Q</infon><infon key="section_type">REF</infon><infon key="source">IEEE Transactions on Intelligent Transportation Systems</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2015</infon><offset>39721</offset><text>ALIMC: Activity Landmark-Based Indoor Mapping via Crowdsourcing[J]</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">4</infon><infon key="name_0">surname:Zhou;given-names:BT</infon><infon key="name_1">surname:Mai;given-names:M</infon><infon key="name_2">surname:Xue;given-names:K</infon><infon key="name_3">surname:Ma;given-names:W</infon><infon key="name_4">surname:Li;given-names:Q</infon><infon key="pub-id_doi">10.1109/LWC.2020.2981793</infon><infon key="section_type">REF</infon><infon key="source">IEEE Wireless Communications Letters</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2020</infon><offset>39788</offset><text>A novel access point placement method for WiFi fingerprinting considering existing APs</text></passage><passage><infon key="name_0">surname:Zollmann;given-names:S</infon><infon key="name_1">surname:Poglitsch;given-names:C</infon><infon key="name_2">surname:Ventura;given-names:J</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>39875</offset><text>VISGIS: dynamic situated visualization for geographic information systems</text></passage><passage><infon key="comment">vol 7431. Springer: Berlin, Heidelberg</infon><infon key="name_0">surname:Zollmann;given-names:S</infon><infon key="name_1">surname:Schall;given-names:G</infon><infon key="name_2">surname:Junghanns;given-names:S</infon><infon key="name_3">surname:Reitmayr;given-names:G</infon><infon key="section_type">REF</infon><infon key="source">Advances in Visual Computing. ISVC 2012. Lecture Notes in Computer Science</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>39949</offset><text>Comprehensible and interactive visualizations of GIS data in augmented reality</text></passage></document></collection>
