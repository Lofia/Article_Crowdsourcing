<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201218</date><key>pmc.key</key><document><id>6764203</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1523/JNEUROSCI.0776-19.2019</infon><infon key="article-id_pmc">6764203</infon><infon key="article-id_pmid">31391262</infon><infon key="article-id_publisher-id">0776-19</infon><infon key="fpage">7703</infon><infon key="issue">39</infon><infon key="kwd">attention auditory scene analysis microsaccades pupil dilation superior colliculus</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</infon><infon key="lpage">7714</infon><infon key="name_0">surname:Zhao;given-names:Sijia</infon><infon key="name_1">surname:Yum;given-names:Nga Wai</infon><infon key="name_2">surname:Benjamin;given-names:Lucas</infon><infon key="name_3">surname:Benhamou;given-names:Elia</infon><infon key="name_4">surname:Yoneya;given-names:Makoto</infon><infon key="name_5">surname:Furukawa;given-names:Shigeto</infon><infon key="name_6">surname:Dick;given-names:Fred</infon><infon key="name_7">surname:Slaney;given-names:Malcolm</infon><infon key="name_8">surname:Chait;given-names:Maria</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">39</infon><infon key="year">2019</infon><offset>0</offset><text>Rapid Ocular Responses Are Modulated by Bottom-up-Driven Auditory Salience</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>75</offset><text>Despite the prevalent use of alerting sounds in alarms and human–machine interface systems and the long-hypothesized role of the auditory system as the brain's “early warning system,” we have only a rudimentary understanding of what determines auditory salience—the automatic attraction of attention by sound—and which brain mechanisms underlie this process. A major roadblock has been the lack of a robust, objective means of quantifying sound-driven attentional capture. Here we demonstrate that: (1) a reliable salience scale can be obtained from crowd-sourcing (N = 911), (2) acoustic roughness appears to be a driving feature behind this scaling, consistent with previous reports implicating roughness in the perceptual distinctiveness of sounds, and (3) crowd-sourced auditory salience correlates with objective autonomic measures. Specifically, we show that a salience ranking obtained from online raters correlated robustly with the superior colliculus-mediated ocular freezing response, microsaccadic inhibition (MSI), measured in naive, passively listening human participants (of either sex). More salient sounds evoked earlier and larger MSI, consistent with a faster orienting response. These results are consistent with the hypothesis that MSI reflects a general reorienting response that is evoked by potentially behaviorally important events regardless of their modality.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>1472</offset><text>SIGNIFICANCE STATEMENT Microsaccades are small, rapid, fixational eye movements that are measurable with sensitive eye-tracking equipment. We reveal a novel, robust link between microsaccade dynamics and the subjective salience of brief sounds (salience rankings obtained from a large number of participants in an online experiment): Within 300 ms of sound onset, the eyes of naive, passively listening participants demonstrate different microsaccade patterns as a function of the sound's crowd-sourced salience. These results position the superior colliculus (hypothesized to underlie microsaccade generation) as an important brain area to investigate in the context of a putative multimodal salience hub. They also demonstrate an objective means for quantifying auditory salience.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2255</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2268</offset><text>Our perception of our surroundings is governed by a process of competition for limited resources. This involves an interplay between task-focused and bottom-up-driven processes that automatically bias perception toward certain aspects of the world, to which our brain, through experience or evolution, has been primed to assign particular significance. Understanding the neural processes that underlie such involuntary attentional capture is a topic of intense investigation in systems neuroscience.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2768</offset><text>Research in vision has capitalized on the fact that attentional allocation can be “objectively” decoded from ocular dynamics: Observers free-viewing complex visual scenes tend to demonstrate consistent fixation, saccade, and microsaccade patterns that can be used to infer the attributes that attract bottom-up visual attention. The underlying network for (micro-)saccade generation is centered on the superior colliculus (SC) with a contribution from the frontal eye fields, consistent with a well established role for these regions in computing the visual salience map and controlling overt attention.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3376</offset><text>The appearance of new events is also associated with two types of rapid orienting responses: (1) an “ocular freezing” (“microsaccadic inhibition”; MSI) response—a rapid transient decrease in the incidence of microsaccades, hypothesized to arise through suppression of ongoing activity in the SC by new sensory inputs; and (2) a phasic pupil dilation response (PDR). The PDR has been linked to potentially SC-mediated spiking activity in the locus ceruleus, which constitutes the source of norepinephrine (noradrenaline) to the central nervous system and therefore controls global vigilance and arousal.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3989</offset><text>Both MSI and PDR have been shown to systematically vary with visual salience and are theorized to reflect the operation of an interrupt process that halts ongoing activities so as to accelerate an attentional shift toward a potentially survival-critical event.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4250</offset><text>Interestingly, sounds can also drive these ocular responses. Abrupt, or otherwise out-of-context auditory events evoke pupil dilation, and cause MSI, consistent with a proposal that these responses reflect the operation of a modality-general orienting mechanism. In fact, sounds cause faster responses than visual stimuli, consistent with the “early warning system” role of hearing. However, because only very simple stimuli have been used, the degree to which sound-evoked ocular responses reflect acoustic properties beyond loudness remains unknown.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4806</offset><text>Here, we sought to determine whether MSI and PDR are sensitive to auditory salience. We used crowd-sourcing to obtain a “subjective” (e.g., ratings-based) salience ranking of a set of brief environmental sounds. The obtained salience scale was also verified with a small, “in-laboratory” replication. Then in a laboratory setting, a group of naive participants passively listened to these sounds while their ocular dynamics and pupil dilation were recorded. We demonstrate that MSI (but not PDR) is systematically modulated by auditory salience. This is consistent with the hypothesis that MSI indexes a rapid, multimodal orienting mechanism that is sensitive to not just the onset, but also the specific perceptual distinctiveness of brief sounds.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>5563</offset><text>Materials and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>5585</offset><text>Stimuli</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>5593</offset><text>Eighteen environmental sounds drawn from, with a subset from and, were used. All stimuli were 500 ms long and RMS equated (see individual spectrograms in Fig. 1A and Fig. 1-1 in the extended data for sound files).</text></passage><passage><infon key="file">zns9991919310001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5807</offset><text>Crowd-sourced “subjective” salience rating for brief environmental sounds. A, Spectrograms for all 18 sounds are displayed in order of ranking in B. See Figure 1-1 in the extended data section, for sound files. B, Crowd-sourced rating collected from MTurk (N = 911). The sounds used in the in-laboratory replication are indicated by orange-colored bars. Error bars are 1 SD from bootstrap resampling. See Figure 1-2 in the extended data section, for details of the instructions to online participants and MTurk page layout. C, Crowd-sourced salience rating is strongly correlated with the in-laboratory salience ranking. The dashed line indicates identical ranks. D, Crowd-sourced salience rating is strongly correlated with acoustic “roughness.” All correlations are conducted using the Spearman rank method.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>6625</offset><text>Subjective salience via crowd-sourcing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>6664</offset><text>Experimental design and statistical analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6710</offset><text>The experiment was conducted on the Amazon Mechanical Turk (MTurk) platform, an online marketplace that allows remote participants (“workers” per MTurk nomenclature) to perform tasks via a web interface. Raters were selected from a large pool of prequalified “workers” in the United States who were judged to be reliable over the course of many previous (non-salience-related) experiments. Each session is delivered through a “human intelligence task” (HIT) page, which contains instructions and the stimuli for that session (see Fig. 1-2 in the extended data section for an example of a HIT page used in the present experiment). Because we were interested in the extent to which a relatively free listening environment can result in meaningful data, we did not impose constraints on sound delivery or level (though it was suggested that the participants listen over headphones).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7602</offset><text>Participants made relative salience judgments on sounds presented in pairs. In total, a pool of 7038 pairs (153 possible pairs × 2 orders to control for order effects × 23 repetitions) were generated. The pairs were then arranged in 207 different HITs by randomly selecting subsets of 34 different pairs from the above pool. Each HIT also included six randomly interspersed “catch trials” in which the two sounds were identical. Each HIT therefore contained 40 sound pairs along with task instructions. Each pair had its own “Play” button, which when pressed started the presentation of the corresponding sound pair, with a 500 ms silent gap between the two sounds. Participants were asked which sound was “more salient or noticeable.” “Which sound would you think is more distracting or catches your attention?” Participants could only listen to each pair once before responding by selecting one of the “first,” “second,” or “identical” buttons to progress to the next sound pair. Participants were instructed to choose the “identical” button only if the sounds were physically identical (catch trials). Failure to respond appropriately to the catch trials (or choosing the “identical” response for the noncatch trials) indicated lack of engagement with the experiment and resulted in the data from that session being excluded from analysis (∼10% exclusion rate, see below). Participants were offered financial compensation approximately equal to the minimum U.S. wage and prorated for the 5 min experiment time. To encourage participant engagement, we paid a small bonus when participants correctly responded to identical sounds and subtracted a small amount for each miss. Each HIT was run by five unique workers for an overall number of 1035 sessions. The time limit for task completion was set to 60 min, though we expected the experiment to last an average of 3 min. Figure 2A plots the actual duration distribution. Most sessions were completed within 3 min.</text></passage><passage><infon key="file">zns9991919310002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>9608</offset><text>MTurk task data. A, Distribution of time spent on each HIT. B, Distribution of number of HITs completed per worker.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9724</offset><text>Each participant was free to complete up to a maximum of nine different HITs. A distribution of HITs per worker is in Figure 2B. Most (71.4%) completed one HIT only, whereas 52 workers (12.4%) completed the maximum number of HITs. We did not find any relationship between participants” number of HITs and performance on catch trials. From the total of 1035 sessions completed, 57 included a single missed catch trial, 11 included two missed catch trials and 11 included more than two missed catch trials. Fifty-eight sessions contained false positives. Overall, 124 sessions were excluded. The remaining sessions were composed of 384 unique workers, of which 270 completed only one HIT and the rest completed multiple HITs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10450</offset><text>Salience ranking was computed by pooling across all HITs and counting the proportion of pairs on which each sound was judged as more salient. Variability was estimated using bootstrap resampling (1000 iterations), where on each iteration, one session for each of the 207 unique HITs was randomly selected for the ranking analysis. The error bars in Figure 1B are one standard deviation from this analysis. The same ranking was obtained after removing sessions with durations exceeding the 90th percentile (14.09 min, n = 820 remaining) or the 75th percentile (5.98 min, n = 683 remaining).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>11040</offset><text>Acoustic analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11059</offset><text>The salience data were analyzed to examine possible correlations with several key acoustic features previously implicated in perceptual salience.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11205</offset><text>An overall loudness measure was produced by a model in which the acoustic signal was filtered with a bank of bandpass filters of width 1 ERB and center frequencies spaced 1/2 ERB from 30 Hz to 16 kHz. The instantaneous power of each filter output was smoothed with a 20 ms window and elevated to power 0.3 to approximate specific loudness. Outputs were then averaged across channels to produce a single value. This model was preceded by a combination of high-pass and low-pass filters to approximate effects of outer and middle ear filtering.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11748</offset><text>Several key measures of salience derived from the model of were examined. Paralleling work in the visual modality, this model produces an auditory saliency map in the form of a frequency x time representation, indicating the spectrotemporal loci that are hypothesized to be particularly perceptually salient. The representation is computed by independently extracting several key auditory features (loudness, spectral and temporal contrast), which are normalized to create a feature-independent scale and then linearly combined together to create the overall map. For the present analysis, we extracted several parameters from the salience map computed for each sound-token: the maximum value within the saliency map (this is the parameter used in the experiments reported in), the mean saliency value across the entire map, and max/mean gradient across the frequency and/or time dimensions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12640</offset><text>Roughness was calculated from the modulation power spectrum, computed using the approach described in (see also). Roughness is associated with energy in the high end (&gt;30 Hz) of the amplitude modulation spectrum, though it also depends on modulation depth and other spectral factors. As is typical of natural wide-band sounds, in our sound set we found a strong correlation (Spearman r = 0.808, p &lt; 0.0001) between power at high modulation rates (30–100 Hz) and those below 30 Hz (0–30 Hz). We also noted that salience (MTurk derived; see Results) significantly correlated with power at modulations between 30 and 100 Hz (Spearman, r = 0.585 p = 0.01) but not with the low frequency (0–30 Hz) modulations (Spearman r = 0.222 p = 0.376). Controlling for low frequency modulations as a covariate (partial correlation), yielded a substantially stronger correlation (Spearman r = 0.707 p = 0.002), suggesting that the high modulation rates, independently of overall modulation power, contributed to salience. Therefore, to specifically isolate the contribution of high modulation rates, and control for overall power across the modulation spectrum, “roughness” was quantified as the ratio between power at modulations between 30 and 100 Hz and power between 0 and 100 Hz (i.e., across the full range). See also Figure 3, for a comparison of how roughness in the present set relates to the range of roughness (calculated in the same way) obtained from a diverse set of environmental sounds.</text></passage><passage><infon key="file">zns9991919310003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>14136</offset><text>To estimate the range of “roughness” values that we might expect to encounter in the environment, we quantified roughness (see Materials and Methods) for a large set of diverse natural sounds (N = 274; sound duration = 500 ms to match that in the present experiment) from a set described previously. This information is presented in histogram form (gray bars). Roughness values for the sounds used in the present study are indicated by black diamonds. We also include roughness calculated for the scream sounds from (red diamonds). This analysis confirms that the set of sounds we used spans the range of roughness obtained from a diverse set of environmental sounds. The sounds at the top of the roughness range in our set, overlap with the roughness range defined by the scream sounds from.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>14933</offset><text>Subjective salience via in-laboratory replication</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>14983</offset><text>Participants.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14997</offset><text>To verify that the online salience ranking data also hold when tested in a more controlled environment, 18 paid participants (15 females, average age 23.8, range 18–31) took part in an in-lab replication study; all reported normal hearing and no history of neurological disorders. Experimental procedures were approved by the research ethics committee of University College London and written informed consent was obtained from each participant.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>15445</offset><text>Experimental design and statistical analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15491</offset><text>We again used a pairwise task with identical presentation parameters as in the MTurk experiment, but that every participant was presented with the full set of all possible pairs (78 pairs × 2 possible orders × 2 repetitions) for a total of 312 pairs of sounds. These were presented in a random order in six consecutive blocks (∼8 min). Each block also contained eight randomly interspersed catch trials of identical sounds. Participants were allowed a short rest between blocks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15974</offset><text>The stimuli were delivered to the participants” ears by Sennheiser HD558 headphones via a UA-33 sound card (Roland) at a comfortable listening level self-adjusted by each participant. Stimulus presentation and response recording were controlled with the Psychtoolbox package (Psychophysics Toolbox Version 3) with MATLAB (The MathWorks). Participants were tested in a darkened and acoustically shielded room (IAC triple-walled sound-attenuating booth). The session lasted for 1 h, starting with the same instructions given in the MTurk experiment. Participants were instructed to fixate their gaze on a white cross at the center of the computer screen while listening to the stimuli, and to respond by pressing one of 3 keyboard buttons to indicate “sound A more salient”/“sound B more salient”/“identical sounds.” The participant's response initiated the following trial with a random intertrial interval of 1.5 to 2 s. Blocks featuring incorrect responses—whether a miss or a false alarm—to any of the eight catch trials indicated lack of engagement and the whole block was discarded from the analysis. In this instance, all participants performed perfectly with a 100% hit rate and 0% false alarm rate resulting in no exclusions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>17225</offset><text>Eye tracking</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>17238</offset><text>Participants.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17252</offset><text>This experiment was performed by a total of 30 paid participants (28 females; aged 18–29, average 23.33), with 15 participants initially (14 females; aged 21∼28, average 23.53), subsequently supplemented by an additional group of 15 participants so as to have a better measure of variability across the population. No participants were excluded from this experiment. All reported normal hearing and no history of neurological disorders. All participants were naive to the aims of the experiment and none had participated in the in-laboratory ranking experiment described above. Experimental procedures were approved by the research ethics committee of University College London and written informed consent was obtained from each participant.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>17999</offset><text>Experimental design and statistical analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18045</offset><text>Sixteen sounds out of the original set were used in this experiment. Sound #3 and sound #9 (Fig. 1) were excluded due to experiment length constraints.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18197</offset><text>The effective onset (the time point to which the eye tracking analysis is time locked) was adjusted for each sound-token to a time where level exceeded a fixed threshold. The threshold was defined as the 20th percentile of the distribution of power (per time sample; over the initial 50 ms) pooled across sound tokens. Further controls for onset energy, based on correlating loudness at onset with the various eye tracking measures, are described in the Results section.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18668</offset><text>Participants listened passively to the sounds, which were presented in random order, with a randomized intertrial interval between 6 and 7 s. In total, 320 trials (16 sound-tokens × 20 repetitions of each) were presented. Stimuli were diotically delivered to participants' ears using Sennheiser HD558 headphones via a Creative Sound Blaster X-Fi sound card (Creative Technology) at a comfortable listening level self-adjusted by each participant. Stimulus presentation and response recording were controlled with the Psychtoolbox package (Psychophysics Toolbox Version 3) on MATLAB (The MathWorks R2018a). Participants sat with their head fixed on a chinrest in front of a monitor (viewing distance 65 cm) in a dimly lit and acoustically shielded room (IAC triple-walled sound-attenuating booth). They were instructed to continuously fixate at a black cross presented at the center of the screen against a gray background and to passively listen to the sounds (no task was performed). A 24-inch monitor (BENQ XL2420T) with 1920 × 1080 pixel resolution and 60 Hz refresh rate presented the fixation cross and feedback. The visual display remained the same throughout. To avoid pupillary light reflex effects, display and ambient room luminance were kept constant throughout the experiment. To reduce fatigue, the experiment was divided into 9 4 min blocks, each separated by a 4 min rest period.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>20065</offset><text>Pupil measurement.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20084</offset><text>An infrared eye-tracking camera (Eyelink 1000 Desktop Mount, SR Research) positioned just below the monitor continuously tracked gaze position and recorded pupil diameter, focusing binocularly with a sampling rate of 1000 Hz. The standard five-point calibration procedure for the Eyelink system was conducted before each experimental block. Participants were instructed to blink naturally. They were also encouraged to rest their eyes briefly during intertrial intervals. Before each trial, the eye tracker automatically checked that the participants” eyes were open and fixated appropriately; trials would not start unless this was confirmed.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>20730</offset><text>Analysis of eye blinks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20754</offset><text>Eye blinks are commonly observed as an involuntary response to abrupt sounds, part of the brainstem-mediated startle reflex. The elicitation of blinks has been shown to be sensitive to a range of stimulus manipulations and it was, therefore, important to relate eye-blink incidence to the measures of salience used here.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21075</offset><text>Because the blink reflex occurs rapidly after stimulus presentation, we analyzed data from the first 500 ms after sound onset. For each subject and sound token, eye-blink incidence was computed by tallying the number of trials that contained a blink (defined as full or partial eye closure). Although the incidence of blinks was low overall (&lt;10%), it varied substantially across participants. For the correlation analyses reported below (Fig. 4), the average rate across participants was computed for each sound condition.</text></passage><passage><infon key="file">zns9991919310004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>21599</offset><text>The incidence of eye blinks (i.e., the proportion of trials containing any blinks in the initial 500 ms after sound onset per condition across subjects). The blink rate was not correlated with the crowd-sourced salience rating (A), loudness (B), or MSI latency (C). All correlations are conducted using the Spearman rank method.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>21928</offset><text>Analysis of pupil diameter data.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21961</offset><text>To measure the sound-evoked pupil dilation responses (Fig. 5), the pupil diameter data of each trial were epoched from 0.5 s before to 3 s after sound onset. Intervals where the eye tracker detected full or partial eye closure (manifested as loss of the pupil signal) were automatically treated as missing data and recovered with shape-preserving piecewise cubic interpolation; epochs with &gt;50% missing data were excluded from analysis. On average, less than two trials per participant were rejected.</text></passage><passage><infon key="file">zns9991919310005.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>22462</offset><text>Measures of pupillary dilation are not correlated with crowd-sourced salience. A, The PDR obtained from the full group (N = 30). The solid lines represent the average normalized pupil diameter as a function of time relative to the onset of the sound. The line color indicates the MTurk salience ranking; more salient sounds are labeled in increasingly warmer colors. The solid black line is the grand-average across all conditions. The dashed line marks the peak average PDR. B, The PDR derivative. C, Correlations between crowd-sourced salience and peak PDR amplitude (left) and maximum of PDR derivative (right). None of the effects were significant. A similar analysis based on sound-token specific peaks (as opposed to based on the grand average) also did not yield significant effects. All correlations are conducted using the Spearman rank method.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23316</offset><text>To compare results across blocks, conditions, and participants, the epoched data within each block were z-score normalized. A baseline correction was then applied by subtracting the median pupil size over the pre-onset period; subsequently, data were smoothed with a 150 ms Hanning window.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>23606</offset><text>Microsaccade analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23629</offset><text>Microsaccade detection was based on the algorithm proposed by. In short, microsaccades were extracted from the continuous horizontal eye-movement data based on the following criteria: (a) a velocity threshold of λ = 6 times the median-based standard deviation within each block; (b) above-threshold velocity lasting for longer than 5 ms but &lt;100 ms; (c) the events are binocular (detected in both eyes) with onset disparity &lt;10 ms; and (d) the interval between successive microsaccades is longer than 50 ms.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24142</offset><text>Extracted microsaccade events were represented as unit pulses (Dirac delta). Two complementary analysis approaches were employed. The first involved tallying MS events, collapsed across subjects and trials (for more details, see the Results section). The second approach entailed analyzing MS rate time series: For each sound token in each participant, the event time series were summed and normalized by the number of trials and the sampling rate. Then, a causal smoothing kernel, ω(τ) = α2 × τ × e−ατ, was applied with a decay parameter of α = 1/50 ms, paralleling a similar technique for computing neural firing rates from neuronal spike trains (; see also). The obtained time series was then baseline corrected over the pre-onset interval. Due to the low baseline incidence of microsaccades per participant (approximately two events per second) and the small number of presentations per sound token (n = 20; required to prevent perceptual adaptation) a within-subject analysis was not possible. Mean microsaccade rate time series, obtained by averaging across participants for each sound token, are used for the analyses reported here. Robustness is verified using bootstrap resampling (see results).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_4</infon><offset>25369</offset><text>Correlation analysis.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25391</offset><text>To control for outlier effects, all reported bivariate and partial correlations were performed using the conservative Spearman's rank correlation method (two-tailed). The one exception to this was the direct comparison of MSI- and PDR-based correlations, where we computed Pearson correlations between crowd-sourced salience and the various eye tracking measures discussed in the results (MSI latency, PDR peak amplitude, PDR derivative peak amplitude; see defined below). Differences in Pearson correlation coefficients were tested using the procedures for testing statistical differences between correlations using the implementation in the R package cocor.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>26051</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26059</offset><text>Crowd-sourced salience ranking yielded a meaningful and stable salience scale</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26137</offset><text>Eighteen environmental sounds (Fig. 1A for spectrograms and Fig. 1-1 in the Extended data section for sound files) originally used by, were selected for this study. The stimulus set represents the variety of sounds that may be encountered in an urban acoustic environment, including animal sounds, human nonspeech sounds (kiss, sneeze), musical instrumental sounds, impact sounds (golf ball, tennis, impact, coins), and an assortment of mechanical sounds (car alarm, alarm clock, camera shutter, pneumatic drill, lawn mower etc.). All stimuli were 500 ms long and RMS equated.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26714</offset><text>We obtained salience rankings of this sound set from 911 online participants via the Amazon Mechanical Turk (MTurk) platform (see Materials and Methods). Sounds were presented in pairs, and participants were required to report which one was “more salient or attention-grabbing.”</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26997</offset><text>Over all responses, a small but robust order-of-presentation bias for selecting the second sound in a pair was observed (t = −9.240, p &lt; 0.001; mean probability to choose the first sound = 0.47, mean probability to choose the second sound = 0.53). However, because the order of presentation was counterbalanced across pairs, this bias did not affect the rating results.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27369</offset><text>To derive a relative measure of salience for each sound, we counted the proportion of pairs (collapsed across all data from all participants) on which each sound was judged as more salient, producing a measure of relative salience ranging between 0 and 1 (Fig. 1B). It is striking that a clear scale of subjective salience can be captured across these 18 brief, arbitrarily selected sounds. Variability was estimated using bootstrap resampling (1000 iterations), where on each iteration, salience was computed over a subset of the data (see Materials and Methods). The error bars in Figure 1B are 1 SD from this analysis.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>27991</offset><text>Crowd-sourced salience scale is strongly correlated with in-laboratory salience judgements</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28082</offset><text>An in-laboratory replication was conducted to validate the salience scale obtained from the MTurk experiment. The paradigm was essentially identical, but the experiments were performed in a laboratory setting and under a controlled listening environment. The main differences were that, to reduce test time, the sound set was reduced to 13 sounds (selected to capture the salience range of the full set and indicated in orange bars; Fig. 1B) and all participants listened to all sound pairs during an hour-long session.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28602</offset><text>Because the “in-laboratory” experiment was designed to mirror the MTurk experiment, the planned analysis involved collapsing across trials and subjects in the same way as described above. The in-laboratory ranking showed a strong correlation with the online ranking (r = 0.857, p &lt; 0.0001; Fig. 1C).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28906</offset><text>The small number of trials per sound-pair (n = 4), which was necessary to reduce perceptual adaptation (and to fit into time constraints), produces rather noisy subject-level data. Regardless, we attempted to assess the association between MTurk and in-laboratory rating on an individual level using a repeated measures correlation analysis (rmcorr package in R). The results confirmed that subject level correlation with the MTurk data was significant (Pearson r = 0.428, p &lt; 0.0001; Spearman r = 0.455, p &lt; 0.0001), though, as expected, accounting for noise at the individual subject level resulted in a decreased explained variance. Overall, the individual level analysis supports the conclusion that the group level data can be taken as representative of single subjects.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>29682</offset><text>Crowd-sourced salience correlates with acoustic roughness</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29740</offset><text>Although the present set of sounds is too small to systematically pinpoint the sound features that contribute to auditory salience, we sought to understand whether the obtained “subjective” salience scale correlates with several key acoustic features, previously hypothesized as contributing to salience:</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30049</offset><text>We found that, despite the fact that loudness is known to be a prominent contributor to perceptual salience, the crowd-sourced salience scale in the present set did not significantly correlate with loudness (r = 0.428, p = 0.078; see Materials and Methods for details about the loudness measure). This may be partly because the level of these sounds was RMS-normalized thus removing some of the larger differences in loudness between sounds.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30491</offset><text>Next, we tested the relationship between crowd-sourced salience and measures of salience derived from the model of. Several relevant parameters were examined (see Materials and Methods). Only correlations with the gradient along the frequency dimension were significant (Spearman's r = 0.525 p = 0.027 for the maximum gradient and r = 0.488, p = 0.049 for the mean gradient; for the rest of the comparisons p ≥ 0.155), indicating that perceptual salience may be associated with salience maps in which salient regions are sparsely spread across the spectrum.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31051</offset><text>Motivated by previous work, we also investigated the correlation between perceptual salience and roughness -a perceptual quality that is associated with energy in the high end (&gt;30 Hz) of the amplitude modulation spectrum (e.g.,; see Materials and Methods). Here, the correlation between crowd-sourced salience and roughness yielded a significant effect (r = 0.709, p = 0.001; Fig. 1D), consistent with accumulating evidence that roughness is a major contributor to salience.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>31527</offset><text>Crowd-sourced salience correlates with objective measures from ocular dynamics</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31606</offset><text>Next, we investigated whether acoustic salience automatically (i.e., without a remit from a task) modulates ocular orienting responses. A subset of 16 of the 18 original sounds (two sounds, 3 and 9, were excluded due to experimental time constraints) were presented to naive, centrally-fixating subjects who listened passively to the sounds, without performing any task, while their gaze position and pupil diameter were continuously tracked. Sounds were presented in random order, and with a random intersound interval between 6 and 7 s. Overall, each sound was presented 20 times across the experimental session. This small number of repetitions was chosen so as to minimize potential effects of perceptual adaptation to the stimuli. The analysis is therefore based on group-level correlations. Resampling based analyses were conducted to derive an estimate of the distribution of correlation strengths in the population.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32530</offset><text>We analyzed two types of rapid orienting responses: the “ocular freezing” (MSI) response and the PDR. We also analyzed the incidence of eye blinks and their possible relationship to perceptual salience. Eye blinks are a component of the brainstem-mediated startle reflex, hypothesized to reflect an automatic defensive response to abrupt or threatening stimuli. The startle eye blink response is commonly elicited by loud, rapidly rising sounds, but has been shown to be sensitive to a range of stimulus manipulations.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>33053</offset><text>Incidence of eye blinks was not correlated with crowd-sourced salience</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33124</offset><text>The incidence of eye blinks was low overall (&lt;10%) and did not significantly correlate with any of the measures reported here.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>33251</offset><text>Measures of pupillary dilation were not correlated with crowd-sourced salience</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33330</offset><text>The temporal evolution of the normalized pupil diameter (the pupil dilation response, PDR) is presented in Figure 5. The pupil starts to dilate around 0.5 s after sound onset, and peaks at ∼1.12 s (ranging from 1.02 to 1.33 s). We did not observe significant correlation between crowd-sourced salience rating and any key parameters associated with PDR dynamics (see Fig. 5C for statistics), including the PDR peak amplitude and the peak of the PDR derivative (maximum rate of change of the PDR).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>33828</offset><text>Crowd-sourced salience is correlated with MSI</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33874</offset><text>The microsaccade results are shown in Figure 6. Consistent with previous demonstrations, we observed an abrupt inhibition of microsaccadic activity after sound presentation. The drop in microsaccade rate began at ∼0.3 s after onset and reached a minimum at 0.45 s.</text></passage><passage><infon key="file">zns9991919310006.jpg</infon><infon key="id">F6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34141</offset><text>MSI is correlated with crowd-sourced salience. A, Raster plot of microsaccade events (pooled across all participants) as a function of time relative to sound onset. The y-axis represents single trials; each dot indicates the onset of a microsaccade. Trials are grouped by sound-token and arranged according to the MTurk-derived salience scale (increasingly hot colors indicate rising salience). The region of MSI, between 0.2 and 0.7 s post-sound onset, is highlighted with a black rectangle. B, Over this time interval, the MS rate (number of MS events per second) is correlated significantly with the crowd-sourced salience rating. The result of bootstrap resampling is shown as the distribution of correlation coefficients (C) and the distribution of associated p-values (D). The vertical red dashed line indicates p = 0.05. E, Average microsaccade rate time series for each sound (F) focusing on the MSI region. The solid black curve is the grand-average MS rate across all sound tokens. MSI commences at ∼0.3 s after sound onset (open circle) and peaks around 0.45 s (solid black circle). The horizontal dashed line indicates the mid-slope of the grand average (amplitude = −0.04 a.u., time = 0.37 s). Black crosses mark the time at which the response to each sound intersects with this line, as a measure of MSI latency. G, Correlation between these values and the crowd-sourced salience rating. All correlations are conducted using the Spearman rank method. Note identical correlation values in G and B are a chance occurrence (the two analyses are independent).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35715</offset><text>We conducted two different analyses to determine the extent to which MSI differs across sounds. The first approach is based on pooling MS data across trials and subjects and counting MS events. We defined a window spanning a 500 ms interval from 200 to 700 ms after sound onset. This window encompasses the interval before the beginning of MSI and after it has settled (Fig. 6A; the extent of the interval is also shown in Fig. 6F). We then tallied the MS events for each sound token. This measure correlated with crowd-sourced salience such that more salient sounds were associated with fewer MS events (i.e., a lower MS rate) within the window (r = −0.627, p = 0.009; Fig. 6B). As can be seen in Figure 6B the number of MS events is small overall and the differences between conditions are narrow, reflecting the low incidence of micro saccades. The robustness of the observed correlation was confirmed with bootstrap resampling (see Fig. 6C; 5000 iterations; balanced) where on each iteration we selected 30 participants with replacement to compute the tally. This analysis (Fig. 6C) confirmed a negatively skewed distribution of r values centered around −0.5 (median r = −0.458), with 98.72% of r values smaller than 0 (p = 0.013) and a left skewed distribution of associated p-values (Fig. 6D). This effect was maintained for windows spanning up to 1 s from sound onset.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37097</offset><text>The MS rate demonstrated a significant correlation with roughness (r = −0.607 p = 0.013) but not with loudness (r-0.353 p = 0.18). The correlation between MS rate and crowd-sourced salience was no longer significant when controlling for roughness as a covariate (partial correlation r(13) = −0.350 p = 0.201), suggesting that dependence on roughness is a major contributor to the correlation between MSI and crowd-sourced salience.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37533</offset><text>Next, we aimed to understand more precisely how the dynamics of MSI vary with salience by quantifying the MSI latency for each sound. This was accomplished by computing a MS rate time series for each token (Fig. 6E; see Materials and Methods). MSI latency was then determined by computing a grand-mean microsaccade rate time series (averaged across sound tokens; see Fig. 6E), identifying its mid-slope amplitude (horizontal dashed line in Fig. 6F), and obtaining the time at which the microsaccade rate time series associated with each sound token intersected with this value. This latency, hereafter referred to as the MSI latency, correlated with the crowd-sourced salience rating (r = −0.627, p = 0.009; Fig. 6G), such that increasing salience was associated with earlier MSI.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38316</offset><text>The correlation between MSI latency and crowd-sourced salience was significantly different from the PDR correlations with crowd-sourced salience reported above (MSI vs PDR: z = 2.6369, p = 0.0042; MSI vs PDR derivative: z = 3.0640, p = 0.0011; see Materials and Methods). It was further confirmed that MSI latency did not significantly correlate with blink rates within the first 500 ms of sound onset (Fig. 4C).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38729</offset><text>Additional analyses to confirm effect robustness (Fig. 7) used bootstrap resampling to estimate the stability of the correlation between MSI latency and crowd-sourced salience across the subject pool. This involved computing a distribution of p- and r-values for subgroup sizes of 30 and 15 subjects (with replacement). We iteratively (5000 iterations) selected n samples (n = 15 or 30) from the full pool of N = 30. For each subset, we computed the correlation between MSI latency and crowd-sourced salience. The distribution of associated correlation coefficients demonstrated a moderate correlation (median r = −0.5063 for N = 15, r = −0.4615 for N = 30) between MSI latency and crowd-sourced salience. The distributions of p-values are significantly left-skewed (Fisher's method; p &lt; 0.0001; further details in the figure), indicative of a true effect.</text></passage><passage><infon key="file">zns9991919310007.jpg</infon><infon key="id">F7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>39590</offset><text>Estimation of the stability of the correlation between MSI latency and crowd-sourced salience. Left, Distribution of the Spearman correlation coefficients derived from resampling analyses with subgroup sizes of 15 or 30 participants. In both cases, the distribution peaks around r = −0.5. Right, Distribution of the p-values associated with each n. The red vertical dashed line indicates p = 0.05. A uniform distribution is expected under the null. The left skewed pattern observed here indicates a true effect. Skewness was formally confirmed by a χ2 test on p-values &lt;0.05 (n = 30: χ2(1066) = 1405.42, p &lt; 0.0001; n = 15: χ2(748) = 903.2, p &lt; 0.0001).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40250</offset><text>To determine what acoustic information might have driven the observed microsaccade effect, we correlated the MSI latency with the measures obtained from the model (see Materials and Methods). This analysis revealed no significant correlations (p ≥ 0.203 for all).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40516</offset><text>We also correlated MSI latency for each sound with roughness and loudness estimates computed between 0 and 300 ms (window sizes of 50, 100, 150, 200, 250 and 300 ms) - e.g., over the interval between sound onset and the average onset time of ocular inhibition. For loudness, none of the correlations reached significance (p &gt; 0.152), This suggests that though the sounds used had clearly differing distributions of power at onset, this did not contribute primarily to the correlation with MSI. The correlation between MSI latency and crowd-sourced salience was maintained even when controlling for loudness at onset (0–50 ms window from onset; partial correlation; r(13) = −0.666, p = 0.007; same holds for longer intervals).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41246</offset><text>In contrast to the lack of a stable link between loudness and MSI latency, a significant correlation with roughness was present from 250 ms onwards (p ≤ 0.028, r ≥ −0.547), confirming the previous observations of a strong link between roughness and MSI rate. The correlation between MSI latency and salience was no longer significant when controlling for roughness as a covariate (partial correlation r(13) = −0.455 p = 0.088), suggesting that dependence on roughness is a major contributor to the correlation between MSI and crowd-sourced salience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>41804</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>41815</offset><text>The main aim of this work was to understand whether/how ocular orienting responses in naive listeners are modulated by acoustic salience. We showed that a crowd-sourced 'subjective” (i.e., rating based) salience ranking of brief, nonspatial, environmental sounds robustly correlated with the ocular freezing response measured in naive, passively listening participants. Sounds ranked by a large group of online participants as more salient evoked earlier MSI, consistent with a faster orienting response (Fig. 6). These results establish that information about auditory salience is conveyed to the SC, the primary generator of micro saccades, within ∼300 ms after sound onset. That sounds systematically modulated microsaccade activity demonstrates that the mechanisms that drive MSI are sensitive to a broad range of salient events beyond the visual modality.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>42680</offset><text>Crowd-sourced salience</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42703</offset><text>We demonstrated that a robust measure of perceptual salience can be obtained from a web-based mass-participant experimental platform. Online experimenting is gaining popularity within cognitive science (for review, see), including in the auditory modality. However, there are various potential drawbacks to this approach relating to lack of control over the participants” listening devices and environment. These may be especially severe for perceptual judgment experiments that demand a high level of engagement from participants. However, the limitations are offset by important unique advantages, including the opportunity of obtaining a large amount of data in a short period of time, and running brief “one-shot” experiments that are critical for avoiding perceptual adaptation. Furthermore, in the context of salience, the variability of the sound environment may in fact provide “real-world” validity to the obtained scale. Here, we established that despite the various concerns outlined above, capitalizing on big numbers makes it possible to acquire a stable, informative, salience scale with relatively minimal control of the listeners and their environment. Indeed, the salience scale obtained online correlated robustly with in-laboratory ranking measures as well as with certain acoustic features previously established as contributing to perceptual salience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>44086</offset><text>Specifically, we found a strong correlation with “roughness,” the perceptual attribute that is associated with “raspy,” “buzzing,” or “harsh” sounds. This correlation arose “organically” in the sense that the sounds in the present study were not selected to vary across this or other acoustic dimensions. The link between roughness and salience is consistent with previous reports establishing a clear role for this feature in determining the perceptual prominence of sounds. Most recently, this was demonstrated in the context of the distinctiveness of screams (; though the authors used the term “fearful” as opposed to “salient” in their experiments).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>44769</offset><text> have proposed a model for auditory salience, inspired in its architecture by the well established model for visual salience. We found limited correlation between the parameters derived from that model and the present crowd-sourced scale. This is possibly because the Kayser model is better suited to capturing “pop-out”-like saliency, associated with attentional capture by an object that stands out from its background. Instead, here we focused on brief sounds reflecting single acoustic sources.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>45272</offset><text>It is important to stress that the present sound set is too small for an extensive exploration of the features that might drive perceptual salience. Roughness likely stood out here because of the primacy of that feature and because our sounds spanned a large enough roughness range (Fig. 3). The robustness of the crowd-sourced judgements suggests that a similar crowd-sourcing approach but with a larger, and perhaps more controlled, set of sounds may reveal other relevant sound features. In particular, recent advances in sound synthesis technologies make it possible to systematically and independently vary acoustic features toward a controlled investigation of acoustic salience.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>45958</offset><text>Acoustic salience did not modulate pupil responses</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>46009</offset><text>The PDR indexes activity within the LC-norepinephrine system, which is proposed to play a key role in controlling global vigilance and arousal. In previous work reporting an association between the PDR and sound salience, the dominant driving feature for the correlation was loudness. In contrast, differences along this dimension were minimized in the present stimuli to allow us to focus on subtler, but potentially behaviorally important, contributors to perceptual salience. Our failure to observe a modulation of the PDR by salience suggests that, at least in the context of auditory inputs, pupil dilation may reflect a nonspecific arousal response, evoked by stimuli that cross a certain salience threshold. This account is consistent with the relatively late timing of the PDR (peaking approximately 1 s after sound onset) thereby potentially reflecting a later stage of processing than that captured by microsaccades (see below).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>46948</offset><text>MSI is a correlate of acoustic salience</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>46988</offset><text>We revealed a robust correlation between MSI latency and crowd-sourced salience: Sounds judged by online raters as more salient were associated with a more rapid (Fig. 6G) and extensive (as reflected by decreased incidence; Fig. 6B) inhibition of microsaccadic activity. The effect arose early, from ∼350 ms after sound onset, pointing to fast underlying circuitry. Correlation analyses indicated that the bulk of this effect is driven by a correlation with roughness, suggesting that this information is computed sufficiently early to affect the body's automatic reorienting response.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>47576</offset><text>The brain mechanisms that respond to acoustic roughness are poorly understood. Response signatures have been observed in both auditory cortical and subcortical areas, including the amygdala, a key brain center for fear/risk processing. reported that the amygdala, but not auditory cortex, exhibited specific sensitivity to temporal modulations within the roughness range. This was interpreted as suggesting that rough sounds activate neural systems associated with the processing of danger. The present findings, demonstrating an association between salience/roughness and rapid orienting responses, are consistent with this conclusion.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48213</offset><text>Microsaccades are increasingly understood to index an active attentional sampling mechanism that is mediated by the SC. Accumulating work suggests that MS occurrence is not automatic but rather modulated by the general state of the participant and by the availability of computational capacity, such that microsaccade incidence is reduced under high load. MSI is an extreme case for such an effect of attentional capture on ocular dynamics, interpreted as reflecting an interruption of ongoing attentional sampling so as to prioritize the processing of a potentially important sensory event. The dominant account for MSI is that sensory input to the SC causes an interruption to ongoing activity by disturbing the balance of inhibition and excitation. Previously reported effects of visual salience on MSI were therefore interpreted as indicating that visual salience may be coded at the level of the SC (see also). We showed that the perceptual salience of sounds also modulates this response, consistent with a well-established role for the SC as a multisensory hub. Importantly, this effect was observed during diotic presentation—sounds did not differ spatially and were perceived centrally, within the head.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49428</offset><text>The present results thus suggest that an investigation of SC responses to sound may provide important clues to understanding auditory salience. There is evidence for projections from the auditory cortex to the SC that might mediate the effects observed here, or they may arise via a subcortical pathway with the IC or the amygdala as an intermediary.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49779</offset><text>Finally, the present experiments focused on the salience of brief sounds presented in silence. However, the ongoing context within which sounds are presented is known to play a critical role in determining their perceptual distinctiveness. In the future, the paradigm established here can be easily expanded to more complex figure–ground situations or to tracking salience within realistic sound mixtures. A further question relates to understanding whether ocular dynamics reflect perceptual salience primarily linked to basic, evolutionary-driven sound features such as roughness or whether they can also be modulated by arbitrary sounds endowed with salience via association or reinforcement (e.g., ones' mobile ring tone).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>50508</offset><text>This work was supported by an EC Horizon 2020 grant to M.C. and a Biotechnology and Biological Sciences Research Council international partnering award to M.C.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">footnote</infon><offset>50668</offset><text>The authors declare no competing financial interests.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>50722</offset><text>References</text></passage><passage><infon key="fpage">5879</infon><infon key="lpage">5891</infon><infon key="name_0">surname:Adolphs;given-names:R</infon><infon key="name_1">surname:Tranel;given-names:D</infon><infon key="name_2">surname:Damasio;given-names:H</infon><infon key="name_3">surname:Damasio;given-names:AR</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.15-09-05879.1995</infon><infon key="pub-id_pmid">7666173</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">1995</infon><offset>50733</offset><text>Fear and the human amygdala</text></passage><passage><infon key="fpage">2051</infon><infon key="lpage">2056</infon><infon key="name_0">surname:Arnal;given-names:LH</infon><infon key="name_1">surname:Flinker;given-names:A</infon><infon key="name_2">surname:Kleinschmidt;given-names:A</infon><infon key="name_3">surname:Giraud;given-names:AL</infon><infon key="name_4">surname:Poeppel;given-names:D</infon><infon key="pub-id_doi">10.1016/j.cub.2015.06.043</infon><infon key="pub-id_pmid">26190070</infon><infon key="section_type">REF</infon><infon key="source">Curr Biol</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">2015</infon><offset>50761</offset><text>Human screams occupy a privileged niche in the communication soundscape</text></passage><passage><infon key="fpage">403</infon><infon key="lpage">450</infon><infon key="name_0">surname:Aston-Jones;given-names:G</infon><infon key="name_1">surname:Cohen;given-names:JD</infon><infon key="pub-id_doi">10.1146/annurev.neuro.28.061604.135709</infon><infon key="pub-id_pmid">16022602</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Neurosci</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2005</infon><offset>50833</offset><text>An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance</text></passage><passage><infon key="fpage">145</infon><infon key="lpage">150</infon><infon key="name_0">surname:Bach;given-names:DR</infon><infon key="name_1">surname:Schächinger;given-names:H</infon><infon key="name_2">surname:Neuhoff;given-names:JG</infon><infon key="name_3">surname:Esposito;given-names:F</infon><infon key="name_4">surname:Di Salle;given-names:F</infon><infon key="name_5">surname:Lehmann;given-names:C</infon><infon key="name_6">surname:Herdener;given-names:M</infon><infon key="name_7">surname:Scheffler;given-names:K</infon><infon key="name_8">surname:Seifritz;given-names:E</infon><infon key="pub-id_doi">10.1093/cercor/bhm040</infon><infon key="pub-id_pmid">17490992</infon><infon key="section_type">REF</infon><infon key="source">Cereb Cortex</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2008</infon><offset>50937</offset><text>Rising sound intensity: an intrinsic warning cue activating the amygdala</text></passage><passage><infon key="fpage">456</infon><infon key="name_0">surname:Bakdash;given-names:JZ</infon><infon key="name_1">surname:Marusich;given-names:LR</infon><infon key="pub-id_doi">10.3389/fpsyg.2017.00456</infon><infon key="pub-id_pmid">28439244</infon><infon key="section_type">REF</infon><infon key="source">Front Psychol</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017</infon><offset>51010</offset><text>Repeated measures correlation</text></passage><passage><infon key="fpage">607</infon><infon key="lpage">611</infon><infon key="name_0">surname:Blumenthal;given-names:TD</infon><infon key="pub-id_doi">10.1111/j.1469-8986.1988.tb01897.x</infon><infon key="pub-id_pmid">3186888</infon><infon key="section_type">REF</infon><infon key="source">Psychophysiology</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">1988</infon><offset>51040</offset><text>The startle response to acoustic stimuli near startle threshold: effects of stimulus rise and fall time, duration, and intensity</text></passage><passage><infon key="fpage">1417</infon><infon key="lpage">1431</infon><infon key="name_0">surname:Blumenthal;given-names:TD</infon><infon key="pub-id_doi">10.1111/psyp.12506</infon><infon key="pub-id_pmid">26283146</infon><infon key="section_type">REF</infon><infon key="source">Psychophysiology</infon><infon key="type">ref</infon><infon key="volume">52</infon><infon key="year">2015</infon><offset>51169</offset><text>Presidential address 2014: the more-or-less interrupting effects of the startle response</text></passage><passage><infon key="fpage">296</infon><infon key="lpage">306</infon><infon key="name_0">surname:Blumenthal;given-names:TD</infon><infon key="name_1">surname:Goode;given-names:CT</infon><infon key="pub-id_doi">10.1111/j.1469-8986.1991.tb02198.x</infon><infon key="pub-id_pmid">1946895</infon><infon key="section_type">REF</infon><infon key="source">Psychophysiology</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">1991</infon><offset>51258</offset><text>The startle eyeblink response to low intensity acoustic stimuli</text></passage><passage><infon key="fpage">11</infon><infon key="name_0">surname:Bonneh;given-names:YS</infon><infon key="name_1">surname:Adini;given-names:Y</infon><infon key="name_2">surname:Polat;given-names:U</infon><infon key="pub-id_doi">10.1167/15.9.11</infon><infon key="pub-id_pmid">26223023</infon><infon key="section_type">REF</infon><infon key="source">J Vis</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2015</infon><offset>51322</offset><text>Contrast sensitivity revealed by microsaccades</text></passage><passage><infon key="fpage">767</infon><infon key="name_0">surname:Bonneh;given-names:Y</infon><infon key="name_1">surname:Fried;given-names:M</infon><infon key="name_2">surname:Arieli;given-names:A</infon><infon key="name_3">surname:Polat;given-names:U</infon><infon key="pub-id_doi">10.1167/14.10.767</infon><infon key="section_type">REF</infon><infon key="source">J Vis</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2014</infon><offset>51369</offset><text>Microsaccades and drift are similarly modulated by stimulus contrast and anticipation</text></passage><passage><infon key="fpage">433</infon><infon key="lpage">436</infon><infon key="name_0">surname:Brainard;given-names:DH</infon><infon key="pub-id_doi">10.1163/156856897X00357</infon><infon key="pub-id_pmid">9176952</infon><infon key="section_type">REF</infon><infon key="source">Spat Vis</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">1997</infon><offset>51455</offset><text>The psychophysics toolbox</text></passage><passage><infon key="fpage">277</infon><infon key="lpage">282</infon><infon key="name_0">surname:Ciocchi;given-names:S</infon><infon key="name_1">surname:Herry;given-names:C</infon><infon key="name_10">surname:Müller;given-names:C</infon><infon key="name_11">surname:Lüthi;given-names:A</infon><infon key="name_2">surname:Grenier;given-names:F</infon><infon key="name_3">surname:Wolff;given-names:SB</infon><infon key="name_4">surname:Letzkus;given-names:JJ</infon><infon key="name_5">surname:Vlachos;given-names:I</infon><infon key="name_6">surname:Ehrlich;given-names:I</infon><infon key="name_7">surname:Sprengel;given-names:R</infon><infon key="name_8">surname:Deisseroth;given-names:K</infon><infon key="name_9">surname:Stadler;given-names:MB</infon><infon key="pub-id_doi">10.1038/nature09559</infon><infon key="pub-id_pmid">21068837</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">468</infon><infon key="year">2010</infon><offset>51481</offset><text>Encoding of conditioned fear in central amygdala inhibitory circuits</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">107</infon><infon key="name_0">surname:Cummings;given-names:A</infon><infon key="name_1">surname:Ceponiene;given-names:R</infon><infon key="name_2">surname:Koyama;given-names:A</infon><infon key="name_3">surname:Saygin;given-names:AP</infon><infon key="name_4">surname:Townsend;given-names:J</infon><infon key="name_5">surname:Dick;given-names:F</infon><infon key="pub-id_doi">10.1016/j.brainres.2006.07.050</infon><infon key="pub-id_pmid">16962567</infon><infon key="section_type">REF</infon><infon key="source">Brain Res</infon><infon key="type">ref</infon><infon key="volume">1115</infon><infon key="year">2006</infon><offset>51550</offset><text>Auditory semantic networks for words and natural sounds</text></passage><passage><infon key="fpage">6</infon><infon key="name_0">surname:Dalmaso;given-names:M</infon><infon key="name_1">surname:Castelli;given-names:L</infon><infon key="name_2">surname:Scatturin;given-names:P</infon><infon key="name_3">surname:Galfano;given-names:G</infon><infon key="pub-id_doi">10.1167/17.3.6</infon><infon key="pub-id_pmid">28278311</infon><infon key="section_type">REF</infon><infon key="source">J Vis</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>51606</offset><text>Working memory load modulates microsaccadic rate</text></passage><passage><infon key="fpage">287</infon><infon key="lpage">351</infon><infon key="name_0">surname:Davis;given-names:M</infon><infon key="name_1">surname:Eaton;given-names:RC</infon><infon key="section_type">REF</infon><infon key="source">Neural Mechanisms of Startle Behavior</infon><infon key="type">ref</infon><infon key="year">1984</infon><offset>51655</offset><text>The Mammalian Startle Response</text></passage><passage><infon key="name_0">surname:Dayan;given-names:P</infon><infon key="name_1">surname:Abbott;given-names:LF</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2001</infon><offset>51686</offset><text>Theoretical neuroscience</text></passage><passage><infon key="fpage">9</infon><infon key="name_0">surname:Dick;given-names:F</infon><infon key="name_1">surname:Bussiere;given-names:J</infon><infon key="section_type">REF</infon><infon key="source">Cent Res Lang Newsl</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2002</infon><offset>51711</offset><text>The effects of linguistic mediation on the identification of environmental sounds</text></passage><passage><infon key="fpage">e0121945</infon><infon key="name_0">surname:Diedenhofen;given-names:B</infon><infon key="name_1">surname:Musch;given-names:J</infon><infon key="pub-id_doi">10.1371/journal.pone.0121945</infon><infon key="pub-id_pmid">25835001</infon><infon key="section_type">REF</infon><infon key="source">PLoS One</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2015</infon><offset>51793</offset><text>cocor: a comprehensive solution for the statistical comparison of correlations</text></passage><passage><infon key="fpage">e1000302</infon><infon key="name_0">surname:Elliott;given-names:TM</infon><infon key="name_1">surname:Theunissen;given-names:FE</infon><infon key="pub-id_doi">10.1371/journal.pcbi.1000302</infon><infon key="pub-id_pmid">19266016</infon><infon key="section_type">REF</infon><infon key="source">PLoS Comput Biol</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2009</infon><offset>51872</offset><text>The Modulation Transfer Function for Speech Intelligibility</text></passage><passage><infon key="fpage">1035</infon><infon key="lpage">1045</infon><infon key="name_0">surname:Engbert;given-names:R</infon><infon key="name_1">surname:Kliegl;given-names:R</infon><infon key="pub-id_doi">10.1016/S0042-6989(03)00084-1</infon><infon key="pub-id_pmid">12676246</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2003</infon><offset>51932</offset><text>Microsaccades uncover the orientation of covert attention</text></passage><passage><infon key="fpage">3</infon><infon key="name_0">surname:Gao;given-names:X</infon><infon key="name_1">surname:Yan;given-names:H</infon><infon key="name_2">surname:Sun;given-names:H</infon><infon key="pub-id_doi">10.1167/15.3.3</infon><infon key="pub-id_pmid">25740876</infon><infon key="section_type">REF</infon><infon key="source">J Vis</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2015</infon><offset>51990</offset><text>Modulation of microsaccade rate by task difficulty revealed through between- and within-trial comparisons</text></passage><passage><infon key="fpage">2533</infon><infon key="lpage">2545</infon><infon key="name_0">surname:Hafed;given-names:ZM</infon><infon key="name_1">surname:Clark;given-names:JJ</infon><infon key="pub-id_doi">10.1016/S0042-6989(02)00263-8</infon><infon key="pub-id_pmid">12445847</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2002</infon><offset>52096</offset><text>Microsaccades as an overt measure of covert attention shifts</text></passage><passage><infon key="fpage">16220</infon><infon key="lpage">16235</infon><infon key="name_0">surname:Hafed;given-names:ZM</infon><infon key="name_1">surname:Ignashchenkova;given-names:A</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.2240-13.2013</infon><infon key="pub-id_pmid">24107954</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2013</infon><offset>52157</offset><text>On the dissociation between microsaccade rate and direction after peripheral cues: microsaccadic inhibition revisited</text></passage><passage><infon key="fpage">940</infon><infon key="lpage">943</infon><infon key="name_0">surname:Hafed;given-names:ZM</infon><infon key="name_1">surname:Goffart;given-names:L</infon><infon key="name_2">surname:Krauzlis;given-names:RJ</infon><infon key="pub-id_doi">10.1126/science.1166112</infon><infon key="pub-id_pmid">19213919</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">323</infon><infon key="year">2009</infon><offset>52275</offset><text>A neural mechanism for microsaccade generation in the primate superior colliculus</text></passage><passage><infon key="fpage">167</infon><infon key="name_0">surname:Hafed;given-names:ZM</infon><infon key="name_1">surname:Chen;given-names:CY</infon><infon key="name_2">surname:Tian;given-names:X</infon><infon key="pub-id_doi">10.3389/fnsys.2015.00167</infon><infon key="pub-id_pmid">26696842</infon><infon key="section_type">REF</infon><infon key="source">Front Syst Neurosci</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2015</infon><offset>52357</offset><text>Vision, perception, and attention through the lens of microsaccades: mechanisms and implications</text></passage><passage><infon key="fpage">3491</infon><infon key="lpage">3502</infon><infon key="name_0">surname:Hartmann;given-names:WM</infon><infon key="pub-id_pmid">8969472</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">100</infon><infon key="year">1996</infon><offset>52454</offset><text>Pitch, periodicity, and auditory organization</text></passage><passage><infon key="fpage">2163</infon><infon key="lpage">2176</infon><infon key="name_0">surname:Huang;given-names:N</infon><infon key="name_1">surname:Elhilali;given-names:M</infon><infon key="pub-id_doi">10.1121/1.4979055</infon><infon key="pub-id_pmid">28372080</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">141</infon><infon key="year">2017</infon><offset>52500</offset><text>Auditory salience using natural soundscapes</text></passage><passage><infon key="fpage">1489</infon><infon key="lpage">1506</infon><infon key="name_0">surname:Itti;given-names:L</infon><infon key="name_1">surname:Koch;given-names:C</infon><infon key="pub-id_doi">10.1016/S0042-6989(99)00163-7</infon><infon key="pub-id_pmid">10788654</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2000</infon><offset>52544</offset><text>A saliency-based search mechanism for overt and covert shifts of visual attention</text></passage><passage><infon key="fpage">161</infon><infon key="lpage">169</infon><infon key="name_0">surname:Itti;given-names:L</infon><infon key="name_1">surname:Koch;given-names:C</infon><infon key="pub-id_doi">10.1117/1.1333677</infon><infon key="section_type">REF</infon><infon key="source">Journal of Electronic Imaging</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2001</infon><offset>52626</offset><text>Feature combination strategies for saliency-based visual attention systems</text></passage><passage><infon key="fpage">221</infon><infon key="lpage">234</infon><infon key="name_0">surname:Joshi;given-names:S</infon><infon key="name_1">surname:Li;given-names:Y</infon><infon key="name_2">surname:Kalwani;given-names:RM</infon><infon key="name_3">surname:Gold;given-names:JI</infon><infon key="pub-id_doi">10.1016/j.neuron.2015.11.028</infon><infon key="pub-id_pmid">26711118</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">89</infon><infon key="year">2016</infon><offset>52701</offset><text>Relationships between pupil diameter and neuronal activity in the locus coeruleus, colliculi, and cingulate cortex</text></passage><passage><infon key="fpage">327</infon><infon key="name_0">surname:Kaya;given-names:EM</infon><infon key="name_1">surname:Elhilali;given-names:M</infon><infon key="pub-id_pmid">24904367</infon><infon key="section_type">REF</infon><infon key="source">Front Hum Neurosci</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2014</infon><offset>52816</offset><text>Investigating bottom-up auditory attention</text></passage><passage><infon key="fpage">1943</infon><infon key="lpage">1947</infon><infon key="name_0">surname:Kayser;given-names:C</infon><infon key="name_1">surname:Petkov;given-names:CI</infon><infon key="name_2">surname:Lippert;given-names:M</infon><infon key="name_3">surname:Logothetis;given-names:NK</infon><infon key="pub-id_doi">10.1016/j.cub.2005.09.040</infon><infon key="pub-id_pmid">16271872</infon><infon key="section_type">REF</infon><infon key="source">Curr Biol</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2005</infon><offset>52859</offset><text>Mechanisms for allocating auditory attention: an auditory saliency map</text></passage><passage><infon key="fpage">1501</infon><infon key="lpage">1508</infon><infon key="name_0">surname:Killion;given-names:MC</infon><infon key="pub-id_doi">10.1121/1.381844</infon><infon key="pub-id_pmid">690329</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">63</infon><infon key="year">1978</infon><offset>52930</offset><text>Revised estimate of minimum audible pressure: where is the “missing 6 dB”?</text></passage><passage><infon key="fpage">223</infon><infon key="lpage">235</infon><infon key="name_0">surname:Knudson;given-names:IM</infon><infon key="name_1">surname:Melcher;given-names:JR</infon><infon key="pub-id_doi">10.1007/s10162-016-0555-y</infon><infon key="pub-id_pmid">26931342</infon><infon key="section_type">REF</infon><infon key="source">J Assoc Res Otolaryngol</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2016</infon><offset>53009</offset><text>Elevated acoustic startle responses in humans: relationship to reduced loudness discomfort level, but not self-report of hyperacusis</text></passage><passage><infon key="fpage">161</infon><infon key="lpage">175</infon><infon key="name_0">surname:Krauzlis;given-names:RJ</infon><infon key="name_1">surname:Bogadhi;given-names:AR</infon><infon key="name_2">surname:Herman;given-names:JP</infon><infon key="name_3">surname:Bollimunta;given-names:A</infon><infon key="pub-id_doi">10.1016/j.cortex.2017.08.026</infon><infon key="pub-id_pmid">28958417</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">102</infon><infon key="year">2018</infon><offset>53142</offset><text>Selective attention without a neocortex</text></passage><passage><infon key="fpage">46</infon><infon key="lpage">55</infon><infon key="name_0">surname:Krishnan;given-names:S</infon><infon key="name_1">surname:Leech;given-names:R</infon><infon key="name_2">surname:Aydelott;given-names:J</infon><infon key="name_3">surname:Dick;given-names:F</infon><infon key="pub-id_pmid">23518401</infon><infon key="section_type">REF</infon><infon key="source">Hearing Research</infon><infon key="type">ref</infon><infon key="volume">300</infon><infon key="year">2013</infon><offset>53182</offset><text>School-age children's environmental object identification in natural auditory scenes: Effects of masking and contextual congruence</text></passage><passage><infon key="fpage">3183</infon><infon key="lpage">3184</infon><infon key="name_0">surname:Leech;given-names:R</infon><infon key="name_1">surname:Dick;given-names:F</infon><infon key="name_2">surname:Aydelott;given-names:J</infon><infon key="name_3">surname:Gygi;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">121</infon><infon key="year">2007</infon><offset>53313</offset><text>Naturalistic auditory scene analysis in children and adults</text></passage><passage><infon key="fpage">412</infon><infon key="lpage">425</infon><infon key="name_0">surname:Liao;given-names:HI</infon><infon key="name_1">surname:Kidani;given-names:S</infon><infon key="name_2">surname:Yoneya;given-names:M</infon><infon key="name_3">surname:Kashino;given-names:M</infon><infon key="name_4">surname:Furukawa;given-names:S</infon><infon key="pub-id_doi">10.3758/s13423-015-0898-0</infon><infon key="pub-id_pmid">26163191</infon><infon key="section_type">REF</infon><infon key="source">Psychon Bull Rev</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2016</infon><offset>53373</offset><text>Correspondences among pupillary dilation response, subjective salience of sounds, and loudness</text></passage><passage><infon key="fpage">687</infon><infon key="lpage">707</infon><infon key="name_0">surname:Meredith;given-names:MA</infon><infon key="name_1">surname:Clemo;given-names:HR</infon><infon key="pub-id_doi">10.1002/cne.902890412</infon><infon key="pub-id_pmid">2592605</infon><infon key="section_type">REF</infon><infon key="source">J Comp Neurol</infon><infon key="type">ref</infon><infon key="volume">289</infon><infon key="year">1989</infon><offset>53468</offset><text>Auditory cortical projection from the anterior ectosylvian sulcus (field AES) to the superior colliculus in the cat: an anatomical and electrophysiological study</text></passage><passage><infon key="fpage">640</infon><infon key="lpage">662</infon><infon key="name_0">surname:Meredith;given-names:MA</infon><infon key="name_1">surname:Stein;given-names:BE</infon><infon key="pub-id_doi">10.1152/jn.1986.56.3.640</infon><infon key="pub-id_pmid">3537225</infon><infon key="section_type">REF</infon><infon key="source">J Neurophysiol</infon><infon key="type">ref</infon><infon key="volume">56</infon><infon key="year">1986</infon><offset>53630</offset><text>Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration</text></passage><passage><infon key="fpage">3215</infon><infon key="lpage">3229</infon><infon key="name_0">surname:Meredith;given-names:MA</infon><infon key="name_1">surname:Nemitz;given-names:JW</infon><infon key="name_2">surname:Stein;given-names:BE</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.07-10-03215.1987</infon><infon key="pub-id_pmid">3668625</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">1987</infon><offset>53746</offset><text>Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors</text></passage><passage><infon key="fpage">330</infon><infon key="lpage">338</infon><infon key="name_0">surname:Mizzi;given-names:R</infon><infon key="name_1">surname:Michael;given-names:GA</infon><infon key="pub-id_doi">10.1016/j.bbr.2014.05.043</infon><infon key="pub-id_pmid">24880095</infon><infon key="section_type">REF</infon><infon key="source">Behav Brain Res</infon><infon key="type">ref</infon><infon key="volume">270</infon><infon key="year">2014</infon><offset>53839</offset><text>The role of the collicular pathway in the salience-based progression of visual attention</text></passage><passage><infon key="fpage">750</infon><infon key="lpage">753</infon><infon key="name_0">surname:Moore;given-names:BC</infon><infon key="name_1">surname:Glasberg;given-names:BR</infon><infon key="pub-id_doi">10.1121/1.389861</infon><infon key="pub-id_pmid">6630731</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">74</infon><infon key="year">1983</infon><offset>53928</offset><text>Suggested formulae for calculating auditory-filter bandwidths and excitation patterns</text></passage><passage><infon key="fpage">345</infon><infon key="lpage">355</infon><infon key="name_0">surname:Murphy;given-names:S</infon><infon key="name_1">surname:Fraenkel;given-names:N</infon><infon key="name_2">surname:Dalton;given-names:P</infon><infon key="pub-id_pmid">23969299</infon><infon key="section_type">REF</infon><infon key="source">Cognition</infon><infon key="type">ref</infon><infon key="volume">129</infon><infon key="year">2013</infon><offset>54014</offset><text>Perceptual load does not modulate auditory distractor processing</text></passage><passage><infon key="fpage">722</infon><infon key="lpage">726</infon><infon key="name_0">surname:Nader;given-names:K</infon><infon key="name_1">surname:Schafe;given-names:GE</infon><infon key="name_2">surname:Le Doux;given-names:JE</infon><infon key="pub-id_doi">10.1038/35021052</infon><infon key="pub-id_pmid">10963596</infon><infon key="section_type">REF</infon><infon key="source">Nature</infon><infon key="type">ref</infon><infon key="volume">406</infon><infon key="year">2000</infon><offset>54079</offset><text>Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval</text></passage><passage><infon key="fpage">107</infon><infon key="lpage">123</infon><infon key="name_0">surname:Parkhurst;given-names:D</infon><infon key="name_1">surname:Law;given-names:K</infon><infon key="name_2">surname:Niebur;given-names:E</infon><infon key="pub-id_doi">10.1016/S0042-6989(01)00250-4</infon><infon key="pub-id_pmid">11804636</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">42</infon><infon key="year">2002</infon><offset>54171</offset><text>Modeling the role of salience in the allocation of overt visual attention</text></passage><passage><infon key="fpage">e1002531</infon><infon key="name_0">surname:Peel;given-names:TR</infon><infon key="name_1">surname:Hafed;given-names:ZM</infon><infon key="name_2">surname:Dash;given-names:S</infon><infon key="name_3">surname:Lomber;given-names:SG</infon><infon key="name_4">surname:Corneil;given-names:BD</infon><infon key="pub-id_doi">10.1371/journal.pbio.1002531</infon><infon key="pub-id_pmid">27509130</infon><infon key="section_type">REF</infon><infon key="source">PLoS Biol</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2016</infon><offset>54245</offset><text>A causal role for the cortical frontal eye fields in microsaccade deployment</text></passage><passage><infon key="fpage">2397</infon><infon key="lpage">2416</infon><infon key="name_0">surname:Peters;given-names:RJ</infon><infon key="name_1">surname:Iyer;given-names:A</infon><infon key="name_2">surname:Itti;given-names:L</infon><infon key="name_3">surname:Koch;given-names:C</infon><infon key="pub-id_doi">10.1016/j.visres.2005.03.019</infon><infon key="pub-id_pmid">15935435</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2005</infon><offset>54322</offset><text>Components of bottom-up gaze allocation in natural images</text></passage><passage><infon key="fpage">2773</infon><infon key="lpage">2782</infon><infon key="name_0">surname:Pressnitzer;given-names:D</infon><infon key="name_1">surname:McAdams;given-names:S</infon><infon key="pub-id_doi">10.1121/1.426894</infon><infon key="pub-id_pmid">10335629</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">105</infon><infon key="year">1999</infon><offset>54380</offset><text>Two phase effects in roughness perception</text></passage><passage><infon key="fpage">2415</infon><infon key="lpage">2441</infon><infon key="name_0">surname:Rolfs;given-names:M</infon><infon key="pub-id_doi">10.1016/j.visres.2009.08.010</infon><infon key="pub-id_pmid">19683016</infon><infon key="section_type">REF</infon><infon key="source">Vision Res</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2009</infon><offset>54422</offset><text>Microsaccades: small steps on a long way</text></passage><passage><infon key="fpage">427</infon><infon key="lpage">439</infon><infon key="name_0">surname:Rolfs;given-names:M</infon><infon key="name_1">surname:Engbert;given-names:R</infon><infon key="name_2">surname:Kliegl;given-names:R</infon><infon key="pub-id_pmid">16032403</infon><infon key="section_type">REF</infon><infon key="source">Exp Brain Res</infon><infon key="type">ref</infon><infon key="volume">166</infon><infon key="year">2005</infon><offset>54463</offset><text>Crossmodal coupling of oculomotor control and spatial attention in vision and audition</text></passage><passage><infon key="fpage">5</infon><infon key="name_0">surname:Rolfs;given-names:M</infon><infon key="name_1">surname:Kliegl;given-names:R</infon><infon key="name_2">surname:Engbert;given-names:R</infon><infon key="pub-id_doi">10.1167/8.11.5</infon><infon key="pub-id_pmid">18831599</infon><infon key="section_type">REF</infon><infon key="source">J Vis</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2008</infon><offset>54550</offset><text>Toward a model of microsaccade generation: the case of microsaccadic inhibition</text></passage><passage><infon key="fpage">499</infon><infon key="lpage">518</infon><infon key="name_0">surname:Rucci;given-names:M</infon><infon key="name_1">surname:Poletti;given-names:M</infon><infon key="pub-id_doi">10.1146/annurev-vision-082114-035742</infon><infon key="pub-id_pmid">27795997</infon><infon key="section_type">REF</infon><infon key="source">Annu Rev Vis Sci</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2015</infon><offset>54630</offset><text>Control and functions of fixational eye movements</text></passage><passage><infon key="fpage">211</infon><infon key="lpage">223</infon><infon key="name_0">surname:Sara;given-names:SJ</infon><infon key="pub-id_doi">10.1038/nrn2573</infon><infon key="pub-id_pmid">19190638</infon><infon key="section_type">REF</infon><infon key="source">Nat Rev Neurosci</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2009</infon><offset>54680</offset><text>The locus coeruleus and noradrenergic modulation of cognition</text></passage><passage><infon key="fpage">130</infon><infon key="lpage">141</infon><infon key="name_0">surname:Sara;given-names:SJ</infon><infon key="name_1">surname:Bouret;given-names:S</infon><infon key="pub-id_doi">10.1016/j.neuron.2012.09.011</infon><infon key="pub-id_pmid">23040811</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">76</infon><infon key="year">2012</infon><offset>54742</offset><text>Orienting and reorienting: the locus coeruleus mediates cognition through arousal</text></passage><passage><infon key="fpage">314</infon><infon key="lpage">325</infon><infon key="name_0">surname:Sato;given-names:S</infon><infon key="name_1">surname:You;given-names:J</infon><infon key="name_2">surname:Jeon;given-names:JY</infon><infon key="pub-id_doi">10.1121/1.2739440</infon><infon key="pub-id_pmid">17614491</infon><infon key="section_type">REF</infon><infon key="source">J Acoust Soc Am</infon><infon key="type">ref</infon><infon key="volume">122</infon><infon key="year">2007</infon><offset>54824</offset><text>Sound quality characteristics of refrigerator noise in real living environments with relation to psychoacoustical and autocorrelation function parameters</text></passage><passage><infon key="fpage">99</infon><infon key="lpage">110</infon><infon key="name_0">surname:Saygin;given-names:AP</infon><infon key="name_1">surname:Dick;given-names:F</infon><infon key="name_2">surname:Bates;given-names:E</infon><infon key="pub-id_doi">10.3758/BF03206403</infon><infon key="pub-id_pmid">16097349</infon><infon key="section_type">REF</infon><infon key="source">Behav Res Methods</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">2005</infon><offset>54978</offset><text>An on-line task for contrasting auditory processing in the verbal and nonverbal domains and norms for younger and older adults</text></passage><passage><infon key="fpage">37</infon><infon key="name_0">surname:Schnupp;given-names:JW</infon><infon key="name_1">surname:Garcia-Lazaro;given-names:JA</infon><infon key="name_2">surname:Lesica;given-names:NA</infon><infon key="pub-id_doi">10.3389/fncir.2015.00037</infon><infon key="pub-id_pmid">26379508</infon><infon key="section_type">REF</infon><infon key="source">Front Neural Circuits</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2015</infon><offset>55105</offset><text>Periodotopy in the gerbil inferior colliculus: local clustering rather than a gradient map</text></passage><passage><infon key="fpage">e19113</infon><infon key="name_0">surname:Sohoglu;given-names:E</infon><infon key="name_1">surname:Chait;given-names:M</infon><infon key="pub-id_doi">10.7554/eLife.19113</infon><infon key="pub-id_pmid">27602577</infon><infon key="section_type">REF</infon><infon key="source">eLife</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2016</infon><offset>55196</offset><text>Detecting and representing predictable structure during auditory scene analysis</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">103</infon><infon key="name_0">surname:Southwell;given-names:R</infon><infon key="name_1">surname:Chait;given-names:M</infon><infon key="pub-id_doi">10.1016/j.cortex.2018.08.032</infon><infon key="pub-id_pmid">30312781</infon><infon key="section_type">REF</infon><infon key="source">Cortex</infon><infon key="type">ref</infon><infon key="volume">109</infon><infon key="year">2018</infon><offset>55276</offset><text>Enhanced deviant responses in patterned relative to random sound sequences</text></passage><passage><infon key="fpage">736</infon><infon key="lpage">748</infon><infon key="name_0">surname:Stewart;given-names:N</infon><infon key="name_1">surname:Chandler;given-names:J</infon><infon key="name_2">surname:Paolacci;given-names:G</infon><infon key="pub-id_doi">10.1016/j.tics.2017.06.007</infon><infon key="pub-id_pmid">28803699</infon><infon key="section_type">REF</infon><infon key="source">Trends Cogn Sci</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2017</infon><offset>55351</offset><text>Crowd-sourcing samples in cognitive science</text></passage><passage><infon key="fpage">20160113</infon><infon key="name_0">surname:Veale;given-names:R</infon><infon key="name_1">surname:Hafed;given-names:ZM</infon><infon key="name_2">surname:Yoshida;given-names:M</infon><infon key="pub-id_doi">10.1098/rstb.2016.0113</infon><infon key="pub-id_pmid">28044023</infon><infon key="section_type">REF</infon><infon key="source">Philos Trans R Soc Lond B Biol Sci</infon><infon key="type">ref</infon><infon key="volume">372</infon><infon key="year">2017</infon><offset>55395</offset><text>How is visual salience computed in the brain? Insights from behaviour, neurobiology and modelling</text></passage><passage><infon key="fpage">1006</infon><infon key="lpage">1010</infon><infon key="name_0">surname:Wallace;given-names:MT</infon><infon key="name_1">surname:Meredith;given-names:MA</infon><infon key="name_2">surname:Stein;given-names:BE</infon><infon key="pub-id_doi">10.1152/jn.1998.80.2.1006</infon><infon key="pub-id_pmid">9705489</infon><infon key="section_type">REF</infon><infon key="source">J Neurophysiol</infon><infon key="type">ref</infon><infon key="volume">80</infon><infon key="year">1998</infon><offset>55493</offset><text>Multisensory integration in the superior colliculus of the alert cat</text></passage><passage><infon key="fpage">408</infon><infon key="lpage">417</infon><infon key="name_0">surname:Wang;given-names:CA</infon><infon key="name_1">surname:Boehnke;given-names:SE</infon><infon key="name_2">surname:Itti;given-names:L</infon><infon key="name_3">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.3550-13.2014</infon><infon key="pub-id_pmid">24403141</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2014</infon><offset>55562</offset><text>Transient pupil response is modulated by contrast-based saliency</text></passage><passage><infon key="fpage">2822</infon><infon key="lpage">2832</infon><infon key="name_0">surname:Wang;given-names:CA</infon><infon key="name_1">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1111/ejn.12641</infon><infon key="pub-id_pmid">24911340</infon><infon key="section_type">REF</infon><infon key="source">Eur J Neurosci</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2014</infon><offset>55627</offset><text>Modulation of stimulus contrast on the human pupil orienting response</text></passage><passage><infon key="fpage">134</infon><infon key="lpage">140</infon><infon key="name_0">surname:Wang;given-names:CA</infon><infon key="name_1">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1016/j.conb.2015.03.018</infon><infon key="pub-id_pmid">25863645</infon><infon key="section_type">REF</infon><infon key="source">Curr Opin Neurobiol</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2015</infon><offset>55697</offset><text>A circuit for pupil orienting responses: implications for cognitive modulation of pupil size</text></passage><passage><infon key="fpage">36</infon><infon key="lpage">44</infon><infon key="name_0">surname:Wang;given-names:CA</infon><infon key="name_1">surname:Blohm;given-names:G</infon><infon key="name_2">surname:Huang;given-names:J</infon><infon key="name_3">surname:Boehnke;given-names:SE</infon><infon key="name_4">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1016/j.biopsycho.2017.07.024</infon><infon key="pub-id_pmid">28789960</infon><infon key="section_type">REF</infon><infon key="source">Biol Psychol</infon><infon key="type">ref</infon><infon key="volume">129</infon><infon key="year">2017</infon><offset>55790</offset><text>Multisensory integration in orienting behavior: pupil size, microsaccades, and saccades</text></passage><passage><infon key="fpage">382</infon><infon key="lpage">392</infon><infon key="name_0">surname:Wetzel;given-names:N</infon><infon key="name_1">surname:Buttelmann;given-names:D</infon><infon key="name_2">surname:Schieler;given-names:A</infon><infon key="name_3">surname:Widmann;given-names:A</infon><infon key="pub-id_pmid">26507492</infon><infon key="section_type">REF</infon><infon key="source">Dev Psychobiol</infon><infon key="type">ref</infon><infon key="volume">58</infon><infon key="year">2016</infon><offset>55878</offset><text>Infant and adult pupil dilation in response to unexpected sounds</text></passage><passage><infon key="fpage">14263</infon><infon key="name_0">surname:White;given-names:BJ</infon><infon key="name_1">surname:Berg;given-names:DJ</infon><infon key="name_2">surname:Kan;given-names:JY</infon><infon key="name_3">surname:Marino;given-names:RA</infon><infon key="name_4">surname:Itti;given-names:L</infon><infon key="name_5">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1038/ncomms14263</infon><infon key="pub-id_pmid">28117340</infon><infon key="section_type">REF</infon><infon key="source">Nat Commun</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2017a</infon><offset>55943</offset><text>Superior colliculus neurons encode a visual saliency map during free viewing of natural dynamic video</text></passage><passage><infon key="fpage">9451</infon><infon key="lpage">9456</infon><infon key="name_0">surname:White;given-names:BJ</infon><infon key="name_1">surname:Kan;given-names:JY</infon><infon key="name_2">surname:Levy;given-names:R</infon><infon key="name_3">surname:Itti;given-names:L</infon><infon key="name_4">surname:Munoz;given-names:DP</infon><infon key="pub-id_doi">10.1073/pnas.1701003114</infon><infon key="pub-id_pmid">28808026</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci U S A</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2017b</infon><offset>56045</offset><text>Superior colliculus encodes visual saliency before the primary visual cortex</text></passage><passage><infon key="fpage">11152</infon><infon key="lpage">11158</infon><infon key="name_0">surname:Widmann;given-names:A</infon><infon key="name_1">surname:Engbert;given-names:R</infon><infon key="name_2">surname:Schröger;given-names:E</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.1568-14.2014</infon><infon key="pub-id_pmid">25122911</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2014</infon><offset>56122</offset><text>Microsaccadic responses indicate fast categorization of sounds: a novel approach to study auditory cognition</text></passage><passage><infon key="fpage">E3313</infon><infon key="lpage">E3322</infon><infon key="name_0">surname:Woods;given-names:KJP</infon><infon key="name_1">surname:McDermott;given-names:JH</infon><infon key="pub-id_doi">10.1073/pnas.1801614115</infon><infon key="pub-id_pmid">29563229</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci U S A</infon><infon key="type">ref</infon><infon key="volume">115</infon><infon key="year">2018</infon><offset>56231</offset><text>Schema learning for the cocktail party problem</text></passage><passage><infon key="fpage">2064</infon><infon key="lpage">2072</infon><infon key="name_0">surname:Woods;given-names:KJP</infon><infon key="name_1">surname:Siegel;given-names:MH</infon><infon key="name_2">surname:Traer;given-names:J</infon><infon key="name_3">surname:McDermott;given-names:JH</infon><infon key="pub-id_doi">10.3758/s13414-017-1361-2</infon><infon key="pub-id_pmid">28695541</infon><infon key="section_type">REF</infon><infon key="source">Atten Percept Psychophys</infon><infon key="type">ref</infon><infon key="volume">79</infon><infon key="year">2017</infon><offset>56278</offset><text>Headphone screening to facilitate web-based auditory experiments</text></passage><passage><infon key="fpage">7224</infon><infon key="name_0">surname:Xiong;given-names:XR</infon><infon key="name_1">surname:Liang;given-names:F</infon><infon key="name_2">surname:Zingg;given-names:B</infon><infon key="name_3">surname:Ji;given-names:XY</infon><infon key="name_4">surname:Ibrahim;given-names:LA</infon><infon key="name_5">surname:Tao;given-names:HW</infon><infon key="name_6">surname:Zhang;given-names:LI</infon><infon key="pub-id_doi">10.1038/ncomms8224</infon><infon key="pub-id_pmid">26068082</infon><infon key="section_type">REF</infon><infon key="source">Nat Commun</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2015</infon><offset>56343</offset><text>Auditory cortex controls sound-driven innate defense behaviour through corticofugal projections to inferior colliculus</text></passage><passage><infon key="fpage">3999</infon><infon key="name_0">surname:Yablonski;given-names:M</infon><infon key="name_1">surname:Polat;given-names:U</infon><infon key="name_2">surname:Bonneh;given-names:YS</infon><infon key="name_3">surname:Ben-Shachar;given-names:M</infon><infon key="pub-id_doi">10.1038/s41598-017-04391-4</infon><infon key="pub-id_pmid">28638094</infon><infon key="section_type">REF</infon><infon key="source">Sci Rep</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2017</infon><offset>56462</offset><text>Microsaccades are sensitive to word structure: a novel approach to study language processing</text></passage><passage><infon key="fpage">13693</infon><infon key="lpage">13700</infon><infon key="name_0">surname:Yuval-Greenberg;given-names:S</infon><infon key="name_1">surname:Merriam;given-names:EP</infon><infon key="name_2">surname:Heeger;given-names:DJ</infon><infon key="pub-id_doi">10.1523/JNEUROSCI.0582-14.2014</infon><infon key="pub-id_pmid">25297096</infon><infon key="section_type">REF</infon><infon key="source">J Neurosci</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">2014</infon><offset>56555</offset><text>Spontaneous microsaccades reflect shifts in covert attention</text></passage><passage><infon key="fpage">33</infon><infon key="lpage">47</infon><infon key="name_0">surname:Zingg;given-names:B</infon><infon key="name_1">surname:Chou;given-names:XL</infon><infon key="name_2">surname:Zhang;given-names:ZG</infon><infon key="name_3">surname:Mesik;given-names:L</infon><infon key="name_4">surname:Liang;given-names:F</infon><infon key="name_5">surname:Tao;given-names:HW</infon><infon key="name_6">surname:Zhang;given-names:LI</infon><infon key="pub-id_doi">10.1016/j.neuron.2016.11.045</infon><infon key="pub-id_pmid">27989459</infon><infon key="section_type">REF</infon><infon key="source">Neuron</infon><infon key="type">ref</infon><infon key="volume">93</infon><infon key="year">2017</infon><offset>56616</offset><text>AAV-mediated anterograde transsynaptic tagging: mapping corticocollicular input-defined neural pathways for defense behaviors</text></passage></document></collection>
