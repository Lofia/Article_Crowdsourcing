<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20201223</date><key>pmc.key</key><document><id>7085741</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3390/s20051443</infon><infon key="article-id_pmc">7085741</infon><infon key="article-id_pmid">32155807</infon><infon key="article-id_publisher-id">sensors-20-01443</infon><infon key="elocation-id">1443</infon><infon key="issue">5</infon><infon key="kwd">indoor localization area localization crowdsourcing fingerprinting deep learning</infon><infon key="license">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).</infon><infon key="name_0">surname:Laska;given-names:Marius</infon><infon key="name_1">surname:Blankenbach;given-names:Jörg</infon><infon key="name_2">surname:Klamma;given-names:Ralf</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>0</offset><text>Adaptive Indoor Area Localization for Perpetual Crowdsourced Data Collection</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>77</offset><text>The accuracy of fingerprinting-based indoor localization correlates with the quality and up-to-dateness of collected training data. Perpetual crowdsourced data collection reduces manual labeling effort and provides a fresh data base. However, the decentralized collection comes with the cost of heterogeneous data that causes performance degradation. In settings with imperfect data, area localization can provide higher positioning guarantees than exact position estimation. Existing area localization solutions employ a static segmentation into areas that is independent of the available training data. This approach is not applicable for crowdsoucred data collection, which features an unbalanced spatial training data distribution that evolves over time. A segmentation is required that utilizes the existing training data distribution and adapts once new data is accumulated. We propose an algorithm for data-aware floor plan segmentation and a selection metric that balances expressiveness (information gain) and performance (correctly classified examples) of area classifiers. We utilize supervised machine learning, in particular, deep learning, to train the area classifiers. We demonstrate how to regularly provide an area localization model that adapts its prediction space to the accumulating training data. The resulting models are shown to provide higher reliability compared to models that pinpoint the exact position.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1511</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1527</offset><text>In recent years, the usage of location-based services (LBS) has experienced substantial growth. This is mostly caused by the wide adoption of smartphones with the ability to reliably track a user’s location. Global Navigation Satellite Systems (GNSS), such as the Global Positioning System (GPS), are the dominant technology to enable LBS, since they offer accurate and reliable localization performance. However, GNSS do not provide sufficient availability and reliability inside buildings, since the satellite signals are attenuated and scattered by building features. This drawback has led to the development of various alternative indoor localization systems, which utilize a spectrum of techniques and technologies. Until today, there is not any gold standard for indoor localization, which can be stated as the main issue that has prevented indoor LBS from developing their full potential.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2429</offset><text>Indoor localization systems can serve different purposes. In monitor-based systems, the location of a user or entity is passively obtained relative to some anchor node. This can be utilized, for example, to enhance the energy efficiency of buildings by automatically switching-off lighting and heating/cooling in empty rooms. In contrast, in device-based systems, the location information is obtained from a user-centric perspective, which can be utilized, for example, to enable navigation. A variety of technologies and approaches are present in the field of indoor localization. Comprehensive overviews are given in. In general, indoor localization systems can be grouped into (1) autonomous, (2) infrastructure-based and (3) hybrid systems. Autonomous systems apply inertial navigation. In infrastructure-based systems, it can be differentiated between (2.1) analysis of signal propagation to dedicated transmitting stations and (2.2) scene analysis (fingerprinting). The former utilizes proximity, lateration or angulation measurements to estimate the user’s location. This requires line-of-sight and knowledge about the location of the stations. In contrast, fingerprinting does not rely on either. Instead, in an offline phase, the scene is scanned at certain reference points with a sensing device (e.g., smartphone). The observed sensor values at each reference point form so-called fingerprints. Using supervised machine learning (ML), a mapping from fingerprints to locations is learned, which is utilized to estimate the location for unseen fingerprints during online localization. Fingerprinting leverages existing infrastructure, which reduces upfront deployment cost. However, the accuracy of the system strongly depends on the quality of the offline site survey and the up-to-dateness of the fingerprint database.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4278</offset><text>A crowdsourced site survey has been proposed to partition the collection among several participants and thus reduces the manual labeling effort. Users either explicitly tag a fingerprint with a location, or the label is implicitly inferred by the system. The decentralized collection comes with the cost of heterogeneous data, which include among others device heterogeneity, labeling noise and an unequal spatial training data distribution.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4721</offset><text>Area localization can be applied in settings with imperfect data to achieve reliable positioning guarantees. The problem is simplified such that the goal becomes to predict the right area instead of pinpointing the exact location. Existing area localization solutions employ a static segmentation into areas that is independent of the available training data. This approach is not applicable for crowdsoucred data collection, since it features an unbalanced spatial training data distribution that changes over time. A segmentation is required that utilizes the existing training data distribution and adapts when new data is accumulated. The amount and shape of the areas, in particular, the richness of training data per area, affect the accuracy of classification models, which we subsequently call model performance. In addition, the expressive power is determined by the segmentation. If a model predicts one of few but large classes, the information gain of the user is lower compared to models that predict one of many smaller areas. We call the expressive power of the model that is determined by the segmentation expressiveness. Since crowdsourced data is expected to be generated continuously, the segmentation into areas as well as the successive classification model can be continuously improved. The challenge is, therefore, to continuously find a model with the right balance between expressiveness and performance given the most recent crowdsourced map coverage.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6207</offset><text>We introduce the concept of adaptive area localization to enable area classification for crowdsourced data that are continuously generated.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6347</offset><text>We propose the idea of data-aware floor plan segmentation to compute segmentations that benefit subsequent classification. We present a clustering-based algorithm that determines such a segmentation with adjustable granularity.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6575</offset><text>We formulate a metric to compare various area classifiers, such that the model, providing the optimal balance between expressiveness and performance, can be selected. This allows for automatic model building and selection in the setting of continuous crowdsourced data collection.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6856</offset><text>We provide a comprehensive experimental study to validate the concepts on a self-generated and a publicly available crowdsourced data set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6995</offset><text>The main contributions of this paper are summarized as follows:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7060</offset><text>The rest of the paper is organized as follows: we introduce related work in Section 2 focusing on crowdsourced data collection, area classification and deep learning. Subsequently, Section 3 introduces the proposed concepts of adaptive area classification in detail. In Section 4 we present the locally dense cluster expansion (LDCE) algorithm for computing floor plan segmentations with adjustable granularities that are based on the available training data. Section 5 covers details regarding machine learning model building for area classification. In Section 6 the proposed concepts are evaluated on a self-generated as well as a publicly available crowdsourced data set. Finally, we discuss our findings in Section 7 and draw conclusion in Section 8.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>7816</offset><text>2. Related Work</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7833</offset><text>Fingerprinting-based indoor localization commonly utilizes a two stage approach. In the offline phase, radio frequency (RF) signals are collected at certain reference points and tagged with the position of collection. An algorithm is used to find a mapping from unknown fingerprints to locations. This algorithm is then applied during the online phase to localize an RF device. The RF technology of choice for fingerprinting is commonly WLAN, however, solutions have been proposed that utilize alternative RF technologies such as LTE. The most common approach for constructing a WLAN fingerprint is the received signal strength (RSS), which can be used either directly or after feature extraction. Recent studies on fingerprinting also incorporate channel state information (CSI) as input data in order to obtain more accurate prediction results. The underlying assumption states that RSS values do not exploit the subcarriers in an orthogonal frequency-division multiplexing (OFDM). Therefore, CSI contains richer multipath information, which is beneficial for training complex models. However, obtaining CSI data is only achievable with certain Wi-Fi network interface cards (NIC) and thus is currently not suitable for smartphone based data collection like crowdsourcing. In this work, we focus on classical WLAN fingerprinting and utilize the RSS of scanned access points to construct the radio frequency map.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>9250</offset><text>2.1. Crowdsourcing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9269</offset><text>Several approaches have been proposed to reduce the manual labeling effort during crowdsourced data collection for Wi-Fi fingerprinting. Rai et al. were among the first to present a probabilistic model to infer the position of implicitly collected fingerprints. They periodically collected the RSS together with the timestamps of collection. Simultaneously, the system tracks the user utilizing a particle filter. After convergence, the path information is used to annotate the RSS measurements with a location. Radu and Marina additionally integrated activity recognition and Wi-Fi fingerprinting via a particle filter to detect certain anchor points, such as elevator or stairs. He and Chan utilized proximity information to Internet-of-things (IoT) sensing devices and the initially sparse RSS radio map to label fingerprints during implicit crowdsourcing. The IoT devices can be fixed, such as installed beacon transmitters or moving (smartphones of other participants). Santos et al. utilized pedestrian dead reckoning (PDR) techniques to reconstruct the movements of users and classified the resulting trajectories using Wi-Fi measurements. Similar segments have been identified using an adaptive approach based on geomagnetic field distance. Finally, floor plans were reconstructed through a data fusion process and the collected Wi-Fi fingerprints were aligned to physical locations. Zhou et al. abstracted the indoor maps as semantics graph. Crowdsourcing trajectories were mapped to the floor plan by applying activity detection and PDR. The annotated trajectories have been utilized to construct the radio map. Based on unfixed data collection, Jiang et al. proposed the construction of a probabilistic radio map, where each cell was assigned a probability density function (PDF) instead of a mean value as in classical site survey approaches. Wei et al. utilized the knowledge of location during the payment process inside the shops of a mall. They utilized this to annotate collected fingerprints with the current shop to build a hierarchical classification model that provides shop-level localization. In contrast to probabilistic fingerprint annotation, unsupervised learning can be utilized to obtain labeled Wi-Fi fingerprints. Jung and Han utilized unsupervised learning to infer the location of access points together with a path loss model and optimization algorithm, which they presented in. They investigated how to adaptively recalibrate the resulting map to avoid performance degradation of downstream localization models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11831</offset><text>Inaccurate position tags for crowdsourced fingerprints that might occur during manual labeling of non-experts or are caused by automatic labeling via probabilistic models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12003</offset><text>The fluctuating dimensionality of RSS signals caused by varying numbers of hearable access points for various locations.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12124</offset><text>The device heterogeneity that causes RSS to differ across various devices for the same measurement position.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12233</offset><text>The nonuniform spatial data distribution, meaning that some areas feature a larger amount of data, while for others no data was collected.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12372</offset><text>Besides the reduction of labeling effort when collecting data via crowdsourcing, there are several additional challenges that have to be considered. Ye and Wang identifed four major problems, which are:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12576</offset><text>They constructed device-specific grid fingerprints utilizing clustering-based algorithms. For sparse areas fingerprints are interpolated and finally, the samples from several devices are fused to obtain device independent grid fingerprints. Yang et al. additionally identified the short measurement time of crowdsourced sample collection as a typical problem. They utilized the fact that the most-recorded RSS does not differ much, irrespective of the length of measuring, to extract a characteristic fingerprint. In a follow up work, Kim et al. evaluated the system in a case study and demonstrated its effectiveness. Pipelidis et al. proposed an architecture for cross-device radio map construction via crowdsourcing. They utilized data labeled via a simultaneous localization and mapping (SLAM)-like algorithm. The RSS values between devices were calibrated via reference measurements at several landmarks. The data was clustered and subsequently used for classification of areas.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13573</offset><text>2.2. Area Localization</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13597</offset><text>In contrast to localization systems that aim at pinpointing the exact position of a user, the concept of area classification only focuses on estimating the current area of the user, such as the office room or the shop inside a mall. This is particularly suitable for large scale deployments or in situations where the data quality does not allow for accurate localization.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13973</offset><text>Lopez Pastor et al. evaluated a Wi-Fi fingerprinting-based indoor localization system inside a medium sized shopping mall. The system is meant for providing shop-level accuracy, while minimizing the deployment cost and effort. Data is collected by randomly walking in predefined areas, such that all data can be labeled with the corresponding shop. The authors claim that the achieved system performance is sufficiently independent of the device and does not deteriorate over time. Wei et al. adopted a similar approach. They utilized the fact that during payment inside a shop, the location of the user is known. This can be used to annotate Wi-Fi fingerprints collected while paying. The obtained fingerprints can be utilized for shop-level position estimation. Rezgui et al. proposed a variation of a support vector machine (SVM) (normalized rank based SVM) to address the problem of hardware variance and signal fluctuation of Wi-Fi based localization systems. The system achieves room level prediction accuracies. He et al. compared the performance of various classification models, such as SVM, artificial neural network (ANN) and deep belief network (DBN) for various test sites. They addressed the identification of floors, indoor/outdoor and buildings. In a recent follow up work, they also tackled the inside/outside region decision problem and propose solutions for missing AP detection and fingerprint preprocessing. Liu et al. proposed an algorithm for probability estimation over possible areas. By adopting the user’s trajectory and existing map information, they eliminate unreasonable results. The partitioning of the map into areas is done manually based on the different rooms and offices.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15704</offset><text>2.3. Deep Learning for Fingerprinting</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15743</offset><text>Fingerprinting-based indoor localization can be formulated as standard supervised learning problem. It can be modeled as regression problem with the goal to predict the exact position, or as a classification task on predetermined areas. Due to the recent success of deep learning in areas such as image processing or speech recognition, the application of deep models for fingerprinting-based indoor localization has gained attention recently. Nowicki and Wietrzykowski applied stacked autoencoders combined with a feed forward neural network for building and floor prediction. Xiao et al. compared SVM and a deep neural network (DNN) on various publicly available data sets and propose a data augmentation schema as well as an approach for transfer learning. Adege et al. applied regression analysis to fill missing RSS values and utilize linear discriminant analysis for dimensionality reduction. Finally, feed forward neural networks are applied to tackle the regression and classification problem. Kim et al. formulated the problem as multi-label classification problem to predict the building, floor and position with a single network with minimal performance degradation. Mai et al. utilized a convolutional neural network (CNN) on raw RSS data by applying the convolution on time-series data. The data is artificially constructed by combining measurements within a certain cell size that have been captured in temporal intervals not exceeding a certain threshold. By constructing an image of the RSS vector, CNNs that are predominantly used for image classification can be applied. Mittal et al. filtered access point signals that have a low Pearson Correlation Coefficient (PCC) between the access point values and the location vector. The remaining RSS vector is transformed into an image matrix by multiplying each access point vector with the obtained correlation values and arranging as matrix with zero padding. Sinha et al. simply arranged the RSS vector as a matrix to train a standard CNN image classifier. They proposed a data augmentation scheme where single values of the RSS vector are replaced by random values sampled from the interval of the difference of the actual value and the access point mean value.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>17990</offset><text>3. Adaptive Area Classification for Crowdsourced Data</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18045</offset><text>In the following section, we introduce our approach to adaptive area classification. We describe the concept overview and introduce relevant notations. Subsequently, a floor plan segmentation is formally defined and classification models for indoor localization are described. Finally, we propose a novel metric called ACS, which is utilized to select area classifiers with respect to the optimal balance between expressiveness and performance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18492</offset><text>3.1. Concept Overview</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18515</offset><text>The performance of Wi-Fi fingerprinting-based indoor localization systems heavily relies on thorough and up-to-date site survey data. Crowdsourced training data collection continuously provides fresh data, but suffers from poor data quality. Several approaches suggest to maintain an up-to-date radio map, which stores a representative fingerprint or a probabilistic distribution for predefined locations, regions, or grid cells. Missing data for certain locations prohibits equal radio map quality at all areas. This is solved by either enlarging the areas of the radio map or by interpolating fingerprints for sparsely covered areas. The update of such a radio map is a complicated process, since its granularity is static. However, the spatial distribution of available training data is expected to shift over time. Therefore, instead of maintaining a radio map with characteristic fingerprints for predefined areas, we store the entire training data with the noisy position tags. At regular intervals, we dynamically subdivide the floor plan into areas based on the richness of available training data. The training data, which are originally annotated with noisy position tags, are labeled with the corresponding areas based on the computed floor plan segmentation. This enables training of standard supervised machine learning classifiers that predict the correct area. In order to quantify the gain of such an area classifier, two metrics can be utilized.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19985</offset><text>The expressiveness measures the information gain of the user, which is mainly influenced by the extent of each individual area and the total coverage of the model.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20149</offset><text>The performance indicates how reliably the model predicts a certain area.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20223</offset><text>The two metrics are inversely proportional. That means a fine segmentation (high expressiveness) negatively affects the performance of the model and vice versa. We assume that fresh crowdsourced training data is accumulated over time. This enables updates of the floor plan segmentation and the successive area classifier. The workflow for continuously providing area localization models, where the prediction space adapts to the new training data, is illustrated in Figure 1. Over time, the map gets covered with an increasing amount of training data, which is illustrated in the top row of Figure 1. At regular intervals, the goal is to provide an optimal indoor area classification model based on the current map coverage. This process includes the automatic floor plan segmentation into areas and the training of an ML model. Several floor plan segmentations can be determined that influence the expressiveness of the ML model and for each of these segmentations several ML models can be learned. For each epoch, the best combination of segmentation and model is selected. This is done with respect to a metric, called area classification score (ACS). The ACS balances expressiveness and performance and is introduced in Section 3.5.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21467</offset><text>3.2. Data Notations</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21488</offset><text>In the following, we introduce the formal notations that are subsequently used. We assume that at a certain point in time, a set of N labeled training data tuples (fingerprints)  for  has been collected for a given indoor map. Each fingerprint  consists of a M-dimensional feature vector  and is tagged with a position  in two dimensions and the corresponding timestamp  of collection. In the following we focus on Wi-Fi fingerprinting, such that each entry of the vector is the RSS value of the corresponding access point and M is equal to the total amount of access points that are observable for the map. Since not all access points are hearable at all locations,  contains missing entries, which have to be considered during further processing of the data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22250</offset><text>3.3. Floor Plan Segmentation for Area Classification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22304</offset><text>In order to train a classification model, we have to find a floor plan segmentation that assigns each fingerprint tuple  to one of the K areas or classes,  for . A floor plan segmentation determines a mapping , where  might be any two-dimensional shape, such as a rectangle. Given such a mapping , we can label each fingerprint with the class label of the area it is located in. For a given segmentation , we obtain the transformed set , where  and . The goal is now to find a classifier  that determines the correct area of the floor plan segmentation for an unknown RSS fingerprint. We have now arrived at the standard formulation of a supervised learning problem, in particular, a classification problem.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23017</offset><text>3.4. ML Models for Area Classification</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23057</offset><text>Given the transformed set of fingerprints  for a segmentation , we can utilize any standard ML classification model that learns to predict the unknown class  for a fingerprint . We can either construct a discriminant function that directly assigns a class to an unknown fingerprint, or we model the conditional probability distribution . SVMs depict a typical discriminant model used in the domain of indoor localization, while with DNNs, it is possible to model . Both models are utilized in the experimental study (Section 6) as classifiers for the transformed fingerprint sets .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23641</offset><text>3.5. Area Classification Score</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23673</offset><text>In order to properly quantify the quality of the learned area localization model (combination of segmentation and trained classifier), we have to simultaneously investigate the model’s expressiveness as well as its performance. The expressiveness is influenced by the total extent of covered area as well as the size of each individual area. We state that the expressiveness of a model is higher if it predicts classes associated with smaller areas. However, the benefit of a narrow prediction area vanishes if the performance for that specific class, for example the accuracy, is poor. To capture this interplay, we have to look at each predicted class of the classifier individually. We define  as the surface area of the area  that belongs to class . On an individual class level, we define the expressiveness of class  as: where  is the minimal extent that an area might have by definition (set to  in the following) and  is a parameter to adjust the slope of the function. Additionally, a performance metric is required, which measures the accuracy of the model on a class level. We choose the  score, since we are equally interested in precision and recall. Let  be the class-based  score for class , evaluated on a separate test set. The chosen metrics for expressiveness and performance reside in the interval , such that we can multiply them to obtain a value in , which would be optimal, if the predicted class has the minimal extent of  and a -score of 1 on the test set. In order to account for the total covered area, we take the weighted mean of the product of expressiveness and performance using the area of each class. We finally arrive at: which we call area classification score (ACS) in the following. The expressiveness term (1) regulates how much the class score adds to the weighted mean. For , the regularization term vanishes, such that the area size of the specific class has no influence on the amount that is added to the mean. This means that two localization models with constant class-wise classification performance  achieve the same score if they cover the same area , independent of the amount of classes and their individual size:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>25849</offset><text>It follows that if  approaches 0, the metric becomes less sensitive to the individual area sizes. With respect to models covering a similar extent of the map, those that provide a higher performance will be rated higher, independent of the number and individual size of their areas. The closer  gets to 1, the higher is the influence of individual area sizes. High performance on broad areas will not add much to the weighted mean, since they are downscaled by the expressiveness factor. As a consequence, models with finer segmentations score higher, since the influence of area regularization outweighs the performance factor. For , the score is only sensitive to the amount of total segments. The ACS becomes  which will be higher for finer segmentations given that the same total extent of the map is covered. The parameter  can be utilized for fine tuning. By setting it larger than 1, models with overall low performance are penalized. We found that  has a greater impact on the model selection and suffices for our use-cases. Therefore,  is set to 1 during subsequent application of the ACS.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>26957</offset><text>Figure 2 emphasizes how the parameter choice of  affects the ACS for three artificial segmentations (a–c). The rectangular boxes represent the prediction areas of the classifier and the numbers show the class-wise  scores on a separate test set. We stated that  influences the expressiveness. In particular, the closer the value gets to 1, the more each individual class score is downscaled by the size of its area. As a consequence, a low  value targets high performant models with lower expressiveness and a high  value selects models with high expressiveness and lower performance. Given the three segmentations (a–c), we plot the ACS for all possible choices of  in Figure 2d to investigate which model achieves the highest score (illustrated by the color below the curve). As expected, the broad segmentation (a) is selected for low lambda values (0–0.13), the medium segmentation (b) is chosen for values (0.13–0.37) and the fine segmentation (c) is chosen for higher values (0.37–1). In practice, a pool of models is trained such as (a–c). The  parameter is fixed, such that the best scoring model is determined. If the model does not adhere to the required use case requirements,  can be adjusted accordingly, such that a different model is obtained.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>28242</offset><text>4. Floor Plan Segmentation Algorithms</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28281</offset><text>In order to train an area classification model, we have to determine a mapping from areas to classes that we defined as floor plan segmentation. If we neglect the underlying training data distribution, we end up with segmentations where certain classes feature few to zero fingerprint samples. This results in unsatisfying classification performance. The goal should be to leverage the knowledge about available training data to compute a segmentation that benefits subsequent classification but still provides the best possible expressiveness. We call such a segmentation data-aware floor plan segmentation and present an algorithm for this purpose in the following.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>28951</offset><text>Locally Dense Cluster Expansion (LDCE)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>28990</offset><text>In the following, we introduce the LDCE algorithm that computes a floor plan segmentation, in particular, a mapping  that assign each class  a shape . Given , we can label fingerprints  with the class that belongs to the area  in which  is located. Let  for  be a set of training data, we cluster the observations and determine the shapes  based on the position labels of the resulting cluster members.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29398</offset><text>Initially, we detect a set of locally dense base clusters. This serves two purposes: (1) observations that are densely connected to a certain degree should not be separated and (2) fingerprints that are not part of any initially dense cluster should be considered as noise. Both conditions are fulfilled when applying a standard density based clustering algorithm such as the density-based spatial clustering of applications with noise (DBSCAN) algorithm.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>29854</offset><text>The resulting base clusters are subsequently expanded. Each round the closest clusters are determined and merged. Resulting clusters that contain the required amount of  members are deleted from the expansion set and added to the set of final clusters. This process is continued until either no clusters are present in the expansion set, or the smallest minimal distance exceeds the maximal allowed merging distance . Remaining clusters with fewer than  members are postprocessed. By setting the  parameters lower than , those clusters having at least  members are added to the set of final clusters. All other remaining clusters are added to the closest final cluster.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>30526</offset><text>This routine yields clusters with definable bounds for the amount of members. Since clusters with more than  members are excluded from the merging phase, any merged cluster might have at most  members. However, besides the amount of available training data per segment, we require a reasonable segmentation that adheres to the physical floor plan structure. In particular, segmentations should minimize spreads across multiple walls if possible. Furthermore, since the feature vector of subsequent classification consists of the RSS vector, the similarity in RSS signal space should be considered during the segmentation phase. The approach we propose achieves this by constructing a particular distance function between fingerprints and clusters of fingerprints that is used in the previously described algorithm. Given two fingerprints  and , we define their distance as: where  is the set of walls between  and . Note that the main distance factor is the Euclidean distance between the position labels, while the difference between RSS vectors and the number of conflicting walls are used to penalize this base distance. The distance between clusters is based on centroid distance. We add another penalty term to account for final clusters that might lie between merging clusters. Let  and  be two clusters, ,  the average position labels and ,  the average RSS vectors, the distance is then given by: where  is the subset of final clusters, such that  within . In order to prevent merging of far distant clusters, with respect to the penalized distance function, we set a threshold  on the maximal allowed merging distance of two clusters. Note that the choice of  determines the maximal amount of allowed walls between two merging clusters. If we choose , it holds that for any , there will be at most  separating walls between any merging cluster.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>32388</offset><text>After we have determined the clustering, we have to construct the two-dimensional shapes that represent the floor plan segmentation. Those are obtained by using the position labels of the respective cluster members. We can construct the shape by taking the bounding box around the labels, or computing the convex or concave hull. Figure 3 shows stages of an example run of LDCE. The clusters merge over time (a–c) until all clusters have at least  members (d). In the example, the final segments are obtained from the bounding boxes around the labels of the class members. The pseude code of the algorithm can be found in Algorithm 1.</text></passage><passage><infon key="file">no_id_0.xml</infon><infon key="id">no_id_0</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;tbody xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;&lt;bold&gt;Algorithm 1&lt;/bold&gt; LDCE floor plan segmentation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1: &lt;bold&gt;Inputs:&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  Fingerprints: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm102&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹&lt;inline-formula&gt;&lt;mml:math id=&quot;mm103&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  Walls: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm104&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹&lt;inline-formula&gt;&lt;mml:math id=&quot;mm105&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  Main parameters: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm106&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  Distance penalties: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm107&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;ζ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  DBSCAN parameters: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm108&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  Postprocessing: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm109&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2: &lt;bold&gt;Initialize:&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm110&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;[&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;]&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;−&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;ζ&lt;/mml:mi&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;−&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹&lt;inline-formula&gt;&lt;mml:math id=&quot;mm111&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm112&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm113&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; ▹ Main routine&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3: &lt;bold&gt;while&lt;/bold&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm114&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;&amp;gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;and&lt;/bold&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm115&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;&amp;lt;&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;do&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4:  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm116&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;[&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;]&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;−&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;ζ&lt;/mml:mi&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;−&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mover&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹&lt;inline-formula&gt;&lt;mml:math id=&quot;mm117&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5:  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm118&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6:  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm119&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7:  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm120&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∪&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;8:  &lt;inline-formula&gt;&lt;mml:math id=&quot;mm121&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;\&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9:  &lt;bold&gt;if&lt;/bold&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm122&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;&amp;gt;&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;then&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10:    &lt;inline-formula&gt;&lt;mml:math id=&quot;mm123&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∪&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11:  &lt;bold&gt;else&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12:    &lt;inline-formula&gt;&lt;mml:math id=&quot;mm124&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∪&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;13:  &lt;bold&gt;end if&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14: &lt;bold&gt;end while&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹ Postprocessing&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15: &lt;bold&gt;for all&lt;/bold&gt; C in &lt;inline-formula&gt;&lt;mml:math id=&quot;mm125&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;do&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16:  &lt;bold&gt;if&lt;/bold&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm126&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;&amp;gt;&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;then&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;17:    &lt;inline-formula&gt;&lt;mml:math id=&quot;mm127&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∪&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18:    &lt;inline-formula&gt;&lt;mml:math id=&quot;mm128&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;\&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19:  &lt;bold&gt;end if&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20: &lt;bold&gt;end for&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;21: &lt;bold&gt;for all&lt;/bold&gt; C in &lt;inline-formula&gt;&lt;mml:math id=&quot;mm129&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;do&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22:  Add C to closest &lt;inline-formula&gt;&lt;mml:math id=&quot;mm130&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;bold&gt;if&lt;/bold&gt; closer than &lt;inline-formula&gt;&lt;mml:math id=&quot;mm131&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;·&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23: &lt;bold&gt;end for&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹ Determine final shapes&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;24: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm132&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;{&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;}&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;▹&lt;inline-formula&gt;&lt;mml:math id=&quot;mm133&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25: &lt;inline-formula&gt;&lt;mml:math id=&quot;mm134&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26: &lt;bold&gt;return&lt;/bold&gt;
&lt;italic&gt;A&lt;/italic&gt;&lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;td align=&quot;right&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;
</infon><offset>33031</offset><text>Algorithm 1 LDCE floor plan segmentation	 	1: Inputs:	 	 	  Fingerprints: 	▹	 	  Walls: 	▹	 	  Main parameters: 	 	 	  Distance penalties: 	 	 	  DBSCAN parameters: 	 	 	  Postprocessing: 	 	 	 	 	 	2: Initialize:	 	 	  	▹	 	  	 	 	  	 	 	 	 	 	 ▹ Main routine	 	 	3: whileanddo	 	 	4:  	▹	 	5:  	 	 	6:  	 	 	7:  	 	 	8:  	 	 	9:  ifthen	 	 	10:    	 	 	11:  else	 	 	12:    	 	 	13:  end if	 	 	14: end while	 	 	 	 	 	▹ Postprocessing	 	 	15: for all C in do	 	 	16:  ifthen	 	 	17:    	 	 	18:    	 	 	19:  end if	 	 	20: end for	 	 	21: for all C in do	 	 	22:  Add C to closest if closer than 	 	 	23: end for	 	 	 	 	 	▹ Determine final shapes	 	 	24: 	▹	 	25: 	 	 	26: returnA	 	 	 	 	 	</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>33901</offset><text>5. Machine Learning Model Building</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>33937</offset><text>The complete pipeline of ML model building comprises (1) preprocessing of the data, (2) model training and (3) model selection and evaluation. Each step is explained in the following.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>34121</offset><text>5.1. Preprocessing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>34140</offset><text>5.1.1. Feature Preprocessing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>34169</offset><text>The applied machine learning models require inputs of fixed dimensions. Each access point that is observed during data collection represents one dimension of the input vector. Having observed a total amount of M access points, we can construct a feature vector , where  for  and  represents the RSS value of the i-th access point of the n-th measurement. Given a collected training sample, there is not a RSS value for each access point. This can have two reasons: (1) the access point cannot be observed at the measuring position because it is out of range, or (2) the access point is in general observable for the given location, however, its RSS value could not be recorded in that specific sample. The second reason is caused by the response rate of an access point, which is correlated with the average observable RSS value for a location. For both causes of unobservable access points, an artificial value has to be chosen as entry for the feature vector. A common practice, which neglects the response rate of access points, is to simply set all missing values to a low RSS value, such as -110dB. This approach is adopted in our experiments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>35318</offset><text>For gradient-based learning algorithms such as DNNs or distance-based algorithms such as k-nearest neighbor (k-NN), it is crucial to normalize or standardize each feature column. This speeds up the learning phase and prevents features with a longer range to outweigh other features. It can be distinguished between feature scaling/normalization and feature standardization (z-score normalization). Scaling linearly transforms the data into the interval , while standardization transforms the data to have zero mean and standard deviation equal to one. Standardization is especially useful if the range of the features are unknown or the feature contains many outliers. For choosing the right normalization technique, we have to investigate the influence of the given map coverage. Let  and  be two access points that are far away, such that there is no location where both can be observed simultaneously. Let  and  be the areas where either signals of  or  are received. A map coverage that contains much more samples of  does only have few samples with signal of . When standardizing the data of the map coverage, we encode a strong bias into the preprocessed data, since the feature column of  is strongly influenced by the vast amount of zero entries. Such a bias might be tolerable if the distribution of training data matches the test data distribution. However, during online localization, users might request their position mostly within , which would result in worse performance. In order to prevent this bias towards the given map coverage, we simply apply column-wise feature scaling. For each AP it is likely that a sample exist which could not register any signal strength for the AP. As a conclusion, the minimum RSS value for all columns is equal to the supplementary value for missing data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>37124</offset><text>5.1.2. Floor Plan Segmentation (Parameter Choice)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>37174</offset><text>To obtain the class labeled set , we partition the floor plan with the introduced LDCE algorithm. The choice of certain parameters of the LDCE algorithm depends on the given floor plan and the spatial distribution of available training data. The parameters  and  determine the starting clusters that result from the initial DBSCAN execution. They should be chosen empirically, such that the sizes of starting clusters do not exceed the stop_size member threshold and not too many observations are considered as noise. The value of  and the wall penalty should also be chosen empirically based on the given floor plan dimensions and the amount of walls that should be allowed within segments. The penalty term  is set to 2, since higher values might yield overlapping clusters during the initial DBSCAN execution.  is set to the highest penalty value of 20 to avoid intersecting final clusters. After those parameters are fixed, we can vary the  and  parameters to obtain multiple segmentations with various granularities. An overview of the parameters can be found in Table 1. Those parameters that depend on the given test site are revisited in the corresponding Section 6.2 and Section 6.3.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>38367</offset><text>5.1.3. Label Preprocessing</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38394</offset><text>For training of regression models, the labels consist of the set of positions , , where each label is a two-dimensional vector representing the position tag. In case of area classification, the labels  with , , for the set  are the one-hot encoded areas of the floor plan segmentation, where  and 0 at all other positions. K represents the amount of segments of the given floor plan segmentation .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>38792</offset><text>5.2. Model Training</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>38813</offset><text>In the upcoming case study in Section 6, we focus on three types of supervised machine learning models that are suitable to predict the area of unknown fingerprints. After hyperparameter tuning we end up with a DNN model that has 3 hidden layers (HL) and 512 hidden units (HU) per layer and utilizes rectified linear unit (ReLU) as activation function between layers. In order to learn the conditional probability distribution , we apply softmax activation function for the output layer together with multiclass cross-entropy loss. This choice can be derived by following a maximum likelihood approach. The Adam optimizer, a variant of stochastic gradient descent (SGD), is utilized for iterative learning of the weights. To prevent overfitting, we apply early stopping, which stops the training phase if the performance on a separate validation data set does not increase for a specified amount of epochs. Furthermore, weight regularization within the loss function and dropout are applied. The complete parameterization of the tuned DNN is given in Table 2. In addition, we train a CNN with similar hyperparameters as suggested by, which consists of two convolutional layers of size (16 × 16), a Maxpool layer of size (8 × 8), a convolutional layer of size (8 × 8) and a Maxpool layer of size (8 × 8). In-between layers, we add dropout layers with dropping probability of 0.25 and utilize ReLu as activation function. Finally, a fully connected dense layer of size 128 is used with output softmax activation function. We found that rearranging the RSS vector as matrix with zero padding outperforms the proposed preprocessing method of that utilize the PCC to reduce the dimensionality and scale the data per access point. Furthermore, we fit a SVM with RBF kernel, which we utilize as discriminative model to directly predict .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>40648</offset><text>Additionally, we select two regression models (k-NN and DNN(reg)). The DNN regression model has the same configuration as the DNN classifier but uses a linear output activation function and mean squared error as loss function. The k-NN models apply the weighted version of the algorithm and are evaluated for three values of k, namely, 2,3 and 5. To validate whether explicitly training a classifier provides valuable results, we label the regression outputs with the closest area of the floor plan segmentation during postprocessing and compare them to the output of the area classifiers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>41238</offset><text>5.3. Model Evaluation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>41261</offset><text>For model evaluation, we require a splitting strategy into training and test data as well as a metric that indicates how well a model performs. Those are introduced for the different model types in the following. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>41475</offset><text>Area classifiers: The training data is labeled according to the computed floor plan segmentations. We apply k-fold cross validation with k=5, such that we arrive at 20% test data per fold. We utilize the stratified version to obtain a good representative of the whole data set in each split.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>41767</offset><text>Regression models: We choose a subset of testing positions by applying DBSCAN on the position labels only. Based on the resulting clusters we apply 5-fold cross validation, such that 20% of the clusters are used as testing data in each fold.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42009</offset><text> Splitting strategy:  </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42032</offset><text> Metric: </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42042</offset><text>As metrics, we compute error vectors for the vectors of predictions and ground truth labels. Those error vectors can be visualized via an empirical cumulative distribution function, which we will refer to as CDF in the following.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42272</offset><text>Area classifiers: The error vector consists of the pairwise distances between the centers of the predicted areas and the ground truth areas, which is zero in case of a correct prediction. The y-intercept of the CDF corresponds to the machine learning accuracy metric (ACC). The curve yields additional knowledge about the significance of misclassification. Furthermore, we report the  score (F1).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42669</offset><text>Regression models: In case of exact position estimation, the error vector consists of the pairwise distances between predictions and ground truth positions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>42826</offset><text>Selection via ACS: During model selection, we utilize the ACS as metric. This requires computing the class-wise  scores of the predicted and ground truth areas.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>42987</offset><text>6. Experimental Evaluation</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43015</offset><text>Does adaptive area localization based on a data-aware floor plan segmentation provide more robust results than the standard regression approach for exact position estimation? In particular, is it suited for arbitrarily collected training data via crowdsourcing?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43277</offset><text>When crowdsourced training data is generated continuously, the area classifier has to adapt to the current data basis. This is accomplished by recomputing the underlying floor plan segmentation and retraining a classification model on the data labeled with the corresponding areas. In this setting, is the proposed ACS suited for automatic model selection among a pool of models that provide varying performances and expressivenesses?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43712</offset><text>The subsequent experimental case study targets two separate questions:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>43783</offset><text>6.1. Study Design</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43802</offset><text>In order to answer these questions, we conduct two experiments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>43866</offset><text>Static performance analysis (Section 6.2.1 and Section 6.3.1): we compute two floor plan segmentations with varying granularities for a snapshot of collected training data. For each segmentation we train and evaluate various classification models. In addition, the performance of the proposed area classifiers is compared to standard regression models that aim at pinpointing the exact location.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>44262</offset><text>Model selection via ACS for continuous data collection (Section 6.2.2 and Section 6.3.2): we subdivide all available training data into 5 epochs that contain roughly the same amount of additional data to simulate the continuous data collection. For each epoch we compute a pool of floor plan segmentations, where we choose the parameters  and  empirically to obtain segmentations with various granularities. Subsequently, we optimize a classifier on the data labeled with the areas. The parameter  has to be chosen according to the use case requirements. We exemplarily choose the outer bounds (0 and 1), where 0 provides high performance and low expressiveness and 1 targets models with higher expressiveness. Furthermore,  is chosen to select a balanced model. We demonstrate how to utilize the ACS to automatically select the optimal model for the given use case requirements.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45142</offset><text>Both experiments are conducted on two different data sets. The first one has been collected in our university building. The second one utilizes the publicly available benchmark dataset for indoor localization using crowdsourced data, which was captured in Tampere, Finland. In the following we report the results grouped by the different test sites.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>45492</offset><text>6.2. Case Study: RWTH Aachen University Building</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>45542</offset><text>The test environment for the data that we collected by ourselves is the 4th floor of the civil engineering building of the RWTH Aachen university, Germany. The floor contains several offices and a long hall. The total area is roughly 1500 m. Two smartphones (Oneplus and LG) are used to collect labeled fingerprints with continuous position tags. In a period of 9 months (from December 2018 to August 2019), a total amount of above 1000 fingerprints have been collected. The initial performance analysis utilizes the entire training data as static data set.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>46100</offset><text>6.2.1. Static Performance Analysis</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46136</offset><text>By applying the LDCE algorithm with two different parameterizations, we obtain two floor plan segmentations, which differ in granularity.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46274</offset><text>The segmentations are shown in Figure 4, where the segments are represented by the shapes with black boundaries. The grey points represent fingerprint locations. We sum the amount of data per 2 × 2 m square and plot a heatmap to visualize the training data distribution. The initial DBSCAN is performed with  and , which yields reasonably sized start clusters. We choose a wall penalty of 10 such that given , there will be at most 2 separating walls between merging clusters. The first segmentation (Figure 4a) sets stop_size equal to 80, such that clusters are excluded from the expansion set when they reach more than 80 members. The second segmentation (Figure 4b) is obtained by setting stop_size to 50.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>46984</offset><text>We label the data set according to both segmentations and train the models described in Section 5.2 to predict the right area.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>47111</offset><text>The resulting CDF is illustrated in Figure 5.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>47157</offset><text>The CNN and the DNN achieve the best classification performance with an accuracy of above 97% on the broad segmentation and almost 95% on the finer segmentation. While the SVM achieves acceptable results for the broad segmentation its performance significantly decreases when using a finer segmentation. All regression model results are mapped to the closest class. They achieve lower performance than the CNN and DNN classifiers. A comprehensive overview of the model comparison can be found in Table 3. The lowest mean error is achieved by the DNN classifier with values of 0.43m and 0.66m respectively. For illustration purposes we plotted the class-wise  score of the best performing model as green numbers for each segment in Figure 4.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>47898</offset><text>In addition, we evaluate the performance of training a standard regression model for exact position estimation.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48010</offset><text>The results are presented in Figure 6. The best regression model (DNN) guarantees that in 95% of the cases, the estimated position will not differ more than 10 m. In comparison the area classification models guarantee a correct area prediction in 95% of the cases and thus achieve more robust results. This is achieved by lowering the expressiveness and utilizing the knowledge about available training data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>48419</offset><text>6.2.2. Model Selection via ACS</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48451</offset><text>In the following we present the results when applying the ACS for model selection as described in Section 6.1.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>48562</offset><text>Figure 7 shows the ACS score of the trained models on the pool of segmentations for the three choices of . The figure is interpreted by fixing a choice for  depending on the use case. At each epoch, we can now deliver the model with the highest ACS, since it provides the best balance between expressiveness and performance. Note that for the first two epochs, the segmentations obtained from  result in a single cluster, since too few data is available and are thus discarded. When inspecting the score for , we see that at the second and third epoch, we would use the segmentation obtained by LDCE (5:20), while in epoch four the highest score is achieved on LDCE (10:40). Finally, for the last epoch, the classifier that was optimized on LDCE (40:80) is selected.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>49329</offset><text>The changes in ACS are discussed epoch-wise in the following. While epoch 1 contains only training data of the lower left offices, in epoch 2 additional training data along the hall has been collected. This allows for additional areas. LDCE (5:20) yields much more new segments among the hall, which causes the high increase for . For , those small segments do not affect the score, however, the achieved class-wise  score does, which is slightly lower for LDCE (10:40). Between epoch 2 and 3, only few new areas are covered, however, the lower left offices feature additional data. LDCE (20:60) and LDCE (40:80) are equal, which can also be observed from their similar ACS values. In LDCE (10:40), the lower offices have already been split in epoch 2, which yielded a bad performance. The additional data allows for improved model performance, which explains the increased ACS. Between epoch 3 and 4, only data in previously uncovered areas is added. This causes an increased ACS value for all segmentations and  values. For the broadest segmentation LDCE (40:80), the previous areas remain the same, while the other segmentations adopt a finer granularity. Therefore, the highest relative increase for  is observed for LDCE (40:80). Between epoch 4 and 5, no additional areas are covered with training data. However, segmentation LDCE (40:80) rearranges its area shapes, such that the total covered area increases. While the class-wise  scores remain roughly the same, this causes the jump in ACS value for . The other segmentations remain mainly unchanged, since only the  scores of the models slightly change.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>50943</offset><text>6.3. Case Study: Tampere, Finland</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>50977</offset><text>In addition to the data collected by ourselves, we evaluate our approach on a publicly available fingerprinting dataset that was generated via crowdsourcing. The original dataset consists of 4648 fingerprints collected by 21 devices in a university building in Tampere, Finland. The fingerprints are distributed over five floors, while the 1st floor contains the highest sample density. Therefore, we select the data of the 1st floor as subset to conduct our experiments.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>51449</offset><text>6.3.1. Static Performance Analysis</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>51485</offset><text>Using the entire data collected on the 1st floor, we construct two floor plan segmentations based on the LDCE algorithm, which can be found in Figure 8. The initial DBSCAN is performed with  and . Note that in contrast to the other site, we slightly increase the  parameter to obtain reasonably sized start clusters. This is justified because the overall training data distribution is more sparse and the map has more than 5 times the extent of the other test site. Following the same logic, we increase the  parameter to 50. We use the same penalties as before but lowered the wall penalty to 5, since we want to allow clusters to span several office rooms. The remaining parameters can be found in Table 1. The broad segmentation was obtained by choosing a stop_size of 100 and for the fine segmentation we set stop_size equal to 60.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>52321</offset><text>The dataset is published with a predetermined train test split, which consists of 20% training data and 80% testing data. When plotting the training data of the 1st floor, we noted that only a single region contains training samples, which makes the proposed split impractical. Therefore, we apply the splitting strategy described in Section 5.3. The CDF of the class-wise error vectors is presented in Figure 9. Similar to the other dataset, the DNN classification models achieve the best results independent of the segmentation. On the broad segmentation, an accuracy of 89% is reached and in 97% of the cases the predicted centroid of the area is less than 30 m off from the centroid of the true area. A comprehensive overview of the individual model performance can be found in Table 4. The DNN achieves the lowest mean centroid error and has the lowest standard deviation. The prediction performance with respect to individual areas is illustrated in Figure 8. The green numbers represent the class-wise  scores that the best model achieved.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53368</offset><text>For comparison with exact position estimation, we evaluate the performance of training standard regression models. The results are presented in Figure 10. While the DNN regression model achieves an error below 10 m with 90% probability, we achieve a correct area prediction in ~90% of the cases on the broad floor plan segmentation. Thus, for the goal of coarse localization the area classifiers provide higher guarantees.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>53791</offset><text>6.3.2. Model Selection via ACS</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>53823</offset><text>In the following we present the results when applying the ACS for model selection as described in Section 6.1. Figure 11 illustrates the obtained ACS scores of the trained models on the pool of segmentations for the three choices of . Using the ACS as selective feature, we can state the following observations. For  (high performance), the model trained on LDCE (15:40) is chosen for the first epoch and LDCE (40:80) is selected for the second and third epoch. For the entire training data the classifier trained on LDCE (60:100) is chosen. For  (balance between expressiveness and performance), LDCE (15:40) provides the selected segmentation for the first four epochs and is replaced by the slightly broader segmentation LDCE (25:60) in the last epoch. The highest expressiveness is given for , which selects the model trained on the finest segmentation LDCE (5:20) for all epochs.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>54708</offset><text>In the following the ACS graphs are analyzed epoch-wise. In the first epoch LDCE (25:60) and LDCE (15:40) consist of only two broad segments, on which the models achieve the same class-wise  scores. This can be observed, since both have the same scores for a fixed  value. Since they cover a larger total area than the finer LDCE (5:20), they score higher for low and medium  values. However, the larger number of segments of LDCE (5:20) causes the higher ACS value for . In epoch 2 LDCE (5:20) adds the most additional segments, while the number of added segments is the same for LDCE (15:40) and LDCE (25:60). This explains the scores observed for . While for the three segmentations the number of segments increases, high class-wise  scores can be maintained for LDCE (15:40) and LDCE (25:60). However, the finest segmentation LDCE (5:20) sacrifices performance for expressiveness and thus scores lower for . For  the score does not change much, since the total covered area remains mostly constant. However, LDCE (40:80), which is present in epoch 2 for first time, covers a much wider total area, since it only consists of few large segments and therefore scores considerably higher for . Between epoch 2 and 3, data is collected in previously uncovered areas, which allows for finer segmentations independent of the chosen parameters. This can be observed by the significant increase in ACS for . On the contrary, between epoch 3 and 4, mostly data within previously covered areas is collected, which allows for slightly higher performance. Finally, in the last epoch, the segmentations change again, while especially LDCE (60:100) computes a segmentation that covers a much larger total extent than the other segmentations. This explains the high increase in ACS value for .</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>56490</offset><text>7. Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>56504</offset><text>In the following the findings of our work are discussed. The results of the case study are analyzed with emphasis on the proposed concepts. Subsequently, the benefits of adaptive area localization are highlighted in comparison to existing solutions. And finally, potential applications of the proposed concept are described.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>56829</offset><text>7.1. Case Study Results</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>56854</offset><text> Model performance: </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>56875</offset><text>Independent of the test site, the DNN area classifiers outperformed all other models with respect to standard classification metrics, such as accuracy and  score. The  metric indicates that the model provides high precision and recall scores, which means that each individual area is detected properly and in case it is selected the prediction is trustable. CNN models are especially useful to learn tasks where inputs are locally connected, such as adjacent pixels in images. When randomly arranging the access point vector as a matrix, it cannot be claimed that a comparable relation between adjacent matrix entries exists. Therefore, the additional feature extraction should not provide any benefits, which is empirically demonstrated by the results. The SVM model can only be used as multi-class classifier by training several individual classifiers and following a certain voting scheme. We applied the one-vs-one strategy, which results in  classifiers if we want to detect K areas. Besides, the high computational effort, the results are worse than a simple k-NN classifier, which is also observed in. </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>57985</offset><text> LDCE floor plan segmentation algorithm: </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>58027</offset><text>During the second experiment, it was demonstrated that the proposed LDCE algorithm is capable of providing a pool of segmentations with various granularities. Those can be utilized in combination with the proposed ACS to select the best area classifier with respect to the right balance between expressiveness and performance. The algorithm requires certain parameters to be chosen empirically based on the given site. </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>58447</offset><text> ACS model selection metric: </text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>58477</offset><text>The effect of  on the ACS was theoretically evaluated and demonstrated for three values in the experiments. However, explicit values cannot be associated with qualitative terms, yet. In particular, it cannot be stated which exact value is optimal for a certain use case. However, the ACS is lazily computed. Once an area classifier has been trained, its ACS can be computed for several choices of  by utilizing the stored prediction and ground truth vectors. This means that it is computationally inexpensive to compute the ACS for a pool of trained models and a large set of  values. An initial  value is guessed. When the retrieved model does not meet the requirements,  can be adjusted to match the right balance between expressiveness and performance.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>59233</offset><text>7.2. Adaptive Area Localization</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59266</offset><text>It is determined independent of the available training data.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59327</offset><text>It is statically determined, mostly prior to data collection.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59389</offset><text>Area localization has been proposed for large-scale deployments of fingerprinting-based solutions or when the data quality does not allow for exact position estimation. The objective is to provide higher positioning guarantees by lowering the expressiveness of the model. In related work, the segmentation during area classification features two characteristics:</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>59752</offset><text>Both features are unfavorable when working with crowdsourced data that is continuously collected and solutions to apply area localization in such settings are currently missing in the literature. Crowdsourced data collection results in a spatially non-uniform data distribution. Training a classifier on data where certain areas (classes) feature only few or no samples results in poor performance. A segmentation that is determined independent of the training data might result in such sparsely covered areas. Therefore, we introduce the concept of data-aware floor plan segmentation and propose the LDCE algorithm that computes such a segmentation. A data-aware floor plan segmentation introduces a trade-off between expressiveness and performance, which has not been quantified in the literature, yet. However, such a quantification is required to measure how well an area classifier performs given that the underlying segmentation is not static. Therefore, we propose the ACS that captures this trade-off. Furthermore, during crowdsourcing, data is accumulated over time. The segmentation determined for a given snapshot of data might become unfavorable once additional data has been collected. It is crucial to regularly recompute the segmentation into areas. In summary, our proposed concepts enable area localization for crowdsourced data and we empirically demonstrate that this achieves higher reliability than exact position estimation. The model adapts to the accumulating training data and finds the right balance between expressiveness and performance.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>61318</offset><text>7.3. Potential Applications</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>61347</offset><text>Depending on the use case, localization systems might have distinct requirements. A system with the objective to provide proximity based services (e.g., inside a shopping mall) requires a coarse-grained position estimation with high guarantees. In contrast, a localization system utilized for navigation of people with visual impairments might benefit from a more fine-grained position estimation. Given a base of crowdsourced training data, our approach allows to automatically construct area localization models for any required tradeoff between expressiveness and performance. Furthermore, it adapts to the accumulating training data that results from continuous crowdsourced data collection. To the best of our knowledge, generating such adaptive localization models based on fingerprinting has not been proposed in the literature, yet.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>62188</offset><text>In addition, absolute location information can be merged with systems that iteratively determine the position of a user such as PDR. WLAN fingerprinting is already employed in sensor fusion solutions. The granularity and level of guarantee of the fingerprinting model might impact initialization and convergence time of the fused model. With our approach, the fingerprinting-based localization model with the optimal granularity in that regards can be trained and deployed in the fused model.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>62681</offset><text>8. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>62696</offset><text>In this work, we propose the concept of adaptive area localization to achieve reliable position estimations using crowdsourced data that is accumulated over time. Existing area localization solutions employ a static segmentation into areas that is independent of the available training data. This approach is not applicable for crowdsoucred data collection, since it features an unbalanced spatial training data distribution that changes over time. To solve this, we propose the LDCE algorithm that computes data-aware floor plan segmentations with various granularities. The underlying segmentation influences the model performance as well as its expressiveness. We introduce the ACS to select the area classifier that provides the best trade-off between them. With those concepts, we can now regularly compute a pool of segmentations and train classifiers on the data labeled with the corresponding areas. We select the best model with the ACS and deploy it for localization.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>63674</offset><text>The proposed concepts are validated on a self-collected as well as on a publicly available crowdsourced data set. We demonstrate that the proposed area classifiers provide higher positioning guarantees than models for exact position estimation. Furthermore, we show that they adapt to the accumulating data base. In future work, we want to utilize PDR techniques and sensor fusion to automate the data collection process and to enhance the positioning quality during localization. In addition, our approach is not limited to WLAN RSS fingerprinting, but can be extended to support magnetic and light sensors or bluetooth, which we want to demonstrate in future work.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title</infon><offset>64341</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>64362</offset><text>M.L., J.B. and R.K. designed the methodology; M.L. conceived and conducted the experiments; J.B. and R.K. administrated and supervised the research project; M.L. wrote the paper, J.B. and R.K. reviewed the text and offered valuable suggestions for improving the manuscript. All authors have read and agreed to the published version of the manuscript.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>64713</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>64721</offset><text>This research received no external funding.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title</infon><offset>64765</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>64787</offset><text>The authors declare no conflict of interest.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>64832</offset><text>References</text></passage><passage><infon key="fpage">2568</infon><infon key="lpage">2599</infon><infon key="name_0">surname:Zafari;given-names:F.</infon><infon key="name_1">surname:Gkelias;given-names:A.</infon><infon key="name_2">surname:Leung;given-names:K.K.</infon><infon key="pub-id_doi">10.1109/COMST.2019.2911558</infon><infon key="section_type">REF</infon><infon key="source">Commun. Surv. Tutorials IEEE</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2019</infon><offset>64843</offset><text>A Survey of Indoor Localization Systems and Technologies</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">12</infon><infon key="name_0">surname:Basiri;given-names:A.</infon><infon key="name_1">surname:Lohan;given-names:E.S.</infon><infon key="name_2">surname:Moore;given-names:T.</infon><infon key="name_3">surname:Winstanley;given-names:A.</infon><infon key="name_4">surname:Peltola;given-names:P.</infon><infon key="name_5">surname:Hill;given-names:C.</infon><infon key="name_6">surname:Amirian;given-names:P.</infon><infon key="name_7">surname:Figueiredo e Silva;given-names:P.</infon><infon key="pub-id_doi">10.1016/j.cosrev.2017.03.002</infon><infon key="section_type">REF</infon><infon key="source">Comput. Sci. Rev.</infon><infon key="type">ref</infon><infon key="volume">24</infon><infon key="year">2017</infon><offset>64900</offset><text>Indoor location based services challenges, requirements and usability of current solutions</text></passage><passage><infon key="fpage">106</infon><infon key="lpage">117</infon><infon key="name_0">surname:Wang;given-names:Y.</infon><infon key="name_1">surname:Shao;given-names:L.</infon><infon key="pub-id_doi">10.1016/j.buildenv.2016.12.015</infon><infon key="section_type">REF</infon><infon key="source">Build. Environ.</infon><infon key="type">ref</infon><infon key="volume">114</infon><infon key="year">2017</infon><offset>64991</offset><text>Understanding occupancy pattern and improving building energy efficiency through Wi-Fi based indoor positioning</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:D’Aloia;given-names:M.</infon><infon key="name_1">surname:Cortone;given-names:F.</infon><infon key="name_2">surname:Cice;given-names:G.</infon><infon key="name_3">surname:Russo;given-names:R.</infon><infon key="name_4">surname:Rizzi;given-names:M.</infon><infon key="name_5">surname:Longo;given-names:A.</infon><infon key="pub-id_doi">10.1109/EESMS.2016.7504811</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 IEEE Workshop on Environmental, Energy, and Structural Monitoring Systems (EESMS)</infon><infon key="type">ref</infon><offset>65103</offset><text>Improving energy efficiency in building system using a novel people localization system</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Ahmetovic;given-names:D.</infon><infon key="name_1">surname:Murata;given-names:M.</infon><infon key="name_2">surname:Gleason;given-names:C.</infon><infon key="name_3">surname:Brady;given-names:E.</infon><infon key="name_4">surname:Takagi;given-names:H.</infon><infon key="name_5">surname:Kitani;given-names:K.</infon><infon key="name_6">surname:Asakawa;given-names:C.</infon><infon key="pub-id_doi">10.1145/3058555.3058560</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 14th Web for All Conference on The Future of Accessible Work—W4A ’17</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>65191</offset><text>Achieving Practical and Accurate Indoor Navigation for People with Visual Impairments</text></passage><passage><infon key="fpage">92</infon><infon key="lpage">98</infon><infon key="name_0">surname:Ho;given-names:T.W.</infon><infon key="name_1">surname:Tsai;given-names:C.J.</infon><infon key="name_2">surname:Hsu;given-names:C.C.</infon><infon key="name_3">surname:Chang;given-names:Y.T.</infon><infon key="name_4">surname:Lai;given-names:F.</infon><infon key="name_5">surname:Ben-Othman;given-names:J.</infon><infon key="name_6">surname:Gang;given-names:F.</infon><infon key="name_7">surname:Liu;given-names:J.S.</infon><infon key="name_8">surname:Arai;given-names:M.</infon><infon key="pub-id_doi">10.1145/3162957.3162971</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 3rd International Conference on Communication and Information Processing—ICCIP ’17</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>65277</offset><text>Indoor navigation and physician-patient communication in emergency department</text></passage><passage><infon key="fpage">311</infon><infon key="lpage">317</infon><infon key="name_0">surname:Kárník;given-names:J.</infon><infon key="name_1">surname:Streit;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.ifacol.2016.12.055</infon><infon key="section_type">REF</infon><infon key="source">IFAC-PapersOnLine</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2016</infon><offset>65355</offset><text>Summary of available indoor location techniques</text></passage><passage><infon key="fpage">466</infon><infon key="lpage">490</infon><infon key="name_0">surname:He;given-names:S.</infon><infon key="name_1">surname:Chan;given-names:S.H.G.</infon><infon key="pub-id_doi">10.1109/COMST.2015.2464084</infon><infon key="section_type">REF</infon><infon key="source">Commun. Surv. Tutorials IEEE</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2016</infon><offset>65403</offset><text>Wi-Fi Fingerprint-Based Indoor Positioning: Recent Advances and Comparisons</text></passage><passage><infon key="elocation-id">135</infon><infon key="name_0">surname:Xia;given-names:S.</infon><infon key="name_1">surname:Liu;given-names:Y.</infon><infon key="name_2">surname:Yuan;given-names:G.</infon><infon key="name_3">surname:Zhu;given-names:M.</infon><infon key="name_4">surname:Wang;given-names:Z.</infon><infon key="pub-id_doi">10.3390/ijgi6050135</infon><infon key="section_type">REF</infon><infon key="source">ISPRS Int. J. Geo-Inf.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2017</infon><offset>65479</offset><text>Indoor Fingerprint Positioning Based on Wi-Fi: An Overview</text></passage><passage><infon key="fpage">473</infon><infon key="lpage">478</infon><infon key="name_0">surname:Batistic;given-names:L.</infon><infon key="name_1">surname:Tomic;given-names:M.</infon><infon key="pub-id_doi">10.23919/MIPRO.2018.8400090</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</infon><infon key="type">ref</infon><offset>65538</offset><text>Overview of indoor positioning system technologies</text></passage><passage><infon key="fpage">1327</infon><infon key="lpage">1346</infon><infon key="name_0">surname:Yassin;given-names:A.</infon><infon key="name_1">surname:Nasser;given-names:Y.</infon><infon key="name_2">surname:Awad;given-names:M.</infon><infon key="name_3">surname:Al-Dubai;given-names:A.</infon><infon key="name_4">surname:Liu;given-names:R.</infon><infon key="name_5">surname:Yuen;given-names:C.</infon><infon key="name_6">surname:Raulefs;given-names:R.</infon><infon key="name_7">surname:Aboutanios;given-names:E.</infon><infon key="pub-id_doi">10.1109/COMST.2016.2632427</infon><infon key="section_type">REF</infon><infon key="source">Commun. Surv. Tutorials IEEE</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2017</infon><offset>65589</offset><text>Recent Advances in Indoor Localization: A Survey on Theoretical Approaches and Applications</text></passage><passage><infon key="fpage">293</infon><infon key="lpage">304</infon><infon key="name_0">surname:Rai;given-names:A.</infon><infon key="name_1">surname:Chintalapudi;given-names:K.K.</infon><infon key="name_2">surname:Padmanabhan;given-names:V.N.</infon><infon key="name_3">surname:Sen;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th Annual International Conference on Mobile Computing and Networking (Mobicom ’12)</infon><infon key="type">ref</infon><offset>65681</offset><text>Zee: Zero-Effort Crowdsourcing for Indoor Localization</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Radu;given-names:V.</infon><infon key="name_1">surname:Marina;given-names:M.K.</infon><infon key="pub-id_doi">10.1109/IPIN.2013.6817916</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the International Conference on Indoor Positioning and Indoor Navigation</infon><infon key="type">ref</infon><offset>65736</offset><text>HiMLoc: Indoor smartphone localization via activity aware Pedestrian Dead Reckoning with selective crowdsourced WiFi fingerprinting</text></passage><passage><infon key="elocation-id">919</infon><infon key="name_0">surname:Santos;given-names:R.</infon><infon key="name_1">surname:Barandas;given-names:M.</infon><infon key="name_2">surname:Leonardo;given-names:R.</infon><infon key="name_3">surname:Gamboa;given-names:H.</infon><infon key="pub-id_doi">10.3390/s19040919</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2019</infon><offset>65868</offset><text>Fingerprints and Floor Plans Construction for Indoor Localisation Based on Crowdsourcing</text></passage><passage><infon key="fpage">24224</infon><infon key="lpage">24238</infon><infon key="name_0">surname:Ye;given-names:Y.</infon><infon key="name_1">surname:Wang;given-names:B.</infon><infon key="pub-id_doi">10.1109/ACCESS.2018.2830415</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2018</infon><offset>65957</offset><text>RMapCS: Radio Map Construction From Crowdsourced Samples for Indoor Localization</text></passage><passage><infon key="fpage">232</infon><infon key="lpage">243</infon><infon key="name_0">surname:He;given-names:S.</infon><infon key="name_1">surname:Tan;given-names:J.</infon><infon key="name_2">surname:Chan;given-names:S.H.G.</infon><infon key="name_3">surname:Lukowicz;given-names:P.</infon><infon key="name_4">surname:Krüger;given-names:A.</infon><infon key="name_5">surname:Bulling;given-names:A.</infon><infon key="name_6">surname:Lim;given-names:Y.K.</infon><infon key="name_7">surname:Patel;given-names:S.N.</infon><infon key="pub-id_doi">10.1145/2971648.2971689</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing - UbiComp ’16</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>66038</offset><text>Towards area classification for large-scale fingerprint-based system</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Lopez-Pastor;given-names:J.A.</infon><infon key="name_1">surname:Ruiz-Ruiz;given-names:A.J.</infon><infon key="name_2">surname:Martinez-Sala;given-names:A.S.</infon><infon key="name_3">surname:Luis Gomez-Tornero;given-names:J.</infon><infon key="pub-id_doi">10.1109/IPIN.2019.8911822</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN)</infon><infon key="type">ref</infon><offset>66107</offset><text>Evaluation of an indoor positioning system for added-value services in a mall</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">10</infon><infon key="name_0">surname:Wei;given-names:J.</infon><infon key="name_1">surname:Zhou;given-names:X.</infon><infon key="name_2">surname:Zhao;given-names:F.</infon><infon key="name_3">surname:Luo;given-names:H.</infon><infon key="name_4">surname:Ye;given-names:L.</infon><infon key="pub-id_doi">10.1109/UPINLBS.2018.8559708</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 Ubiquitous Positioning, Indoor Navigation and Location-Based Services (UPINLBS)</infon><infon key="type">ref</infon><offset>66185</offset><text>Zero-cost and map-free shop-level localization algorithm based on crowdsourcing fingerprints</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">19</infon><infon key="name_0">surname:Rezgui;given-names:Y.</infon><infon key="name_1">surname:Pei;given-names:L.</infon><infon key="name_2">surname:Chen;given-names:X.</infon><infon key="name_3">surname:Wen;given-names:F.</infon><infon key="name_4">surname:Han;given-names:C.</infon><infon key="pub-id_doi">10.1155/2017/6268797</infon><infon key="section_type">REF</infon><infon key="source">Mob. Inf. Syst.</infon><infon key="type">ref</infon><infon key="volume">2017</infon><infon key="year">2017</infon><offset>66278</offset><text>An Efficient Normalized Rank Based SVM for Room Level Indoor WiFi Localization with Diverse Devices</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">5</infon><infon key="name_0">surname:Liu;given-names:H.X.</infon><infon key="name_1">surname:Chen;given-names:B.A.</infon><infon key="name_2">surname:Tseng;given-names:P.H.</infon><infon key="name_3">surname:Feng;given-names:K.T.</infon><infon key="name_4">surname:Wang;given-names:T.S.</infon><infon key="pub-id_doi">10.1109/VTCSpring.2015.7145926</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2015 IEEE 81st Vehicular Technology Conference (VTC Spring)</infon><infon key="type">ref</infon><offset>66378</offset><text>Map-Aware Indoor Area Estimation with Shortest Path Based on RSS Fingerprinting</text></passage><passage><infon key="name_0">surname:Torres-Solis;given-names:J.</infon><infon key="name_1">surname:Falk;given-names:T.</infon><infon key="name_2">surname:Chau;given-names:T.</infon><infon key="name_3">surname:Villanueva Molina;given-names:F.J.</infon><infon key="pub-id_doi">10.5772/8678</infon><infon key="section_type">REF</infon><infon key="source">Ambient Intelligence</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>66458</offset><text>A Review of Indoor Localization Technologies: Towards Navigational Assistance for Topographical Disorientation</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Pecoraro;given-names:G.</infon><infon key="name_1">surname:Di Domenico;given-names:S.</infon><infon key="name_2">surname:Cianca;given-names:E.</infon><infon key="name_3">surname:de Sanctis;given-names:M.</infon><infon key="pub-id_doi">10.1109/WiMOB.2017.8115803</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE 13th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)</infon><infon key="type">ref</infon><offset>66569</offset><text>LTE signal fingerprinting localization based on CSI</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">7</infon><infon key="name_0">surname:Xiao;given-names:L.</infon><infon key="name_1">surname:Behboodi;given-names:A.</infon><infon key="name_2">surname:Mathar;given-names:R.</infon><infon key="pub-id_doi">10.1109/ATNAC.2017.8215428</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 27th International Telecommunication Networks and Applications Conference (ITNAC)</infon><infon key="type">ref</infon><offset>66621</offset><text>A deep learning approach to fingerprinting indoor localization solutions</text></passage><passage><infon key="elocation-id">554</infon><infon key="name_0">surname:Sinha;given-names:R.S.</infon><infon key="name_1">surname:Lee;given-names:S.M.</infon><infon key="name_2">surname:Rim;given-names:M.</infon><infon key="name_3">surname:Hwang;given-names:S.H.</infon><infon key="pub-id_doi">10.3390/electronics8050554</infon><infon key="section_type">REF</infon><infon key="source">Electronics</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2019</infon><offset>66694</offset><text>Data Augmentation Schemes for Deep Learning in an Indoor Positioning Application</text></passage><passage><infon key="fpage">2481</infon><infon key="lpage">2489</infon><infon key="name_0">surname:Yang;given-names:S.</infon><infon key="name_1">surname:Dessai;given-names:P.</infon><infon key="name_2">surname:Verma;given-names:M.</infon><infon key="name_3">surname:Gerla;given-names:M.</infon><infon key="pub-id_doi">10.1109/INFCOM.2013.6567054</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2013 Proceedings IEEE INFOCOM</infon><infon key="type">ref</infon><offset>66775</offset><text>FreeLoc: Calibration-free crowdsourced indoor localization</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">18</infon><infon key="name_0">surname:Kim;given-names:W.</infon><infon key="name_1">surname:Yang;given-names:S.</infon><infon key="name_2">surname:Gerla;given-names:M.</infon><infon key="name_3">surname:Lee;given-names:E.K.</infon><infon key="pub-id_doi">10.1155/2016/4916563</infon><infon key="section_type">REF</infon><infon key="source">Mob. Inf. Syst.</infon><infon key="type">ref</infon><infon key="volume">2016</infon><infon key="year">2016</infon><offset>66834</offset><text>Crowdsource Based Indoor Localization by Uncalibrated Heterogeneous Wi-Fi Devices</text></passage><passage><infon key="fpage">117</infon><infon key="lpage">122</infon><infon key="name_0">surname:Mittal;given-names:A.</infon><infon key="name_1">surname:Tiku;given-names:S.</infon><infon key="name_2">surname:Pasricha;given-names:S.</infon><infon key="name_3">surname:Chen;given-names:D.</infon><infon key="name_4">surname:Homayoun;given-names:H.</infon><infon key="name_5">surname:Taskin;given-names:B.</infon><infon key="pub-id_doi">10.1145/3194554.3194594</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 on Great Lakes Symposium on VLSI—GLSVLSI ’18</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>66916</offset><text>Adapting Convolutional Neural Networks for Indoor Localization with Smart Mobile Devices</text></passage><passage><infon key="elocation-id">1062</infon><infon key="name_0">surname:Adege;given-names:A.</infon><infon key="name_1">surname:Lin;given-names:H.P.</infon><infon key="name_2">surname:Tarekegn;given-names:G.</infon><infon key="name_3">surname:Jeng;given-names:S.S.</infon><infon key="pub-id_doi">10.3390/app8071062</infon><infon key="section_type">REF</infon><infon key="source">Appl. Sci.</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2018</infon><offset>67005</offset><text>Applying Deep Neural Network (DNN) for Robust Indoor Localization in Multi-Building Environment</text></passage><passage><infon key="fpage">1666</infon><infon key="lpage">1671</infon><infon key="name_0">surname:Wang;given-names:X.</infon><infon key="name_1">surname:Gao;given-names:L.</infon><infon key="name_2">surname:Mao;given-names:S.</infon><infon key="name_3">surname:Pandey;given-names:S.</infon><infon key="pub-id_doi">10.1109/WCNC.2015.7127718</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2015 IEEE Wireless Communications and Networking Conference (WCNC)</infon><infon key="type">ref</infon><offset>67101</offset><text>DeepFi: Deep learning for indoor fingerprinting using channel state information</text></passage><passage><infon key="fpage">763</infon><infon key="lpage">776</infon><infon key="name_0">surname:Wang;given-names:X.</infon><infon key="name_1">surname:Gao;given-names:L.</infon><infon key="name_2">surname:Mao;given-names:S.</infon><infon key="name_3">surname:Pandey;given-names:S.</infon><infon key="pub-id_doi">10.1109/TVT.2016.2545523</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Veh. Technol.</infon><infon key="type">ref</infon><infon key="volume">66</infon><infon key="year">2016</infon><offset>67181</offset><text>CSI-based Fingerprinting for Indoor Localization: A Deep Learning Approach</text></passage><passage><infon key="fpage">18066</infon><infon key="lpage">18074</infon><infon key="name_0">surname:Chen;given-names:H.</infon><infon key="name_1">surname:Zhang;given-names:Y.</infon><infon key="name_2">surname:Li;given-names:W.</infon><infon key="name_3">surname:Tao;given-names:X.</infon><infon key="name_4">surname:Zhang;given-names:P.</infon><infon key="pub-id_doi">10.1109/ACCESS.2017.2749516</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>67256</offset><text>ConFi: Convolutional Neural Networks Based Indoor Wi-Fi Localization Using Channel State Information</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">32</infon><infon key="name_0">surname:Yang;given-names:Z.</infon><infon key="name_1">surname:Zhou;given-names:Z.</infon><infon key="name_2">surname:Liu;given-names:Y.</infon><infon key="pub-id_doi">10.1145/2543581.2543592</infon><infon key="section_type">REF</infon><infon key="source">ACM Comput. Surv. (CSUR)</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2013</infon><offset>67357</offset><text>From RSSI to CSI</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">9</infon><infon key="name_0">surname:He;given-names:S.</infon><infon key="name_1">surname:Chan;given-names:S.H.G.</infon><infon key="pub-id_doi">10.1109/SAHCN.2017.7964901</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)</infon><infon key="type">ref</infon><offset>67374</offset><text>Towards Crowdsourced Signal Map Construction via Implicit Interaction of IoT Devices</text></passage><passage><infon key="elocation-id">864</infon><infon key="name_0">surname:Zhou;given-names:B.</infon><infon key="name_1">surname:Li;given-names:Q.</infon><infon key="name_2">surname:Mao;given-names:Q.</infon><infon key="name_3">surname:Tu;given-names:W.</infon><infon key="pub-id_doi">10.3390/s17040864</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>67459</offset><text>A Robust Crowdsourcing-Based Indoor Localization System</text></passage><passage><infon key="fpage">3764</infon><infon key="lpage">3774</infon><infon key="name_0">surname:Jiang;given-names:Q.</infon><infon key="name_1">surname:Ma;given-names:Y.</infon><infon key="name_2">surname:Liu;given-names:K.</infon><infon key="name_3">surname:Dou;given-names:Z.</infon><infon key="pub-id_doi">10.1109/JSEN.2016.2535250</infon><infon key="section_type">REF</infon><infon key="source">IEEE Sensors J.</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2016</infon><offset>67515</offset><text>A Probabilistic Radio Map Construction Scheme for Crowdsourcing-Based Fingerprinting Localization</text></passage><passage><infon key="fpage">2892</infon><infon key="lpage">2906</infon><infon key="name_0">surname:Jung;given-names:S.h.</infon><infon key="name_1">surname:Moon;given-names:B.c.</infon><infon key="name_2">surname:Han;given-names:D.</infon><infon key="pub-id_doi">10.1109/TMC.2015.2506585</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Mob. Comput.</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2016</infon><offset>67613</offset><text>Unsupervised Learning for Crowdsourced Indoor Localization in Wireless Networks</text></passage><passage><infon key="fpage">1764</infon><infon key="lpage">1777</infon><infon key="name_0">surname:Jung;given-names:S.h.</infon><infon key="name_1">surname:Han;given-names:D.</infon><infon key="pub-id_doi">10.1109/ACCESS.2017.2780243</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2018</infon><offset>67693</offset><text>Automated Construction and Maintenance of Wi-Fi Radio Maps for Crowdsourcing-Based Indoor Positioning Systems</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Pipelidis;given-names:G.</infon><infon key="name_1">surname:Tsiamitros;given-names:N.</infon><infon key="name_2">surname:Ustaoglu;given-names:E.</infon><infon key="name_3">surname:Kienzler;given-names:R.</infon><infon key="name_4">surname:Nurmi;given-names:P.</infon><infon key="name_5">surname:Flores;given-names:H.</infon><infon key="name_6">surname:Prehofer;given-names:C.</infon><infon key="pub-id_doi">10.1109/IPIN.2019.8911766</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN)</infon><infon key="type">ref</infon><offset>67803</offset><text>Cross-Device Radio Map Generation via Crowdsourcing</text></passage><passage><infon key="fpage">290</infon><infon key="lpage">304</infon><infon key="name_0">surname:Chow;given-names:K.H.</infon><infon key="name_1">surname:He;given-names:S.</infon><infon key="name_2">surname:Tan;given-names:J.</infon><infon key="name_3">surname:Chan;given-names:S.H.G.</infon><infon key="pub-id_doi">10.1109/TMC.2018.2839112</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Mob. Comput.</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2019</infon><offset>67855</offset><text>Efficient Locality Classification for Indoor Fingerprint-Based Systems</text></passage><passage><infon key="fpage">575</infon><infon key="lpage">584</infon><infon key="name_0">surname:Nowicki;given-names:M.</infon><infon key="name_1">surname:Wietrzykowski;given-names:J.</infon><infon key="name_2">surname:Szewczyk;given-names:R.</infon><infon key="name_3">surname:Zieliński;given-names:C.</infon><infon key="name_4">surname:Kaliczyńska;given-names:M.</infon><infon key="pub-id_doi">10.1007/978-3-319-54042-9_57</infon><infon key="section_type">REF</infon><infon key="source">Automation 2017, Advances in Intelligent Systems and Computing, Warsaw, Poland, 15–17 March 2017</infon><infon key="type">ref</infon><infon key="volume">Volume 550</infon><infon key="year">2017</infon><offset>67926</offset><text>Low-Effort Place Recognition with WiFi Fingerprints Using Deep Learning</text></passage><passage><infon key="fpage">466</infon><infon key="name_0">surname:Kim;given-names:K.S.</infon><infon key="name_1">surname:Lee;given-names:S.</infon><infon key="name_2">surname:Huang;given-names:K.</infon><infon key="pub-id_doi">10.1186/s41044-018-0031-2</infon><infon key="section_type">REF</infon><infon key="source">Big Data Anal.</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2018</infon><offset>67998</offset><text>A scalable deep neural network architecture for multi-building and multi-floor indoor localization based on Wi-Fi fingerprinting</text></passage><passage><infon key="fpage">01044</infon><infon key="lpage">01049</infon><infon key="name_0">surname:Ibrahim;given-names:M.</infon><infon key="name_1">surname:Torki;given-names:M.</infon><infon key="name_2">surname:ElNainay;given-names:M.</infon><infon key="pub-id_doi">10.1109/ISCC.2018.8538530</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 IEEE Symposium on Computers and Communications (ISCC)</infon><infon key="type">ref</infon><offset>68127</offset><text>CNN based Indoor Localization using RSS Time-Series</text></passage><passage><infon key="elocation-id">356</infon><infon key="name_0">surname:Song;given-names:C.</infon><infon key="name_1">surname:Wang;given-names:J.</infon><infon key="pub-id_doi">10.3390/ijgi6110356</infon><infon key="section_type">REF</infon><infon key="source">ISPRS Int. J. Geo-Inf.</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2017</infon><offset>68179</offset><text>WLAN Fingerprint Indoor Positioning Strategy Based on Implicit Crowdsourcing and Semi-Supervised Learning</text></passage><passage><infon key="name_0">surname:Bishop;given-names:C.M.</infon><infon key="section_type">REF</infon><infon key="source">Pattern Recognition and Machine Learning (Information Science and Statistics)</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>68285</offset></passage><passage><infon key="fpage">1</infon><infon key="lpage">11</infon><infon key="name_0">surname:Dong;given-names:K.</infon><infon key="name_1">surname:Ling;given-names:Z.</infon><infon key="name_2">surname:Xia;given-names:X.</infon><infon key="name_3">surname:Ye;given-names:H.</infon><infon key="name_4">surname:Wu;given-names:W.</infon><infon key="name_5">surname:Yang;given-names:M.</infon><infon key="pub-id_doi">10.1155/2017/1268515</infon><infon key="section_type">REF</infon><infon key="source">Wirel. Commun. Mob. Comput.</infon><infon key="type">ref</infon><infon key="volume">2017</infon><infon key="year">2017</infon><offset>68286</offset><text>Dealing with Insufficient Location Fingerprints in Wi-Fi Based Indoor Location Fingerprinting</text></passage><passage><infon key="name_0">surname:Han;given-names:J.</infon><infon key="name_1">surname:Kamber;given-names:M.</infon><infon key="name_2">surname:Pei;given-names:J.</infon><infon key="section_type">REF</infon><infon key="series">Morgan Kaufmann Series in Data Management Systems</infon><infon key="source">Data Mining: Concepts and Techniques</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>68380</offset></passage><passage><infon key="name_0">surname:Goodfellow;given-names:I.</infon><infon key="name_1">surname:Bengio;given-names:Y.</infon><infon key="name_2">surname:Courville;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Deep Learning</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>68381</offset></passage><passage><infon key="elocation-id">32</infon><infon key="name_0">surname:Lohan;given-names:E.</infon><infon key="name_1">surname:Torres-Sospedra;given-names:J.</infon><infon key="name_2">surname:Leppäkoski;given-names:H.</infon><infon key="name_3">surname:Richter;given-names:P.</infon><infon key="name_4">surname:Peng;given-names:Z.</infon><infon key="name_5">surname:Huerta;given-names:J.</infon><infon key="pub-id_doi">10.3390/data2040032</infon><infon key="section_type">REF</infon><infon key="source">Data</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2017</infon><offset>68382</offset><text>Wi-Fi Crowdsourced Fingerprinting Dataset for Indoor Positioning</text></passage><passage><infon key="fpage">2352</infon><infon key="lpage">2449</infon><infon key="name_0">surname:Rawat;given-names:W.</infon><infon key="name_1">surname:Wang;given-names:Z.</infon><infon key="pub-id_doi">10.1162/neco_a_00990</infon><infon key="pub-id_pmid">28599112</infon><infon key="section_type">REF</infon><infon key="source">Neural Comput.</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2017</infon><offset>68447</offset><text>Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review</text></passage><passage><infon key="elocation-id">3766</infon><infon key="name_0">surname:Rana;given-names:S.P.</infon><infon key="name_1">surname:Prieto;given-names:J.</infon><infon key="name_2">surname:Dey;given-names:M.</infon><infon key="name_3">surname:Dudley;given-names:S.</infon><infon key="name_4">surname:Corchado;given-names:J.M.</infon><infon key="pub-id_doi">10.3390/s18113766</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">18</infon><infon key="year">2018</infon><offset>68531</offset><text>A Self Regulating and Crowdsourced Indoor Positioning System through Wi-Fi Fingerprinting for Multi Storey Building</text></passage><passage><infon key="comment">Lecture Notes in Electrical Engineering</infon><infon key="fpage">729</infon><infon key="lpage">739</infon><infon key="name_0">surname:Chang;given-names:Q.</infon><infon key="name_1">surname:van de Velde;given-names:S.</infon><infon key="name_2">surname:Wang;given-names:W.</infon><infon key="name_3">surname:Li;given-names:Q.</infon><infon key="name_4">surname:Hou;given-names:H.</infon><infon key="name_5">surname:Heidi;given-names:S.</infon><infon key="name_6">surname:Sun;given-names:J.</infon><infon key="name_7">surname:Liu;given-names:J.</infon><infon key="name_8">surname:Fan;given-names:S.</infon><infon key="name_9">surname:Lu;given-names:X.</infon><infon key="pub-id_doi">10.1007/978-3-662-46632-2_63</infon><infon key="section_type">REF</infon><infon key="source">China Satellite Navigation Conference (CSNC) 2015 Proceedings: Volume III</infon><infon key="type">ref</infon><infon key="volume">Volume 342</infon><infon key="year">2015</infon><offset>68647</offset><text>Wi-Fi Fingerprint Positioning Updated by Pedestrian Dead Reckoning for Mobile Phone Indoor Localization</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">4</infon><infon key="name_0">surname:Zou;given-names:H.</infon><infon key="name_1">surname:Chen;given-names:Z.</infon><infon key="name_2">surname:Jiang;given-names:H.</infon><infon key="name_3">surname:Xie;given-names:L.</infon><infon key="name_4">surname:Spanos;given-names:C.</infon><infon key="pub-id_doi">10.1109/ISISS.2017.7935650</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 IEEE International Symposium on Inertial Sensors and Systems (INERTIAL)</infon><infon key="type">ref</infon><offset>68751</offset><text>Accurate indoor localization and tracking using mobile phone inertial sensors, WiFi and iBeacon</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">2</infon><infon key="name_0">surname:Jin;given-names:F.</infon><infon key="name_1">surname:Liu;given-names:K.</infon><infon key="name_2">surname:Zhang;given-names:H.</infon><infon key="name_3">surname:Feng;given-names:L.</infon><infon key="name_4">surname:Chen;given-names:C.</infon><infon key="name_5">surname:Wu;given-names:W.</infon><infon key="pub-id_doi">10.1109/SAHCN.2018.8397155</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 15th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)</infon><infon key="type">ref</infon><offset>68847</offset><text>Towards Scalable Indoor Localization with Particle Filter and Wi-Fi Fingerprint</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:Wang;given-names:X.</infon><infon key="name_1">surname:Yu;given-names:Z.</infon><infon key="name_2">surname:Mao;given-names:S.</infon><infon key="pub-id_doi">10.1109/ICC.2018.8422562</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2018 IEEE International Conference on Communications (ICC)</infon><infon key="type">ref</infon><offset>68927</offset><text>DeepML: Deep LSTM for Indoor Localization with Smartphone Magnetic and Light Sensors</text></passage><passage><infon key="fpage">7</infon><infon key="lpage">13</infon><infon key="name_0">surname:Zhang;given-names:W.</infon><infon key="name_1">surname:Sengupta;given-names:R.</infon><infon key="name_2">surname:Fodero;given-names:J.</infon><infon key="name_3">surname:Li;given-names:X.</infon><infon key="pub-id_doi">10.1109/ICMLA.2017.0-185</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</infon><infon key="type">ref</infon><offset>69012</offset><text>DeepPositioning: Intelligent Fusion of Pervasive Magnetic Field and WiFi Fingerprinting for Smartphone Indoor Localization via Deep Learning</text></passage><passage><infon key="elocation-id">812</infon><infon key="name_0">surname:Kanaris;given-names:L.</infon><infon key="name_1">surname:Kokkinis;given-names:A.</infon><infon key="name_2">surname:Liotta;given-names:A.</infon><infon key="name_3">surname:Stavrou;given-names:S.</infon><infon key="pub-id_doi">10.3390/s17040812</infon><infon key="section_type">REF</infon><infon key="source">Sensors</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">2017</infon><offset>69153</offset><text>Fusing Bluetooth Beacon Data with Wi-Fi Radiomaps for Improved Indoor Localization</text></passage><passage><infon key="file">sensors-20-01443-g001.jpg</infon><infon key="id">sensors-20-01443-f001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69236</offset><text>Concept of adaptive area classification for crowdsourced map coverage.</text></passage><passage><infon key="file">sensors-20-01443-g002.jpg</infon><infon key="id">sensors-20-01443-f002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69307</offset><text>Illustration of the impact of  on the ACS for pool of example models.</text></passage><passage><infon key="file">sensors-20-01443-g003.jpg</infon><infon key="id">sensors-20-01443-f003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69377</offset><text>Illustration of LDCE segmentation. Clusters expand over time (a–c) until all clusters have reached a size greater than the  threshold (d).</text></passage><passage><infon key="file">sensors-20-01443-g004.jpg</infon><infon key="id">sensors-20-01443-f004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69518</offset><text>Floor plan segmentations of RWTH Aachen university building. The black lined shapes represent areas of classifier. The green numbers represent the class-wise -score of the best model. The grey dots are the fingerprint locations. The amount of training data per 2 × 2 m grid cell is illustrated via the heatmap color.</text></passage><passage><infon key="file">sensors-20-01443-g005.jpg</infon><infon key="id">sensors-20-01443-f005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69836</offset><text>CDF of classification error. Error vector build from distances between centroids of true and predicted areas.</text></passage><passage><infon key="file">sensors-20-01443-g006.jpg</infon><infon key="id">sensors-20-01443-f006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>69946</offset><text>Performance of regression models. (a) shows the CDF of the prediction errors and (b) holds mean, standard deviation, minimal and maximal error.</text></passage><passage><infon key="file">sensors-20-01443-g007.jpg</infon><infon key="id">sensors-20-01443-f007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70090</offset><text>Area classification score (ACS) for three choices of . Per epoch the model with the highest score is chosen. The legend shows the (: ) parameters used during segmentation. The other parameters of LDCE are chosen as presented in Table 1.</text></passage><passage><infon key="file">sensors-20-01443-g008.jpg</infon><infon key="id">sensors-20-01443-f008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70327</offset><text>Floor plan segmentations of 1st floor of public dataset. The black lined shapes represent areas of the classifier. The green numbers represent the class-wise -score of the best model. The grey dots are the fingerprint locations. The amount of training data per 4x4m grid cell is illustrated via the heatmap color.</text></passage><passage><infon key="file">sensors-20-01443-g009.jpg</infon><infon key="id">sensors-20-01443-f009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70641</offset><text>CDF of classification error. Error vector build from distances between centroids of true and predicted areas.</text></passage><passage><infon key="file">sensors-20-01443-g010.jpg</infon><infon key="id">sensors-20-01443-f010</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70751</offset><text>Performance of regression models. (a) shows the CDF of the prediction errors and (b) holds mean, standard deviation, minimal and maximal error.</text></passage><passage><infon key="file">sensors-20-01443-g011.jpg</infon><infon key="id">sensors-20-01443-f011</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>70895</offset><text>Area classification score (ACS) for three choices of . Per epoch the model with the highest score is chosen. The legend shows the (: ) parameters used during segmentation. The other parameters of LDCE are chosen as presented in Table 1.</text></passage><passage><infon key="file">sensors-20-01443-t001.xml</infon><infon key="id">sensors-20-01443-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>71132</offset><text>Parameter choice of LDCE for experiments.</text></passage><passage><infon key="file">sensors-20-01443-t001.xml</infon><infon key="id">sensors-20-01443-t001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Data Set&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Main&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Postprocessing&lt;/th&gt;&lt;th colspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Penalties&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;DBSCAN&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;stop_size&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;max_eps&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;minMembers&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm229&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mstyle mathvariant=&quot;bold&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mstyle&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm230&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mstyle mathvariant=&quot;bold&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;ζ&lt;/mml:mi&gt;&lt;/mml:mstyle&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;inline-formula&gt;&lt;mml:math id=&quot;mm231&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mstyle mathvariant=&quot;bold&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;η&lt;/mml:mi&gt;&lt;/mml:mstyle&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt;
&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;eps&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;minPts&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;RWTH Aachen&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;{80, 50}&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;{40,20}&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Tampere, Finnland&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;{100, 60}&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;{60, 40}&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>71174</offset><text>Data Set	Main	Postprocessing	Penalties	DBSCAN	 	stop_size	max_eps	minMembers				eps	minPts	 	RWTH Aachen	{80, 50}	30	{40,20}	10	2	20	2	3	 	Tampere, Finnland	{100, 60}	50	{60, 40}	5	2	20	5	3	 	</text></passage><passage><infon key="file">sensors-20-01443-t002.xml</infon><infon key="id">sensors-20-01443-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>71367</offset><text>DNN model hyperparameter configuration.</text></passage><passage><infon key="file">sensors-20-01443-t002.xml</infon><infon key="id">sensors-20-01443-t002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HU&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;HL&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Dropout&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Reg. Penalty&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;lr&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Batch&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Epochs&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Loss&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Activation&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Optimizer&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;512&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.06&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0007&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;200&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cat. cross-entropy&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ReLU&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Adam&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>71407</offset><text>HU	HL	Dropout	Reg. Penalty	lr	Batch	Epochs	Loss	Activation	Optimizer	 	512	3	0.2	0.06	0.0007	32	200	Cat. cross-entropy	ReLU	Adam	 	</text></passage><passage><infon key="file">sensors-20-01443-t003.xml</infon><infon key="id">sensors-20-01443-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>71539</offset><text>Performance of classification models on both segmentations. The upper three models are explicitly trained to predict one of the underlying areas, while the other models (reg-&gt;class) are regression models where we assign the closest area of the regression prediction during postprocessing.</text></passage><passage><infon key="file">sensors-20-01443-t003.xml</infon><infon key="id">sensors-20-01443-t003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Segmentation&lt;/th&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Parameter&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Area Center Error&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Std&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Min&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Max&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ACC&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;7&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;broad&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.43&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.28&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.42&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.97&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.32&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.17&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.42&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.97&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.46&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;45.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.96&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.36&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.93&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.82&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.93&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.87&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;49.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.93&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.92&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.56&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25.09&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;7&quot; align=&quot;left&quot; valign=&quot;middle&quot; colspan=&quot;1&quot;&gt;fine&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.66&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.18&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;55.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.91&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.74&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.91&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.80&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.88&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.79&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.91&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.99&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.94&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.90&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.91&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.87&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.54&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.34&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.91&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.07&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42.50&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.87&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>71828</offset><text>Segmentation	Model	Parameter	Area Center Error	Classification	 	Mean	Std	Min	Max	ACC	F1	 	broad	CNN		0.43	3.28	0.0	47.42	0.97	0.97	 	DNN		0.32	2.17	0.0	47.42	0.97	0.97	 	SVM		0.54	3.46	0.0	45.25	0.96	0.95	 	k-NN (reg- &gt; class)	k = 2	0.85	4.36	0.0	49.95	0.94	0.93	 	k-NN (reg- &gt; class)	k = 3	0.82	4.30	0.0	49.95	0.94	0.93	 	k-NN (reg- &gt; class)	k = 5	0.87	3.94	0.0	49.95	0.93	0.92	 	DNN (reg- &gt; class)		0.56	2.88	0.0	25.09	0.95	0.95	 	fine	CNN		0.66	4.18	0.0	55.12	0.95	0.91	 	DNN		0.54	3.74	0.0	59.90	0.95	0.91	 	SVM		1.12	4.80	0.0	59.90	0.88	0.79	 	k-NN (reg- &gt; class)	k = 2	1.15	5.47	0.0	59.90	0.91	0.84	 	k-NN (reg- &gt; class)	k = 3	0.99	4.94	0.0	59.90	0.91	0.87	 	k-NN (reg- &gt; class)	k = 5	1.00	4.54	0.0	48.34	0.91	0.86	 	DNN (reg- &gt; class)		0.71	3.07	0.0	42.50	0.92	0.87	 	</text></passage><passage><infon key="file">sensors-20-01443-t004.xml</infon><infon key="id">sensors-20-01443-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>72604</offset><text>Performance of classification models on both segmentations. The upper three models are explicitly trained to predict one of the underlying areas, while the other models (reg-&gt;class) are regression models where we assign the closest area of the regression prediction during postprocessing.</text></passage><passage><infon key="file">sensors-20-01443-t004.xml</infon><infon key="id">sensors-20-01443-t004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Segmentation&lt;/th&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Model&lt;/th&gt;&lt;th rowspan=&quot;2&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;Parameter&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Area Center Error&lt;/th&gt;&lt;th colspan=&quot;2&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-top:solid thin;border-bottom:solid thin&quot; rowspan=&quot;1&quot;&gt;Classification&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Mean&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Std&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Min&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Max&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ACC&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F1&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;7&quot; align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; colspan=&quot;1&quot;&gt;broad&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.70&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.15&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.87&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.21&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.60&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.89&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.88&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.92&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;65.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.83&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.55&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.02&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.87&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.97&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.48&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.26&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.86&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.34&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;65.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.85&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.83&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.62&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11.17&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;65.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.83&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.81&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;7&quot; align=&quot;left&quot; valign=&quot;middle&quot; colspan=&quot;1&quot;&gt;fine&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.65&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.11&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.83&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.81&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.53&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.75&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.84&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.81&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12.36&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;100.44&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.71&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.72&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.12&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.82&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.79&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3.95&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.30&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;90.47&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.81&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.77&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k-NN (reg- &amp;gt; class)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;k = 5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;4.25&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9.55&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.80&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.76&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DNN(reg)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5.00&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;10.07&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.79&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.76&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.72&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>72893</offset><text>Segmentation	Model	Parameter	Area Center Error	Classification	 	Mean	Std	Min	Max	ACC	F1	 	broad	CNN		3.70	10.15	0.0	69.26	0.87	0.86	 	DNN		3.21	9.60	0.0	69.26	0.89	0.88	 	SVM		4.30	10.92	0.0	65.84	0.85	0.83	 	k-NN (reg- &gt; class)	k = 2	3.55	10.02	0.0	69.26	0.87	0.86	 	k-NN (reg- &gt; class)	k = 3	3.97	10.48	0.0	69.26	0.86	0.85	 	k-NN (reg- &gt; class)	k = 5	4.34	10.84	0.0	65.84	0.85	0.83	 	DNN (reg- &gt; class)		4.62	11.17	0.0	65.84	0.83	0.81	 	fine	CNN		3.65	9.11	0.0	90.47	0.83	0.81	 	DNN		3.53	9.00	0.0	91.75	0.84	0.81	 	SVM		7.00	12.36	0.0	100.44	0.71	0.56	 	k-NN (reg- &gt; class)	k = 2	3.72	9.12	0.0	90.47	0.82	0.79	 	k-NN (reg- &gt; class)	k = 3	3.95	9.30	0.0	90.47	0.81	0.77	 	k-NN (reg- &gt; class)	k = 5	4.25	9.55	0.0	69.79	0.80	0.76	 	DNN(reg)		5.00	10.07	0.0	69.79	0.76	0.72	 	</text></passage></document></collection>
