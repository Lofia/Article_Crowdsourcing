<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220617</date><key>pmc.key</key><document><id>9159300</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.3389/frai.2022.828187</infon><infon key="article-id_pmc">9159300</infon><infon key="article-id_pmid">35664506</infon><infon key="elocation-id">828187</infon><infon key="kwd">crowdsourcing annotation labeling guidelines ambiguity clarification machine learning artificial intelligence</infon><infon key="license">This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</infon><infon key="name_0">surname:Pradhan;given-names:Vivek Krishna</infon><infon key="name_1">surname:Schaekermann;given-names:Mike</infon><infon key="name_2">surname:Lease;given-names:Matthew</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">5</infon><infon key="year">2022</infon><offset>0</offset><text>In Search of Ambiguity: A Three-Stage Workflow Design to Clarify Annotation Guidelines for Crowd Workers</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>105</offset><text>We propose a novel three-stage FIND-RESOLVE-LABEL workflow for crowdsourced annotation to reduce ambiguity in task instructions and, thus, improve annotation quality. Stage 1 (FIND) asks the crowd to find examples whose correct label seems ambiguous given task instructions. Workers are also asked to provide a short tag that describes the ambiguous concept embodied by the specific instance found. We compare collaborative vs. non-collaborative designs for this stage. In Stage 2 (RESOLVE), the requester selects one or more of these ambiguous examples to label (resolving ambiguity). The new label(s) are automatically injected back into task instructions in order to improve clarity. Finally, in Stage 3 (LABEL), workers perform the actual annotation using the revised guidelines with clarifying examples. We compare three designs using these examples: examples only, tags only, or both. We report image labeling experiments over six task designs using Amazon's Mechanical Turk. Results show improved annotation accuracy and further insights regarding effective design for crowdsourced annotation tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1212</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1228</offset><text>While crowdsourcing now enables labeled data to be obtained more quickly, cheaply, and easily than ever before (Snow et al.,; Sorokin and Forsyth,; Alonso,), ensuring data quality remains something of an art, challenge, and perpetual risk. Consider a typical workflow for annotating data on Amazon Mechanical Turk (MTurk): a requester designs an annotation task, asks multiple workers to complete it, and then post-processes labels to induce final consensus labels. Because the annotation work itself is largely opaque, with only submitted labels being observable, the requester typically has little insight into what if any problems workers encounter during annotation. While statistical aggregation (Hung et al.,; Sheshadri and Lease,; Zheng et al.,) and multi-pass iterative refinement (Little et al.,; Goto et al.,) methods can be employed to further improve initial labels, there are limits to what can be achieved by post-hoc refinement following label collection. If initial labels are poor because many workers were confused by incomplete, unclear, or ambiguous task instructions, there is a significant risk of “garbage in equals garbage out” (Vidgen and Derczynski,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2410</offset><text>In contrast, consider a more traditional annotation workflow involving trusted annotators, such as practiced by the Linguistic Data Consortium (LDC) (Griffitt and Strassel,). Once preliminary annotation guidelines are developed, an iterative process ensues in which: (1) a subset of data is labeled based on current guidelines; (2) annotators review corner cases and disagreements, review relevant guidelines, and reach a consensus on appropriate resolutions; (3) annotation guidelines are updated; and (4) the process repeats. In comparison to the simple crowdsourcing workflow above, this traditional workflow iteratively debugs and refines task guidelines for clarity and completeness in order to deliver higher quality annotations. However, it comes at the cost of more overhead, with a heavier process involving open-ended interactions with trusted annotators. Could we somehow combine these for the best of both worlds?</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3336</offset><text>In this study, we propose a novel three-stage FIND-RESOLVE-LABEL design pattern for crowdsourced annotation which strikes a middle-ground between the efficient crowdsourcing workflow on one hand and the high quality LDC-style workflow on the other. Similar to prior study (Gaikwad et al.,; Bragg and Weld,; Manam and Quinn,), we seek to design a light-weight process for engaging the workers themselves to help debug and clarify the annotation guidelines. However, existing approaches typically intervene in a reactive manner after the annotation process has started, or tend to be constrained to a specific dataset or refinement of textual instruction only. By contrast, our approach is proactive and open-ended. It leverages crowd workers' unconstrained creativity and intelligence to identify ambiguous examples through an Internet search on the Internet and enriches task instructions with these concrete examples proactively upfront before the annotation process commences. Overall, we envision a partnership between the requester and workers in which each party has complementary strengths and responsibilities in the annotation process, and we seek to maximize the relative strengths of each party to ensure data quality while preserving efficiency.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4593</offset><text>Figure 1 depicts our overall workflow. In Stage 1 (FIND), workers are shown initial guidelines for an annotation task and asked to search for data instances that appear ambiguous given the guidelines. For each instance workers find, they are also asked to provide a short tag that describes the concept embodied by the specific instance which is ambiguous given the guidelines. Next, in Stage 2 (RESOLVE), the requester selects one or more of the ambiguous instances to label as exemplars. Those instances and their tags are then automatically injected back into the annotation guidelines in order to improve clarity. Finally, in Stage 3 (LABEL), workers perform the actual annotation using the revised guidelines with clarifying examples. The requester can run the LABEL stage on a sample of data, assess label quality, and then decide how to proceed. If quality is sufficient, the remaining data can simply be labeled according to the guidelines. Otherwise, Stages 1 and 2 can be iterated in order to further refine the guidelines.</text></passage><passage><infon key="file">frai-05-828187-g0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>5627</offset><text>Our Three-Stage FIND-RESOLVE-LABEL workflow is shown above. Stage 1 (FIND) asks the crowd to find examples whose correct label seems ambiguous given the task instructions (e.g., using external Internet search or database lookup). In Stage 2 (RESOLVE), the requester selects and labels one or more of these ambiguous examples. These are then automatically injected back into task instructions in order to improve clarity. Finally, in Stage 3 (LABEL), workers perform the actual annotation using the revised guidelines with clarifying examples. If Stage 3 labeling quality is insufficient, we can return to Stage 1 to find more ambiguous examples to further clarify instructions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6305</offset><text>To evaluate our three-stage task design, we construct six different image labeling tasks with different levels of difficulty and intuitiveness. We construct a test dataset that contains different ambiguous and unambiguous concepts. Starting from simple and possibly ambiguous task instructions, we then improve instructions via our three-stage workflow. Given expert (gold) labels for our dataset for each of the six tasks, we can evaluate how well-revised instructions compare to original instructions by measuring the accuracy of the labels obtained from the crowd.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>6873</offset><text>1.1. Contributions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6892</offset><text>We provide initial evidence suggesting that the crowd can find and provide useful ambiguous examples which can be used to further clarify task instructions and that these examples may have the potential to be utilized to improve annotation accuracy. Our experiments further seem to suggest that workers can perform better when shown key ambiguous examples as opposed to randomly chosen examples. Finally, we provide an analysis of workers' performance for different intents of the same classification task and different concepts of ambiguity within each intent.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7454</offset><text>Our article is organized as follows. Section 2 presents Motivation and Background. Next, section 3 details our 3-Stage FIND-RESOLVE-LABEL workflow. Next, section 4 explains our experimental setup. Section 5 then presents the results. Finally, section 6 discusses conclusions and future directions.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>7752</offset><text>2. Motivation and Background</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7781</offset><text>Consider the task of labeling images for object detection. For example, on MTurk one might post a task such as, “Is there a dog in this image?” Such a task appears to be quite simple, but is it? For example, is a wolf a dog? What about more exotic and unusual wild breeds of dogs? Does the dog need to be a real animal or merely a depiction of one? What about a museum model of an ancient but extinct dog breed, or a realistic wax sculpture What if the dog is only partially visible in the image? Ultimately, what is it that the requester really wants? For example, a requester interested in anything and everything dog-related might have very liberal inclusion criteria. On the other hand, a requester training a self-driving car might only care about animals to be avoided, while someone training a product search engine for an e-commerce site might want to include dog-style children's toys (Kulesza et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8698</offset><text>As this seemingly simple example illustrates, annotation tasks that seem straightforward to a requester may in practice embody a variety of subtle nuances and ambiguities to be resolved. Such ambiguities can arise for many reasons. The requester may have been overly terse or rushed in posting a task. They may believe the task is obvious and that no further explanation should be needed. They likely also have their own implicit biases (of which they may be unaware) that provide a different internal conception of the task than others might have. For example, the requester might be ignorant of the domain (e.g., is a wolf a type of dog?) or have not fully defined what they are looking for. For example, in information retrieval, users' own conception and understanding of what they are looking for often evolve during the process of search and browsing (Cole,). We describe our own experiences with this in section 5.1.1. Annotators, on the other hand, also bring with them their own variety of implicit biases which the requester may not detect or understand (Ipeirotis et al.,; Sen et al.,; Dumitrache et al.,; Geva et al.,; Al Kuwatly et al.,; Fazelpour and De-Arteaga,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>9877</offset><text>2.1. Helping Requesters Succeed</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>9909</offset><text>2.1.1. Best Practices</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9931</offset><text>A variety of tutorials, surveys, introductions, and research articles offer how-to advice for successful microtask crowdsourcing with platforms such as MTurk (Jones,; Marshall and Shipman,; Egelman et al.,; Kovashka et al.,). For example, it is often recommended that requesters invest time browsing and labeling some data themselves before launching a task in order to better define and debug it (Alonso,). Studies have compared alternative task designs to suggest best practices (Grady and Lease,; Kazai et al.,; Papoutsaki et al.,; Wu and Quinn,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>10482</offset><text>2.1.2. Templates and Assisted Design</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10519</offset><text>Rather than start task design from scratch, MTurk offers templates and has suggested that requesters share successful templates for others' use (Chen et al.,). Similarly, classic research on software design patterns (Gamma et al.,) has inspired ideas for similar crowdsourcing design patterns which could be reused across different data collection tasks. For example, FIND-FIX-VERIFY (Bernstein et al.,) is a well-known example that partially inspired our study. Other researchers have suggested improved tool support for workflow design (Kittur et al.,) or engaging the crowd itself in task design or decomposition (Kittur et al.,; Kulkarni et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>11171</offset><text>2.1.3. Automating Task Design</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11201</offset><text>Other researchers have gone further still to propose new middleware and programmable APIs to let requesters define tasks more abstractly and leave some design and management tasks to the middleware (Little et al.,; Ahmad et al.,; Franklin et al.,; Barowy et al.,; Chen et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11480</offset><text>2.2. Understanding Disagreement</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>11512</offset><text>2.2.1. Random Noise vs. Bias</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11541</offset><text>Since annotators are human, even trusted annotators will naturally make mistakes from time to time. Fortunately, random error is exactly the kind of disagreement that aggregation (Hung et al.,; Sheshadri and Lease,) can easily resolve; assuming such mistakes are relatively infrequent and independent, workers will rarely err at the same instance, and therefore, techniques as simple as majority voting can address random noise. On the other hand, if workers have individual biases, they will make consistent errors; e.g., a teenager vs. a protective parent might have liberal vs. conservative biases in rating movies (Ipeirotis et al.,). In this case, it is useful to detect such consistent biases and re-calibrate worker responses to undo such biases. Aggregation can also work provided that workers do not share the same biases. However, when workers do share systematic biases, the independence assumption underlying aggregation is violated, and so aggregation can amplify bias rather than resolve it. Consequently, it is important that task design annotation guidelines should be vetted to ensure they identify cases in which annotator biases conflict with desired labels and particularly establish clear expectations for how such cases should be handled (Draws et al.,; Nouri et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>12833</offset><text>2.2.2. Objective vs. Subjective Tasks</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12871</offset><text>In fully-objective tasks, we assume each question has a single correct answer, and any disagreement with the gold standard reflects error. Label aggregation methods largely operate in this space. On the other extreme, purely-subjective (i.e., opinion) tasks permit a wide range of valid responses with little expectation of agreement between individuals (e.g., asking about one's favorite color or food). Between these simple extremes, however, lies a wide, interesting, and important space of partially-subjective tasks in which answers are only partially-constrained (Tian and Zhu,; Sen et al.,; Nguyen et al.,). For example, consider rating item quality: while agreement tends to be high for items having extremely good or bad properties, instances with more middling properties naturally elicit a wider variance in opinion. In general, because subjectivity permits a valid diversity of responses, it can be difficult to detect if an annotator does not undertake a task in good faith, complicating quality assurance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>13891</offset><text>2.2.3. Difficulty vs. Ambiguity</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13923</offset><text>Some annotation tasks are more complex than others, just as some instances within each task are more difficult to label than other instances. A common concern with crowdsourcing is whether inexpert workers have sufficient expertise to successfully undertake a given annotation task. Intuitively, more guidance and scaffolding are likely necessary with more skilled tasks and fewer expert workers (Huang et al.,). Alternatively, if we use sufficiently expert annotators, we assume difficult cases can be handled (Retelny et al.,; Vakharia and Lease,). With ambiguity, on the other hand, it would be unclear even to an expert what to do. Ambiguity is an interaction between data instances and annotation guidelines; effectively, an ambiguous instance is a corner-case with respect to guidelines. Aggregation can helpfully identify the majority interpretation but that interpretation may or may not be what is actually desired. Both difficult and ambiguous cases can lead to label confusion. Krivosheev et al. developed mechanisms to efficiently detect label confusion in classification tasks and demonstrated that alerting workers of the risk of confusion can improve annotation performance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_3</infon><offset>15113</offset><text>2.2.4. Static vs. Dynamic Disagreement</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15152</offset><text>As annotators undertake a task, their understanding of work evolves as they develop familiarity with both the data and the guidelines. In fact, prior study has shown that annotators interpret and implement task guidelines in different ways as annotation progresses (Scholer et al.,; Kalra et al.,). Consequently, different sorts of disagreement can occur at different stages of annotation. Temporally-aware aggregation can partially ameliorate this (Jung and Lease,), as can implementing data collection processes to train, “burn-in,” or calibrate annotators, controlling, and/or accelerating their transition from an initial learning state into a steady state (Scholer et al.,). For example, we emphasize identifying key boundary cases and expected labels for them.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15923</offset><text>2.3. Mitigating Imperfect Instructions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15962</offset><text>Unclear, confusing, and ambiguous task instructions are commonplace phenomena on crowdsourcing platforms (Gadiraju et al.,; Wu and Quinn,). In early study, Alonso et al. recommended collecting optional, free-form, task-level feedback from workers. While Alonso et al. found that some workers did provide example-specific feedback, the free-form nature of their feedback request elicited a variety of response types, which is difficult to check or to invalidate spurious responses. Alonso et al. also found that requiring such feedback led many workers to submit unhelpful text that was difficult to automatically cull. Such feedback was, therefore, recommended to be kept entirely optional.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16653</offset><text>While crowd work is traditionally completed independently to prevent collusion and enable statistical aggregation of uncorrelated work (Hung et al.,; Sheshadri and Lease,; Zheng et al.,), a variety of work has explored collaboration mechanisms by which workers might usefully help each other complete a task more effectively (Dow et al.,; Kulkarni et al.,; Drapeau et al.,; Chang et al.,; Manam and Quinn,; Schaekermann et al.,; Chen et al.,; Manam et al.,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17112</offset><text>Drapeau et al. proposed an asynchronous two-stage Justify-Reconsider method. In the Justify task, workers provide a rationale along with their answer referring to the task guidelines taught during training. For the Reconsider task, workers are confronted with an argument for the opposing answer submitted by another worker and then asked to reconsider (i.e., confirm or change) their original answer. The authors report that their Justify-Reconsider method generally yields higher accuracy but that requesting justifications requires additional cost. Consequently, they find that simply collecting more crowd annotations yields higher accuracy in a fixed-budget setting.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17784</offset><text>Chang et al. proposed a three-step approach in which crowd workers label the data, provide justifications for cases in which they disagree with others, and then review others' explanations. They evaluate their method on an image labeling task and report that requesting only justifications (without any further processing) does not increase the crowd accuracy. Their open-ended text responses can be subjective and difficult to check.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18219</offset><text>Kulkarni et al. provide workers with a chat feature that supports workers in dealing with inadequate task explanations, suggesting additional examples to be given to requesters, teaching other workers how to use the UI, and verifying their hypotheses of the underlying task intent. Schaekermann et al. investigate the impact of discussion among crowd workers on the label quality using a chat platform allowing synchronous group discussion. While the chat platform allows workers to better express their justification than text excerpts, the discussion increases task completion times. In addition, chatting does not impose any restriction on the topic, limiting discussion from unenthusiastic workers and efficacy. Chen et al. also proposed a workflow allowing simultaneous discussion among crowd workers, and designed task instructions and a training phase to achieve effective discussions. While their method yields high labeling accuracy, the increased cost due to the discussion limits its task scope. Manam and Quinn evaluated both asynchronous and synchronous Q&amp;A between workers and requesters to allow workers to ask questions to resolve any uncertainty about overall task instructions or specific examples. Bragg and Weld proposed an iterative workflow in which data instances with the low inter-rater agreement are put aside and either used as difficult training examples (if considered resolvable with respect to the current annotation guidelines) or used to refine the current annotation guidelines (if considered ambiguous).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19758</offset><text>Other study has explored approaches to address ambiguities even before the annotation process commences. For example, Manam et al. proposed a multi-step workflow enlisting the help of crowd workers to identify and resolve ambiguities in textual instructions. Gadiraju et al. and Nouri et al. both developed predictive models to automatically score textual instructions for their overall level of clarity and Nouri et al. proposed an interactive prototype to surface the predicted clarity scores to requesters in real-time as they draft and iterate on the instructions. Our approach also aims to resolve ambiguities upfront but focuses on identifying concrete visual examples of ambiguity and automatically enriching the underlying set of textual instructions with those examples.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20538</offset><text>Ambiguity arises from the interaction between annotation guidelines and particular data instances. Searching for ambiguous data instances within large-scale datasets or even the Internet can amount to finding a needle in a haystack. There exists an analogous problem of identifying “unknown unknowns” or “blind spots” of machine learning models. Prior study has proposed crowdsourced or hybrid human-machine approaches for spotting and mitigating model blind spots (Attenberg et al.,; Vandenhof,; Liu et al.,). Our study draws inspiration from these workflows. We leverage the scale, intelligence, and common sense of the crowd to identify potential ambiguities within annotation guidelines and may, thus, aid in the process of mitigating blind spots in downstream model development.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21330</offset><text>2.4. Crowdsourcing Beyond Data Labeling</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21370</offset><text>While data labeling represents the most common use of crowdsourcing in regard to training and evaluating machine learning models, human intelligence can be tapped in a much wider and more creative variety of ways. For example, the crowd might verify output from machine learning models, identify, and categorize blind spots (Attenberg et al.,; Vandenhof,) and other failure modes (Cabrera et al.,), and suggest useful features for a machine learning classifier (Cheng and Bernstein,).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21855</offset><text>One of the oldest crowdsourcing design patterns is utilizing the scale of the crowd for efficient, distributed exploration or filtering of large search spaces. Classic examples include the search for extraterrestrial intelligence1, for Jim Gray's sailboat (Vogels,) or other missing people (Wang et al.,), for DARPA's red balloons (Pickard et al.,), for astronomical events of interest (Lintott et al.,), and for endangered wildlife (Rosser and Wiggins,) or bird species (Kelling et al.,). Across such examples, what is being sought must be broadly recognizable so that the crowd can accomplish the search task without the need for subject matter expertise (Kinney et al.,). In the 3-stage FIND-FIX-VERIFY crowdsourcing workflow (Bernstein et al.,), the initial FIND stage directs the crowd to identify “patches” in an initial text draft where more work is needed.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22724</offset><text>Our asking the crowd to search for ambiguous examples given task guidelines further explores the potential of this same crowd design pattern for distributed search. Rather than waiting for ambiguous examples to be encountered by chance during the annotation process, we instead seek to rapidly identify corner-cases by explicitly searching for them. We offload to the crowd the task of searching for ambiguous cases, and who better to identify potentially ambiguous examples than the same workforce that will be asked to perform the actual annotation? At the same time, we reduce requester work, limiting their effort to labeling corner-cases rather than adjusting the textual guidelines.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>23413</offset><text>3. Workflow Design</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23432</offset><text>In this study, we propose a three-stage FIND-RESOLVE-LABEL workflow for clarifying ambiguous corner cases in task instructions, investigated in the specific context of a binary image labeling task. An illustration of the workflow is shown in Figure 1. In Stage 1 (FIND), workers are asked to proactively collect ambiguous examples and concept tags given task instructions (section 3.1). Next, in Stage 2 (RESOLVE), the requester selects and labels one or more of the ambiguous examples found by the crowd. These labeled examples are then automatically injected back into task instructions in order to improve clarity (section 3.2). Finally, in Stage 3 (LABEL), workers perform the actual annotation task using the revised guidelines with clarifying examples (section 3.3). Requesters run the final LABEL stage on a sample of data, assess label quality, and then decide how to proceed. If quality is sufficient the remaining data can be labeled according to the current revision of the guidelines. Otherwise, Stages 1 and 2 can be repeated in order to further refine the clarity of annotation guidelines.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>24536</offset><text>3.1. Stage 1: Finding Ambiguous Examples</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24577</offset><text>In Stage 1 (FIND), workers are asked to collect ambiguous examples given the task instructions. For each ambiguous example, workers are also asked to generate a concept tag. The concept tag serves multiple purposes. First, it acts as a rationale (McDonnell et al.,; Kutlu et al.,), requiring workers to justify their answers and thus nudging them toward high-quality selections. Rationales also provide a form of transparency to help requesters better understand worker intent. Second, the concept tag provides a conceptual explanation of the ambiguity which can then be re-injected into annotation guidelines to help explain corner cases to future workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25235</offset><text>Figure 2 shows the main task interface for Stage 1 (FIND). The interface presents the annotation task (e.g., “Is there a dog in this image?”) and asks workers: “Can you find ambiguous examples for this task?” Pilot experiments revealed that workers had difficulty understanding the task based on this textual prompt alone. We, therefore, make the additional assumption that requesters provide a single ambiguous example to clarify the FIND task for workers. For example, the FIND stage for a dog annotation task could show the image of a Toy Dog as an ambiguous seed example. Workers are then directed to use Google Image Search to find these ambiguous examples. Once an ambiguous image is uploaded, another page (not shown) asks workers to provide a short concept tag summarizing the type of ambiguity represented by the example (e.g., Toy Dog).</text></passage><passage><infon key="file">frai-05-828187-g0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26090</offset><text>In the Stage 1 (FIND) task, workers are asked to search for examples they think would be ambiguous given task instructions. In this case, “Is there a dog in this image?” In collaboration conditions (section 3.1.1), workers will see additional ambiguous examples found by past workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>26379</offset><text>3.1.1. Exploring Collaboration</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26410</offset><text>To investigate the potential value of worker collaboration in finding higher quality ambiguities, we explore a light-weight, iterative design in which workers do not directly interact with each other, but are shown examples found by past workers (in addition to the seed example provided by the requester). For example, worker 2 would see an example selected by worker 1, and worker 3 would see examples found by workers 1 and 2, etc. Our study compares three different collaboration conditions described in section 4.4.1 below.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>26939</offset><text>3.2. Stage 2: Resolving Ambiguous Examples</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26982</offset><text>After collecting ambiguous examples in Stage 1 (FIND), the requester then selects and labels one or more of these examples. The requester interface for Stage 2 (RESOLVE) is shown in Figure 3. Our interface design affords a low-effort interaction in which requesters toggle examples between three states via mouse click: (1) selected as a positive example, (2) selected as a negative example, (3) unselected. Examples are unselected by default. The selected (and labeled) examples are injected back into the task instructions for Stage 3 (LABEL).</text></passage><passage><infon key="file">frai-05-828187-g0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27528</offset><text>For Stage 2 (RESOLVE), our interface design lets a requester easily select and label images. Each mouse click on an example toggles between unselected, selected positive, and selected negative states.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>27729</offset><text>3.3. Stage 3: Labeling With Clarifying Examples</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27777</offset><text>Best practices suggest that along with task instructions, requesters should include a set of examples and their correct annotations (Wu and Quinn,). We automatically append to task instructions the ambiguous examples selected by the requester in Stage 2 (RESOLVE), along with their clarifying labels (Figure 4). Positive examples are shown first (“you should select concepts like these”), followed by negative examples (“and NOT select concepts like these”). Note that this stage does not require additional effort (e.g., instruction drafting) from the side of the requester because it merely augments the pre-existing task instruction template with the resulting list of clarifying examples.</text></passage><passage><infon key="file">frai-05-828187-g0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>28478</offset><text>For Stage 3 (LABEL), we combine the ambiguous instances and/or tags collected in Stage 1 (FIND) with the requester labels from Stage 2 (RESOLVE) and automatically inject the labeled examples back into task instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>28698</offset><text>4. Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28709</offset><text>Experiments were conducted in the context of binary image classification. In particular, we designed six annotation tasks representing different variations of labeling for the presence or absence of dog-related concepts. Similar to prior study by Kulesza et al., we found this seemingly simplistic domain effective for our study because non-expert workers bring prior intuition as to how the classification could be done, but the task is characterized by a variety of subtle nuances and inherent ambiguities. We employed a between-subjects design in which each participant was assigned to exactly one experimental condition to avoid potential learning effects. This design was enforced using “negative” qualifications (Amazon Mechanical Turk,) preventing crowd workers from participating in more than a single task. For the purpose of experimentation, authors acted as requesters. This included the specification of task instructions and intents and performing Stage 2 (RESOLVE), i.e., selecting clarifying examples for use in Stage 3 (LABEL).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>29757</offset><text>4.1. Participant Recruitment and Quality Control</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>29806</offset><text>We recruited participants on Amazon's Mechanical Turk using workers from the US who had completed at least 1,000 tasks with a 95% acceptance rate. This filter served as a basic quality assurance mechanism to increase the likelihood of recruiting good-faith workers over “spammers.” No further quality control mechanism was employed in our study to emulate imperfect, yet commonplace crowdsourcing practices for settings where definitive gold standard examples are not readily available for quality assessment. For ecological validity, we opted to not collect demographic information about participants prior to the annotation tasks.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>30443</offset><text>4.2. Dataset</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>30456</offset><text>All experiments utilized the same set of 40 images. The image set was designed to encompass both easy, unambiguous cases and a range of difficult, ambiguous cases with respect to the question “Is there a dog in this image?” We first assembled a set of candidate images using a combination of (1) an online image search conducted by the authors to identify a set of clear positive and clear negative examples, (2) the Stage 1 (FIND) mechanism in which crowd workers on Amazon's Mechanical Turk identified difficult, ambiguous cases. Similar to Kulesza et al., we identified a set of underlying, dog-related categories via multiple passes of structured labeling on the data. From this process, 11 categories of dog-related concepts emerged: (1) dogs, (2) small dog breeds, (3) similar animals (easy to confuse with dogs), (4) cartoons, (5) stuffed toys, (6) robots, (7) statues, (8) dog-related objects (e.g., dog-shaped cloud), (9) miscellaneous (e.g., hot dog, the word “dog”), (10) different animals (difficult to confuse with dogs), and (11) planes (the easiest category workers should never confuse with dogs). Each image was assigned to exactly one category.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>31627</offset><text>4.3. Annotation Tasks</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>31649</offset><text>When users of a search engine type in the query “apple,” are they looking for information about the fruit, the company, or something else entirely? Despite the paucity of detail provided by a typical terse query, search result accuracy is assessed based on how well results match the user's underlying intent. Similarly, requesters on crowdsourcing platforms expect workers to understand the annotation “intent” underlying the explicit instructions provided. Analogously, worker accuracy is typically evaluated with respect to how well-annotations match that requester's intent even if instructions are incomplete, unclear, or ambiguous.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32295</offset><text>To represent this common scenario, we designed three different annotation tasks. For each task, the textual instructions exhibit a certain degree of ambiguity such that adding clarifying examples to instructions can help clarify requester intent to workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>32553</offset><text>For each of the three tasks, we also selected two different intents, one more intuitive than the other in order to assess the effectiveness of our workflow design under intents of varying intuitiveness. In other words, we intentionally included one slightly more esoteric intent for each task hypothesizing that these would require workers to adapt to classification rules in conflict with their initial assumptions about requester intent. For each intent below, we list the categories constituting the positive class. All other categories are part of the negative class for the given intent.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33146</offset><text>For each of our six binary annotation tasks below, we partitioned examples into positive vs. negative classes given the categories included in the intent. We then measured worker accuracy in correctly labeling images according to positive and negative categories for each task.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>33424</offset><text>4.3.1. Task 1: Is There a Dog in This Image?</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33469</offset><text>Intent a (more intuitive): dogs, small dog breeds</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33519</offset><text>Intent b (less intuitive): dogs, small dog breeds, similar animals. Scenario: The requester intends to train a machine learning model for avoiding animals and believes the model may also benefit from detecting images of wolves and foxes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>33757</offset><text>4.3.2. Task 2: Is There a Fake Dog in This Image?</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33807</offset><text>Intent a (more intuitive): similar animals. Scenario: The requester is looking for animals often confused with dogs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>33924</offset><text>Intent b (less intuitive): cartoons, stuffed toys, robots, statues, objects. Scenario: The requester is looking for inanimate objects representing dogs.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>34077</offset><text>4.3.3. Task 3: Is There a Toy Dog in This Image?</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34126</offset><text>Intent a (less intuitive): small dog breeds. Scenario: Small dogs, such as Chihuahua or Yorkshire Terrier, are collectively referred to as “toy dog” breeds2. However, this terminology is not necessarily common knowledge making this intent less intuitive.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34385</offset><text>Intent b (more intuitive): stuffed toys, robots. Scenario: The requester is looking for children's toys, e.g., to train a model for an e-commerce site.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>34537</offset><text>4.4. Evaluation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>34553</offset><text>4.4.1. Qualitative Evaluation of Ambiguous Examples From Stage 1 (FIND)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34625</offset><text>For Stage 1 (FIND), we evaluated crowd workers' ability to find ambiguous images and concept tags for Task 1: “Is there a dog in this image?”. Through qualitative coding, we analyzed worker submissions based on three criteria: (1) correctness; (2) uniqueness; and (3) usefulness.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>34909</offset><text>Correctness captures our assessment of whether the worker appeared to have understood the task correctly and submitted a plausible example of ambiguity for the given task. Any incorrect examples were excluded from consideration for uniqueness or usefulness.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35167</offset><text>Uniqueness captures our assessment of how many distinct types of ambiguity workers found across correct examples. For example, we deemed “Stuffed Dog” and “Toy Dog” sufficiently close as to represent the same concept.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35393</offset><text>Usefulness captures our assessment of which of the unique ambiguous concepts found were likely to be useful in the annotation. For example, while an image of a hot dog is valid and unique, it is unlikely that many annotators would find it ambiguous in practice.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35655</offset><text>Our study compares two different collaboration conditions for Stage 1 (FIND). In both conditions, workers were shown one or more ambiguous examples with associated concept tags and were asked to add another, different example of ambiguity, along with a concept tag for that new example:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>35942</offset><text>No collaboration. Each worker sees the task interface seeded with a single ambiguous example and its associated concept tag provided by the requester. Workers find additional ambiguous examples independently from other workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36170</offset><text>Collaboration. Workers see all ambiguous examples and their concept tags previously found by other workers. There is no filtering mechanism involved, so workers may be presented with incorrect and/or duplicated examples. This workflow configuration amounts to a form of unidirectional, asynchronous communication among workers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36498</offset><text>For both collaboration conditions, a total of 15 ambiguous examples (from 15 unique workers) were collected and evaluated with respect to the above criteria.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>36656</offset><text>4.4.2. Quantitative Evaluation of Example Effectiveness in Stage 3 (LABEL)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>36731</offset><text>To evaluate the effectiveness of enriching textual instructions with ambiguous examples from Stage 1 (FIND) and to assess the relative utility of presenting workers with images and/or concept tags from ambiguous examples, we compared the following five conditions. The conditions varied in how annotation instructions were presented to workers in Stage 3 (LABEL):</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37095</offset><text>B0: No examples were provided along with textual instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37158</offset><text>B1: A set of randomly chosen examples were provided along with textual instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37243</offset><text>IMG: Only images (but no concept tags) of ambiguous examples were shown to workers along with textual instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37359</offset><text>TAG: Only concept tags (but no images) of ambiguous examples were shown to workers along with textual instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37475</offset><text>IMG+TAG: Both images and concept tags of ambiguous examples were shown to workers along with textual instructions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>37590</offset><text>Each of the five conditions above was completed by nine unique workers. Each task consisted of classifying 10 images. Workers were asked to classify each of the 10 images into either the positive or the negative class.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>37809</offset><text>5. Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>37820</offset><text>5.1. Can Workers Find Ambiguous Concepts?</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>37862</offset><text>In this section, we provide insights from pilots of Stage 1 (FIND) followed by a qualitative analysis of ambiguous examples identified by workers in this stage.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>38023</offset><text>5.1.1. Pilot Insights</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>38045</offset><text>5.1.1.1. Task Design</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38066</offset><text>Initial pilots of Stage 1 (FIND) revealed two issues: (1) duplicate concepts, and (2) misunderstanding of the task. Some easy-to-find and closely related concepts were naturally repeated multiple times. One type of concept duplication was related to the seed example provided by requesters to clarify the task objective. In particular, some workers searched for additional examples of the same ambiguity rather than finding distinct instances of ambiguity. Another misunderstanding led some workers to submit generally ambiguous images, i.e., similar to Google Image Search results for search term “ambiguous image,” rather than images that were ambiguous relative to the specific task instruction “Is there a dog in this image?” We acknowledge that our own task design was not immune to ambiguity, so we incorporated clarifications to instruct workers to find ambiguous examples distinct from the seed example and specific to the task instructions provided.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>39033</offset><text>5.1.1.2. Unexpected Ambiguous Concepts</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39072</offset><text>However, our pilots also revealed workers' ability to identify surprising examples of ambiguous concepts we had not anticipated. Some of these examples were educational and helped the paper authors learn about the nuances of our task. For example, one worker returned an image of a Chihuahua (a small dog breed) along with the concept tag “toy dog.” In trying to understand the worker's intent, we learned that the term “toy dog” is a synonym for small dog breeds (see text footnote 2). Prior to that, our interpretation of the “toy dog” concept was limited to children's toys. This insight inspired Task 3 (“Is there a toy dog in this image?”) with two different interpretations (section 4.3). Another unexpected ambiguous example was the picture of a man (Figure 5). We initially jumped to the conclusion that the worker's response was spam, but on closer inspection discovered that the picture displayed reality show celebrity “Dog the Bounty Hunter”3 These instances are excellent illustrations of the possibility that crowd workers may interpret task instructions in valid and original ways entirely unanticipated by requesters.</text></passage><passage><infon key="file">frai-05-828187-g0005.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40226</offset><text>Ambiguous examples and concept tags provided by workers in Stage 1 (FIND) for the task “Is there a dog in this image?” We capitalize tags here for presentation but use raw worker tags without modification in our evaluation.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>40454</offset><text>5.1.2. Qualitative Assessment of Example Characteristics</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40511</offset><text>We employed qualitative coding to assess whether worker submissions met each of the quality criteria (Correctness, Uniqueness, and Usefulness). Table 1 shows the percentage of ambiguous examples meeting these criteria for the two conditions with and without collaboration, respectively. Our hypothesis that collaboration among workers can help produce higher quality ambiguous examples is supported by our results. Results show that, compared to no collaboration, a collaborative workflow produced substantially greater proportions of correct (93 vs. 60%), unique (40 vs. 27%), and useful (33 vs. 27%) ambiguous examples. A potential explanation for this result is that exposing workers to a variety of ambiguous concepts upfront may assist them in exploring the space of yet uncovered ambiguities more effectively.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>41327</offset><text>Percentage of correct, unique, and useful examples from Stage 1 (FIND).</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Correct&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Unique&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Useful&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;No collaboration&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;60.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;26.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Collaboration&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;93.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;40.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;33.3&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>41399</offset><text>	Correct	Unique	Useful	 	No collaboration	60.0	26.7	26.7	 	Collaboration	93.0	40.0	33.3	 	</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>41490</offset><text>The bold value indicates the largest values per column.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>41546</offset><text>5.2. Can Ambiguous Examples Improve Annotation Accuracy?</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41603</offset><text>Next, we report quantitative results on how ambiguous examples—found in Stage 1 and selected and labeled in Stage 2—can be used as instructional material to improve annotation accuracy in Stage 3. We also provide an analysis of annotation errors.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>41854</offset><text>5.2.1. Effectiveness of Ambiguous Examples</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41897</offset><text>Our hypothesis is that these examples can be used to help delineate the boundary of our annotation task and, hence, teach annotation guidelines to crowd workers better than randomly chosen examples. Table 2 reports crowd annotation accuracy for each of the six tasks broken down by experimental condition.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>42203</offset><text>Worker accuracy [%] for all six tasks by condition.</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Design&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 1a&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 1b&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 2a&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 2b&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 3a&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task 3b&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;70.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;47.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;69.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;83.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;66.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;78.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;68.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;85.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;89.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;91.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;79.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;87.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;91.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;96.9&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG+TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;91.4&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;87.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;81.8&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.6&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>42255</offset><text>Design	Task 1a	Task 1b	Task 2a	Task 2b	Task 3a	Task 3b	 	B0	75.6	70.1	47.8	78.7	69.1	92.9	 	B1	83.0	66.4	59.0	85.5	78.7	96.0	 	IMG	88.0	89.5	68.5	85.8	89.2	93.5	 	TAG	91.0	91.0	79.0	87.0	91.0	96.9	 	IMG+TAG	91.4	87.0	81.8	86.4	88.3	96.6	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>42495</offset><text>The bold value indicates the largest values per column.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>42551</offset><text>5.2.1.1. Using Examples to Teach Annotation Guidelines</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42606</offset><text>Intuitively, providing examples to workers helps them to better understand the intended labeling task (Wu and Quinn,). Comparing designs B0 and B1 in Table 2, we clearly see that providing examples (B1) almost always produces more accurate labeling than a design that provides no examples (B0). In addition to this, the IMG design performs better than B1. This shows that the kind of examples that are provided is also important. Showing ambiguous examples is clearly superior to showing randomly chosen examples. This supports our hypothesis: ambiguous examples appear to delineate labeling boundaries for the task better than random examples.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>43251</offset><text>5.2.1.2. Instances vs. Concepts</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43283</offset><text>Best practices suggest that requesters provide examples when designing their tasks (Wu and Quinn,). We include this design in our evaluation as B1. An alternate design is to show concepts as examples instead of specific instances; this is our design TAG, shown in Table 2. For example, for the task “Is there a Dog in this image?”, instead of showing a dog statue image, we could simply provide the example concept “Inanimate Objects” should be labeled as NO. Results in Table 2 show that TAG consistently outperforms IMG, showing that teaching via example concepts can be superior to teach via example instances.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>43905</offset><text>5.2.1.3. Concepts Only vs. Concepts and Examples</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>43954</offset><text>Surprisingly, workers who were presented shown only the concept tags performed better than workers who were shown concept tags along with an example image for each concept. Hence, the particular instance chosen may not represent the concept well. This might be overcome by better selecting a more representative example for a concept or showing more examples for each concept. We leave such questions for future study.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>44373</offset><text>5.2.2. Sources of Worker Errors</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>44405</offset><text>5.2.2.1. Difficult vs. Subjective Questions</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>44449</offset><text>Table 3 shows accuracy for categories “Similar Animal” and “Cartoon” for Task 1b (section 4.3). We see that some concepts appear more difficult, such as correctly labeling a wolf or a fox. Annotators appear to need some world knowledge or training of differences between species in order to correctly distinguish such examples vs. dogs. Such concepts seem more difficult to teach; even though the accuracy improves, the improvement is less than we see with other concepts. In contrast, for Cartoon Dog (an example of a subjective question), adding this category to the illustrative examples greatly reduces the ambiguity for annotators. Other concepts like “Robot” and “Statue” also show large improvements in accuracy.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>45185</offset><text>Worker accuracy [%] on Task 1b, broken down by concept category.</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Design&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Similar animal&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Stuffed toy&lt;xref rid=&quot;TN1&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Robot&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Statue&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Cartoon&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;⋯ &lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;37.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;33.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;18.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;62.2&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;14.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;29.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;57.8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;44.4&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;92.6&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;74.1&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG+TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;92.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;97.8&lt;/td&gt;&lt;td rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Design&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Objects&lt;xref rid=&quot;TN1&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Unseen&lt;/bold&gt;&lt;break/&gt;
&lt;bold&gt;ambiguity&lt;xref rid=&quot;TN1&quot; ref-type=&quot;table-fn&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/xref&gt;&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Small&lt;/bold&gt;&lt;break/&gt;
&lt;bold&gt;breed&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Plane&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Dog&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;bold&gt;Another&lt;/bold&gt;&lt;break/&gt;
&lt;bold&gt;animal&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;border-top: thin solid #000000;&quot;&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;74.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;88.9&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;59.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.2&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;94.4&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;80.0&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;94.4&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;100.0&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;IMG+TAG&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;77.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;91.7&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.3&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;95.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;88.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>45250</offset><text>Design	Similar animal	Stuffed toy*	Robot	Statue	Cartoon	⋯ 	 	B0	37.0	22.2	33.3	18.5	62.2		 	B1	14.8	22.2	25.9	29.6	57.8		 	IMG	44.4	100.0	100.0	92.6	100.0		 	TAG	74.1	100.0	88.9	88.9	97.8		 	IMG+TAG	48.1	88.9	92.6	77.8	97.8		 	Design	Objects*	Unseenambiguity*	Smallbreed	Plane	Dog	Anotheranimal	 	B0	74.1	88.9	88.9	100.0	100.0	100.0	 	B1	59.3	82.2	94.4	100.0	100.0	100.0	 	IMG	100.0	75.6	88.9	100.0	95.6	100.0	 	TAG	100.0	80.0	94.4	100.0	91.1	100.0	 	IMG+TAG	96.3	77.8	91.7	96.3	95.6	88.9	 	</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>45746</offset><text>We see that some hard concepts cannot be easily disambiguated, e.g., Similar Animal. Concepts for which workers were not shown any examples are marked with an asterisk (*).</text></passage><passage><infon key="file">T3.xml</infon><infon key="id">T3</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>45919</offset><text>The bold value indicates the largest values per column.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>45975</offset><text>5.2.2.2. Learning Closely Related Concepts</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46018</offset><text>To see if crowdworkers learn closely related concepts without being explicitly shown examples, consider “Robot Dog” and “Stuffed Toy” as two types of a larger “Toy Dog” children's toy concept. In Task 1b, the workers are shown the concept “Robot Dog” as examples labeled as NO, without being shown an example for “Stuffed Toy.” Table 3 shows that workers learn the related concept “Stuffed Toy” and accurately label the instances that belong to this concept. The performance gain for the concept “Toy Dog” is the same as the gain for “Robot Dog,” when we compare design IMG+TAG and B1. Other similarly unseen concepts [marked with an asterisk (*) in the table] show that workers are able to learn the requester's intent for unseen concepts if given examples of other, similar concepts.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_4</infon><offset>46836</offset><text>5.2.2.3. Peer Agreement With Ambiguous Examples</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>46884</offset><text>It is not always possible or cost-effective to obtain expert/gold labels for tasks, so requesters often rely on peer-agreement between workers to estimate worker reliability. Similarly, majority voting or weighted voting is often used to aggregate worker labels for consensus (Hung et al.,; Sheshadri and Lease,). However, we also know that when workers have consistent, systematic group biases, the aggregation will serve to reinforce and amplify the group bias rather than mitigate it (Ipeirotis et al.,; Sen et al.,; Dumitrache et al.,; Fazelpour and De-Arteaga,).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>47452</offset><text>While we find agreement often correlates with accuracy, and so have largely omitted to report it in this study, we do find several concepts for which the majority chooses wrong answers, producing high agreement but low accuracy. Recall that our results are reported over nine workers per example, whereas typical studies use a plurality of three or five workers. Also recall that Tasks 1b and 2a (section 4.3) represent two of our less intuitive annotation tasks for which requester intent may be at odds with worker intuition, requiring greater task clarity for over-coming worker bias.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>48040</offset><text>Table 4 shows majority vote accuracy for these tasks for the baseline B1 design which (perhaps typical of many requesters) includes illustrative examples but not necessarily the most informative ones. Despite collecting labels from nine different workers, the majority is still wrong, with majority vote accuracy on ambiguous examples falling below 50%.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>48394</offset><text>Worker accuracy [%] on ambiguous vs unambiguous categories with baseline B1.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Task&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Unambiguous&lt;/bold&gt;
&lt;/th&gt;&lt;th valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Ambiguous&lt;/bold&gt;
&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1a&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;96.1&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;72.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1b&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;41.7&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2a&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;86.8&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;42.2&lt;/bold&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2b&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3a&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;82.5&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;75.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3b&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;98.6&lt;/td&gt;&lt;td valign=&quot;top&quot; align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;93.8&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>48471</offset><text>Task	Unambiguous	Ambiguous	 	1a	96.1	72.2	 	1b	98.6	41.7	 	2a	86.8	42.2	 	2b	98.5	82.5	 	3a	82.5	75.8	 	3b	98.6	93.8	 	</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>48591</offset><text>For Tasks 1b and 2a, the majority vote (among nine workers) on ambiguous examples is wrong.</text></passage><passage><infon key="file">T4.xml</infon><infon key="id">T4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>48683</offset><text>The bold values indicate substantially lower than the other values in the table.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>48764</offset><text>6. Conclusion and Future Work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_2</infon><offset>48794</offset><text>6.1. Summary and Contributions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>48825</offset><text>Quality assurance for labeled data remains a challenge today. In chasing the potential advantages crowdsourcing has to offer, some important quality assurance best practices from traditional export annotation workflows may be lost. Our work adds to the existing literature on mechanisms to disambiguate nuanced class categories and, thus, improve labeling guidelines and, in effect, classification decisions in crowdsourced data annotation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49266</offset><text>In this study, we presented a three-stage FIND-RESOLVE-LABEL workflow as a novel mapping of traditional annotation processes, involving iterative refinement of guidelines by expert annotators, onto a light-weight, structured task design suitable for crowdsourcing. Through careful task design and intelligent distribution of effort between crowd workers and requesters, it may be possible for the crowd to play a valuable role in reducing requester effort while also helping requesters to better understand the nuances and edge cases of their intended annotation taxonomy in order to generate clearer task instructions for the crowd. In contrast to prior work, our approach is proactive and open-ended, leveraging crowd workers' unconstrained creativity and intelligence to identify ambiguous examples online through an Internet search, proactively enriching task instructions with these examples upfront before the annotation process commences.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>50212</offset><text>While including illustrative examples in instructions is known to be helpful (Wu and Quinn,), we have shown that not all examples are equally informative to annotators and that intelligently selecting ambiguous corner-cases can improve labeling quality. Our results revealed that the crowd performed worst on ambiguous instances and, thus, can benefit the most from help for cases where requester intents run counter to annotators' internal biases or intuitions. For some instances of ambiguity, we observed high agreement among workers on answers contrary to what the requester defines as correct. Such tasks are likely to produce an incorrect label even when we employ intelligent answer aggregation techniques. Techniques like ours to refine instruction clarity are particularly critical in such cases.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>51018</offset><text>Finally, we found that workers were able to infer the correct labels for concepts closely related to the target concept. This result suggests that it may not be necessary to identify and clarify all ambiguous concepts that could potentially be encountered during the task. An intelligently selected set of clarifying examples may enable the crowd to disambiguate labels of unseen examples accurately even if not all instances of ambiguity are exhaustively covered.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_2</infon><offset>51483</offset><text>6.2. Limitations</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>51500</offset><text>In this study, we propose a novel workflow for addressing the issue of inherent ambiguity in data classification settings. However, our study is not without limitations. First, our study focuses on a specific type of annotation task (image classification). While our workflow design targets data classification tasks in general, further study is needed to empirically validate the usefulness of this approach for other data annotation settings.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>51945</offset><text>Second, our approach is based on the assumption that characteristics of ambiguous instances contributed by the crowd via external Internet search will match those of the dataset being evaluated. However, this assumption may not always be met depending on the domain and modality of the dataset. Certain datasets may not be represented via publicly available external search. In that case, additional building blocks would be needed in the workflow to enable effective search over a private data repository. Existing solutions for external search may already cluster results based on representative groups or classes. Our empirical results leave open the question of to what extent this feature could have influenced or facilitated the task for workers.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>52698</offset><text>Third, our evaluation is limited in terms of datasets, task types, and the size of our participant sample. Caution is warranted in generalizing our results beyond the specific evaluation setting, e.g., since characteristics of the dataset can influence the results. Given the limited size of our participant sample and the fact that crowd populations can be heterogeneous, our empirical data was amenable only to descriptive statistics but not to null hypothesis significance tests. In conclusion, our results should be considered indicative of the potential usefulness of our approach rather than being fully definitive. Further study is needed to validate our approach in a more statistically robust and generalizable manner via larger samples.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_2</infon><offset>53445</offset><text>6.3. Future Study</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>53463</offset><text>While we evaluate our strategy on an image labeling task, our approach is more general and could be usefully extended to other domains and tasks. For example, consider collecting document relevance judgments in information retrieval (Alonso et al.,; Scholer et al.,; McDonnell et al.,), where user information needs are often subjective, vague, and incomplete. Such generalization may raise new challenges. For example, the image classification task used in our study allows us to point workers to online image searches. However, other domains may require additional or different search tools (e.g., access to collections of domain-specific text documents) for workers to be able to effectively identify ambiguous corner cases.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>54191</offset><text>Alonso proposes having workers perform practice tasks to get familiarized with the data and thus increase annotation performance for future tasks. While our experimental setup prevented workers from performing more than one task to avoid potential learning effects, future study may explore and leverage workers' ability to improve their performance for certain types of ambiguity over time. For example, we may expect that workers who completed Stage 1 are better prepared for Stage 3 given that they have already engaged in the mental exercise of critically exploring the decision boundary of the class taxonomy in question.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>54818</offset><text>Another best practice from LDC is deriving a decision tree for common ambiguous cases which annotators can follow as a principled and consistent way to determine the label of ambiguous examples (Griffitt and Strassel,). How might we use the crowd to induce such a decision tree? Prior design study in effectively engaging the crowd in clustering (Chang et al.,) can guide design considerations for this challenge.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>55232</offset><text>In our study, Stage 2 RESOLVE required requesters to select ambiguous examples. Future study may explore variants of Stage 1 FIND where requesters filter the ambiguous examples provided by the crowd. There is an opportunity for saving requester effort if both of these stages are combined. For instance, examples selected in the filtering step of Stage 1 can be fed forward to reduce the example set considered for labeling in Stage 2. Another method would be to have requesters perform labeling simultaneously with filtering in Stage 1, eliminating Stage 2 altogether. Finally, if the requester deems the label quality of Stage 3 insufficient and initiates another cycle of ambiguity reduction via Stages 1 and 2 those stages could start with examples already identified in the prior cycle.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>56024</offset><text>A variety of other directions can be envisioned for further reducing requester effort. For example, the crowd could be called upon to verify and prune ambiguous examples collected in the initial FIND stage. Examples flagged as spam or assigned a low ambiguity rating could be automatically discarded to minimize requester involvement in Stage 2. Crowd ambiguity ratings could also be used to rank examples for guiding requesters' attention in Stage 2. A more ambitious direction for future study would be to systematically explore how well and under what circumstances the crowd is able to correctly infer requester intent. Generalizable insights about this question would enable researchers to design strategies that eliminate requester involvement altogether under certain conditions.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>56811</offset><text>Data Availability Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>56839</offset><text>The datasets presented in this article are not readily available because of an unrecoverable data loss. Data that is available will be shared online upon acceptance at https://www.ischool.utexas.edu/~ml/publications/. Requests to access the datasets should be directed to ML, ml@utexas.edu.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>57130</offset><text>Ethics Statement</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>57147</offset><text>Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. Written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>57456</offset><text>Author Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>57477</offset><text>VP: implementation, experimentation, data analysis, and manuscript preparation. MS: manuscript preparation. ML: advising on research design and implementation and manuscript preparation. All authors contributed to the article and approved the submitted version.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title_1</infon><offset>57739</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>57747</offset><text>This research was supported in part by the Micron Foundation and by Good Systems (https://goodsystems.utexas.edu), a UT Austin Grand Challenge to develop responsible AI technologies.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title_1</infon><offset>57930</offset><text>Author Disclaimer</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>57948</offset><text>Any opinions, findings, and conclusions or recommendations expressed by the authors are entirely their own and do not represent those of the sponsors.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>58099</offset><text>Conflict of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>58120</offset><text>ML is engaged as an Amazon Scholar, proposed methods are largely general across paid and volunteer labeling platforms. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>58422</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>58439</offset><text>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>58786</offset><text>1https://setiathome.berkeley.edu/.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>58821</offset><text>2https://en.wikipedia.org/wiki/Toy_dog.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">footnote</infon><offset>58861</offset><text>3http://www.dogthebountyhunter.com.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>58897</offset><text>References</text></passage><passage><infon key="fpage">53</infon><infon key="lpage">64</infon><infon key="name_0">surname:Ahmad;given-names:S.</infon><infon key="name_1">surname:Battle;given-names:A.</infon><infon key="name_2">surname:Malkani;given-names:Z.</infon><infon key="name_3">surname:Kamvar;given-names:S.</infon><infon key="pub-id_doi">10.1145/2047196.2047203</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>58908</offset><text>“The jabberwocky programming environment for structured social computing,”</text></passage><passage><infon key="fpage">184</infon><infon key="lpage">190</infon><infon key="name_0">surname:Al Kuwatly;given-names:H.</infon><infon key="name_1">surname:Wich;given-names:M.</infon><infon key="name_2">surname:Groh;given-names:G.</infon><infon key="pub-id_doi">10.18653/v1/2020.alw-1.21</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Fourth Workshop on Online Abuse and Harms (at EMNLP)</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>58987</offset><text>“Identifying and measuring annotator bias based on annotators' demographic characteristics,”</text></passage><passage><infon key="fpage">1089</infon><infon key="lpage">1092</infon><infon key="name_0">surname:Alonso;given-names:O.</infon><infon key="pub-id_doi">10.1145/2766462.2776778</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>59084</offset><text>“Practical lessons for gathering quality labels at scale,”</text></passage><passage><infon key="fpage">9</infon><infon key="lpage">15</infon><infon key="name_0">surname:Alonso;given-names:O.</infon><infon key="name_1">surname:Rose;given-names:D. E.</infon><infon key="name_2">surname:Stewart;given-names:B.</infon><infon key="pub-id_doi">10.1145/1480506.1480508</infon><infon key="section_type">REF</infon><infon key="source">ACM SigIR Forum, Vol. 42</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>59147</offset><text>“Crowdsourcing for relevance evaluation,”</text></passage><passage><infon key="section_type">REF</infon><infon key="source">Tutorial: Best Practices for Managing Workers in Follow-Up Surveys or Longitudinal Studies</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>59193</offset></passage><passage><infon key="fpage">2</infon><infon key="lpage">7</infon><infon key="name_0">surname:Attenberg;given-names:J.</infon><infon key="name_1">surname:Ipeirotis;given-names:P. G.</infon><infon key="name_2">surname:Provost;given-names:F.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 11th AAAI Conference on Human Computation, AAAIWS'11-11</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>59194</offset><text>“Beat the machine: challenging workers to find the unknown unknowns,”</text></passage><passage><infon key="fpage">102</infon><infon key="lpage">109</infon><infon key="name_0">surname:Barowy;given-names:D. W.</infon><infon key="name_1">surname:Curtsinger;given-names:C.</infon><infon key="name_2">surname:Berger;given-names:E. D.</infon><infon key="name_3">surname:McGregor;given-names:A.</infon><infon key="pub-id_doi">10.1145/2927928</infon><infon key="section_type">REF</infon><infon key="source">Commun. ACM</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2016</infon><offset>59268</offset><text>Automan: a platform for integrating human-based and digital computation</text></passage><passage><infon key="fpage">313</infon><infon key="lpage">322</infon><infon key="name_0">surname:Bernstein;given-names:M. S.</infon><infon key="name_1">surname:Little;given-names:G.</infon><infon key="name_2">surname:Miller;given-names:R. C.</infon><infon key="name_3">surname:Hartmann;given-names:B.</infon><infon key="name_4">surname:Ackerman;given-names:M. S.</infon><infon key="name_5">surname:Karger;given-names:D. R.</infon><infon key="pub-id_doi">10.1145/1866029.1866078</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>59340</offset><text>“Soylent: a word processor with a crowd inside,”</text></passage><passage><infon key="fpage">165</infon><infon key="lpage">176</infon><infon key="name_0">surname:Bragg;given-names:J.</infon><infon key="name_1">surname:Weld;given-names:D. S.</infon><infon key="pub-id_doi">10.1145/3242587.3242598</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>59393</offset><text>“Sprout: crowd-powered task design for crowdsourcing,”</text></passage><passage><infon key="fpage">CSCW2</infon><infon key="name_0">surname:Cabrera;given-names:A. A.</infon><infon key="name_1">surname:Druck;given-names:A. J.</infon><infon key="name_2">surname:Hong;given-names:J. I.</infon><infon key="name_3">surname:Perer;given-names:A.</infon><infon key="pub-id_doi">10.1145/3479569</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM Hum.-Comput. Interact</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2021</infon><offset>59452</offset><text>Discovering and validating ai errors with crowdsourced failure reports</text></passage><passage><infon key="fpage">2334</infon><infon key="lpage">2346</infon><infon key="name_0">surname:Chang;given-names:J. C.</infon><infon key="name_1">surname:Amershi;given-names:S.</infon><infon key="name_2">surname:Kamar;given-names:E.</infon><infon key="pub-id_doi">10.1145/3025453.3026044</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>59523</offset><text>“Revolt: collaborative crowdsourcing for labeling machine learning datasets,”</text></passage><passage><infon key="fpage">3180</infon><infon key="lpage">3191</infon><infon key="name_0">surname:Chang;given-names:J. C.</infon><infon key="name_1">surname:Kittur;given-names:A.</infon><infon key="name_2">surname:Hahn;given-names:N.</infon><infon key="pub-id_doi">10.1145/2858036.2858411</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>59605</offset><text>“Alloy: clustering with crowds and computation,”</text></passage><passage><infon key="name_0">surname:Chen;given-names:J. J.</infon><infon key="name_1">surname:Menezes;given-names:N. J.</infon><infon key="name_2">surname:Bradley;given-names:A. D.</infon><infon key="name_3">surname:North;given-names:T.</infon><infon key="section_type">REF</infon><infon key="source">ACM CHI Workshop on Crowdsourcing and Human Computation</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>59658</offset><text>“Opportunities for crowdsourcing research on amazon mechanical Turk,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">14</infon><infon key="name_0">surname:Chen;given-names:Q.</infon><infon key="name_1">surname:Bragg;given-names:J.</infon><infon key="name_2">surname:Chilton;given-names:L. B.</infon><infon key="name_3">surname:Weld;given-names:D. S.</infon><infon key="pub-id_doi">10.1145/3290605.3300761</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 ACM CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>59732</offset><text>“Cicero: multi-turn, contextual argumentation for accurate crowdsourcing,”</text></passage><passage><infon key="fpage">102</infon><infon key="lpage">108</infon><infon key="name_0">surname:Chen;given-names:Y.</infon><infon key="name_1">surname:Ghosh;given-names:A.</infon><infon key="name_2">surname:Kearns;given-names:M.</infon><infon key="name_3">surname:Roughgarden;given-names:T.</infon><infon key="name_4">surname:Vaughan;given-names:J. W.</infon><infon key="pub-id_doi">10.1145/2960403</infon><infon key="section_type">REF</infon><infon key="source">Commun. ACM</infon><infon key="type">ref</infon><infon key="volume">59</infon><infon key="year">2016</infon><offset>59811</offset><text>Mathematical foundations for social computing</text></passage><passage><infon key="fpage">600</infon><infon key="lpage">611</infon><infon key="name_0">surname:Cheng;given-names:J.</infon><infon key="name_1">surname:Bernstein;given-names:M. S.</infon><infon key="pub-id_doi">10.1145/2675133.2675214</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>59857</offset><text>“Flock: hybrid crowd-machine learning classifiers,”</text></passage><passage><infon key="fpage">1216</infon><infon key="lpage">1231</infon><infon key="name_0">surname:Cole;given-names:C.</infon><infon key="pub-id_doi">10.1002/asi.21541</infon><infon key="pub-id_pmid">12105693</infon><infon key="section_type">REF</infon><infon key="source">J. Assoc. Inform. Sci. Technol</infon><infon key="type">ref</infon><infon key="volume">62</infon><infon key="year">2011</infon><offset>59913</offset><text>A theory of information need for information retrieval that connects information to knowledge</text></passage><passage><infon key="fpage">1013</infon><infon key="lpage">1022</infon><infon key="name_0">surname:Dow;given-names:S.</infon><infon key="name_1">surname:Kulkarni;given-names:A.</infon><infon key="name_2">surname:Klemmer;given-names:S.</infon><infon key="name_3">surname:Hartmann;given-names:B.</infon><infon key="pub-id_doi">10.1145/2145204.2145355</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>60007</offset><text>“Shepherding the crowd yields better work,”</text></passage><passage><infon key="name_0">surname:Drapeau;given-names:R.</infon><infon key="name_1">surname:Chilton;given-names:L. B.</infon><infon key="name_2">surname:Bragg;given-names:J.</infon><infon key="name_3">surname:Weld;given-names:D. S.</infon><infon key="section_type">REF</infon><infon key="source">Fourth AAAI Conference on Human Computation and Crowdsourcing</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>60055</offset><text>“Microtalk: using argumentation to improve crowdsourcing accuracy,”</text></passage><passage><infon key="fpage">48</infon><infon key="lpage">59</infon><infon key="name_0">surname:Draws;given-names:T.</infon><infon key="name_1">surname:Rieger;given-names:A.</infon><infon key="name_2">surname:Inel;given-names:O.</infon><infon key="name_3">surname:Gadiraju;given-names:U.</infon><infon key="name_4">surname:Tintarev;given-names:N.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 9</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>60127</offset><text>“A checklist to combat cognitive biases in crowdsourcing,”</text></passage><passage><infon key="name_0">surname:Dumitrache;given-names:A.</infon><infon key="name_1">surname:Inel;given-names:O.</infon><infon key="name_2">surname:Aroyo;given-names:L.</infon><infon key="name_3">surname:Timmermans;given-names:B.</infon><infon key="name_4">surname:Welty;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Crowdtruth 2.0: Quality metrics for crowdsourcing with disagreement. arXiv [preprint] arXiv:1808.06080</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>60190</offset></passage><passage><infon key="fpage">267</infon><infon key="lpage">289</infon><infon key="name_0">surname:Egelman;given-names:S.</infon><infon key="name_1">surname:Chi;given-names:E. H.</infon><infon key="name_2">surname:Dow;given-names:S.</infon><infon key="pub-id_doi">10.1007/978-1-4939-0378-8_11</infon><infon key="section_type">REF</infon><infon key="source">Ways of Knowing in HCI</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>60191</offset><text>“Crowdsourcing in HCI research,”</text></passage><passage><infon key="name_0">surname:Fazelpour;given-names:S.</infon><infon key="name_1">surname:De-Arteaga;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">arXiv [Preprint].</infon><infon key="type">ref</infon><infon key="year">2022</infon><offset>60228</offset><text>Diversity in sociotechnical machine learning systems</text></passage><passage><infon key="fpage">61</infon><infon key="lpage">72</infon><infon key="name_0">surname:Franklin;given-names:M. J.</infon><infon key="name_1">surname:Kossmann;given-names:D.</infon><infon key="name_2">surname:Kraska;given-names:T.</infon><infon key="name_3">surname:Ramesh;given-names:S.</infon><infon key="name_4">surname:Xin;given-names:R.</infon><infon key="pub-id_doi">10.1145/1989323.1989331</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>60281</offset><text>“Crowddb: answering queries with crowdsourcing,”</text></passage><passage><infon key="fpage">5</infon><infon key="lpage">14</infon><infon key="name_0">surname:Gadiraju;given-names:U.</infon><infon key="name_1">surname:Yang;given-names:J.</infon><infon key="name_2">surname:Bozzon;given-names:A.</infon><infon key="pub-id_doi">10.1145/3078714.3078715</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 28th ACM Conference on Hypertext and Social Media</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>60334</offset><text>“Clarity is a worthwhile quality: on the role of task clarity in microtask crowdsourcing,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">4</infon><infon key="name_0">surname:Gaikwad;given-names:S. N. S.</infon><infon key="name_1">surname:Whiting;given-names:M. E.</infon><infon key="name_2">surname:Gamage;given-names:D.</infon><infon key="name_3">surname:Mullings;given-names:C. A.</infon><infon key="name_4">surname:Majeti;given-names:D.</infon><infon key="name_5">surname:Goyal;given-names:S.</infon><infon key="pub-id_doi">10.1145/3022198.3023270</infon><infon key="section_type">REF</infon><infon key="source">Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>60429</offset><text>“The daemo crowdsourcing marketplace,”</text></passage><passage><infon key="name_0">surname:Gamma;given-names:E.</infon><infon key="name_1">surname:Helm;given-names:R.</infon><infon key="name_2">surname:Johnson;given-names:R.</infon><infon key="name_3">surname:Vlissides;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">Design Patterns: Elements of Reusable Object-oriented Software</infon><infon key="type">ref</infon><infon key="year">1995</infon><offset>60472</offset></passage><passage><infon key="fpage">1161</infon><infon key="lpage">1166</infon><infon key="name_0">surname:Geva;given-names:M.</infon><infon key="name_1">surname:Goldberg;given-names:Y.</infon><infon key="name_2">surname:Berant;given-names:J.</infon><infon key="pub-id_doi">10.18653/v1/D19-1107</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>60473</offset><text>“Are we modeling the task or the annotator? An investigation of annotator bias in natural language understanding datasets,”</text></passage><passage><infon key="name_0">surname:Goto;given-names:S.</infon><infon key="name_1">surname:Ishida;given-names:T.</infon><infon key="name_2">surname:Lin;given-names:D.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 4</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>60601</offset><text>“Understanding crowdsourcing workflow: modeling and optimizing iterative and parallel processes,”</text></passage><passage><infon key="fpage">172</infon><infon key="lpage">179</infon><infon key="name_0">surname:Grady;given-names:C.</infon><infon key="name_1">surname:Lease;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data With Amazon's Mechanical Turk</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>60703</offset><text>“Crowdsourcing document relevance assessment with mechanical Turk,”</text></passage><passage><infon key="name_0">surname:Griffitt;given-names:K.</infon><infon key="name_1">surname:Strassel;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">LREC</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>60775</offset><text>“The query of everything: developing open-domain, natural-language queries for bolt information retrieval,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">7</infon><infon key="name_0">surname:Huang;given-names:G.</infon><infon key="name_1">surname:Wu;given-names:M.-H.</infon><infon key="name_2">surname:Quinn;given-names:A. J.</infon><infon key="pub-id_doi">10.1145/3411763.3443447</infon><infon key="section_type">REF</infon><infon key="source">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>60887</offset><text>“Task design for crowdsourcing complex cognitive skills,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">15</infon><infon key="name_0">surname:Hung;given-names:N. Q. V.</infon><infon key="name_1">surname:Tam;given-names:N. T.</infon><infon key="name_2">surname:Tran;given-names:L. N.</infon><infon key="name_3">surname:Aberer;given-names:K.</infon><infon key="pub-id_doi">10.1007/978-3-642-41154-0_1</infon><infon key="section_type">REF</infon><infon key="source">International Conference on Web Information Systems Engineering</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>60949</offset><text>“An evaluation of aggregation techniques in crowdsourcing,”</text></passage><passage><infon key="fpage">64</infon><infon key="lpage">67</infon><infon key="name_0">surname:Ipeirotis;given-names:P. G.</infon><infon key="name_1">surname:Provost;given-names:F.</infon><infon key="name_2">surname:Wang;given-names:J.</infon><infon key="pub-id_doi">10.1145/1837885.1837906</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM SIGKDD Workshop on Human Computation</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>61013</offset><text>“Quality management on amazon mechanical Turk,”</text></passage><passage><infon key="fpage">132</infon><infon key="lpage">154</infon><infon key="name_0">surname:Jones;given-names:G. J.</infon><infon key="pub-id_doi">10.1007/978-3-642-36415-0_9</infon><infon key="section_type">REF</infon><infon key="source">Information Retrieval Meets Information Visualization</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>61065</offset><text>“An introduction to crowdsourcing for language and multimedia technology research,”</text></passage><passage><infon key="fpage">83</infon><infon key="lpage">91</infon><infon key="name_0">surname:Jung;given-names:H. J.</infon><infon key="name_1">surname:Lease;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 3rd AAAI Conference on Human Computation (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>61153</offset><text>“Modeling temporal crowd work quality with limited supervision,”</text></passage><passage><infon key="name_0">surname:Kalra;given-names:K.</infon><infon key="name_1">surname:Patwardhan;given-names:M.</infon><infon key="name_2">surname:Karande;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Human Computation and Crowdsourcing (HCOMP): Works-in-Progress Track</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>61222</offset><text>“Shifts in rating bias due to scale saturation,”</text></passage><passage><infon key="fpage">205</infon><infon key="lpage">214</infon><infon key="name_0">surname:Kazai;given-names:G.</infon><infon key="name_1">surname:Kamps;given-names:J.</infon><infon key="name_2">surname:Koolen;given-names:M.</infon><infon key="name_3">surname:Milic-Frayling;given-names:N.</infon><infon key="pub-id_doi">10.1145/2009916.2009947</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>61275</offset><text>“Crowdsourcing for journal search evaluation: impact of hit design on comparative system ranking,”</text></passage><passage><infon key="name_0">surname:Kelling;given-names:S.</infon><infon key="name_1">surname:Gerbracht;given-names:J.</infon><infon key="name_2">surname:Fink;given-names:D.</infon><infon key="name_3">surname:Lagoze;given-names:C.</infon><infon key="name_4">surname:Wong;given-names:W.-K.</infon><infon key="name_5">surname:Yu;given-names:J.</infon><infon key="pub-id_doi">10.1609/aimag.v34i1.2431</infon><infon key="section_type">REF</infon><infon key="source">AI Mag</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>61378</offset><text>A human/computer learning network to improve biodiversity conservation and research</text></passage><passage><infon key="fpage">591</infon><infon key="lpage">598</infon><infon key="name_0">surname:Kinney;given-names:K. A.</infon><infon key="name_1">surname:Huffman;given-names:S. B.</infon><infon key="name_2">surname:Zhai;given-names:J.</infon><infon key="pub-id_doi">10.1145/1458082.1458160</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 17th ACM Conference on Information and Knowledge Management</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>61462</offset><text>“How evaluator domain expertise affects search result relevance judgments,”</text></passage><passage><infon key="fpage">1033</infon><infon key="lpage">1036</infon><infon key="name_0">surname:Kittur;given-names:A.</infon><infon key="name_1">surname:Khamkar;given-names:S.</infon><infon key="name_2">surname:André;given-names:P.</infon><infon key="name_3">surname:Kraut;given-names:R.</infon><infon key="pub-id_doi">10.1145/2145204.2145357</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>61542</offset><text>“Crowdweaver: visually managing complex crowd work,”</text></passage><passage><infon key="fpage">43</infon><infon key="lpage">52</infon><infon key="name_0">surname:Kittur;given-names:A.</infon><infon key="name_1">surname:Smus;given-names:B.</infon><infon key="name_2">surname:Khamkar;given-names:S.</infon><infon key="name_3">surname:Kraut;given-names:R. E.</infon><infon key="pub-id_doi">10.1145/2047196.2047202</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2011</infon><offset>61599</offset><text>“Crowdforge: crowdsourcing complex work,”</text></passage><passage><infon key="fpage">177</infon><infon key="lpage">243</infon><infon key="name_0">surname:Kovashka;given-names:A.</infon><infon key="name_1">surname:Russakovsky;given-names:O.</infon><infon key="name_2">surname:Fei-Fei;given-names:L.</infon><infon key="name_3">surname:Grauman;given-names:K.</infon><infon key="pub-id_doi">10.1561/0600000071</infon><infon key="section_type">REF</infon><infon key="source">Found. Trends Comput. Graph. Vis</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2016</infon><offset>61645</offset><text>Crowdsourcing in computer vision</text></passage><passage><infon key="fpage">2522</infon><infon key="lpage">2535</infon><infon key="name_0">surname:Krivosheev;given-names:E.</infon><infon key="name_1">surname:Bykau;given-names:S.</infon><infon key="name_2">surname:Casati;given-names:F.</infon><infon key="name_3">surname:Prabhakar;given-names:S.</infon><infon key="pub-id_doi">10.14778/3407790.3407842</infon><infon key="section_type">REF</infon><infon key="source">Proc. VLDB Endow</infon><infon key="type">ref</infon><infon key="volume">13</infon><infon key="year">2020</infon><offset>61678</offset><text>Detecting and preventing confused labels in crowdsourced data</text></passage><passage><infon key="fpage">3075</infon><infon key="lpage">3084</infon><infon key="name_0">surname:Kulesza;given-names:T.</infon><infon key="name_1">surname:Amershi;given-names:S.</infon><infon key="name_2">surname:Caruana;given-names:R.</infon><infon key="name_3">surname:Fisher;given-names:D.</infon><infon key="name_4">surname:Charles;given-names:D.</infon><infon key="pub-id_doi">10.1145/2556288.2557238</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>61740</offset><text>“Structured labeling for facilitating concept evolution in machine learning,”</text></passage><passage><infon key="fpage">28</infon><infon key="lpage">35</infon><infon key="name_0">surname:Kulkarni;given-names:A.</infon><infon key="name_1">surname:Gutheim;given-names:P.</infon><infon key="name_2">surname:Narula;given-names:P.</infon><infon key="name_3">surname:Rolnitzky;given-names:D.</infon><infon key="name_4">surname:Parikh;given-names:T.</infon><infon key="name_5">surname:Hartmann;given-names:B.</infon><infon key="pub-id_doi">10.1109/MIC.2012.72</infon><infon key="section_type">REF</infon><infon key="source">IEEE Intern. Comput</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2012b</infon><offset>61822</offset><text>Mobileworks: designing for quality in a managed crowdsourcing architecture</text></passage><passage><infon key="fpage">1003</infon><infon key="lpage">1012</infon><infon key="name_0">surname:Kulkarni;given-names:A.</infon><infon key="name_1">surname:Can;given-names:M.</infon><infon key="name_2">surname:Hartmann;given-names:B.</infon><infon key="pub-id_doi">10.1145/2145204.2145354</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work</infon><infon key="type">ref</infon><infon key="year">2012a</infon><offset>61897</offset><text>“Collaboratively crowdsourcing workflows with Turkomatic,”</text></passage><passage><infon key="fpage">143</infon><infon key="lpage">189</infon><infon key="name_0">surname:Kutlu;given-names:M.</infon><infon key="name_1">surname:McDonnell;given-names:T.</infon><infon key="name_2">surname:Elsayed;given-names:T.</infon><infon key="name_3">surname:Lease;given-names:M.</infon><infon key="pub-id_doi">10.1613/jair.1.12012</infon><infon key="section_type">REF</infon><infon key="source">J. Artif. Intell. Res</infon><infon key="type">ref</infon><infon key="volume">69</infon><infon key="year">2020</infon><offset>61960</offset><text>Annotator rationales for labeling tasks in crowdsourcing</text></passage><passage><infon key="fpage">1179</infon><infon key="lpage">1189</infon><infon key="name_0">surname:Lintott;given-names:C. J.</infon><infon key="name_1">surname:Schawinski;given-names:K.</infon><infon key="name_2">surname:Slosar;given-names:A.</infon><infon key="name_3">surname:Land;given-names:K.</infon><infon key="name_4">surname:Bamford;given-names:S.</infon><infon key="name_5">surname:Thomas;given-names:D.</infon><infon key="pub-id_doi">10.1111/j.1365-2966.2008.13689.x</infon><infon key="section_type">REF</infon><infon key="source">Monthly Not. R. Astron. Soc</infon><infon key="type">ref</infon><infon key="volume">389</infon><infon key="year">2008</infon><offset>62017</offset><text>Galaxy zoo: morphologies derived from visual inspection of galaxies from the Sloan digital sky survey</text></passage><passage><infon key="fpage">68</infon><infon key="lpage">76</infon><infon key="name_0">surname:Little;given-names:G.</infon><infon key="name_1">surname:Chilton;given-names:L. B.</infon><infon key="name_2">surname:Goldman;given-names:M.</infon><infon key="name_3">surname:Miller;given-names:R. C.</infon><infon key="pub-id_doi">10.1145/1837885.1837907</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the ACM SIGKDD Workshop on Human Computation</infon><infon key="type">ref</infon><infon key="year">2010a</infon><offset>62119</offset><text>“Exploring iterative and parallel human computation processes,”</text></passage><passage><infon key="fpage">57</infon><infon key="lpage">66</infon><infon key="name_0">surname:Little;given-names:G.</infon><infon key="name_1">surname:Chilton;given-names:L. B.</infon><infon key="name_2">surname:Goldman;given-names:M.</infon><infon key="name_3">surname:Miller;given-names:R. C.</infon><infon key="pub-id_doi">10.1145/1866029.1866040</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2010b</infon><offset>62187</offset><text>“Turkit: human computation algorithms on mechanical Turk,”</text></passage><passage><infon key="fpage">2432</infon><infon key="lpage">2442</infon><infon key="name_0">surname:Liu;given-names:A.</infon><infon key="name_1">surname:Guerra;given-names:S.</infon><infon key="name_2">surname:Fung;given-names:I.</infon><infon key="name_3">surname:Matute;given-names:G.</infon><infon key="name_4">surname:Kamar;given-names:E.</infon><infon key="name_5">surname:Lasecki;given-names:W.</infon><infon key="pub-id_doi">10.1145/3366423.3380306</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of The Web Conference 2020, WWW '20</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>62250</offset><text>“Towards hybrid human-AI workflows for unknown detection,”</text></passage><passage><infon key="name_0">surname:Manam;given-names:V. C.</infon><infon key="name_1">surname:Quinn;given-names:A. J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 6th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>62313</offset><text>“Wingit: efficient refinement of unclear task instructions,”</text></passage><passage><infon key="fpage">1121</infon><infon key="lpage">1130</infon><infon key="name_0">surname:Manam;given-names:V. K. C.</infon><infon key="name_1">surname:Jampani;given-names:D.</infon><infon key="name_2">surname:Zaim;given-names:M.</infon><infon key="name_3">surname:Wu;given-names:M.-H.</infon><infon key="name_4">surname:J. Quinn;given-names:A.</infon><infon key="pub-id_doi">10.1145/3308560.3317081</infon><infon key="section_type">REF</infon><infon key="source">Companion Proceedings of The 2019 World Wide Web Conference</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>62378</offset><text>“Taskmate: a mechanism to improve the quality of instructions in crowdsourcing,”</text></passage><passage><infon key="fpage">234</infon><infon key="lpage">243</infon><infon key="name_0">surname:Marshall;given-names:C. C.</infon><infon key="name_1">surname:Shipman;given-names:F. M.</infon><infon key="pub-id_doi">10.1145/2464464.2464485</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 5th Annual ACM Web Science Conference</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>62463</offset><text>“Experiences surveying the crowd: reflections on methods, participation, and reliability,”</text></passage><passage><infon key="name_0">surname:McDonnell;given-names:T.</infon><infon key="name_1">surname:Lease;given-names:M.</infon><infon key="name_2">surname:Elsayad;given-names:T.</infon><infon key="name_3">surname:Kutlu;given-names:M.</infon><infon key="pub-id_doi">10.24963/ijcai.2017/692</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>62558</offset><text>“Why is that relevant? Collecting annotator rationales for relevance judgments,”</text></passage><passage><infon key="fpage">149</infon><infon key="lpage">158</infon><infon key="name_0">surname:Nguyen;given-names:A. T.</infon><infon key="name_1">surname:Halpern;given-names:M.</infon><infon key="name_2">surname:Wallace;given-names:B. C.</infon><infon key="name_3">surname:Lease;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>62643</offset><text>“Probabilistic modeling for crowdsourcing partially-subjective ratings,”</text></passage><passage><infon key="fpage">165</infon><infon key="lpage">175</infon><infon key="name_0">surname:Nouri;given-names:Z.</infon><infon key="name_1">surname:Gadiraju;given-names:U.</infon><infon key="name_2">surname:Engels;given-names:G.</infon><infon key="name_3">surname:Wachsmuth;given-names:H.</infon><infon key="pub-id_doi">10.1145/3465336.3475109</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 32nd ACM Conference on Hypertext and Social Media</infon><infon key="type">ref</infon><infon key="year">2021a</infon><offset>62720</offset><text>“What is unclear? Computational assessment of task clarity in crowdsourcing,”</text></passage><passage><infon key="name_0">surname:Nouri;given-names:Z.</infon><infon key="name_1">surname:Prakash;given-names:N.</infon><infon key="name_2">surname:Gadiraju;given-names:U.</infon><infon key="name_3">surname:Wachsmuth;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 9th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2021b</infon><offset>62802</offset><text>“iclarify-a tool to help requesters iteratively improve task descriptions in crowdsourcing,”</text></passage><passage><infon key="name_0">surname:Papoutsaki;given-names:A.</infon><infon key="name_1">surname:Guo;given-names:H.</infon><infon key="name_2">surname:Metaxa-Kakavouli;given-names:D.</infon><infon key="name_3">surname:Gramazio;given-names:C.</infon><infon key="name_4">surname:Rasley;given-names:J.</infon><infon key="name_5">surname:Xie;given-names:W.</infon><infon key="section_type">REF</infon><infon key="source">Third AAAI Conference on Human Computation and Crowdsourcing</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>62899</offset><text>“Crowdsourcing from scratch: a pragmatic experiment in data collection by novice requesters,”</text></passage><passage><infon key="fpage">509</infon><infon key="lpage">512</infon><infon key="name_0">surname:Pickard;given-names:G.</infon><infon key="name_1">surname:Pan;given-names:W.</infon><infon key="name_2">surname:Rahwan;given-names:I.</infon><infon key="name_3">surname:Cebrian;given-names:M.</infon><infon key="name_4">surname:Crane;given-names:R.</infon><infon key="name_5">surname:Madan;given-names:A.</infon><infon key="pub-id_doi">10.1126/science.1205869</infon><infon key="pub-id_pmid">22034432</infon><infon key="section_type">REF</infon><infon key="source">Science</infon><infon key="type">ref</infon><infon key="volume">334</infon><infon key="year">2011</infon><offset>62997</offset><text>Time-critical social mobilization</text></passage><passage><infon key="fpage">75</infon><infon key="lpage">85</infon><infon key="name_0">surname:Retelny;given-names:D.</infon><infon key="name_1">surname:Robaszkiewicz;given-names:S.</infon><infon key="name_2">surname:To;given-names:A.</infon><infon key="name_3">surname:Lasecki;given-names:W. S.</infon><infon key="name_4">surname:Patel;given-names:J.</infon><infon key="name_5">surname:Rahmati;given-names:N.</infon><infon key="pub-id_doi">10.1145/2642918.2647409</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology</infon><infon key="type">ref</infon><infon key="year">2014</infon><offset>63031</offset><text>“Expert crowdsourcing with flash teams,”</text></passage><passage><infon key="name_0">surname:Rosser;given-names:H.</infon><infon key="name_1">surname:Wiggins;given-names:A.</infon><infon key="pub-id_doi">10.24251/HICSS.2019.637</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 52nd Hawaii International Conference on System Sciences</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>63076</offset><text>“Crowds and camera traps: genres in online citizen science projects,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">19</infon><infon key="name_0">surname:Schaekermann;given-names:M.</infon><infon key="name_1">surname:Goh;given-names:J.</infon><infon key="name_2">surname:Larson;given-names:K.</infon><infon key="name_3">surname:Law;given-names:E.</infon><infon key="pub-id_doi">10.1145/3274423</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM Hum. Comput. Interact</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2018</infon><offset>63150</offset><text>Resolvable vs. irresolvable disagreement: a study on worker deliberation in crowd work</text></passage><passage><infon key="fpage">623</infon><infon key="lpage">632</infon><infon key="name_0">surname:Scholer;given-names:F.</infon><infon key="name_1">surname:Kelly;given-names:D.</infon><infon key="name_2">surname:Wu;given-names:W.-C.</infon><infon key="name_3">surname:Lee;given-names:H. S.</infon><infon key="name_4">surname:Webber;given-names:W.</infon><infon key="pub-id_doi">10.1145/2484028.2484090</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>63237</offset><text>“The effect of threshold priming and need for cognition on relevance calibration and assessment,”</text></passage><passage><infon key="fpage">826</infon><infon key="lpage">838</infon><infon key="name_0">surname:Sen;given-names:S.</infon><infon key="name_1">surname:Giesel;given-names:M. E.</infon><infon key="name_2">surname:Gold;given-names:R.</infon><infon key="name_3">surname:Hillmann;given-names:B.</infon><infon key="name_4">surname:Lesicko;given-names:M.</infon><infon key="name_5">surname:Naden;given-names:S.</infon><infon key="pub-id_doi">10.1145/2675133.2675285</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>63339</offset><text>“Turkers, scholars, arafat and peace: cultural communities and algorithmic gold standards,”</text></passage><passage><infon key="fpage">156</infon><infon key="lpage">164</infon><infon key="name_0">surname:Sheshadri;given-names:A.</infon><infon key="name_1">surname:Lease;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 1st AAAI Conference on Human Computation (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>63435</offset><text>“SQUARE: a benchmark for research on computing crowd consensus,”</text></passage><passage><infon key="fpage">254</infon><infon key="lpage">263</infon><infon key="name_0">surname:Snow;given-names:R.</infon><infon key="name_1">surname:O'Connor;given-names:B.</infon><infon key="name_2">surname:Jurafsky;given-names:D.</infon><infon key="name_3">surname:Ng;given-names:A. Y.</infon><infon key="pub-id_doi">10.3115/1613715.1613751</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the Conference on Empirical Methods in Natural Language Processing</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>63504</offset><text>“Cheap and fast–but is it good?: evaluating non-expert annotations for natural language tasks,”</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">8</infon><infon key="name_0">surname:Sorokin;given-names:A.</infon><infon key="name_1">surname:Forsyth;given-names:D.</infon><infon key="pub-id_doi">10.1109/CVPRW.2008.4562953</infon><infon key="section_type">REF</infon><infon key="source">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2008, CVPRW'08</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>63606</offset><text>“Utility data annotation with amazon mechanical Turk,”</text></passage><passage><infon key="fpage">226</infon><infon key="lpage">234</infon><infon key="name_0">surname:Tian;given-names:Y.</infon><infon key="name_1">surname:Zhu;given-names:J.</infon><infon key="pub-id_doi">10.1145/2339530.2339571</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</infon><infon key="type">ref</infon><infon key="year">2012</infon><offset>63665</offset><text>“Learning from crowds in the presence of schools of thought,”</text></passage><passage><infon key="name_0">surname:Vakharia;given-names:D.</infon><infon key="name_1">surname:Lease;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the iConference</infon><infon key="type">ref</infon><infon key="year">2015</infon><offset>63731</offset><text>“Beyond mechanical Turk: an analysis of paid crowd work platforms,”</text></passage><passage><infon key="fpage">180</infon><infon key="lpage">187</infon><infon key="name_0">surname:Vandenhof;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>63803</offset><text>“A hybrid approach to identifying unknown unknowns of predictive models,”</text></passage><passage><infon key="fpage">e0243300</infon><infon key="name_0">surname:Vidgen;given-names:B.</infon><infon key="name_1">surname:Derczynski;given-names:L.</infon><infon key="pub-id_doi">10.1371/journal.pone.0243300</infon><infon key="pub-id_pmid">33370298</infon><infon key="section_type">REF</infon><infon key="source">PLoS ONE</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2020</infon><offset>63881</offset><text>Directions in abusive language training data, a systematic review: garbage in, garbage out</text></passage><passage><infon key="name_0">surname:Vogels;given-names:W.</infon><infon key="section_type">REF</infon><infon key="source">Help Find Jim Gray</infon><infon key="type">ref</infon><infon key="year">2007</infon><offset>63972</offset></passage><passage><infon key="fpage">45</infon><infon key="lpage">53</infon><infon key="name_0">surname:Wang;given-names:F.-Y.</infon><infon key="name_1">surname:Zeng;given-names:D.</infon><infon key="name_2">surname:Hendler;given-names:J. A.</infon><infon key="name_3">surname:Zhang;given-names:Q.</infon><infon key="name_4">surname:Feng;given-names:Z.</infon><infon key="name_5">surname:Gao;given-names:Y.</infon><infon key="pub-id_doi">10.1109/MC.2010.216</infon><infon key="pub-id_pmid">21379365</infon><infon key="section_type">REF</infon><infon key="source">Computer</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2010</infon><offset>63973</offset><text>A study of the human flesh search engine: crowd-powered expansion of online knowledge</text></passage><passage><infon key="name_0">surname:Wu;given-names:M.-H.</infon><infon key="name_1">surname:Quinn;given-names:A. J.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 5th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>64059</offset><text>“Confusing the crowd: task instruction quality on amazon mechanical Turk,”</text></passage><passage><infon key="fpage">541</infon><infon key="lpage">552</infon><infon key="name_0">surname:Zheng;given-names:Y.</infon><infon key="name_1">surname:Li;given-names:G.</infon><infon key="name_2">surname:Li;given-names:Y.</infon><infon key="name_3">surname:Shan;given-names:C.</infon><infon key="name_4">surname:Cheng;given-names:R.</infon><infon key="pub-id_doi">10.14778/3055540.3055547</infon><infon key="section_type">REF</infon><infon key="source">Proc. VLDB Endow</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2017</infon><offset>64138</offset><text>Truth inference in crowdsourcing: is the problem solved?</text></passage></document></collection>
