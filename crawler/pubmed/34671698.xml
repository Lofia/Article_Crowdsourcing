<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20211102</date><key>pmc.key</key><document><id>8528079</id><infon key="license">CC BY</infon><passage><infon key="article-id_arxiv">arXiv:2110.07531v1</infon><infon key="article-id_pmc">8528079</infon><infon key="article-id_pmid">34671698</infon><infon key="article-id_publisher-id">2110.07531</infon><infon key="elocation-id">arXiv:2110.07531v1</infon><infon key="license">This work is licensed under a Creative Commons Attribution 4.0 International License, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.</infon><infon key="name_0">surname:Wayment-Steele;given-names:Hannah K.</infon><infon key="name_1">surname:Kladwang;given-names:Wipapat</infon><infon key="name_10">surname:Gao;given-names:Jiayang</infon><infon key="name_11">surname:Onodera;given-names:Kazuki</infon><infon key="name_12">surname:Fujikawa;given-names:Kazuki</infon><infon key="name_13">surname:Mao;given-names:Hanfei</infon><infon key="name_14">surname:Vandewiele;given-names:Gilles</infon><infon key="name_15">surname:Tinti;given-names:Michele</infon><infon key="name_16">surname:Steenwinckel;given-names:Bram</infon><infon key="name_17">surname:Ito;given-names:Takuya</infon><infon key="name_18">surname:Noumi;given-names:Taiga</infon><infon key="name_19">surname:He;given-names:Shujun</infon><infon key="name_2">surname:Watkins;given-names:Andrew M.</infon><infon key="name_20">surname:Ishi;given-names:Keiichiro</infon><infon key="name_21">surname:Lee;given-names:Youhan</infon><infon key="name_22">surname:Öztürk;given-names:Fatih</infon><infon key="name_23">surname:Chiu;given-names:Anthony</infon><infon key="name_24">surname:Öztürk;given-names:Emin</infon><infon key="name_25">surname:Amer;given-names:Karim</infon><infon key="name_26">surname:Fares;given-names:Mohamed</infon><infon key="name_27">surname:Participants;given-names:Eterna</infon><infon key="name_28">surname:Das;given-names:Rhiju</infon><infon key="name_3">surname:Kim;given-names:Do Soon</infon><infon key="name_4">surname:Tunguz;given-names:Bojan</infon><infon key="name_5">surname:Reade;given-names:Walter</infon><infon key="name_6">surname:Demkin;given-names:Maggie</infon><infon key="name_7">surname:Romano;given-names:Jonathan</infon><infon key="name_8">surname:Wellington-Oguri;given-names:Roger</infon><infon key="name_9">surname:Nicol;given-names:John J.</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="year">2021</infon><offset>0</offset><text>Predictive models of RNA degradation through dual crowdsourcing</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>64</offset><text>Messenger RNA-based medicines hold immense potential, as evidenced by their rapid deployment as COVID-19 vaccines. However, worldwide distribution of mRNA molecules has been limited by their thermostability, which is fundamentally limited by the intrinsic instability of RNA molecules to a chemical degradation reaction called in-line hydrolysis. Predicting the degradation of an RNA molecule is a key task in designing more stable RNA-based therapeutics. Here, we describe a crowdsourced machine learning competition (“Stanford OpenVaccine”) on Kaggle, involving single-nucleotide resolution measurements on 6043 102–130-nucleotide diverse RNA constructs that were themselves solicited through crowdsourcing on the RNA design platform Eterna. The entire experiment was completed in less than 6 months. Winning models demonstrated test set errors that were better by 50% than the previous state-of-the-art DegScore model. Furthermore, these models generalized to blindly predicting orthogonal degradation data on much longer mRNA molecules (504–1588 nucleotides) with improved accuracy over DegScore and other models. Top teams integrated natural language processing architectures and data augmentation techniques with predictions from previous dynamic programming models for RNA secondary structure. These results indicate that such models are capable of representing in-line hydrolysis with excellent accuracy, supporting their use for designing stabilized messenger RNAs. The integration of two crowdsourcing platforms, one for data set creation and another for machine learning, may be fruitful for other urgent problems that demand scientific discovery on rapid timescales.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1751</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1764</offset><text>The chemical instability of RNA sets a fundamental limit on the stability of RNA-based therapeutics such as mRNA-based vaccines. Better methods to develop thermostable RNA therapeutics would allow for increasing their potency, increasing the equitability of their distribution, and reducing their cost. A key prediction task underpinning the design of thermostable RNA therapeutics is the prediction of RNA hydrolysis from sequence and structure. Previous models for RNA degradation have assumed that the probability of any RNA nucleotide linkage being cleaved is proportional to the probability of the 5’ nucleotide being unpaired. Computational studies with this model suggested that at least a two-fold increase in stability could be achieved through sequence design, while maintaining a wide diversity of sequences and features related to translatability, immunogenicity, and global structure. However, it is unlikely that degradation depends only on the probability of a nucleotide being unpaired: local sequence-and structure-specific contexts may vary widely.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2833</offset><text>We wished to understand the maximum predictive power achievable for RNA degradation on a short timescale for model development. To do this, we combined two crowdsourcing platforms: Eterna, an RNA design platform, and Kaggle, a platform for machine learning competitions. Eterna has previously been able to solve near-intractable problems in RNA design, and the diversity of resulting structures on its platform have more recently contributed to advancing RNA secondary structure prediction. We reasoned that crowdsourcing the problem of obtaining data on a wide diversity of sequences and structures would rapidly lead to a diverse dataset, and that crowdsourcing the second problem of obtaining a machine learning architecture would result in a model capable of expressing the resulting complexity of sequence- and structure-dependent degradation patterns. We hypothesized this “dual crowdsourcing” would lead to stringent and independent tests of the models developed, minimizing interplay between the individuals designing the constructs to test (Eterna participants) and the individuals building the models (Kaggle participants) and leading to better generalizability on independent data sets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4035</offset><text>The resulting models were subjected to two blind prediction challenges: the first was in the context of the Kaggle competition, where the data that Kaggle participants were scored on was not produced until after the challenge was set to the participants. The resulting models also demonstrated increased predictive power in a completely independent challenge of predicting the overall degradation of full-length mRNAs encoding the protein nanoluciferase, which were experimentally tested in a different wet lab pipeline. The models therefore appear immediately useful for guiding design of low degradation mRNA molecules. Analysis of model performance suggests that the task of predicting RNA degradation patterns is limited by both the amount of data available as well as the accuracy of the structure prediction tools used to create input features. Further developments in experimental data and secondary structure prediction, when combined with network architectures such as those developed here, will further advance RNA degradation prediction and therapeutic design.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>5107</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>5115</offset><text>Dual-crowdsourced competition design and assessment.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>5168</offset><text>The aim of the OpenVaccine Kaggle competition (Fig. 1A) was to develop computational models for predicting RNA degradation patterns. We asked participants on the Eterna platform to submit RNA designs using a web-browser design window (Fig. 1B), which resulted in a diversity of sequences and structures (Fig. 1C). 150 participants in total (Table S1) submitted sequences. A secondary motivation was an opportunity for participants to receive feedback on RNA fragments they may wish to use in mRNA design challenges described in ref.. 3029 RNA designs of length 107 nt were collected in the first “Roll-Your-Own-Structure” round I (RYOS-I), which was opened March 26, 2020, and closed upon reaching 3029 constructs on June 19, 2020 (Fig. 1D).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>5914</offset><text>We then obtained nucleotide-level degradation profiles for the first 68 nucleotides of these RNAs using In-line-seq, a novel method for characterizing in-line RNA degradation in high-throughput for the purposes of designing stabilized RNA therapeutics. Degradation profiles were collected in four different accelerated degradation conditions, and the structures of the constructs were also characterized via selective 2’ hydroxyl acylation with primer extension (SHAPE; termed “Reactivity” below), a technique to characterize RNA secondary structure. The Kaggle competition was designed to create models that would have predictive power for all five of these data types, given RNA sequence and secondary structure as input (Fig. 1E). In total, each independent construct of length 68 required predicting 5×68 values for the 5 data types. In addition to these experimental data, Kaggle participants were also provided with features related to RNA secondary structure computed from available biophysical models to use if they wished. These features included 107×107 base pairing probability matrices from EternaFold6, a recently developed package with state-of-the-art performance on RNA structural ensembles; 107-character strings representing the minimum free energy (MFE) RNA secondary structure from the more widely used Vienna RNA package; and a six-character featurization of the MFE structure developed for the bpRNA database.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>7354</offset><text>We developed training and public test datasets from the RYOS-I dataset (Fig. 2). The 3029 constructs were filtered for those with mean signal-to-noise values greater than 1, resulting in 2218 constructs (Fig. 2, dark blue track). These constructs were segmented into splits of 1179 in the public training dataset, 400 constructs in the public test set, and 639 in the private test set. The sequences that did not pass the signal-to-noise filter were also provided to Kaggle participants with the according description. The RYOS-I data contained some “clusters” of sequences where Eterna players included many small variations on a single sequence (clusters visible in Fig. 1C). To mitigate the possibility of sequence motifs in these clusters biasing evaluation, we segmented the RYOS-I data into a public training, public test, and private test sets by clustering the sequences and including only sequences that were singly, doubly, or triply-clustered in the private test set. This strategy was described to Kaggle participants.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>8389</offset><text>To ensure that the majority of the data used for the private test set was fully blind, we initiated a second “Roll-your-own-structure” challenge that was launched for Eterna design collection on August 18, 2020. Design collection was closed on September 7th, three days before the launch of the Kaggle challenge on September 10th. The RYOS-II wet-lab experiments were conducted concurrently with the Kaggle challenge, enabling a completely blind test for the models developed on Kaggle. The Kaggle competition was closed on October 6th. The RYOS-II was similarly clustered and filtered to ensure that the test set used for scoring consisted primarily of singly- and double-clustered constructs. Models were scored on the mean column RMSE (MCRMSE) across three data types. While the submission format required that all 5 be predicted, only three data types were collected in the testing wet-lab experiments (SHAPE; 10 mM Mg2+, pH 10, 1 day, 24 °C; and no Mg2+, pH 10, 1 day, 50 °C) and only these data types were scored: Reactivity, deg_Mg_pH10, and deg_Mg_50C. Given that useful models for degradation should be agnostic to RNA length, we designed the constructs in RYOS-II to be 34 nucleotides longer (102 vs. 68 nts) than the constructs in RYOS-I to discourage modelling methods that would overfit to constructs of length 68.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>9723</offset><text>Performance of Kaggle teams and common attributes of top-performing models.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>9799</offset><text>During the three week competition period, 1,636 teams submitted 35,806 solutions. Overall performance of teams vs. baseline models for RNA degradation are depicted in Figure 3A. Kaggle entries significantly outperformed the “DegScore” linear regression model for RNA degradation reported in ref., by more than 50% in MCRMSE (Fig. 3A). Kaggle participants developed feature encodings beyond what was provided. One of the most widely-used community-developed featurizations was a graph-based distance embedding depicted in Fig. 3B. Many architectures used a combined autoencoder/GNN/GRU model, including the architectures of the top two solutions (Fig. 3C and Fig. 3D, respectively). Many teams cited pseudolabeling and generating additional mock data as being integral to their solutions. The machine learning practice of pseudolabeling involves using predictions from one model as “mock ground truth” labels for another model. Effective pseudolabeling usually requires a high level of accuracy of the primary model and is most frequently used with classification problems. To generate additional mock data, participants generated random RNAs as well as structure featurizations using 5 different secondary structure prediction algorithms using the package Arnie (https://github.com/DasLab/arnie) and iteratively scored based on these predictions for their model as well (see Supplement for more detailed descriptions of solutions from Kaggle teams).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>11257</offset><text>Ensembling models.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>11276</offset><text>Motivated by applications to design of stabilized mRNA, we explored whether increased accuracy in modeling might be achieved by combining models. A common feature of Kaggle competitions is that winning solutions are dissimilar enough that ensembled models frequently improve predictive ability. We used a genetic algorithm to ensemble maximally 10 of the top 100 models. The score on the public dataset was used to optimize, with the final ensembled model evaluated on the private dataset. With this method, ensembling achieved a Public score of 0.2237 (compared to the best Public LB score of 0.2276) and a Private score of 0.3397 (compared to the best Private test set score of 0.3420). In comparison, averaging the outputs of the top two models gave a result of 0.2244 Public, 0.33788 Private. Blending the top two solutions with the 3rd solution did not improve the result. An estimated bound of ensembling can be found by optimizing directly to the Private ensemble score. With this method, it was possible to achieve a Private ensemble score of 0.3382 (again, vs LB 0.3420). The improvement of 0.0038 over the leaderboard for this last approach is about the distance between the 1st place and 10th place teams, and the “correct” way gives an improvement that is the distance between the 1st and 5th place teams. All these experiments suggest that most of the signal has been captured by the top two models, and that the use of further ensembling provides, at best, modest improvements. The seemingly puzzling result that the simple ensemble of the top two models outperforms the genetic algorithm blend of the top 10 (on the private test set) can be attributed to increase of the search space. The search space for the 10 different blending weights is substantially larger than for just a single weight, and it is very likely that the algorithm found a local, rather than global, minimum.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>13175</offset><text>Top models are capable of deep representation of RNA experimental motifs.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>13249</offset><text>We analyzed predictions from the first-place model (“Nullrecurrent”) in depth to better understand its performance. Across all nucleotides in the private test set, 41% of nucleotide-level predictions for SHAPE reactivity agreed with experimental measurements within an error that was lower than experimental uncertainty; for comparison, if experimental errors are distributed as normal distributions, a perfect predictor would agree with experimental values over 68% of data points. For Deg_Mg_pH10, 28% of predictions were within error, and for Deg_Mg_50C, 42% of predictions were. The nucleotides with the largest root mean squared error (RMSE) in the Deg_Mg_pH10 data type were any nucleotide type in bulges, and U’s in any unpaired context. Fig. 4A depicts representative constructs with the lowest RMSE for the Deg_Mg_pH10 data type out of the private test data, demonstrating that a diverse set of structures and structure motifs were capable of being predicted correctly. Aggregating the predictions from the Nullrecurrent model over secondary structure motifs (Fig. 4B) demonstrates that the Nullrecurrent predictions by motif captured patterns observed in the experimental signal in human expert analysis -- e.g., asymmetric loops exhibited higher degradation than symmetric loops. Constructs with the highest RMSE demonstrate indicators that the provided structure features were incorrect. Fig. 4C depicts two constructs with the highest RMSE for the SHAPE modification prediction. The SHAPE data for the first construct, “2204Sept042020”, has high reactivity in predicted stem areas, indicating the stems were unfolded in solution. In contrast, construct “Triple UUUU Tetraloops” has experimentally low reactivity in the exterior loop, suggesting that a stem was present. However, we found no correlation between the EternaScore, an indicator of how closely the experimental reactivity signal matches the predicted structure, and RMSE summed per construct for the private test constructs, suggesting that in general, quality of the input structure features was not a limitation in model training.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>15371</offset><text>Kaggle models show improved performance in independent mRNA degradation prediction.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>15455</offset><text>As an independent test for the top two Kaggle models, we ran predictions for a dataset of full-length messenger RNAs (mRNAs) from ref.. These data were not publicly available at the time of the Kaggle competition. The lengths of these mRNAs ranged from 504 to 1588 with a median length of 928, nearly 10-fold times longer than the longest RNA fragments used in the OpenVaccine Kaggle challenge (full dataset, attributes, and calculations in Table S2). The PERSIST-seq method was developed to determine the degradation rate of the coding sequence of an mRNA. To compare the Kaggle predictors, which make predictions per nucleotide, to this single value for degradation, we made predictions for all nucleotides in the full mRNA constructs and summed the predictions from the region that was captured in the PERSIST-seq method by reverse-transcription PCR which, in most cases, included the mRNA’s 5’ untranslated region (UTR) and coding sequence (CDS) (Fig. 5A). Carrying out predictions on the full RNA sequence and then summing over the probed window allows to account for interactions between the untranslated regions and CDS, as can be seen for two example constructs in Fig. 5B -- nucleotides in the 5’ and 3’ UTRs are predicted to pair with the CDS. We made predictions for 188 mRNAs in 4 classes: a short multi-epitope vaccine (MEV), the model protein Nanoluciferase, with one class consisting of varied UTRs and a second consisting of varied CDSs, and enhanced Green Fluorescent Protein (eGFP). We found that the Kaggle second-place “Kazuki2” model exhibited the highest correlation to fit half-lives, followed by the Kaggle 1st-place “Nullrecurrent” model (Fig. 5C), with Spearman correlation coefficients of −0.48 and −0.41, respectively. Both Kaggle models outperformed unpaired probability values from EternaFold (R=−0.31), the same kind of inputs provided to participants in the competition, the DegScore linear regression model (R=−0.36), and an additional benchmark model prepared after the Kaggle competition exploring the use of XGBoost training with the DegScore featurization (R=−0.42). An ensemble of the Nullrecurrent and Kazuki2 models did not outperform the Kazuki2 model (R=−0.45), again suggesting that the models themselves had reached their predictive potential. In comparison, resampling the measured degradation rates from within error and calculating this correlation coefficient, as a measure of the upper limit of experimental noise, resulted in a Spearman correlation of −0.95 (Table 1).</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>18004</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>18015</offset><text>The OpenVaccine competition uniquely leveraged resources from two complementary crowd-sourcing platforms: Kaggle and Eterna. The participants in the Kaggle competition were tasked with predicting stability measurements of individual RNA nucleotides. The urgency of timely development of a stable COVID-19 mRNA vaccine necessitated that the competition be run on a relatively short timeframe of three weeks, as opposed to three months, which is more common with the Kaggle competitions. Kaggle competitions with relatively small datasets can be subject to serious overfitting to the public leaderboard, which often leads to a major “shake up” of the leaderboard when the results on the unseen test set are announced. In this competition the shakeup was minimal - most of the top teams were ranked close to the same position on the private leaderboard as they were on the public leaderboard. As the private leaderboard was determined on data that had not been collected at the time of the competition launch, this result suggests that the models that were developed are robust and generalizable. Furthermore, the models generalized to the task of predicting degradation for full-length mRNA molecules that were ten-fold longer than the constructs used for training. We speculate that the use of a separate, independently collected data set for the private leaderboard tests -- a true blind prediction challenge -- was important for ensuring generalizability. The winning solutions all used neural network architectures that are commonly used with modeling of 1D sequential data: recurrent NNs (LSTMs and GRUs) and 1D CNNs. The effectiveness of pseudolabeling suggests two things: more data will likely benefit any future modeling efforts, and the simple neural networks that were used have enough capacity to benefit from more data. Future directions for model development includes training such models on larger chemical mapping datasets from more diverse experimental sources, and integrating into inference frameworks for RNA structure prediction.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>20068</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20076</offset><text>Initial feature generation.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20104</offset><text>As a starting point for Kaggle teams, we supplied a collection of features for each RNA sequence, including the minimum free energy (MFE) structure according to the ViennaRNA 2 energy model, loop type assignments generated with bpRNA (S=Stem, E=External Loop, I=Internal loop, B=Bulge, H=Hairpin, M=Multiloop, X=Dangle) and the base pair probability matrix according to the EternaFold energy model. These features were generated using Arnie (https://github.com/DasLab/arnie).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20580</offset><text>Experimental data generation.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20610</offset><text>The first experimental dataset used in this work, for the public training and test set, resulted from the “Roll-Your-Own-Structure” Round I lab on Eterna, and had been generated previously in ref..</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20812</offset><text>The second experimental dataset used in this work, for the private test set, was generated for this work specifically. To produce these data, and for precise consistency with the public training and test set, In-line-seq was carried out as described in ref.), Methods section “High-throughput in-line and SHAPE probing on Eterna-designed RNA fragments (In-line-seq).” Chemical mapping protocols were taken from ref..</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21233</offset><text>As a high-level summary of that method, DNA templates were ordered via custom oligonucleotide pool from Custom Array/Genscript, prepended by the T7 RNA polymerase promoter. Templates were amplified via PCR, transcribed to RNA via the TranscriptAid T7 High Yield Transcription Kit (Thermofisher, K0441), and the purified RNA was subjected to degradation conditions: 1) 50 mM Na-CHES buffer (pH 10.0) at room temperature without added MgCl2; 2) 50 mM Na-CHES buffer (pH 10.0) at room temperature with 10 mM MgCl2; 3) phosphate buffered saline (PBS, pH 7.2; Thermo Fisher Scientific-Gibco 20012027) at 50iC without added MgCl2; and 4) PBS (pH 7.2) at 50iC with 10 mM MgCl2. Reactions were quenched for data collection at 0 and 24 hour time points (+MgCl2) or 0 and 7 day time points (−MgCl2). In parallel, purified RNA was subjected to SHAPE structure probing conditions, and one sample was subjected to the SHAPE protocol absent addition of the 1-methyl-7-nitroisatoic anhydride reagent.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22221</offset><text>cDNA was prepared from the six RNA samples (SHAPE probed, control reaction, and four degradation conditions). We pooled 1.5 μL of each cDNA sample together, ligated with an Illumina adapter, washed, and resuspended the ligated product, which was quantified by qPCR, sequenced using an Illumina Miseq, and analyzed using MAPseeker (https://ribokit.github.io/MAPseeker) following the recommended steps for sequence assignment, peak fitting, background subtraction of the no-modification control, correction for signal attenuation, and reactivity profile normalization.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22789</offset><text>The third experimental dataset used in this work, for an independent test of the top models, was a PERSIST-seq data set for in solution stabilities of full-length mRNAs taken from ref..</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>22975</offset><text>Data availability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22994</offset><text>All datasets are downloadable in raw RDAT format from rmdb.stanford.edu at the following accession numbers: SHAPE_RYOS_0620, RYOS1_NMD_0000, RYOS1_PH10_0000, RYOS1_MGPH_0000, RYOS1_50C_0000, RYOS1_MG50_0000, RYOS2_1M7_0000, RYOS2_MGPH_0000, RYOS2_MG50_0000. Kaggle-formatted train and test sets are downloadable from https://www.kaggle.com/c/stanford-covid-vaccine.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>23360</offset><text>Model availability.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23380</offset><text>Code to run the Nullrecurrent model and the DegScore-XGBoost model is available at www.github.com/eternagame/KaggleOpenVaccine. Code to use and reproduce the linear regression DegScore model is available at www.github.com/eternagame/DegScore.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>23623</offset><text>Supplementary Material</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>23646</offset><text>Conflict of Interest. DSK, WK, and RD hold equity and are employees of a new venture seeking to stabilize mRNA molecules. WR and MD are employees of Kaggle.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>23803</offset><text>References</text></passage><passage><infon key="comment">Edn. 2016/12/18</infon><infon key="fpage">1</infon><infon key="lpage">11</infon><infon key="name_0">surname:Kramps;given-names:T.</infon><infon key="name_1">surname:Elbers;given-names:K.</infon><infon key="pub-id_pmid">27987140</infon><infon key="section_type">REF</infon><infon key="source">Methods Mol Biol</infon><infon key="type">ref</infon><infon key="volume">1499</infon><infon key="year">2017</infon><offset>23814</offset></passage><passage><infon key="fpage">100766</infon><infon key="name_0">surname:Verbeke;given-names:R.</infon><infon key="name_1">surname:Lentacker;given-names:I.</infon><infon key="name_2">surname:De Smedt;given-names:S.C.</infon><infon key="name_3">surname:Dewitte;given-names:H.</infon><infon key="section_type">REF</infon><infon key="source">Nano Today</infon><infon key="type">ref</infon><infon key="volume">28</infon><infon key="year">2019</infon><offset>23815</offset><text>Three decades of messenger RNA vaccine development</text></passage><passage><infon key="comment">e1216</infon><infon key="fpage">1271</infon><infon key="lpage">1283</infon><infon key="name_0">surname:Zhang;given-names:N.N.</infon><infon key="pub-id_pmid">32795413</infon><infon key="section_type">REF</infon><infon key="source">Cell</infon><infon key="type">ref</infon><infon key="volume">182</infon><infon key="year">2020</infon><offset>23866</offset><text>A thermostable mRNA vaccine against COVID-19</text></passage><passage><infon key="name_0">surname:Wu;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>23911</offset><text>mRNA-1273 vaccine induces neutralizing antibodies against spike mutants from global SARS-CoV-2 variants</text></passage><passage><infon key="fpage">10604</infon><infon key="lpage">10617</infon><infon key="name_0">surname:Wayment-Steele;given-names:H.K.</infon><infon key="pub-id_pmid">34520542</infon><infon key="section_type">REF</infon><infon key="source">Nucleic Acids Res</infon><infon key="type">ref</infon><infon key="volume">49</infon><infon key="year">2021</infon><offset>24015</offset><text>Theoretical basis for stabilizing messenger RNA through secondary structure design</text></passage><passage><infon key="name_0">surname:Leppek;given-names:K.</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>24098</offset><text>Combinatorial optimization of mRNA structure, stability, and translation for RNA-based therapeutics</text></passage><passage><infon key="fpage">2122</infon><infon key="lpage">2127</infon><infon key="name_0">surname:Lee;given-names:J.</infon><infon key="pub-id_pmid">24469816</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci U S A</infon><infon key="type">ref</infon><infon key="volume">111</infon><infon key="year">2014</infon><offset>24198</offset><text>RNA design rules from a massive open laboratory</text></passage><passage><infon key="fpage">748</infon><infon key="lpage">757</infon><infon key="name_0">surname:Anderson-Lee;given-names:J.</infon><infon key="pub-id_pmid">26902426</infon><infon key="section_type">REF</infon><infon key="source">J Mol Biol</infon><infon key="type">ref</infon><infon key="volume">428</infon><infon key="year">2016</infon><offset>24246</offset><text>Principles for Predicting RNA Secondary Structure Design Difficulty</text></passage><passage><infon key="name_0">surname:Wayment-Steele;given-names:H.K.</infon><infon key="name_1">surname:Kladwang;given-names:W.</infon><infon key="name_2">surname:Participants;given-names:E.</infon><infon key="name_3">surname:Das;given-names:R.</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>24314</offset><text>RNA secondary structure packages ranked and improved by high-throughput experiments</text></passage><passage><infon key="fpage">95</infon><infon key="lpage">117</infon><infon key="name_0">surname:Seetin;given-names:M.G.</infon><infon key="name_1">surname:Kladwang;given-names:W.</infon><infon key="name_2">surname:Bida;given-names:J.P.</infon><infon key="name_3">surname:Das;given-names:R.</infon><infon key="pub-id_pmid">24136600</infon><infon key="section_type">REF</infon><infon key="source">Methods Mol Biol</infon><infon key="type">ref</infon><infon key="volume">1086</infon><infon key="year">2014</infon><offset>24398</offset><text>Massively parallel RNA chemical mapping with a reduced bias MAP-seq protocol</text></passage><passage><infon key="fpage">1610</infon><infon key="lpage">1616</infon><infon key="name_0">surname:Wilkinson;given-names:K.A.</infon><infon key="name_1">surname:Merino;given-names:E.J.</infon><infon key="name_2">surname:Weeks;given-names:K.M.</infon><infon key="pub-id_pmid">17406453</infon><infon key="section_type">REF</infon><infon key="source">Nat Protoc</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2006</infon><offset>24475</offset><text>Selective 2’-hydroxyl acylation analyzed by primer extension (SHAPE): quantitative RNA structure analysis at single nucleotide resolution</text></passage><passage><infon key="fpage">5381</infon><infon key="lpage">5394</infon><infon key="name_0">surname:Danaee;given-names:P.</infon><infon key="pub-id_pmid">29746666</infon><infon key="section_type">REF</infon><infon key="source">Nucleic Acids Res</infon><infon key="type">ref</infon><infon key="volume">46</infon><infon key="year">2018</infon><offset>24615</offset><text>bpRNA: large-scale automated annotation and analysis of RNA secondary structure</text></passage><passage><infon key="name_0">surname:Chen;given-names:T.</infon><infon key="name_1">surname:G.;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>24695</offset><text>XGBoost: A Scalable Tree Boosting System</text></passage><passage><infon key="fpage">152629</infon><infon key="name_0">surname:Foo;given-names:C.-S.</infon><infon key="name_1">surname:Pop;given-names:C.</infon><infon key="section_type">REF</infon><infon key="source">bioRxiv</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>24736</offset><text>Learning RNA secondary structure (only) from structure probing data</text></passage><passage><infon key="fpage">26</infon><infon key="name_0">surname:Lorenz;given-names:R.</infon><infon key="pub-id_pmid">22115189</infon><infon key="section_type">REF</infon><infon key="source">Algorithms Mol Biol</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2011</infon><offset>24804</offset><text>ViennaRNA Package 2.0</text></passage><passage><infon key="fpage">2579</infon><infon key="lpage">2605</infon><infon key="name_0">surname:van der Maaten;given-names:L</infon><infon key="name_1">surname:H.;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Journal of Machine Learning Research</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2008</infon><offset>24826</offset><text>Visualizing Data using t-SNE</text></passage><passage><infon key="file">nihpp-2110.07531v1-f0001.jpg</infon><infon key="id">F1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24855</offset><text>Dual-crowdsourcing setup for creating predictive models of RNA degradation. A. Screenshot of the OpenVaccine Kaggle competition, public leaderboard. B. Screenshot of an example construct designed by an Eterna participant in the “Roll Your Own Structure” challenge (“rainbow tetraloops 7” by Omei). C. tSNE projection of training sequences of “Roll-Your-Own-Structure” Round I, marker style and colors indicating 150 Eterna participants. Lines indicate example short 68 nt RNA fragments. D. Timelines of dual crowdsourced challenges. Eterna participants designed datasets that were used for training and blind test data for Kaggle machine learning competition to predict RNA chemical mapping signal and degradation. E. Kaggle participants were given RNA sequence and structure information and asked to predict RNA degradation profiles and SHAPE reactivity.</text></passage><passage><infon key="file">nihpp-2110.07531v1-f0002.jpg</infon><infon key="id">F2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25723</offset><text>Signal-noise filtering and hierarchical clustering was used to filter the constructs designed by Eterna participants to create a test set of constructs that were maximally distant from other test constructs. Heatmaps of datatype “deg_Mg_pH10”.</text></passage><passage><infon key="file">nihpp-2110.07531v1-f0003.jpg</infon><infon key="id">F3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25971</offset><text>Deep learning strategies used in competition. (A) Public test vs. private test performance of all teams in Kaggle challenge. Black star: experimental error. Red star: DegScore baseline model (Leppek, 2021). (B) Distance embedding used to represent nucleotide proximity to other nucleotides in secondary structure. (C) Schematic of the single neural net (NN) architecture used by the first place solution. This solution combined two sets of features into a single NN architecture, which combined elements of classic RNNs and CNNs. (D) Schematic of the full solution pipeline for the second place solution. This solution combined single model neural networks, similar to the ones used for the first place solution, with more complex 2nd and 3rd level stacking using XGBoost as the higher level learner.</text></passage><passage><infon key="file">nihpp-2110.07531v1-f0004.jpg</infon><infon key="id">F4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>26772</offset><text>Deep-learning models can represent RNA-structure-based observables. (A) Experimental error for private dataset vs. RMSE from winning Nullrecurrent model. (B) Representative structures from the best-predicted constructs from SHAPE modification (top row) and degradation at 10 mM Mg2+, pH 10, 1 day, 24 °C (Deg_Mg_pH10, bottom row). (B) Nullrecurrent model predictions and experimental signal, averaged over secondary structure motifs. (C) One failure mode for prediction came from constructs whose input secondary structure features were incorrectly predicted.</text></passage><passage><infon key="file">nihpp-2110.07531v1-f0005.jpg</infon><infon key="id">F5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>27333</offset><text>Kaggle models demonstrate improved performance in independent test of degradation of full-length mRNAs. (A) Kaggle models were tested against predictors in ref. in their ability to predict net degradation measurements from the PERSIST-seq technique. The PERSIST-seq technique measures the degradation of the CDS region. Predictions were made for the full constructs, and values from the CDS region summed to compare to PERSIST-seq measurements of CDS region degradation. (B) Representative structures from ref. of a destabilized mRNA (“Yellowstone”, left) and a stabilized mRNA (“LinearDesign-1”, right). (C) Four mRNA types were part of the test dataset: a short Multi-Epitope Vaccine (MEV), Nanoluciferase with varied UTRs, Nanoluciferase with varied CDS regions, and enhanced Green Fluorescent Protein (eGFP). (D) Length-normalized predictions from the Kaggle 1st place “Nullrecurrent” model and Kaggle 2nd place “Kazuki2” model show improved prediction over unpaired probabilities, the DegScore linear regression model, and a version of the DegScore featurization with XGBoost training.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>28440</offset><text>Results from models tested in this work on Kaggle OpenVaccine public leaderboard, private test set, and orthogonal mRNA degradation results.</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;box&quot; rules=&quot;all&quot;&gt;&lt;colgroup span=&quot;1&quot;&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;col align=&quot;left&quot; valign=&quot;middle&quot; span=&quot;1&quot;/&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Public test set (400 constructs, 27200 nucleotides)&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Private test set (1801 constructs, 162316 nucleotides)&lt;/th&gt;&lt;th align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;mRNA degradation prediction from ref.&lt;xref rid=&quot;R6&quot; ref-type=&quot;bibr&quot;&gt;6&lt;/xref&gt; (188 constructs)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Metric&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MCRMSE&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;MCRMSE&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Spearman Correlation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Experimental error&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.12491&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.10571&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.951&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Single Model (blind prediction)&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;DegScore&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.48724&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.52772&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.36&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Nullrecurrent&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.22758&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.34198&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.41&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Kazuki2&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.22756&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.34266&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−&lt;bold&gt;0.48&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;Ensembled models (post hoc)&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Genetic algorithm (10 of top 100 selected)&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.2237&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.3397&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Ensemble top 2 models&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.2244&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;
&lt;bold&gt;0.33788&lt;/bold&gt;
&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;−0.45&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Genetic algorithm on private test set&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;/&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.3382&lt;/td&gt;&lt;td align=&quot;left&quot; valign=&quot;middle&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;--&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>28581</offset><text>	Public test set (400 constructs, 27200 nucleotides)	Private test set (1801 constructs, 162316 nucleotides)	mRNA degradation prediction from ref. (188 constructs)	 	Metric	MCRMSE	MCRMSE	Spearman Correlation	 	Experimental error	0.12491	0.10571	−0.951	 	Single Model (blind prediction)				 	DegScore	0.48724	0.52772	−0.36	 	Nullrecurrent	0.22758	0.34198	−0.41	 	Kazuki2	0.22756	0.34266	−0.48	 	Ensembled models (post hoc)				 	Genetic algorithm (10 of top 100 selected)	0.2237	0.3397		 	Ensemble top 2 models	0.2244	0.33788	−0.45	 	Genetic algorithm on private test set		0.3382	--	 	</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>29174</offset><text>Bootstrapped Spearman correlation of degradation rate (resampled from experimental error) to half-life.</text></passage></document></collection>
