research-article201460458214545896Health Informatics JournalSims et al.







           Original Article


                                                                                               Health Informatics Journal
                                                                                                               1–11
           Provider impressions of the use                                                       © The Author(s) 2014
                                                                                               Reprints and permissions:
           of a mobile crowdsourcing app in                                           sagepub.co.uk/journalsPermissions.nav
                                                                                         DOI: 10.1177/1460458214545896
           medical practice                                                                            jhi.sagepub.com




           Max H Sims

           University of Rochester, USA


           Maria Fagnano, Jill S Halterman and Marc W

           Halterman
           University of Rochester Medical Center, USA




           Abstract
           In our prior work, we conducted a field trial of the mobile application ▯DocCHIRP (Crowdsourcing Health
           Information Retrieval Protocol for Doctors), designed to help clinicians problem-solve at the point of care
           by crowdsourcing their peers. Here, we present the results of our post-trial survey that investigated the

           impressions of participating clinicians regarding the use of medical cro▯wdsourcing and to identify factors
           influencing adoption of the technology. In all, 72 valid surveys were re▯ceived from 85 registered users (85%
           response rate). The majority of clinicians (>80%) felt crowdsourcing ▯would be useful to diagnose unusual
           cases, facilitate patient referrals, and problem-solve at the point of c▯are. Perceived barriers to adoption

           included interruptions in workflow and the reluctance to publicly expose knowledge gaps. While considered
           a useful alternative to existing methods, future studies are needed to i▯nvestigate whether the approach and
           application can be modified to effectively address these barriers, and t▯o determine whether crowdsourcing
           will enhance provider performance and the quality of care delivered.



           Keywords
           clinical decision-making, collaborative work practices, information ▯ nowledge management, IT design
           and development, IT health care evaluation




           Introduction

           Given the pace of medical practice, opportunities for face-to-face colla▯boration between health
           care providers (HCPs) are sporadic, and neither text paging nor email ▯lends themselves towards
                                                       1
           effective collaboration at the point of care. Physicians typically have two questions for every three


           Corresponding author:

           Marc W Halterman, Department of Neurology, Center for Neural Development▯ & Disease, University of Rochester
           Medical Center, 601 Elmwood Avenue, Box 645, Rochester, NY 14642, USA.
           Email: marc_halterman@urmc.rochester.edu




                                          Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
2                                                                           Health Informatics Journal


patients encountered, yet finding relevant articles to address focused questions quickly is difficult
and in practice, providers use medical references fewer than nine times ▯per month.   2

   The “information overload” problem is not specific to the field of▯ medicine, and other disci-    3
plines have started to adopt an evolving form of digital collaboration c▯alled crowdsourcing.
Fueled by expanded access to the Internet and availability of web-enabled smart devices,
crowdsourcing has been used to engage online communities to accomplish t▯asks of varying
complexity. Large organizations crowdsource the collective wisdom of employees and the pub-

lic at large to solve internal operational issues, conduct market research, and ad▯vance their
branding presence. Corporations are also beginning to consider the frequency and quality o▯f
employee engagement with internal crowdsourcing systems as part of the f▯ormal job review
and promotion process.    5To a lesser degree, the medical community has begun to use both

social media and crowdsourcing in practice with doctors using Facebook a▯nd Twitter to engage
their patients and promote their reputation.  6Examples of crowdsourcing have begun to emerge
in the public domain with companies proposing to connect patients with p▯roviders using pro-
grams like HealthTap’s “Talk-to-Docs” or to invite HCPs to solve diagnostic dilemmas through

sites like crowdmed.com. It remains unclear whether physicians would use▯ crowdsourcing for
peer-to-peer collaboration.
   To study this question, we conducted a field trial of the mobile crowdsou▯rcing application
DocCHIRP (Crowdsourcing Health Information Retrieval Protocol for Doctors). In this study, we

report the results from our post-trial provider survey whose primary obj▯ective was to understand
user perspectives regarding the pros and cons of using real-time crowdso▯urcing in clinical practice.
Related objectives were to understand current provider information seeki▯ng behaviors, define user
opinion regarding the value of collaborative interactions, and define th▯e potential barriers to imple-

menting peer-to-peer crowdsourcing into the workflow of medical practice.


Methods

Program design

DocCHIRP is both a mobile and web-based application that allows HCPs to post que▯stions to
trusted colleagues in real time. Details regarding program development a▯nd design have been
previously reported. The mobile application was designed to run on both Apple and Android
devices, and trial participants downloaded the program from theAppleApp store or the android

compatible version directly from the DocCHIRP server. Network access was restricted to users
holding verified server accounts, and providers were able to select and ▯manage members of a
single crowd, set notification preferences (email, texting, or both), ▯and publicly display their
areas of expertise. When faced with a clinical question, the consulting provider (hereafter▯

referred to as the index provider) could send consult questions and car▯ry on one-to-many con-
versations with colleagues in real time. Responses, which were collated ▯according to response
time, were associated with the initial consult question and viewable by ▯all participants in that
provider’s group.


Study population

Both the DocCHIRP field trial and post-trial survey were approved by the University of Ro▯chester

Research Subjects Review Board (RSRB) and designated as minimal risk. T       ▯ rial participants
(n=85) were recruited from the division of Pediatric Neurology and Departments of Neurology,
Pediatrics, Neuroradiology, Psychiatry, Orthopedics, Emergency Medicine, Internal Medicine, and




                                Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
Sims et al.                                                                                        3


Family Medicine. We did not seek parity in either age or gender representation, and consen▯t to
participate was obtained as part of account activation. Participants inc▯luded attending physicians
(n=63), residents (n=13), fellows (n=1), and nurse practitioners (n=8). Subjects were invited to
participate in an email that included a cover letter and the notice of a▯pproval from the RSRB.
Follow-up emails  4 and a written invitation were also sent over a period of 30   days. Participants

received a coffee coupon 1day prior to closing the survey at the end of the month.


Data analysis
The 10-min survey was anonymous and conducted online (https://www.surveymonkey.com).

Closed-ended questions, in which the respondent picked an answer from a ▯given number of mutu-
ally exclusive options (see supplemental materials), were grouped by c▯ategory. Questions included
items regarding provider demographics, current use of mobile technologie▯s in clinical practice,
frequency and modes of provider-to-provider communication, and impressions of medical crowd-
sourcing. To understand factors affecting technology adoption, survey respondents were asked to

self-identify as users or non-users by recalling the frequency of progra▯m engagement (never, occa-
sionally, regularly). Responses were verified against the server transcripts.


Statistical analyses

We performed all analyses using Statistical Product and Service Solutions▯ (SPSS) version 15.0
software (SPSS Inc., Chicago, IL, USA). We used standard summary statistics to describe overall
demographics and survey sub-domains. Data were organized in two-by-two tables, and Fisher’s
exact tests were performed to look for differences between users and non-users. An exact, two-
sided α level of less than 0.05 was considered statistically signific▯ant.


Results

Participant demographics

Of the 85 providers that created DocCHIRP accounts, we received responses from 72 participants,
resulting in an 85percent response rate. DocCHIRPregistrants were divided into “user” and “non-
user” groups by self-report of whether they used DocCHIRP regularly or occasionally versus

never with 40 respondents indicated that they logged in more than once. The median age of study
participants was 43years, and neitherage nor gender had an influence on whether participants used
the mobile application (Table 1). The majority of participants (82%) accessed DocCHIRP using the
iPhone; however, we found no correlation between the preferred mobile device(s) and u▯ser status.


Existing information seeking and communication behaviors

When asked how difficult it is to find good evidence or actionable information to solve cl▯inical
issues when needed, most providers indicated it was either “very easy▯” (17%) or “easy” (62%),
while 22% indicated having difficulty. To understand where clinicians turn when they need to close
knowledge gaps and solve clinical problems, we assessed the range of ref▯erence materials provid-

ers reported using at the point of care on a weekly or daily basis (Table 2). Clinicians relied heavily
on online resources like Up to Date or eMedicine (90.1%) and face-to-face advice from colleagues
(88.6%). We also found no differences between users and non-users in terms of their use of mobile
applications (i.e. Epocrates, 5-Minute Consult), the published literat▯ure, or paging a colleague.




                              Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
4                                                                             Health Informatics Journal


Table 1. Study population.

                         Overall                  User                    Non-user               p value

                         (n=72), n (%)            (n=42), n (%)           (n=30), n (%)
Age (median 43years)

  <Median age            34 (47.2)                23 (54.8)               11 (36.7)              0.810
  >Median age            38 (52.8)                19 (45.2)               19 (63.3)
Gender
  Women                  31 (43.1)                19 (61.3)               12 (38.7)              0.156

  Men                    41 (56.9)                23 (56.1)               18 (43.9)
Education
  MD or DO               53 (73.6)                28 (66.7)               25 (83.3)              0.283
  MD/PhD                 12 (16.7)                 9 (21.4)                3 (10.0)

  NP                       7 (9.7)                 5 (11.9)                2 (6.7)

                         Overall                  User                    Non-user
                         (n=106), n (%)           (n=61), n (%)           (n=45), n (%)
             a
Mobile device
  iPhone                 59 (81.9)                35 (83.3)               24 (80.0)              0.763
  iPad                   34 (47.2)                19 (45.2)               15 (50.0)              0.812
  Droid                    8 (11.1)                5 (11.9)                3 (10.0)              1.000

  Blackberry               1 (1.4)                 1 (2.4)                 0 (0.0)               1.000
  Other                    4 (5.6)                 1 (2.4)                 3 (10.0)              0.301

DocCHIRP: Crowdsourcing Health Information Retrieval Protocol for Doctors.
aUsers interacted with DocCHIRP using more than one device in some cases ▯resulting in a higher number of overall
devices registered relative to the number of respondents.


However, compared to non-users, DocCHIRP users were more likely to engage their colleagues

through face-to-face discussions (92.5% vs 78.6%; p<0.05), by email (45.2% vs 21.4%;p<0.05),
or through texting (21.4% vs 3.6%; p<0.05).
   In terms of their current use of online and mobile communication tools, ▯trial participants indi-

cated being comfortable using both electronic mail (98.6%) and texting▯ (93%), while fewer provid-
ers (57.7%) used social networking tools (Facebook, LinkedIn, Twitter). With respect to their
impact on clinical care, 47.1% of respondents agreed that having access ▯to texting and email

enhanced quality. That said, nearly half felt that they receive non-essential texts and pa▯ges (49.3%)
that disrupt their daily workflow and that these interruptions interfere▯d with quality care (48.6%).
These sentiments were shared among users and non-users alike.


Crowdsourcing in medicine: perceptions and strategies

We next turned to the question of whether perceptions regarding the use o▯f crowdsourcing in the

medical setting differed among trial participants. Over 80        percent of respondents agreed that
crowdsourcing could have a net positive impact on routine patient care, medical education, making
appropriate patient referrals, and diagnosing unusual cases (Figure 1)▯. We also found that more

non-users than users were concerned that the use of near-real-time crowdsourcing could interfere
with personal “off the clock” time (89% vs 65%; p      <0.05). Similarly  , 56percent  of clinicians sur-
veyed indicated real-time crowdsourcing could negatively affect their productivity (Table 3).

Given this result, we were interested to find that over half of the respondents were willing to




                                 Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
Sims et al.                                                                                                 5

Table 2. Provider use of existing communication modalities and information resour▯ces.

                                           Overall, n (%)      User, n (%)      Non-user, n (%)       p value

HCPs use the following resources weekly/daily

   Online references                       64 (90.1)           40 (92.5)        24 (82.8)             0.113
   PubMed                                  49 (69.0)           29 (69.0)        20 (69.0)             1.000
   Print journals                          24 (33.8)           12 (28.6)        12 (41.4)             0.312

   Mobile applications                     34 (48.6)           21 (50.0)        13 (46.4)             0.811
   Face-to-face discussions                62 (88.6)           40 (92.5)        22 (78.6)             0.040
   Email a colleague                       25 (35.7)           19 (45.2)         6 (21.4)             0.047
   Text a colleague                        10 (14.3)            9 (21.4)         1 (3.6)              0.043

   Page a colleague                        23 (32.4)           14 (33.3)         9 (31.0)             1.000
HCPs are comfortable
   Using electronic mail                   71 (98.6)           42 (100.0)       29 (96.7)             0.417
   Texting                                 66 (93.0)           39 (92.9)        27 (93.1)             1.000

   Using social networks                   41 (57.7)           27 (64.3)        14 (48.3)             0.225
   Mobile phone applications               50 (70.4)           32 (76.2)        18 (62.1)             0.290
   Push notifications                      33 (46.5)           21 (50.0)        12 (41.4)             0.629

HCPs agree
   They receive non-essential texts        35 (49.3)           19 (45.2)        16 (55.2)             0.474
   Texting/paging interfere                34 (48.6)           20 (48.8)        14 (48.3)             1.000
   Texts/email enhance care delivery       33 (47.1)           18 (42.9)        15 (53.6)             0.466

HCP: health care provider.

























Figure 1. Health care provider perceptions regarding the utility of crowdsourcing ▯in medical practice.
Providers were asked to select areas in which they felt mobile crowdsourcing could be used to enhance a
range of activities in academic medicine.



respond to a DocCHIRP notification within minutes, if not actively engaged in patient care. M▯ost
providers (66.7%) also indicated it would be acceptable if many of the▯ir posts receive an answer
within several hours rather than minutes.




                                 Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
6                                                                                Health Informatics Journal


Table 3. Factors affecting adoption of crowdsourcing in medical practice.

                                            Overall, n (%)     User, n (%)      Non-user, n (%)      p value

Real-time crowdsourcing would

   Create confusion around clinical         18 (26.9)           8 (20.0)        10 (37.0)            0.163
   decisions
   Increase physician distractions          42 (63.6)          25 (62.5)        17 (64.5)            1.000
   Interfere with personal time             49 (74.2)          26 (65.0)        23 (88.5)            0.045
   Interfere with the flow of care          37 (56.1)          26 (65.0)        11 (42.3)            0.082

I would be more likely to use DocCHIRP if
   My institution guaranteed HIPAA          53 (77.9)          30 (75.0)        23 (82.1)            0.563
   compliance
   It was only available behind the         29 (43.9)          13 (33.3)        16 (59.3)            0.046

   institutional firewall
   I was able to view contributions         41 (100)           34 (100)          7 (100)             1.000
   from my clinical colleagues
   It included a reputation-based           47 (71.2)          32 (82.1)        15 (55.6)            0.028
   system that could recognize high-
   quality contributions

   I could access a database of             57 (85.1)          37 (92.5)        20 (74.1)            0.076
   previous posts
   The system used my institutional         60 (88.2)          33 (82.5)        27 (96.4)            0.128
   login credentials

   My identity is kept anonymous            21 (31.8)           9 (23.1)        12 (44.4)            0.106
   I could receive feedback that my         56 (83.6)          34 (85.0)        22 (81.5)            0.745
   participation had a positive effect
   on patient care
   I had the option to choose/view          65 (91.5)          41 (97.6)        24 (82.8)            0.038

   level of consult urgency
   I could create networks across           57 (86.4)          36 (92.3)        21 (77.8)            0.144
   institutions
HCPs agreed that they would be likely to
   Respond in a few minutes, unless         36 (51.4)          23 (54.8)        13 (46.4)            0.626

   with patient
   Post questions that would require a      33 (47.1)          21 (50.0)        12 (42.9)            0.629
   provider’s response within minutes
   Post questions that would require a      46 (66.7)          31 (73.8)        15 (55.6)            0.128
   provider’s response within hours

HCP: health care provider; HIPAA: Health Insurance Portability and Accou▯ntability Act; DocCHIRP: Crowdsourcing
Health Information Retrieval Protocol for Doctors.



   When asked about particular features that would enhance adoption of DocC▯HIRP , clinicians
cited several factors including the following: (1) receiving feedback ▯that their participation had
a positive effect on patient care (84%), (2) having an institutional guarantee of▯ Health Insurance

Portability andAccountabilityAct (HIPAA) compliance (78%), and (3) being able to use insti-
tutional login credentials to access the system (88%). More users than non-users agreed that
recognition of high-quality contributions by the system would be benefic▯ial (82% vs 56%;

p =0.028) and preferred having the option to choose/view the level of cons▯ult ur           gency (97% vs
83%; p   <0.05). Conversely    , non-users preferred that their employer distribute the system and





                                   Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
Sims et al.                                                                                          7


that it function only behind the institutional firewall (59.3% vs 33.3%▯;    p<0.05). Despite these
concerns, HCPs also expressed an interest in collaborating with providers outside their own
institution.


Consultation practices by clinicians

While the DocCHIRP prototype used in the field trial supported communication between a
single group of colleagues selected by the user, it did not support ad hoc consultation across

medical specialties. In other words, while pediatricians could consult through the application
with other pediatricians, they were unable to consult with neurologists.▯ Not surprising, we
found most clinicians (93% of users vs 78% of non-users) indicating th▯at access to this feature
would enhance the utility of the program. To understand existing collaborative networks, we

next asked providers in Neurology, Pediatrics, and Pediatric Neurology to indicate which of the
various provider groups they have consulted over the past year (Figure ▯2).As expected, patterns
varied between groups where, for example, while neurologists sought cont▯act with colleagues
in neurosurgery and radiology, pediatricians were more likely to seek input from neurologists,

psychiatrists, and infectious disease specialists. We also considered whether providers would
seek the advice from colleagues in the allied health professions. We found that while each dis-
cipline sought input from social work, a range of unique preferences for▯ other allied health
providers was also observed.


Discussion

Although crowdsourcing is widely utilized in the commercial and manufact▯uring sectors by com-
panies including Coca-Cola, General Electric, and Disney,     8 the health care field is beginning to

show signs of innovation in this space. Most activity has occurred in the private sector rather than
within academic institutions, where startups facilitate the client–cr▯owd relationships between
patients and the lay public (https://www.crowdmed.com), patients and physicians (https://www.
healthtap.com), as well as between companies and physicians (www.sermo.com). However, with
recent legislation promoting the creation of regionally based Accountable Care Organizations

(ACOs), it is anticipated that mobile health solutions that foster com▯munication between clinicians
practicing within individualACO structures will become more common.
   Clinicians are constantly presented with questions regarding best practices in caring for their
patients,2and in some settings spend less than 10     min on average with a given patient.    9 These

forces create bottlenecks in HCP workflows that compromise their ability to provide high-quality
care. In this study, we wanted to understand whether providers from the DocCHIRP field trial felt
crowdsourcing could address this problem, and to identify particular fea▯tures of the mobile appli-
cation that would facilitate adoption. Providers judged program success ▯on the basis of receiving
high-quality responses in a timely manner. In the course of these studies, we have identified three

key features essential to the success of near-real-time crowdsourcing among HCPs: (1) to satisfy
the expectation of a timely response, there must be a crowd of sufficient size available; (2) the
crowd must hold the appropriate expertise to opine on the question posed▯; and (3) HCPs must trust
that the information provided will contribute to overall patient benefit▯, without bringing undue

exposure to professional risk or harm to their reputation. And while our results show that while
there was wide agreement that crowdsourcing had the potential to enhance care delivery, this
enthusiasm was tempered by concerns regarding information privacy and th▯e potential ill effects
on provider workflow.





                               Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
8                                                                             Health Informatics Journal







































Figure 2. Historical provider consultation practices. Survey participants were ask▯ed to identify the
medical specialists and allied health professionals with whom they had c▯onsulted in the previous year. Data
for the cohort of neurologists (=23), pediatric neurologists ( =9), and pediatricians (23) are shown.


Adapting near-real-time crowdsourcing to medical practice

While crowdsourcing has typically involved using the collective intellig▯ence of the crowd without
explicit time constraints, advances in “real-time crowdsourcing” h▯ave made it possible to reduce
                                                             10
crowd response to under a few seconds for certain tasks.       Similarly, “near-real-time crowdsourc-
ing” approaches, including the system VizWiz designed to help the visually impaired navigate in a
sighted world, can deliver crowd responses within 5        min. With this in mind, we considered
whether real-time crowdsourcing could also benefit HCPs. We found that 96 percent of providers

felt crowdsourcing could have an overall net positive effect on their practice. In particular, provid-
ers indicated that crowdsourcing could help solve unusual cases, promote▯ medical education, and
facilitate communication and the exchange of clinical information betwee▯n existing collaborative
groups. There was also significant interest in using crowdsourcing to communicat▯e with colleagues

outside their own specialty as well as with allied health professionals.▯
   Potential barriers to adopting crowdsourcing in practice were also ident▯ified. Research and
anecdotal experience tell us that interruptions by pagers, mobile phones, and other devices can
                                                  12
adversely affect the quality of care delivered.     Although providers in our study were willing to



                                 Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
Sims et al.                                                                                            9


answer consults in under 5 min if not otherwise engaged in clinical tasks, more than half recog -
nized the potential for DocCHIRP consult requests to interfere with their workflow. Potential solu-

tions to this problem included having a tagging system, visual prompts, and notification preference
settings to help triage consult urgency. HCPs were also concerned that participation would under-
mine their reputation through public disclosure of their knowledge gaps.▯ We therefore asked par -
ticipants whether they felt having the opportunity to post consults anon▯ymously would mitigate
this problem. Interestingly, most providers felt while it could potentially benefit the index provider,

hiding the identity of the consultant would undermine user confidence in the action plan recom-
mended by the crowd. Users also felt that public recognition of high-qua▯lity contributions would
likely encourage program use.
   Trust in network security was another dominant theme in the post-trial su▯rvey. Wary of the

potential disclosure of protected health information, providers preferre▯d to have their employer
coordinate the deployment and oversee security of the crowdsourcing syst▯em. Despite this,
respondents also saw value in being able to collaborate with colleagues ▯across institutions. In con-
trast to non-users, DocCHIRP users were less concerned about this issue and disagreed with having

the employer involved. It is not surprising that clinicians seek opportu▯nities for open intellectual
interactions with their colleagues in an environment where both their re▯putation and the confiden-
tiality of their patients are preserved. Further study will be required ▯to understand how best to
engineer the system to accommodate these competing interests.

   In the DocCHIRP trial, it was also suggested that incomplete information from the crowd▯ could
mislead the index provider, a criticism voiced elsewhere regarding the safety and utility of curbs▯ide
consultation. 13Prior work from the crowdsourcing literature indicates that large, diverse crowds
outperform smaller groups of experts,    14and at the societal level, such diversity is essential to the
                                                                                  15
retention of culturally acquired skills and the avoidance of societal co▯llapse. While it remains to
be seen whether this will translate into the clinical domain, it seems i▯ntuitive to expect that response
quality may suffer where crowd size is small and where anonymity and casual participati▯on under-
mine accountability.

   While clinicians tend not to trust differential diagnosis programs based on algorithmic approaches,
most providers surveyed were not concerned about the legality of crowdsou       ▯ rced responses. This
sentiment is consistent with recent guidelines from the Food and Drug Administration that deem
DocCHIRP and other medical references as exempt from regulatory oversight. Overall, it appears

that licensed providers consider real-time crowdsourcing with the same sk     ▯ epticism applied to other
resources when making treatment decisions for their patients.


Consultation practices by clinicians

Successful design and integration of crowdsourcing in a complex system r▯equires understanding
both user motivations to collaborate and the existing social networks at▯ play. While our users and
non-users consulted reference materials with equal frequency, DocCHIRP users were more likely
to collaborate with their peers by email or texting and seek face-to-fac▯e discussions. Of note, this

drive was independent of their taste for digital technologies. With regard to the specific groups
that providers wished to collaborate with, some of the consultation patt▯erns could be predicted
based on the specialty. However, we were intrigued to find that collaboration with social work and
other allied health professionals was pervasive, and in some cases speci▯alty specific. And while

not included in our survey, data indicate that physicians value the contributions of medical center
librarians highly. McGowan et al. found that 80percent of the answers provided by the librar-
ians were rated as having a positive effect on provider decision-making. Taken together, we
believe that inclusion of medical informaticists and other allied professionals within provider




                                Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
10                                                                               Health Informatics Journal


crowdsourcing networks will play a critical role as the volume and compl▯exity of available data
regarding best practices continue to expand.


Study limitations and future directions

As with other social media applications, DocCHIRP is subject to network effects, whereby a good or
service becomes more valuable as the size of the user pool grows, reaching the required critical mass
to sustain productive use.Although communication between clinicians did not easily allow clinicians           ▯

to consult outside their group, we expect to observe a wide range of pote      ▯ ntial uses with the implemen-
tation of more robust crowd management systems. Our data indicate that clinicians agree that consul-
tation with a crowd of known colleagues can produce high-yield answers. I          ▯ n this case, these
individuals have a high likelihood of possessing the knowledge in question and are motivated to

respond in a timely fashion. However, it remains to be seen what will happen when users reach out to
larger groups, either within or across disciplines in terms of quality and t▯ he timeliness of responses.
We anticipate this later model, where, for example, generalist practitione     ▯ rs seek information from a
crowd of trusted yet unknown specialists, has the potential to fetch the▯ most useful information. Yet,

while the literature supp19ts the assumption that crowd size is directly▯ proportional to the quality of
the responses received, the outstanding question remains whether clinicians will be motivated to          ▯
connect by virtue of their affiliation within an institution orACO, absent an established personal con-
nection. In addition to serving as the vehicles for information delivery, this and other medical crowd-
sourcing applications will provide a unique window on this novel mode of▯ communication.

   Despite having an 85percent response rate, the total number of providers in our post-trial sam-
ple was relatively small (n =72). Thus, we may not have been able to fully appreciate differences
between user and non-user impressions. Furthermore, our use of a convenience sample from a
subset of available clinical departments limits the generalizability of ▯our findings across the full

range of clinical practice sites and HCP profiles.Although we found “users” were more likely than
“non-users” to engage colleagues in face-to-face discussions, it is possible that “users” are more
consultative or collaborative in nature than the “non-users” group▯ independent of any technologi-
cal intervention. There were also limitations to the program design that prevented us from studying
several important aspects of human–computer interactions. Chief among these was not capturing

the location of the clinical encounter and timing of the response with r▯espect to this event. Since
having access to a provider network with sufficient depth and variety of experience will be essen-
tial to support continued use and growth, we believe that providing flex▯ible options that allow
HCPs to create and manage their networks will be crucial to the success ▯of crowdsourcing in this

domain. Finally, while there is value in assessing provider perception, future studies ▯will be needed
to establish whether implementation of real-time crowdsourcing systems l▯ike DocCHIRP will in
fact benefit key metrics including diagnostic accuracy, the quality of care delivered, provider effi-
ciency, the cost of services provided, or patient outcomes.


Acknowledgements
The authors would like to thank the physicians and nurse practitioners at the University of Rochester who
participated in the trial.


Declaration of conflicting interests
Collaborative Informatics, LLC provided integrated mobile and server sof▯tware used in this study. Dr Marc
W Halterman is co-owner of Collaborative Informatics, LLC, and oversaw t▯he specifications and construction

of the software used in this study. Dr Marc W Halterman has provided the▯ necessary conflict of interest docu-
mentation in keeping with the requirements of the University of Rocheste▯r.



                                  Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015
Sims et al.                                                                                             11


Ethical approval
The DocCHIRP study was reviewed by the Institutional Review Board at the▯ University of Rochester and
received approval posing minimal risk.


Funding

This research received no specific grant from any funding agency in the ▯public, commercial, or not-for-profit
sectors.


References
 1. Bernhardt JM, Mays D and Kreuter MW. Dissemination 2.0: closing the gap ▯between knowledge and

     practice with new media and marketing. J Health Commun 2011; 16(Suppl. 1): 32–44.
 2. Moore M and Librarians AAHS. Teaching physicians to make informed decisions in the face of uncer-
     tainty: librarians and informaticians on the health care team. Acad Med 2011; 86(11): 1345.
 3. Howe J. The rise of crowdsourcing.     Wired, June 2006, 14(6): 1–4. http://archive.wired.com/wired/
     archive/14.06/crowds.html?pg=1&topic=crowds&topic_set=

 4. Dam Rvd, Nelson E and Lozinski Z. The changing face of communication: social networking’s grow-
     ing influence on telecom providers. IBM Global Business Services, 2008, http://www.ibm.com/smarter-
     planet/global/files/nz__en_uk__telecom__gbe03121_usen_socialnetwork.pdf
 5. Bingham T and Conner ML. The new social learning: a guide to transforming organizations through

     social media. Alexandria, VA: ASTD Press/San Francisco, CA: Berrett-Koehler Publishe▯rs, 2010, pp.
     xxii, 193.
 6. Haupt A. How doctors are using social media to connect with patients. US News and World Report, 21
     November 2011, p. 2.
 7. Sims MH, Bigham J, Kautz H, et al. Crowdsourcing medical expertise in near real time. J Hosp Med.

     Epub ahead of print 18 April 2014. DOI: 10.1002/jhm.2204.
 8. Roth Y. 11 of the 12 Best Global Brands use creative crowdsourcing, 2012▯, http://yannigroth.
     com/2012/03/23/xx-of-the-100-best-global-brands-use-creative-crowdsourci▯ng/
 9. Block L, Habicht R, Wu AW, et al. In the wake of the 2003 and 2011 duty ▯hours regulations, how do

     internal medicine interns spend their time? J Gen Intern Med 2013; 28(8): 1042–1047.
10. Bernstein MS, Karger DR, Miller RC, et al. Analytic methods for optimizi▯ng realtime crowdsourcing. In:
     Proceedings of the collective intelligence, Cambridge, MA, 18–20 April 2012.
11. Bigham JP, Jayant C, Ji H, et al. VizWiz: nearly real-time answers to vi▯sual questions, 2010, http://www.
     cs.rochester.edu/hci/pubs/pdfs/vizwiz.pdf

12. Volpp KG and Grande D. Residents’ suggestions for reducing errors in ▯teaching hospitals.N Engl J Med
     2003; 348(9): 851–855.
13. Burden M, Sarcone E, Keniston A, et al. Prospective comparison of curbsi▯de versus formal consulta -
     tions. J Hosp Med 2013; 8(1): 31–35.
14. Brabham DC. Crowdsourcing as a model for problem solving. Convergence 2010; 14(1): 75–90.

15. Derex M, Beugin MP, Godelle B, et al. Experimental evidence for the infl▯uence of group size on cultural
     complexity. Nature. Epub ahead of print 15 November 2013. DOI: 10.1038/nature12774.
16. Bakul P. Mobile medical applications: guidance for industry and food and drug adm▯inistration staff
     (ed U.S. Department of Health and Human Services FDA), 2013, http://ww▯w.fda.gov/downloads/

     MedicalDevices/.../UCM263366.pdf
17. Sollenberger JF and Holloway RG Jr. The evolving role and value of libraries and librarians in health
     care. JAMA 2013; 310: 1231–1232.
18. McGowan J, Hogg W, Campbell C, et al. Just-in-time information improved ▯decision-making in primary
     care: a randomized controlled trial. PloS one 2008; 3(11): e3785.

19. Bachrach Y, Graepel T, Kasneci G, et al. Crowd IQ—aggregating opinions to boost performance. In:
     Proceedings of the 11th international conference on autonomous agents an▯d multiagent systems (ed
     V Conitzer, M Winikoff, L Padgham, et al.), Valencia, 4–8 June 2012.▯ Richland, SC: International
     Foundation for Autonomous Agents and Multiagent Systems, pp. 535–542.▯




                                Downloaded from jhi.sagepub.com at OhioLink on March 23, 2015