                                                                                            Vol. 29 no. 16 2013, pages 1925–1933
BIOINFORMATICS                                            REVIEW                                doi:10.1093/bioinformatics/btt333



Genome analysis                                                                          Advance Access publication June 19, 2013


Crowdsourcing for bioinformatics


Benjamin M. Good and Andrew I. Su

Department of Molecular and Experimental Medicine, The Scripps Research Institute, La Jolla, CA 92037, USA
Associate Editor: Jonathan Wren



ABSTRACT                                                         (usually an employee) and outsourcing it to an undefined, gen-
Motivation: Bioinformatics is faced with a variety of problems thatally large group of people in the form of an open call’ (Howe,
                                                                 2006). Now, it is used to describe a range of activities that span
require human involvement. Tasks like genome annotation, image
analysis, knowledge-base population and protein structure determin-e gamut from volunteers editing wiki pages or tagging astro-
ation all benefit from human input. In some cases, people are neededical images to experienced professionals tackling complex
                                                                 algorithm development challenges. Here, we will focus specific-
in vast quantities, whereas in others, we need just a few with rare
abilities. Crowdsourcing encompasses an emerging collection of   ally on systems for accomplishing directed work that requires
approaches for harnessing such distributed human intelligence.   human intelligence. These human-powered systems are built to
                                                                 solve discrete tasks with clear end points. They are distinct from
Recently, the bioinformatics community has begun to apply crowd-
sourcing in a variety of contexts, yet few resources are available thatcommon, community-driven branches of crowdsourcing,
describe how these human-powered systems work and how to use     such as wikis, in that they allow for top-down control over the
                                                                 work that is conducted. (For an extensive introduction to wikis
them effectively in scientific domains.
Results: Here, we provide a framework for understanding and apply-n biology, see Galperin and Fernandez-Suarez, 2012).
ing several different types of crowdsourcing. The framework considerse tasks discussed here have been historically approached
                                                                 from an artificial intelligence perspective—where algorithms at-
two broad classes: systems for solving large-volume ‘microtasks’ and
systems for solving high-difficulty ‘megatasks’. Within these classes, to mimic human abilities (Sabou et al.,2 2)w,
we discuss system types, including volunteer labor, games with a crowdsourcing gives us access to a new methodology: ‘artificial

purpose, microtask markets and open innovation contests. We illus-rtificial intelligence’ (https://www.mturk.com/). The objective of
trate each system type with successful examples in bioinformatics and review is to give insights into how, from a practical perspec-
                                                                 tive based on recent successes, to use this new force to tackle
conclude with a guide for matching problems to crowdsourcing solu-
tions that highlights the positives and negatives of different   difficult problems in biology.
approaches.                                                        We divide crowdsourcing systems into two major groups:
                                                                 those for solving ‘microtasks’ that are large in number but low
Contact: bgood@scripps.edu
                                                                 in difficulty, and those for solving individually challenging
Received on February 15, 2013; revised on April 30, 2013; accepte‘megatasks’. In Section 2, we present an overview of microtask
on June 5, 2013                                                  solutions with subsections on volunteer systems, casual games,

                                                                 microtask markets, forced labor (workflow sequestration) and
                                                                 education. Section 3 describes crowdsourcing approaches to
1   INTRODUCTION                                                 megatasks with subsections on innovation challenges and hard

Imagine having easy, inexpensive access to a willing team of     games. Section 4 concludes the article with a guide for matching
millions of intelligent workers. What could you accomplish?      problems to potential crowdsourcing solutions, pointers to infor-
Lakhani and colleagues produced 30 new sequence alignment        mation about forms of crowdsourcing not covered here and a

algorithms that each improved on the state-of-the-art, in 2      brief exploration of the potential consequences of crowdsourcing
weeks, for $6000 (Lakhani et al., 2013). Others improved a       on society.
44-species multiple alignment (Kawrykow et al., 2012), de-

veloped a new protein folding algorithm (Khatib et al., 2011a),
produced accurate parasite counts for tens of thousands of       2   CROWDSOURCING MICROTASKS
images of infected blood cells (Luengo-Oroz et al.,2)dan         Microtasks can be solved in a short amount of time (typically a

still others are attempting to translate the entire web into everfew seconds) by any human who is capable of following a short
major language (http://duolingo.com). Crowdsourcing systems      series of instructions. In bioinformatics, microtasks often orient
make these and many other monumental tasks approachable.         around image or text annotation. In these cases, crowdsourcing

Here, we explore what these systems are and how they are         systems provide system designers with access to vast numbers of
being applied in bioinformatics.                                 workers who, working in parallel, can collectively label enor-

  The term ‘crowdsourcing’ was coined in 2006 to describe ‘the   mous volumes of data in a short time. These systems achieve
act of taking a job traditionally performed by a designated agenthigh quality, typically as good as or better than expert annota-
                                                                 tors, through extensive use of redundancy and aggregation.

                                                                 Annotation tasks are presented to multiple workers, and their
                                                                 contributions are integrated, e.g. through voting, to arrive at the
*To whom correspondence should be addressed.                     final solution.



ß The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.pe1925sions@oup.com
B.M.Good and A.I.Su



                                                                    are then integrated back into the full computationally generated
2.1   Volunteer (citizen science)
Perhaps the most surprisingly effective strategy for incentivizing  alignments. In the first 7 months of game-play, Phylo recruited
                                                                    412000 players who collectively completed4254000 puzzles.
large-scale labor in support of scientific objectives is simply to  When the alignment blocks from game players were reassembled,
ask for volunteers. This pattern, often referred to as ‘citizen sci-
ence’, dates back at least to the year 1900, when the annual        they resulted in improvements to 470% of the original
                                                                    alignments.
Christmas bird counts were first organized by the National
Audubon Society (Cohn, 2008). Now, it is exemplified by the
Zooniverse project and its initial product Galaxy Zoo (Lintott      2.2.2 Image annotation   Following shortly after Phylo, two re-
                                                                    search groups independently developed games focused on the
et al., 2008). Galaxy Zoo has successfully used the web to tap      classification of images related to malaria infection. Mavandadi
into a willing community of contributors of previously unimagin-
able scale. Within the first 10 days of its launch in July 2007, theand colleagues describe a web-based game called MOLT that
                                                                    challenges players to label red blood cells from thin blood
Galaxy Zoo web site had captured 8 million morphological clas-      smears as either infected or uninfected (Mavandadi et al.,
sifications of images of distant galaxies (Clery, 2011). After 9
                                                                    2012a, b). Luengo-Oroz and colleagues present a game called
months, 4100000 people had contributed to the classification        MalariaSpot for counting malaria parasites in thick blood
of41 million images—with an average of 38 volunteers viewing        smears (Luengo-Oroz et al., 2012). The similar approaches
each image. Now, the Zooniverse project, in collaboration with
                                                                    taken by both of these systems reflect consistent themes for
Cancer Research UK, is moving into the biomedical domain            microtask platforms; both systems aggregate the responses of
with a project called CellSlider (http://www.cellslider.net).       multiple players (sometimes 420) to produce the annotation
  In CellSlider, volunteers are presented with images of stained
                                                                    for each image and use images with known annotations to
cell populations from cancer patient biopsies and asked to label    benchmark player performance. Using these techniques, both
the kinds and quantities of different cell types. In particular,
volunteers seek out irregularly shaped cells that have been         MOLT and MalariaSpot achieved expert-level performance on
                                                                    their respective tasks. Both systems share a vision of using their
stained yellow based on the level of estrogen receptor expressed    crowdsourcing approach to enable the rapid, accurate and inex-
by the cell. Quantifying the amount of these ‘cancer core’ cells in
                                                                    pensive annotation of medical images from regions without
a particular patient can help to ascertain the extent to which a    access to local pathologists in a process known as ‘tele-path-
treatment is helping the patient, and thus can be used to help      ology’. These systems are also envisioned to play a role in train-
personalize and improve therapy. Launched on October 24,
                                                                    ing both human pathologists and automated computer vision
2012, the initiative has not published its finding yet, but it      algorithms.
claimed to have analyzed 550000 images in its first 3 months
of operation.
                                                                    2.3   Microtask markets

                                                                    Microtask markets are probably the most well-known and thor-
2.2   Casual games                                                  oughly used variety of crowdsourcing. Rather than attempting to
Aside from simply relying on the altruistic urges of the audience,
                                                                    use fun or altruism as incentives, these systems simply use cash
a growing number of crowdsourcing initiatives attempt to            rewards. Where a game like MalariaSpot provides points for
reward participation with fun. In these ‘games with a purpose’,     each labeled image, a microtask market would allow contribu-
microtasks are presented in the context of simple, typically web-
                                                                    tors to earn a small amount of money for each unit of work.
based games (Ahn and Dabbish, 2008). (We distinguish these          Within bioinformatics, microtask markets have so far been used
microtask games from other closely related games designed to        for image and text annotation.
solve difficult problems in Section 3.1.) In these ‘gamified’

crowdsourcing systems, the participants earn points and advance     2.3.1 Image annotation     Although microtask markets have
through levels just like other games, but the objectives in each    enjoyed widespread use for general image annotation tasks
                                                                    since their inception, there are few published examples of appli-
game are closely aligned with its higher-level purpose. To win,
game players have to solve real-world problems with high quality    cations in bioinformatics—though many are in progress. Nguyen
and in large quantities. Casual crowdsourcing games have been       and colleagues provide a prototypical example (Nguyen et al.,
                                                                    2012). They describe the application of the Amazon Mechanical
actively developed by the computer science community since the
ESP Game emerged with great success for general-purpose image       Turk (AMT) crowdsourcing service to detect polyps associated
labeling in 2003 (Ahn and Dabbish, 2004). The first casual games    with colorectal cancer in images generated through computed–

within the realm of bioinformatics address the tasks of multiple    tomographic colonography. Using the AMT, they paid crowd
sequence alignment and image annotation.                            workers to label images of polyp candidates as either true or
                                                                    false. For each task (known as a ‘HIT’ for ‘human intelligence

2.2.1 Multiple sequence alignment Phylo is a game in which          task’), the workers were presented with 11 labeled training
players help to improve large multiple sequence alignments by       images to use to make their judgment on the test image.
completing a series of puzzles representing dubious sections from   Workers were paid $0.01 for each image that they labeled. In

precomputed alignments (Kawrykow et al., 2012). To complete a       the first of two replicate trials with nearly identical results, 150
puzzle, players move Tetris-like, color-coded blocks representing   workers collectively completed 5360 tasks resulting in 20 inde-
nucleotides around until the computed alignment score reaches       pendent assessments of each of 268 polyp candidates. This work

at least a predetermined level, with more points awarded for        was completed in 3.5 days at a total cost of $53.60 (plus some
better alignments. These human-generated alignment sections         small overhead fees paid to Amazon). A straightforward voting



1926
                                                                                                     Crowdsourcing for bioinformatics



strategy was used to combine the classifications made by multiple     meta-services generally offer less control over the operation of

workers for each polyp candidate. The classifications generated       the system but solve many common problems effectively. Aside
by this system were then assessed based on agreement with expert      from these services, a small but growing number of open source
classifications and compared with results from a machine learn-       projects for working with crowdsourcing systems are now avail-

ing algorithm. The results of the crowd-powered system and the        able. For example, see Turkit (Little et al., 2010) and
machine learning system were not significantly different. Both        CrowdForge (Kittur et al., 2011).

systems produced an area under the receiver operating charac-
teristic curve close to 0.85. Although this system did not improve    2.4   Forced labor (workflow sequestration)
on the automated system, it demonstrated that minimally trained
                                                                      If altruism, fun or money is not sufficient to motivate workers, it
AMT workers could perform this expert-level task rapidly and          is sometimes possible to force them to work for free. This strat-
with high quality. In subsequent work, the same research group
reported significant improvements with a new system that inte-        egy has been used most effectively in the omnipresent
                                                                      ReCAPTCHA (Ahn et al., 2008). ReCAPTCHA is a security
grated the automated predictions with those derived from crowd-       system for web sites that requires users to type in two words
sourcing to produce an area under the receiver operating
characteristic curve of 0.91 on the same data (Wang et al., 2011).    that they see in a distorted image. One word is known and
                                                                      thus used for verification that the user is a human (not a pro-
                                                                      gram), and the other is a scanned image of text that needs to be
2.3.2 Text annotation   With much of the world’s biological and
medical knowledge represented in text, natural language process-      digitized. Because this task is difficult to accomplish computa-
                                                                      tionally, it provides organizations with a way to defend against
ing (NLP) is a core component of research in bioinformatics.          automated spammers, thus saving them large amounts of work.
Many tasks in NLP require extensive amounts of expensive lin-
guistic annotation. For example, NLP systems that detect con-         At the same time, the decision by web site owners to use the
                                                                      system effectively forces hundreds of millions of web users to
cepts and relationships often need large corpuses of semantically
tagged text for training (Kim et al., 2003). In seeking a faster,     work on large-scale optical character recognition tasks for free.
                                                                        ReCAPTCHA uses the incentive to complete a task that is
less-expensive method for acquiring these data, the NLP com-          important to the users/workers (logging in to a web site) to mo-
munity was among the first to explore the use of crowdsourcing
for research purposes (Sabou et al., 2012). Early work by Snow        tivate them to complete a task that is important to the system
                                                                      designer (digitize books). McCoy and colleagues recently applied
and colleagues demonstrated that expert-level text annotations        this pattern for clinical knowledge base construction (McCoy
could be collected ‘cheap and fast’ using the AMT platform and
also provided a pattern for correcting biases common to crowd-        et al., 2012). In this study, the ‘crowd’ consisted of the physicians
                                                                      in a large medical community; the incentive was to use an elec-
sourcing systems (Snow et al., 2008). Although this and related       tronic health record system to prescribe medications, and the
work has achieved good results with common language tasks,
                                                                      task was to capture links between medications and patient prob-
biomedical text (with its more challenging vocabulary) is just        lems. To prescribe a medication, the physicians were required to
beginning to be approached through crowdsourcing.
  Yetisgen-Yildiz and colleagues demonstrated that AMT work-          link it to the associated clinical problem. Using this pattern, 867
                                                                      clinicians created 239469 problem-medication links in 1 year,
ers could produce effective annotations of medical conditions,        including 41203 distinct links. To identify problem-medication
medications and laboratory tests within the text of clinical trial
descriptions (Yetisgen-Yildiz et al., 2010). Burger and colleagues    links with high precision, the authors implemented a filtering
                                                                      system that incorporated both the number of patients for
also used the AMT to validate predicted gene mutation relations       which a pair was asserted (voting) and the baseline probability
in MEDLINE abstracts (Burger et al., 2012). They found that
the workers (paid $0.07/task) were easily recruited, responded        of each pair (penalizing pairs likely to co-occur by chance). Using
                                                                      a manually defined threshold intended to minimize false-positive
quickly and (as is typical of all crowdsourcing systems) displayed    findings, this filter yielded 11166 distinct problem-medication
a wide range of abilities and response rates with the best-scoring
                                                                      pairs. Compared with expert review of the associated records,
worker producing an accuracy of 90.5% with respect to a gold          these links had a specificity of 99.6% and a sensitivity of 42.8%.
standard on41000 HITs. Using majority voting to aggregate the           The success of this early study, conceptual articles that de-
responses from each worker, they achieved an overall accuracy
                                                                      scribe similar patterns (Hernandez-Chan et al., 2012) and the
of 83.8% across all 1733 candidate gene mutation relationships        continued increase in adoption of electronic health record sys-
presented for verification. Finally, Zhai and colleagues recently
showed that crowdsourcing could be used for detailed processing       tems suggest that this approach will enjoy widespread applica-
                                                                      tion in the biomedical domain. Within bioinformatics, this kind
of the text from clinical trial announcements including the fol-      of workflow sequestration is, so far, most commonly seen in
lowing: annotating named entities, validating annotations from
other workers and identifying linked attributes, such as side ef-     educational settings as described in the next section.

fects of medications (Zhai et al., 2012).
                                                                      2.5   Crowdsourcing and education
2.3.3 Microtask platforms    The AMT was the first and remains        Genome annotation is a crucial activity in bioinformatics and is

the leading microtask market, but there are a variety of other        one that requires extensive human labor. With an ever-increasing
platforms emerging (Table 1). In addition, meta-services like         supply of genomes to annotate, there is an effectively infinite
Crowdflower help to address standard problems in microtask            amount of work to accomplish and, as this work is non-trivial,

markets, such as spammer identification, worker rating and re-        a need to train large numbers of students to accomplish it.
sponse aggregation. From the task-requestor perspective, the          Killing two birds with one stone, a number of annotation



                                                                                                                                  1927
B.M.Good and A.I.Su
















                                                        ClickoCorkweofr:sirrewt,Cerssedsocrege Kaggle
                                Tools/platforms                                    None     Innocentive, TopCoder,









                                                                                                     ecreasing number of individual tasks that the system must

                                                                                      function contest creators
                                Primoanrtyroqluality                R&A                      None


                                                                                                          innovation contests, but are not listed explicitly because they are not a
                                                                                                       or megatasks like algorithm development. The table describes the dominant






                                                                       tasimopfrtesceal
                                   incentivee  Fun Money          R&AompR&A R&Aanother     Bossa, PyBossaforms: Mechanical Turk,nnotathon.org















                                                                                                     from the top down, in roughly increasing order of difficulty per task and d

                                            Galaxy Zoo)ification (e.g. Phylo, MOLT)or colons (e.g.DuoLingo)EteRNA)
                                         Image classification (e.g. CellSlider,gennotation linking drugsoce, automatic scoring functions could certainly be applied in some





                                                                                                       sks of ReCAPTCHA, whereas the most difficult are the innovation contests f









                                                                                                          t cannot be exhaustive. For exampl



                                            high-task volumeme of work target population needs to executereward for solution.
                                                     Access to sufficient funds for requiredAccess to sufficient resources to provide







                                                        market                                 contest
                                                                                                     Redundancy and aggregation. Types of crowdsourcing systems are displayed,
                                                                                                     ¼
                        Crowdsourcing systems

                                                                                                     : R&A

                        Table 1.Task clasMs icoystem tyVpelunteeronditionTsaskhsrefineresptrotegeneraNosolvceruateisteilneqosirypoeifisceaceaccrsa,taeultgentiaorgiroetsaidentified thus far.




1928
                                                                                                    Crowdsourcing for bioinformatics



projects have incorporated the annotation of new sequences dir-      via both direct manipulation (dragging and twisting pieces of the

ectly into the curriculum of undergraduate courses (Hingamp          protein) and through the execution of small optimization algo-
et al., 2008). Using standard crowdsourcing mechanisms, redun-       rithms like ‘wiggle’. Importantly, these training levels abstract
dancy and aggregation, as well as review by expert curators,         the complex mechanics of protein folding into concepts that

these initiatives have generated thousands of high-quality anno-     are accessible to lay game players.
tations (Bristeret al., 2012).                                         Since its inception in 2008, Foldit has captured the attention of

  From both a social and an economic perspective, this ap-           hundreds of thousands of players, some of whom have achieved
proach has the elegant property of simultaneously accomplishing      remarkable scientific successes. Foldit players have outperformed
the desired work and generating the capital needed to pay the        some of the world’s best automated structure prediction systems

workers. In this case, the capital is the knowledge that they are    and aided in the solution of an important retroviral structure
acquiring by interacting with the system. In contrast to other       that had eluded specialists for decades (Khatib et al., 2011b).
approaches such as the forced labor of ReCAPTCHA, which              In addition to solving naturally occurring protein structures,

may be considered a nuisance or even an exploitation, offering       players have recently succeeded in optimizing the design of en-
education on a topic of interest appears to be a much more fair      gineered enzymes to achieve specific physicochemical goals
exchange. The startup company DuoLingo (founded by the cre-
                                                                     (Eiben et al., 2012).
ator of ReCAPTCHA) now uses this pattern on a massive scale            Although these individual successes are impressive, the greater
by helping millions of students learn foreign languages while        challenge remains to devise algorithms that fold proteins auto-

simultaneously harvesting their efforts to translate web docu-       matically. In addition to the visually oriented puzzle interface,
ments (http://duolingo.com).                                         Foldit introduced a scripting system that allows players to com-
                                                                     pose automated workflows. These scripts string together multiple

                                                                     optimization widgets and may be used in combination with
3   CROWDSOURCING MEGATASKS                                          direct manipulation. In one of the most intriguing developments
                                                                     from this initiative, Foldit players used the provided scripting
In addition to rapidly completing large volumes of simple tasks,
different incarnations of the crowdsourcing paradigm can be          interface to collaboratively write folding algorithms that rival
applied to solve individual tasks that might take weeks or even      professionally designed solutions (Khatib et al., 2011a).

months of expert-level effort to complete. In these cases, the goal    Following directly from Foldit’s success, some of Foldit’s cre-
is to use crowdsourcing to seek out and enable the few talented      ators have released a new game called EteRNA (http://eterna.
individuals from a large candidate population that might,            cmu.edu). In EteRNA, the goal is to design an RNA molecule

through the heterogeneous skills and perspectives that they pro-     that will fold into a particular predefined shape. Design contests
vide, be able to solve problems that continue to stymie trad-        are run every week, and the best designs are evaluated in the
                                                                     laboratory providing real-world feedback. This connection be-
itional research organizations. This shift from high-volume
tasks to high-difficulty tasks affords different requirements for    tween the gamer community and the scientists behind the game
successful crowdsourcing. Two approaches that have generated         has proven effective in recruiting tens of thousands of players—
                                                                     including a few star players that are not only producing valuable
impressive successes in bioinformatics are hard games and innov-
ation contests.                                                      new designs but are also identifying new rules of RNA behavior
                                                                     (Koerner, 2012).

3.1   Hard games                                                       Although much is made of the numbers of players to access
                                                                     these games, it is important to realize that only a small fraction
In contrast to casual games like MalariaSpot that are designed to    of these players contribute directly to any important advance.
complete large volumes of microtasks, the games discussed here
                                                                     These games are portals for recruiting, engaging and enabling a
provide players with access to small numbers of extremely chal-      small number of people with exceptional skills who would never
lenging individual problems. Although casual games tend toward       normally have the opportunity to help solve these problems. In
what the gaming community describes as ‘grinding’, where the
                                                                     essence, these games are as much about discovering latent scien-
players perform highly repetitive actions, hard games provide        tists as they are about making scientific discoveries (Good and
rich interactive environments that promote long-term explor-         Su, 2011).
ation and engagement with a challenge. Thus far, two such
                                                                       Most of the players are not active scientists by trade and typ-
games have been successful in bioinformatics, Foldit and             ically have little to no formal training. Although most do not
EteRNA.
                                                                     contribute directly to solutions, a few bring a different perspec-
  In Foldit, the goal of most games (or puzzles) is typically to     tive that opens up an entirely new way of looking at and solving
find the 3D conformation of a given protein structure that results   the problem. Such a diversity of human intelligence, if filtered
in the lowest calculated free energy (Cooper et al., 2010). To
                                                                     and aggregated effectively, is a powerful and much sought-after
achieve this goal, players interact with a rich desktop game en-     force.
vironment that builds on the Rosetta structure prediction tool
suite (Rohl et al., 2004). In contrast to casual games in which
                                                                     3.2   Open innovation contests
players can play (and contribute solutions) within minutes,
Foldit players must first advance through an extensive series of     Open innovation contests define particular challenges and invite
training levels that can take several hours to complete. These       anyone in the general public to submit candidate solutions. The

introductory levels systematically introduce increasingly complex    solutions are evaluated and if they meet the defined criteria,
game features that allow players to manipulate protein structures    the best solutions are rewarded with cash prizes. The prizes



                                                                                                                                1929
B.M.Good and A.I.Su



and the social prestige garnered by winning a large public contest     A variety of contests exist in the academic sphere, such as the

provide the key incentives driving participation in these            long-running Critical Assessment of Protein Structure Prediction
initiatives.                                                         (CASP) for protein structure prediction and the recent series of
  First pioneered by Innocentive, a 2001 spinoff of Eli Lilly
                                                                     challenges in systems biology operated by the Dialogue for
meant to improve its research pipeline, a variety of platforms       Reverse Engineering Assessments and Methods (DREAM) ini-
for operating these contests have recently emerged. Within bio-      tiative (Marbach et al., 2012). For the most part, these contests

informatics, key open innovation platforms include Innocentive       remain distinct from other innovation contests in that they focus
(which is used on a wide variety of tasks), TopCoder (for soft-      on recruiting submissions specifically from academics, using sci-
ware development and algorithm design) and Kaggle (for data          entific publications as one form of incentive.

analysis).
  As with games, contests make it possible to let enormous num-
bers of potential ‘solvers’ try out their unique abilities on the
                                                                     4   DISCUSSION
specified problem. In contrast to games, which require extensive,    Here, we presented a series of success stories where different
costly development time before any possible reward from the
community might be attained, the up-front cost of running an         forms of crowdsourcing were successfully applied to address
                                                                     key problems in bioinformatics. It is worth noting that crowd-
innovation contest is comparatively small. If no one solves the
posted problem, little is lost by the problem poster. Further,       sourcing is not a panacea. Although it is difficult to find pub-
financial incentives are far easier to tune than game mechanics.     lished examples of crowdsourcing failures in science, clearly not
                                                                     all attempts will succeed. For example, only 57% of Innocentive
The harder and more important the problem is, the bigger the
offered bounty for its solution. Common prizes range from a few      challenges were successfully solved in 2011 (up from 34% in
                                                                     2006) (Spradlin 2012), many attempts to draw in volunteer
thousand dollars for small coding challenges that can be accom-      crowds fail (notably among scientific wikis) and attempts to
plished by individuals in their spare time to million-dollar con-
tests that can require large project teams and/or long-term time     use the Mechanical Turk often face challenges associated with
                                                                     spammers or poorly performing workers. In our own unpub-
commitments.
  Many successes in bioinformatics have already been attained        lished research, we have struggled to find ways to map problems
at the lower end of the prize spectrum. As an example, Lakhani       in bioinformatics to representations that are suitable for
                                                                     gamification. The challenge of successfully orchestrating a scien-
and colleagues recently assessed the potential of the TopCoder
platform on a difficult sequence alignment problem (Lakhani          tific crowdsourcing initiative should not be underestimated. Yet
et al., 2013). To test the hypothesis that ‘big data biology is      the successes described above provide ample evidence that, in
                                                                     many cases, these approaches are worth consideration. As
amenable to prize-based contests’, they posted a challenge related
to immune repertoire profiling on TopCoder with a prize pool of      noted by Innocentive president Dwayne Spradlin, the primary
just $6000. In the 2 weeks that the contest was run, 733 people      challenge to successfully applying crowdsourcing is really in

participated and 122 submitted candidate solutions. In compari-      choosing the right problem for the crowd to solve (Spradlin
son with one prior ‘industry standard’ (NCBI’s MegaBlast), 30        2012). In the next section, we provide a guide for matching prob-
                                                                     lems to potential crowdsourcing-driven solutions, noting both
of the submitted solutions produced more accurate alignments
and all ran substantially faster. None of the participants in the    plusses and minuses associated with each system.
competition was a professional computational biologist, with

most describing themselves as software developers. In addition       4.1   Choosing a crowdsourcing approach
to this academic study, industry representatives report extensive
use of these small-scale coding competitions as part of their bio-   Although diverse in their implementations and goals, the crowd-
                                                                     sourcing systems described in this review each attempt to ad-
informatics research and development pipelines (Merriman et al.,
2012).                                                               vance science by enabling the overwhelming majority of people
  At the upper end of the prize spectrum, one of the first suc-      who reside outside of the ivory tower to participate in the process

cessful million-dollar contests led to the discovery of a novel      (Cooper, 2013). How this process unfolds—how well it solves the
biomarker for amyotrophic lateral sclerosis (Talan, 2011).           problems at hand and how it influences the participants—de-
                                                                     pends deeply on the nature of each problem and the approach
Currently, groups such as Life Technologies and the U.S.
Government’s Defense Threat Reduction Agency (DTRA) are              taken by system architects. Although the diversity of potential
running million-dollar contests for development of novel sequen-     tasks in bioinformatics renders a global rubric for composing
                                                                     crowdsourcing solutions unlikely, the examples presented in
cing technologies and organism detection from complex mixtures
of DNA sequence, respectively.                                       this review and organized in Table 1 suggest some general guide-
  These examples highlight the potential of open innovation          lines (Fig. 1).

contests to focus the attention of large numbers of talented           Crowdsourcing generally begins where automation fails. Tasks
people on solving particular challenging problems. These systems     that can be automated generally should be, and workers should
offer solution seekers with an approach that can be highly cost      be focused on tasks that extend the reach of current computa-

effective in recruiting such talent. As an example, Lakhani and      tional approaches (Kawrykow et al., 2012). As such, the first
colleagues estimate that contest participants spent ▯2684 hours      question to answer when deciding how or if crowdsourcing
working on their problem. Given a 2-week time period and a           may be useful is ‘what tasks (or subtasks) of the larger problem

total cost of $6000, this is a remarkable amount of skilled labor    can currently be solved computationally and which cannot’?
and an incredibly short amount of time.                              Once the tasks that require human abilities are defined, use the



1930
                                                                                                   Crowdsourcing for bioinformatics





























Fig. 1. Crowdsourcing decision tree. When considering a crowdsourcing approach, work through the tree from the top left to identify approaches that
may suit your particular challenge. In many cases there might not be a known crowdsourcing approach that is suitable



following (summarized in Fig. 1) to identify crowdsourcing sys-     Foldit’s free energy calculation, that can link performance in

tems that may be suitable.                                          the game directly to the problem under study. Without such
  Highly granular, repetitive tasks such as image classification    mapping, it will be difficult to provide the players with the feed-
can be approached via volunteer initiatives, casual games, work-    back they need to learn the problem space and thus become

flow sequestration and microtask markets. Games and direct          effective solvers.
volunteer labor are of most value when the number of required          Looking forward, the didactic division used here between sys-
                                                                    tems for completing microtasks and those for solving megatasks
tasks is exceedingly large—too large to pay workers even small
amounts per unit of work. The downsides of depending on vol-        will likely be blurred as new integrated systems arise that take
unteers or game players are that there is no guarantee that they    advantage of key aspects of multiple forms of crowdsourcing

will generate the required amount of labor, and nearly all of the   (Bernstein, 2012). The emergent community-driven processes
potentially substantial cost of building the crowdsourcing solu-    that gave rise to Wikipedia offer some hints at what such
                                                                    future systems might look like (Kittur and Kraut, 2008). Such
tion (the game, the web site) must be paid up-front before any
possible benefit is attained. Depending on the task, workflow       systems will have to promote the rapid formation of extended
sequestration can be a powerful approach, as it not only effect-    communities of participants that display a wide variety of skills
                                                                    and proclivities who come together to achieve a common high-
ively forces the required labor but can also be used to target
specific populations of workers. The downside is that the align-    level goal. For the moment, such problem-solving communities
                                                                    remain difficult to generate and to sustain. But, as the science of
ment of workflows with microtasks will likely not be possible in
many cases. Finally, microtask markets have the benefit of offer-   crowdsourcing advances, it will be increasingly possible for
ing system designers with an immediate workforce of massive         system architects to guide these collective intelligences into exist-
                                                                    ence (Kittur et al., 2011).
scale and precise control of the nature and volume of their activ-
ities. The main negative aspect of microtask markets is that, be-
cause of the per-unit cost of the work, they do not have the        4.2   Related systems

capacity to scale up in the way that the other forms do.            Here, we focused only on crowdsourcing approaches that are
  When it comes to megatasks involving extended work and
                                                                    specifically relevant to common problems in bioinformatics.
specialized skills, innovation contests and hard games can be       For broader reviews, see ‘Crowdsourcing systems on the world
considered. Among these, innovation contests are by far the         wide web’ (Doan et al., 2011), ‘Human computation: a survey
most popular and generalizable framework. These systems
                                                                    and taxonomy of a growing field’ (Quinn and Bederson, 2011)
have repeatedly produced solutions to difficult problems in a       and ‘Crowd-powered systems’ (Bernstein, 2012).
variety of domains at comparatively tiny costs, and we expect          Within   bioinformatics,  two   other   important   emerging

their use to continue to expand. Hard games, like Foldit, are       approaches that depend on the crowd, but not the crowd’s intel-
fascinating for the potential scale, diversity and collaborative    ligence, are distributed computing and online health research.
capacity of the gamer/solver population; however, these benefits
                                                                    Systems like Rosetta@home and the more-general purpose
are not guaranteed and come at a high up-front cost in develop-     Berkeley   Open    Infrastructure   for  Network     Computing
ment time. Furthermore, it simply may not be possible to gamify     (BOINC) use the spare cycles of thousands of personal com-

many important tasks. The tasks most suited to approaches with      puters to advance research in bioinformatics, particularly protein
hard games are those that have scoring functions, such as           folding and docking simulations (Sansom, 2011). In the medical



                                                                                                                               1931
B.M.Good and A.I.Su




domain, the term crowdsourcing is often used to describe large-         Consider protein folding. Foldit changed the number of people
scale patient data collection through online surveys. Personal          thinking about and working on protein-folding problems from

genomics companies, such as 23andme, have surveyed their gen-           perhaps a few thousand to hundreds of thousands. Consider also
otyped ‘crowd’ to enable many new discoveries in genetics               the new phenomenon of ‘crowdfunding’ (Wheat et al., 2013).

(Do et al., 2011; Tung et al., 2011). In addition, a variety of         Now members of the public, not just members of government
initiatives have begun exploring the crowdsourcing of both pa-          grant review panels, have a vote in what science is funded.

tient-initiated and researcher-initiated (non-clinical) patient            The majority of Foldit players will not directly contribute to
trials. Such ‘crowdsourced health research’ is an important and         an important advance, but some will. Perhaps, more import-

growing area, but conceptually distinct from the crowdsourcing          antly, Foldit players and contributors to the various other
applications considered here. For a recent survey of the literature     crowdsourcing initiatives discussed here are much more cogni-
on this topic, see Swan (2012).
                                                                        zant of these scientific problems than they ever were before. If
                                                                        fostered effectively by system architects, a new crowdsourcing-
                                                                        generated awareness will improve how the general public per-
4.3   Social impact
                                                                        ceives science and will affect how they vote and how they
While we have focused primarily on the economic aspects of              encourage future generations.
crowdsourcing, kinds of work and cost, there is another aspect
                                                                           Taken together, the different manifestations of the crowdsour-
that is important to consider. Crowdsourcing is not just a new          cing paradigm open up many new avenues for scientific explor-
way of performing difficult computations rapidly and inexpen-
                                                                        ation. From the high-throughput annotation of millions of
sively; it represents a fundamental change in the way that scien-       images, to the one-off introduction of a novel twist on RNA
tific work is distributed within society. Recalling the original
definition, crowdsourcing is a shift from work done in-house            structure design by a librarian, these new systems are expanding
                                                                        scientific problem-solving capacity in unpredictable ways. To
to work done in the open by anyone that is able. This means
not only that we can often solve more problems more efficiently,        take advantage of these new ways of accomplishing work takes
                                                                        both openness and, in some cases, some amount of humility. The
but also that different people are solving them. As a result, there
are both ethical concerns about worker exploitation that must be        scientific community must be willing to share our greatest prob-
                                                                        lems and step aside to let others help us solve them.
addressed and novel opportunities for societal side benefits that
are important to explore.

  Some have expressed concern for the well-being of players of
scientific crowdsourcing games (Graber and Graber, 2013), and           ACKNOWLEDGEMENTS

it is reasonable to ask about the morality of forcing hundreds of       Thanks to Hassan Masum, Sebastien Boisvert, Jacques Corbeil,
millions of people to solve ReCAPTCHAs to go about their                Mark Wilkinson, Attila Csordas and Twitter correspondents for
daily work. However, the majority of worry about real exploit-
                                                                        helpful comments on an early draft of this manuscript.
ation is related to the workers in microtask markets. In some
cases, people spend significant amounts of time earning wages           Funding: NIH grants GM083924 and GM089820 to A.S.

that amount to5$2/hour (Fort et al., 2011). Although problem-           Conflict of Interest: none declared.
focused, resource-strapped researchers may rejoice at the oppor-

tunity to address the new scientific questions that this workforce
makes possible, it is both socially responsible and vital for long-
                                                                        REFERENCES
term success to remain aware that there are people at the other
end of the line completing these tasks. In fact many of the newer       Ahn,L.V. and Dabbish,L. (2004) Labeling images with a computer game.
crowdsourcing companies, e.g. MobileWorks, now make worker                 Proceedings of the 2004 SIGCHI Conference on Human Factors in Computing

conditions a top priority with guaranteed minimum wages and                Systems. ACM Press, New York, NY, USA, pp. 319–326.
opportunity for advancement within their framework. Keeping             Ahn,L.V. and Dabbish,L. (2008) Designing games with a purpose. Commun. ACM,
                                                                           51, 58–67.
worker satisfaction in mind should not only help encourage fair         Ahn,L.V. et al. (2008) reCAPTCHA: Human-Based Character Recognition via
treatment but will also help designers come up with more effect-           Web Security Measures. Science, 321, 1465–1468.
                                                                        Bernstein,M.S. (2012) Crowd-powered systems. In: Electrical Engineering and
ive crowdsourcing solutions. Paying workers well, building up
long-term relationships with them and providing tasks that                 Computer  Science.MassachusettsInstituteof Technology, Cambridge,
                                                                           Massachusetts, Ph.D. Dissertation.
may provide them with benefits aside from any direct per-task           Brister,J.R. et al. (2012) Microbial virus genome annotation-Mustering the troops
reward in fun or money not only makes for a happier workforce              to fight the sequence onslaught. Virology, 434, 175–180.
                                                                        Burger,J.et al. (2012) Validating candidate gene-mutation relations in MEDLINE
but also makes for a far more powerful one (Kochhar et al.,
2010). While much is made of the power of our visual system                abstracts via crowdsourcing. In: Bodenreider,O. and Rance,B. (eds) Data
in the context of crowdsourcing, our ability to learn is what              Integration in the Life Sciences. Springer, Berlin; Heidelberg, pp. 83–91.
                                                                        Clery,D. (2011) Galaxy evolution. Galaxy zoo volunteers share pain and glory of
separates us from the rest of the animal kingdom. Tapping                  research. Science, 333, 173–175.
into this innate ability and our strong desire to use it will produce   Cohn,J.P. (2008) Citizen science: can volunteers do real research? BioScience, 58,

crowdsourcing systems that not only solve scientific problems              192.
more effectively but, in the process, will end up producing             Cooper,C. (2013) The most stressful science problem. Scientific American Blog,
                                                                           http://blogs.scientificamerican.com/guest-blog/2013/01/10/the-most-stressful-
many more scientifically literate citizens.                                science-problem/ (30 June 2013, date last accessed).
  Before crowdsourcing models started to appear, only a small           Cooper,S.et al. (2010) Predicting protein structures with a multiplayer online game.

fraction of society had a direct input into the advance of science.        Nature, 466, 756–760.



1932
                                                                                                                                  Crowdsourcing for bioinformatics




Do,C.B.et al. (2011) Web-based genome-wide association study identifies two novel         Marbach,D. et al. (2012) Wisdom of crowds for robust gene network inference. Nat.

    loci and a substantial genetic component for Parkinson’s disease. PLoS Genet.,           Methods, 9, 796–804.
    7, e1002141.                                                                          Mavandadi,S. et al. (2012a) Distributed medical image analysis and diagnosis

Doan,A. et al. (2011) Crowdsourcing systems on the world-wide web. Commun.                   through crowd-sourced games: a malaria case study. PloS One, 7,e37245.
    ACM, 54,86.                                                                           Mavandadi,S. et al. (2012b) Crowd-sourced BioGames: managing the big data

Eiben,C.B. et al. (2012) Increased Diels-Alderase activity through backbone                  problem for next-generation lab-on-a-chip platforms. Lab Chip, 12, 4102–4106.
    remodeling guided by Foldit players. Nat. Biotechnol., 30, 190–192.                   McCoy,A.B. et al. (2012) Development and evaluation of a crowdsourcing meth-

Fort,K. et al. (2011) Amazon mechanical turk: gold mine or coal mine? Comput.                odology for knowledge base construction: identifying relationships between
    Ling., 37, 413–420.                                                                      clinical problems and medications. JAMIA, 19, 713–718.

Galperin,M.Y. and Fernandez-Suarez,X.M. (2012) The 2012 nucleic acids research            Merriman,B. et al. (2012) Progress in ion torrent semiconductor chip based sequen-
    database issue and the online molecular biology database collection. Nucleic             cing. Electrophoresis, 33, 3397–3417.

    Acids Res., 40,D1–D8.                                                                 Nguyen,T.B. et al . (2012) Distributed human intelligence for colonic polyp classifi-
Good,B. and Su,A. (2011) Games with a scientific purpose. Genome Biol., 12,135.              cation in computer-aided detection for CT colonography. Radiology, 262,

Graber,M.A. and Graber,A. (2013) Internet-based crowdsourcing and research                   824–833.
    ethics: the case for IRB review. J. Med. Ethics, 39, 115–118.                         Quinn,A.J. and Bederson,B.B. (2011) Human computation: a survey and taxonomy
Hernandez-Chan,G. et al. (2012) Knowledge acquisition for medical diagnosis using
                                                                                             of a growing field. In: CHI ‘11 SIGCHI Conference on Human Factors in
    collective intelligence. J. Med. Syst., 36,5–9.                                          Computing Systems. ACM Press, NY, USA, pp. 1403–1412.
Hingamp,P. et al. (2008) Metagenome annotation using a distributed grid of under-
                                                                                          Rohl,C.A. et al. (2004) Protein structure prediction using Rosetta. Methods
    graduate students. PLoS Biol., 6,e296.                                                   Enzymol, 383,66–93.
Howe,J. (2006) The Rise of Crowdsourcing. Wired, available at http://www.wired.
                                                                                          Sabou,M. et al. (2012) Crowdsourcing research opportunities. In: Proceedings of the
    com/wired/archive/14.06/crowds.html (30 June 2013, date last accessed).                  12th International Conference on Knowledge Management and Knowledge
Kawrykow,A.et al. (2012) Phylo: a citizen science approach for improving multiple
                                                                                             Technologies—i-KNOW ‘12. ACM Press, NY, USA, p. 1.
    sequence alignment. PloS One, 7,e31362.                                               Sansom,C. (2011) The power of many. Nat. Biotechnol., 29, 201–203.
Khatib,F.et al. (2011a) Algorithm discovery by protein folding game players. Proc.
                                                                                          Snow,R. et al. (2008) Cheap and fast—but is it good?: evaluating non-expert an-
    Natl Acad. Sci. USA, 108, 18949–18953.                                                   notations for natural language tasks. In: Proceedings of the 2008 Conference on
Khatib,F.et al. (2011b) Crystal structure of a monomeric retroviral protease solved
                                                                                             Empirical   Methods    in  Natural   Language    Procenrsoisfsianigco.sA
    by protein folding game players. Nat. Struct. Mol. Biol., 18, 1175–1177.                 Computational Linguistics, Stroudsburg, PA, USA, pp. 254–263.
Kim,J.D.et al. (2003) GENIA corpus—semantically annotated corpus for bio-text-
                                                                                          Swan,M. (2012) Crowdsourced health research studies: an important emerging com-
    mining. Bioinformatics, 19, i180–i182.                                                   plement to clinical trials in the public health research ecosystem. J. Med. Internet
Kittur,A. and Kraut,R.E. (2008) Harnessing the wisdom of crowds in wikipedia:
                                                                                             Res., 14,e46.
    quality through coordination. In: Proceedings of the 2008 ACM conference on
    Computer supported cooperative work. ACM, San Diego, CA, USA, pp. 37–46.              Talan,J. (2011) A million dollar ideaﬄ potential biomarker for ALS. Neurology
                                                                                             Today, 11,1.
Kittur,A. et al. (2011) CrowdForge: crowdsourcing complex work. In: Proceedings
    of the 24th annual ACM symposium on User interface software and technology.           Tung,J.Y. et al. (2011) Efficient replication of over 180 genetic associations with
                                                                                             self-reported medical data. PloS One, 6, e23473.
    ACM, Santa Barbara, California, USA, pp. 43–52.
Kochhar,S.et al. (2010) The anatomy of a large-scale human computation engine.            Wang,S. et al. (2011) Fusion of machine intelligence and human intelligence for
                                                                                             colonic polyp detection in CT colonography. In: 2011 IEEE International
    In: Proceedings of the ACM SIGKDD Workshop on Human Computation.ACM,
    Washington DC, pp. 10–17.                                                                Symposium on Biomedical Imaging: From Nano to Macro. IEEE, Chicago,
                                                                                             Illinois, USA, pp. 160–164.
Koerner,B.I. (2012) New videogame lets amateur researchers mess with RNA.
    Wired Science, available at http://www.wired.com/wiredscience/2012/07/ff_             Wheat,R.E. et al. (2013) Raising money for scientific research through crowdfund-
                                                                                             ing. Trends Ecol. Evol., 28, 71–72.
    rnagame/ (30 June 2013, date last accessed).
Lakhani,K.R.et al. (2013) Prize-based contests can provide solutions to computa-          Yetisgen-Yildiz,M. et al. (2010) Preliminary experience with amazon’s mechanical
                                                                                             turk for annotating medical named entities. In: CSLDAMT ‘10 Proceedings of
    tional biology problems. Nat. Biotech., 31, 108–111.
Lintott,C.J. et al. (2008) Galaxy Zoo: morphologies derived from visual inspection           the NAACL HLT 2010 Workshop on Creating Speech and Language Data with
                                                                                             Amazon’s Mechanical Turk. Association for Computational Linguistics,
    of galaxies from the sloan digital sky survey. Mon. Not. R. Astron. Soc., 389,
    1179–1189.                                                                               Stroudsburg, PA, USA, pp. 180–183.
                                                                                          Zhai,H. et al. (2012) Cheap, fast, and good enough for the non-biomedical domain
Little,G. et al. (2010) TurKit. In: Proceedings of the 23nd annual ACM symposium on
    User interface software and technology—UIST ‘10. ACM Press, NY, USA, p. 57.              but is it usable for clinical natural language processing? Evaluating crowdsour-
                                                                                             cing for clinical trial announcement named entity annotations. In: 2012 IEEE
Luengo-Oroz,M.A.et al. (2012) Crowdsourcing malaria parasite quantification: an
    online game for analyzing images of infected thick blood smears. J. Med.                 Second International Conference on Healthcare Informatics, Imaging and Systems
    Internet Res., 14,e167.                                                                  Biology. IEEE, La Jolla, California, USA, pp. 106–106.





































                                                                                                                                                                      1933