                  0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



             ICT Seventh Framework Programme (ICT FP7)



                      Grant Agreement No: 288828

       Bridging Communities for Next Generation Policy-Making











International Research Roadmap on ICT Tools for

            Governance and Policy Modelling


                          Interim Version



                           Internal Deliverable Form
     Project Reference No. ICT FP7 288828

          Deliverable No. D2.2.1

   Relevant Workpackage: WP2

                 Nature: Report

      Dissemination Level: Restricted

       Document version: FINAL V1
                   Date: 16/07/2012

                Authors: David Osimo & Francesco Mureddu (T4I2), Riccardo Onori &
                         Stefano Armenia (CATTID)

              Reviewers: Stefano Armenia (CATTID), Gianluca Carlo Misuraca (IPTS), Andrea
                         Bassi (MI)

    Document description:This deliverable describes the interim version of the new
                         International Research Roadmap on ICT Tools for Governance and
                         Policy Modelling
                         0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




                                            History


Version                 Date                      Reason                  Revised by

1.0                     29/06/2012                1 Version               CATTID-T4I2

                                                   nd
1.1                     02/07/2012                2 Version               CATTID-T4I2

                                                  2nd Version sent for
                        02/07/2012                                        ATC
                                                  Peer Review

                        05/07/2012                Contribution            CATTID, IPTS, MI

                                                   rd
1.2                     13/07/2012                3   Version sent for    T4I2
                                                  final confirmation

                                                                          ATC, CATTID, IPTS, MI,
                        16/07/2012                Partners’ Approval      W3C

                                                  Final Version sent to
1.3                     16/07/2012                                        ATC
                                                  the PO






































                                                                                     2 | P a g e
                                    0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



TABLE OF CONTENTS

LIST OF FIGURES...............................................................................................................................................4

EXECUTIVE SUMMARY.....................................................................................................................................5

1.    BACKGROUND..........................................................................................................................................6

2.    METHODOLOGY .......................................................................................................................................8

   2.1.     Definition................................................................................................................................................9

3.    THE DEMAND SIDE OF POLICY-MAKING 2.0 ...........................................................................................13

   3.1.     The job of policy-makers: the policy cycle............................................................................................13

   3.2.     The traditional tools of policy-making..................................................................................................15

   3.3.     The key challenges of policy-makers ....................................................................................................15

      3.3.1.      Detect and understand problems before they become unsolvable.............................................16

      3.3.2.      Generate high involvement of citizens in policy-making..............................................................16

      3.3.3.      Identify “good ideas” and innovative solutions to long-standing problems ................................17

      3.3.4.      Reduce uncertainty on the possible impacts of policies...............................................................17

      3.3.5.      Ensure long - term thinking ..........................................................................................................20

      3.3.6.      Encourage behavioural change and uptake..................................................................................20

      3.3.7.      Manage crisis and the “unknown unknown”................................................................................21

      3.3.8.      Moving from conversations to action...........................................................................................21

      3.3.9.      Detect non-compliance and mis-spending through better transparency ....................................22

      3.3.10.     Understand the impact of policies................................................................................................22

4.    THE RESEARCH CHALLENGES..................................................................................................................23

   4.1.     Policy Modelling....................................................................................................................................23

      4.1.1.      Systems of Atomized Models........................................................................................................23

      4.1.2.      Collaborative Modelling................................................................................................................28

      4.1.3.      Easy Access to Information and Knowledge Creation ..................................................................38

      4.1.4.      Model Validation ..........................................................................................................................40

      4.1.5.      Immersive Simulation...................................................................................................................43

      4.1.6.      Output Analysis and Knowledge Synthesis...................................................................................45

   4.2.     Data-powered Collaborative Governance ............................................................................................48

      4.2.1.      Big Data.........................................................................................................................................48

      4.2.2.      Opinion Mining and Sentiment Analysis.......................................................................................63

      4.2.3.      Visual Analytics .............................................................................................................................70

      4.2.4.      Serious Gaming for Behavioural Change ......................................................................................78

      4.2.5.      Open Government Data................................................................................................................83

      4.2.6.      Collaborative Governance ............................................................................................................88

      4.2.7.      Participatory Sensing ....................................................................................................................92




                                                                                                                         3 | P a g e
                                     0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


      4.2.8.      Identity Management...................................................................................................................96

5.    CONCLUSIONS: CLOSING THE LOOP OF POLICY-MAKING 2.0..................................................................99

6.    REFERENCES.........................................................................................................................................103




LIST OF FIGURES

Figure 1: Outline of the Process .............................................................................................................................8

Figure 2: Policy Cycle and Related Activities ........................................................................................................14
Figure 3: Total Disasters Reported .......................................................................................................................21

Figure 4: Agricultural Production and Externalities Simulator (APES)..................................................................25

Figure 5: Conversational Modelling Interface ......................................................................................................32
Figure 6: The PADGET Framework........................................................................................................................32

Figure 7: The Time-Space Matrix..........................................................................................................................34

Figure 8: COMA, Collaborative Modelling Architecture .......................................................................................35
Figure 9: OCOPOMO eParticipation Platform.......................................................................................................36

Figure 10: Twitrratr...............................................................................................................................................66
Figure 11: Wordclouds..........................................................................................................................................67

Figure 12: UserVoice.............................................................................................................................................67

Figure 13: CROSSROAD’s Approach......................................................................................................................99
Figure 14: Relation Between Policy-Making Needs and Research Challenges...................................................100









































                                                                                                                            4 | P a g e
                          0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



Executive Summary

This deliverable introduces and describes the interim version of the new International Research
Roadmap on ICT tools for Governance and Policy Modelling, one of the core output of the project,
which is developed under WP2 Content Production. The roadmap aim to establish the scientific and

political basis for long-lasting interest and commitment to next generation policy-making, as well as
to provide a clear outline of what technologies are available now for policy-makers to improve their
work, and what could become available in the future.




















































                                                                                       5 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



1.      BACKGROUND

The CROSSOVER project aims to consolidate and expand the existing community on ICT for
Governance and Policy Modelling (built largely within FP7) by:

    -   Bringing together and reinforcing the links between the different global communities of
        researchers and experts: it will create directories of experts and solutions, and animate

        knowledge exchange across communities of practice both offline and online;
    -   Reaching out and raising the awareness of non-experts and potential users, with special

        regard to high-level policy-makers and policy advisors: it will produce multimedia content, a
        practical handbook and high-level policy conferences with competition for prizes;

    -   Establishing the scientific and political basis for long-lasting interest and commitment to next
        generation policy-making, beyond the mere availability of FP7 funding: it will focus on use
        cases and a demand-driven approach, involving policy-makers and advisors in high-level
        conference, defining a collaborative stakeholders’ declaration and developing a

        sustainability plan.
The CROSSOVER project pursues this goal through a combination of content production, ad hoc and

well-designed online and offline animation; as well as strong links with existing communities outside
the CROSSOVER project and outside the realm of e-Government.

The present deliverable, developed under WP2 Content Production, is one of the core outputs of the
project: the International Research Roadmap on ICT Tools for Governance and Policy Modelling. It
aims at providing a common basis and a common set of concepts for researchers in a highly
multidisciplinary context. But most of all, it aims to provide a clear outline of what technologies are

available now for policy-makers to improve their work, and what could become available tomorrow.
CROSSOVER builds on the results of the CROSSROAD project , which elaborated a research roadmap

on the same topic along the whole of 2010. With respect to the previous roadmap, this document is
firstly a revised and updated version. Beside this, it contains some fundamental novelties:

    -   A demand-driven approach: rather than focussing on the technology, the present roadmap
        starts from the needs and the activities of policy-making and then links the research
        challenges to them

    -   An additional emphasis on cases and applications: for each research challenge, we indicate
        relevant cases and practical solutions

    -   A clearer thematic focus on ICT for Governance and Policy-Modelling, by dropping more
        peripheral grand challenges of Government Service Utility and Scientific Base for ICT-
        enabled Governance

    -   A global coverage: while CROSSROAD focussed on Europe, CROSSOVER includes cases and
        experiences from all over the world

    -   A living roadmap: the present deliverable is accompanied by an online repositories of tools,
        people and applications

The document is structured as follows.

Section 2, the methodology for the research roadmap is described.



1
 http://CROSSROAD.epu.ntua.gr/


                                                                                             6 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

Section 3, analyses the current policy-making process, and its’ main needs related to improvement

of evidence base policy; (improvement of quality, speed and participation);

Section 4, describes the proposed research challenges on policy-making 2.0.
Finally, Section 5 sums up the analysis, by linking each research challenge of section 4 to the specific

policy-making task and needs outlined in section 3.

























































                                                                                                 7 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


2.      METHODOLOGY

The present Research Roadmap on Policy-Making 2.0 is developed with a sequential approach based

on the existing research roadmap developed by the CROSSROAD project.
In the first phase of the project, up to M6, the consortium started a collection of literature,

information about software tools and applications cases. The present document starts to summarize
this information in order to update and expand the previous roadmap. In addition to this desk-based
review, the document has benefited from the informal discussions being held on the Linkedin group
of the project (Policy-making 2.0), where more than 200 practitioners and researchers are discussing

the practices and the challenges of policy-making.
The present deliverable represents the start, rather than the end, of the project. The publication of

the deliverable kicks off the engagement activities of the project, designed to provide further input
and to improve the roadmap:

    -   As soon as it is released, the preliminary version of the roadmap will be published in
        commentable format on the project website http://www.CROSSOVER-project.eu/.
        Animators will stimulate discussion about it and generate comments by researchers and
        practitioners alike. This participatory process will help enriching the roadmap, which will be

        published in its final version after validation by the community/ies of practitioners and policy
        makers

    -   Two workshops organised by the project aimed at gathering input on the research
        challenges and feedback on the proposed roadmap     2

    -   An online survey, as well as a focus groups and meetings with practitioners from civil society
        and government will help to focus the roadmap on the actual needs

All the input provided by the online and offline activities will then be included n the final version of
the research roadmap, to be released in M18.





















                                    Figure 1: Outline of the Process



2 The first workshop, “USING OPEN DATA: policy modeling, citizen empowerment, data journalism” took place on 19 - 20

June 2012. A full report will follow as a deliverable of the project


                                                                                           8 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

In particular, the methodological approach integrates demand-driven needs and technology-push

opportunities. It starts from spelling out the key challenges of policy-making 2.0 (based on review of
literature and cases). When it then analyses the research challenges, it highlights how each research
challenges is likely to help meeting these needs. In fact, each research challenge in section 4 is
deliberately linked to a policy-making need.


    2.1.        Definition

Policy-making 2.0 refers to a set of methodologies and technological solutions aimed at innovating
policy-making. As we will describe in section 3.1, the scope goes well beyond the “policy adoption”
notion typical of eParticipation, and encompass all phases of the policy cycle. The main goal is

limited to improving the quality of policies, not of making them more consensual or representative.
Policy-making 2.0 is a new term that we have coined to express in more understandable terms the

somehow technical notion of “ICT for governance and policy modelling”. Its usage in the course of
the project proved more effective than the latter when discussing with stakeholders. Thereby from
now on we will refer to the roadmap as the Research Roadmap on Policy-Making 2.0.

The full set of methodologies and tools has been spelled out in the taxonomy in WP1 :  3

1.1.    Open government information & intelligence for transparency

        1.1.1. Open & Transparent Information Management
                1.1.1.1.Open data policy

                1.1.1.2.Open data licence

                1.1.1.3.Open data portal

                1.1.1.4.Code list

                1.1.1.5.Vocabulary/ontology
                1.1.1.6.Reference data

                1.1.1.7.Data cleaning and reconciliation tool

        1.1.2. Data published on the Web under an open licence

                1.1.2.1.Human-readable data

                1.1.2.2.Machine readable data in proprietary format
                1.1.2.3.Machine-readable data published in a non-proprietary format

                1.1.2.4.Data published in RDF

                1.1.2.5.SPARQL endpoint for querying RDF data

                1.1.2.6.RDF data linked to other data sets

        1.1.3. Visual Analytics
                1.1.3.1.Visualisation of a single, static, embedded data set

                1.1.3.2.Visualisation of multiple static data sets

                1.1.3.3.Visualisation of a single live data feed or updating data set


3
 The taxonomy presented here builds on CROSSROAD taxonomy, which has been expanded, reviewed and updated by the
members of the Consortium


                                                                                             9 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


                1.1.3.4.Visualisation of multiple data points, including live feeds or updates

1.2.    Social computing, citizen engagement and inclusion

        1.2.1. Social Computing
                1.2.1.1.Collaborative writing and annotation

                1.2.1.2.Content syndication

                1.2.1.3.Feedback and reputation management systems

                1.2.1.4.Social Network Analysis

                1.2.1.5.Participatory sensing
        1.2.2. Citizen Engagement

                1.2.2.1.Online deliberation

                1.2.2.2.Argumentation support

                1.2.2.3.Petition, Polling and voting
                1.2.2.4.Serious games

                1.2.2.5.Opinion mining

        1.2.3. Public Opinion-Mining & Sentiment Analysis

                1.2.3.1.Opinion tracking
                1.2.3.2.Multi-lingual and Multi-Cultural opinion extraction and filtering

                1.2.3.3.Real-time opinion visualisation

                1.2.3.4.Collective Wisdom Analysis and Exploitation

1.3.    Policy Assessment
        1.3.1. Policy Context Analysis

                1.3.1.1.Forecasting

                1.3.1.2.Foresight

                1.3.1.3.Back-Casting
                1.3.1.4.Now-Casting

                1.3.1.5.Early Warning Systems

                1.3.1.6.Technology Road-Mapping (TRM)

        1.3.2. Policy Modelling
                1.3.2.1.Group Model Building

                1.3.2.2.Systems Thinking & Behavioural Modelling

                1.3.2.3.System Dynamics

                1.3.2.4.Agent-Based Modelling
                1.3.2.5.Stochastic Modelling

                1.3.2.6.Cellular Automata

        1.3.3. Policy Simulation


                                                                                          10 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


                1.3.3.1.Multi-level & micro-simulation models

                1.3.3.2.Discrete Event Simulation

                1.3.3.3.Autonomous Agents, ABM Simulation, Multi-Agent Systems (MAS)
                1.3.3.4.Virtual Worlds, Virtual Reality & Gaming Simulation

                1.3.3.5.Model Integration

                1.3.3.6.Model Calibration & Validation

        1.3.4. Policy Evaluation

                1.3.4.1.Impact Assessment
                1.3.4.2.Scenarios

                1.3.4.3.Model Quality Evaluation

                1.3.4.4.Multi-Criteria Decision Analysis

1.4.    Identity, privacy and trust in governance
        1.4.1. Identity Management

                1.4.1.1.Federated Identity Management Systems

                1.4.1.2.User centric, self-managed and lightweight credentials

                1.4.1.3.Legal-social aspects of eIdentity management
                1.4.1.4.Mobile Identity (Portability)

        1.4.2. Privacy

                1.4.2.1.Privacy and Data Protection

                1.4.2.2.Privacy Enhancing Technologies
                1.4.2.3.Anonymity and Pseudonymity

                1.4.2.4.Open data management (including Citizen Profiling, 'digital shadow' tracing
                and tracking

        1.4.3. Trust

                1.4.3.1.Legal Informatics

                1.4.3.2.Digital Rights Management
                1.4.3.3.Digital Citizenship Rights and feedback loops

                1.4.3.4.Intellectual Property in the digital era

                1.4.3.5.Trust-building Services (including data processing and profiling by private
                actors for public services)

1.5.    Future internet for collaborative governance

        1.5.1. Cloud Computing
                1.5.1.1.Cloud service level requirements

                1.5.1.2.Business models in the cloud

                1.5.1.3.Cloud interoperability



                                                                                           11 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


                1.5.1.4.Security and authentication in the cloud

                1.5.1.5.Data confidentiality and auditability

                1.5.1.6.Cloud legal implications
        1.5.2. Pervasive Computing & Internet of Things in Public Services

                1.5.2.1.Ambient intelligence

                1.5.2.2.Exploiting smart objects

                1.5.2.3.Standardization

                1.5.2.4.Business models for pervasive technologies
                1.5.2.5.Privacy implications and risks

        1.5.3. Provision of next generation public e-services

                1.5.3.1.Fixed and mobile network access technologies

                1.5.3.2.Mobile web
                1.5.3.3.Models for information dissemination

                1.5.3.4.Management of scarce network capacity and congestion problems

                1.5.3.5.Large-scale resource sharing

                1.5.3.6.Interworking of different technologies for seamless connectivity of users
        1.5.4. Future Human/Computer Interaction Applications & Systems

                1.5.4.1.Web accessibility

                1.5.4.2.User-centered design

                1.5.4.3.Augmented cognition
                1.5.4.4.Human senses recognition



Policy-making 2.0 encompasses clearly a wide set of methodologies and tools. At first sight, it might
appear unclear what the common denominator is. In our view, what they share is that they are
designed to use technology in order to design more effective public policies. In particular, these
technologies share a common approach in taking into account and dealing with the full complexity

of human nature. As spelled out originally in the CROSSOVER project proposal: “traditional
policy‐making tools are limited insofar they assume an abstract and unrealistic human being:
rational (utility maximizing), consistent (not heterogeneous), atomised (not connected), wise
(thinking long‐term) and politically committed (as Lisa Simpson)”. Policy-making 2.0 thus accounts

for this diversity. Its methodologies and tools are designed not to impose change and artificial
structures, rather to interact with this diversity. Agent-based models account for the interaction
between agents that are different in nature and values; systems thinking accounts for long-term
interacting impacts; social network analysis deals with the mutual influences between people rather
than fully rational choices; big data analyses observed behaviour rather than theoretical models;

persuasive technologies deal with the complex psychology of individuals and introduces gaming
values to involve more “casual” participants. Moreover, policy-making 2.0 tools allow all
stakeholders to participate to the decision-making process





                                                                                          12 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



3. The demand side of policy-making 2.0

In the following sections we introduce the needs and activities of policy-making in order to make
sure that the research roadmap meets the real needs of policy-makers. We start by arguing why

policy-making is more complex and more important than ever in nowadays globalized world. We
then describe the typical process of policy-making, and finally analyse the key challenges to the
process. This analysis is based on desk analysis and the on-going survey of users’ needs.

Our first consideration is that policy-making is more important and complex than ever. The role of
government has substantially changed over the last twenty years. Governments have to re-design
their role in areas where they were directly involved in service provision, such as utilities but also

education and health. This is not simply a matter of privatisation, or of a linear trend towards smaller
government. Indeed, even before the recent financial turmoil and nationalisation of parts of the
financial system, government role in the European societies was not simply “diminishing”, but rather
being transformed. At the same time, it is increasingly recognized that the emergence of new and

complex problems requires government to increasingly collaborate wi4h non-governmental actors in
the understanding and in the addressing of these challenges . As an OECD report states the
following:

“Government has a larger role in the OECD countries than two decades ago. But the nature of public
policy problems and the methods to deal with them are still undergoing deep change. Governments
are moving away from the direct provision of services towards a greater role for private and non-
profit entities and increased regulation of markets. Government regulatory reach is also extending in

new socio-economic areas. This expansion of regulation reflects the increasing complexity of
societies. At the same time, through technological advances, government’s ability to accumulate
information in these areas has increased significantly. As government face more new and complex
problems that cannot be dealt with easily by direct public service provision, more ambitious policies

require more complex interventions and collaboration with non-governmental parties”
This is particularly challenging in our "complex" societies. “Complex” systems are those where “the

behaviour of the system as a whole cannot be determined by partitioning it and understanding the
behaviour of each of the parts separately, which is the classic strategy of the reductionist physical
sciences”. The present challenges governments must face, as described by the OECD, are complex as
they are characterised by many non-linear interactions between agents; they emerge from these

interactions and are therefore difficult to predict. The financial crisis is probably the foremost
example of a complex problem, which proved impossible to predict with traditional decision-making
tools.


    3.1.        The job of policy-makers: the policy cycle
Policy-making is typically carried out through a set of activities described as "policy-cycle" (Howard

2005). In this document we propose a new way of implementing policies, by first assessing their
impacts in a virtual environment.

While different versions of the cycle are proposed in literature, in this context we adopt a simple
version articulated in 5 phases:

    -   agenda setting encompasses the basic analysis on the nature and size of problems at stakes
        are addressed, including the causal relationships between the different factors



4
 See Ostrom: http://www.nobelprize.org/nobel_prizes/economics/laureates/2009/ostrom-lecture.html


                                                                                           13 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    -    policy design includes the development of the possible solutions, the analysis of the
         potential impact of these solutions , the development and revision of a policy proposal

    -    adoption is the cut-off decision on the policy. This is the most delicate and sensitive area,
         where accountability and representativeness are needed. It is also the area most covered by

         existing research on e-democracy
    -    implementation is often considered the most challenging phase, as it needs to translate the

         policy objectives in concrete activities, that have to deal with the complexity of the real
         world . It includes ensuring a broader understanding, the change of behaviour and the active
         collaboration of all stakeholders.

    -    Monitoring and evaluation make use of implementation data to assess whether the policy is
         being implemented as planned, and is achieving the expected objectives.

The figure below (authors’ elaboration based on Howard 2005 and EC 2009) illustrates the main

phases of the policy cycle (in the internal circle) and the typical concrete activities (external circle)
that accompany this cycle. In particular, the identified activities are based on the Impact Assessment
Guidelines of the European Commission (EC 2009).




































                                  Figure 2: Policy Cycle and Related Activities



5 A very important element in policy design and formulation is given by ex-ante evaluation. In this respect ICT tools for
policy-making can play an important role, simulating alternative policy options and impacts before implementing a policy
action



                                                                                                  14 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Traditionally, the focus about the impact of technology in policy-making has been on the adoption
phase, analysing the implications of ICT for direct democracy. In the context of the CROSSOVER

project, we adopt a broader conceptual framework that embraces all phases of policy-making.

    3.2.        The traditional tools of policy-making

But what are the methodologies and tools already traditionally adopted in policy-making?

Typically, in the agenda-setting phase, statistics are analysed by government and experts contracted
by government in order to understand the problems at stake and the underlying causes of the
problems. Survey and consultations, including online, are frequently used to assess the stakeholders’
priorities, and typically analyzed in-house. Linear, general-equilibrium models are used to identify
causal relationships between different factors.

Once the problems and its causes are defined, the policy design phase is typically articulated through
an ex-ante impact assessment approach. A limited set of policy options are formulated in house with
the involvement of experts and stakeholders. For each option, econometric and other simulations

are carried out in order to forecast possible sectoral and cross-sectoral impacts. These simulations
are typically carried out by general-equilibrium models if the time frame is focused on short and
medium term economic impacts of policy implementation. Based on the simulated impact, the best
option is submitted for adoption.

The adoption phase is typically carried out by the official authority, either legislative or executive
(depending on the type of policy). In some cases, decision is left to citizens through direct
democracy, through a referendum or tools such as participatory budgeting; or to stakeholders

through self-regulation.
The implementation phase typically is carried out directly by government, using incentives and
coercion. It benefits from technology mainly in terms of monitoring and surveillance, in order to

manage incentives and coercion, for example through the database used for social security or taxes
revenues.

The monitoring and evaluation phase is supported by mathematical simulation studies and analysis
of government data, typically carried out in-house or by contractors. Final results are published in
report format, and fed back to the agenda setting phase.


    3.3.        The key challenges of policy-makers
Needless to say, the current policy-making process is seldom based on objective evidence and not all

views are necessarily represented. Dramatic crisis seem to happen too often, and governments
struggle to anticipate and deal with them, as the financial crisis has shown. Citizens feel a sense of
mistrust towards government, as shown by the decrease in voters turnout in the elections.

In this section, we analyze and identify the specific challenges of policy-making. The goal is to clearly
spell out "what is the problem" that policy-making 2.0 tools can help to solve.

The challenges have been identified on desk-based research of "government failure" in a variety of
contexts, and are illustrated by real-life examples.

One first overarching challenge is the emergence of a distributed governance model. The
traditional division of “market” and “state” no longer fits a reality where public decision and action is
effectively carried out by a plurality of actors. Traditionally, the policy cycle is designed as a set of
activities belonging to government, from the agenda setting to the delivery and evaluation. However
in recent years it has been increasingly recognized that public governance involves a wide range of
stakeholders, who are increasingly involved not only in agenda-setting but in designing the policies,



                                                                                         15 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


adopting them (through the increasing role of self-regulation), implementing them (through
collaboration, voluntary action, corporate social responsibility), and evaluating them (such as in the
case of civil society as watchdog of government). As Elinor Ostrom stated in her lecture delivered
                                              6
when receiving the Nobel Prize in Economics : “A core goal of public policy should be to facilitate the
development of institutions that bring out the best in humans. We need to ask how diverse
polycentric institutions help or hinder the innovativeness, learning, adapting, trustworthiness, levels
of cooperation of participants, and the achievement of more effective, equitable, and sustainable

outcomes at multiple scales”. This acknowledgement leads to important implications for the
CROSSOVER roadmap: policy-making 2.0 tools are not just tools for government, but for all
stakeholders to participate in the policy-making process .7



        3.3.1. Detect and understand problems before they become unsolvable

The continuous struggle for evidence-based policy-making can have some important and potentially
negative implications in terms of the capacity of prompt identification of problems. Policy-makers
have to balance the need for prompt reaction with the need for justified action, by distinguishing

signal from noise. Delayed actions are often ineffective; at the same time, short-term evidence can
lead to opposite effects. In any case, government have scarce resources and need to prioritize
interventions on the most important problems.

For instance the significant underestimation of the risks of the housing bubble in the late 2000s, and
the systemic reaction that it would lead to, led to delayed reactions. The detection of the ozone hole

was delayed because satellite detection instruments were calibrated to consider as "errors"
measurements outside a certain boundary; it turned out that correct low measurement of ozone
were assessed as false negative.

Systemic changes do not happen gradually, but become visible only when it's too late to intervene or
the cost of intervening is too high. For example, ICT is today recognized as a key driver of

productivity and growth, but evidence to prove this became available at a distance of years from the
initial investment. In fact the initial lack of correlation between ICT investment and productivity
growth was mostly due to incorrect measurement of ICT capital prices and quality. Subsequent
methodologies found that computer hardware played an increasing role as a source of economic

growth (see inter al. Colecchia and Schreyer 2002, Jorgenson and Stiroh 2000, Oliner and Sichel
2000).

The problem is in this case is therefore twofold: to collect data more rapidly; and to analyze them
with a wider variety of models that account for systemic, long term effects and that are able to
detect and anticipate weak signals or unexpected wild cards.



        3.3.2. Generate high involvement of citizens in policy-making

The involvement of citizens in policy-making remains too often associated with short-termism and
populism.

It is difficult to engage citizens in policy discussions in the first place: public policy issues are not
generally appealing and interesting as citizens fail to understand the relevance of the issues and to



6
 http://www.nobelprize.org/nobel_prizes/economics/laureates/2009/ostrom-lecture.html
7However in our project we mainly focus on tools that are used or can be adopted by Governments, otherwise we would
risk to enlarge too much the scope of the research roadmap



                                                                                            16 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

see "what's in it for me". The decline in voters turnout and the lack of trust in politicians reflects this.

More importantly, there are innumerable cases where the "right" policies are not adopted because
citizens "would not understand" or because it is not politically acceptable.

While the Internet has long promised an opportunity for widespread involvement, e-participation
initiatives often struggle to generate participation. Participation is often limited to those that are
already interested in politics, rather than involving those that are not.

When participation occurs, online debates tend to focus on eye-catching issues and polarized
positions, in part because of the limits of the technology available. It is extremely difficult and time

consuming to generate open, large scale and meaningful discussion.


        3.3.3. Identify “good ideas” and innovative solutions to long-standing problems

Innovation in policy-making is a slow process. Because of the technical nature of issues at hand, the
policy discussion is often limited to restricted circles. Innovative policies tend to be "imported"

through "institutional isomorphism". Innovative ideas, from both civil servants and citizens, fail to
surface to the top hierarchy and are often blocked for institutional resistance.

Existing instruments for large-scale brainstorming remain limited in usage, and fail to surface the
most innovative ideas. Crowdsourcing typically focus on the most “attractive” ideas, rather than the
most insightful.



        3.3.4. Reduce uncertainty on the possible impacts of policies

When policy options have been developed, simulations are carried out to anticipate the likely impact
of policies. The option with the most positive impact is normally the one that is proposed for
adoption.

Most existing methodologies and tools for the simulation of policy impacts work decently with well
known, linear phenomena. However, they are not effective in times of crisis and fast change, which
unfortunately turn out to be exactly the situations where government intervention is most needed.

As an example nowadays the European Central Bank bases its analysis of the EURO Area economy
and monetary policy on a derived version of the DSGE model developed by Frank Smet and Raf
                   8
Wouters in 2003 . Smet and Wouters’ model is deeply microfounded, allowing for a rigorous
theoretical structure of the model. Moreover in this setting the reduced form parameters are
related to deep structural parameters in order to mitigate Lucas’ critique, while the utility of agents
can be taken as a measure of welfare in the economics (Phelps ed. 1970).

However, the DSGE models suffer from several shortcomings jeopardizing their ability to predict, let
alone to prevent, a global crisis:

        Agents are assumed to be perfectly rational, having perfect access to information and

        adapting instantly to new situations in order to maximize their long-run personal advantage
        So far agents have entered the models as homogeneous representative entities, while it

        would be a step forward being able to take into account agents heterogeneity
        Canonical models consider atomistic agents with little or no interactions and thereby are not

        able to cope with network externalities


8
 http://www.ecb.int/home/html/researcher_swm.en.html


                                                                                            17 | P a g e
0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling








































































                                                                       18 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


But most of all it is the very notion of equilibrium which prevents standard models from dealing with
crisis. Stable steady state equilibrium is a condition according to which the behaviour of a dynamical

system does not change over time or in which a change in one direction is a mere temporary
deviation. This condition is proper of general equilibrium theory, in which a stable steady state is
believed to be the norm rather than the exception. When in the canonical model we are out of
equilibrium, the situation is seen just as a short lapse before the return to the steady state. This is in
sharp contrast with the very notion of crisis, which represents a steady deviation from the

equilibrium. Loosely speaking, the crisis phenomenon is not even conceived within the framework of
standard models.

All these flaws are not only related to DSGE models, Computational General Equilibrium (CGE) or
macro-econometric forecasting models, but generally affect the traditional policy making tools. In
this view it would be very important to find new frameworks capable of avoiding those shortcuts.
Some of such methodologies and methods already exists, e.g. System Dynamics and other hybrid

models, and some governments are using them. Our aim is to push forward in that direction.
We need to move away from the equilibrium paradigm in order to be able to assess other issues:
evolutionary dynamics; heterogeneity of technologies and firm; political and legal determinants of

social stability; incentive structures; better modelling technological change, innovation diffusion and
economic systems (taking into account finance, debt and insurance); interactions between
heterogeneous economic agents (firms and households) and central governments; heterogeneous
responses to government incentives; economic dependence from the ecosystem.

    Trichet, the former head of ECB, clearly put it: “This doesn't mean we have to abandon
    DSGE...(but)...atomistic rational agents don't capture behaviour during a crisis...rational

    expectations theory has brought macroeconomics a long way ... but there is a clear case to re-
    examine the assumptions”
But the need for new policy making tools is not limited to the economic realm: in the future it will

become more and more important to anticipate non-linear potentially catastrophic impacts from
phenomena such as: climate change (draught and global warming); threshold climate effects such as
poles’ sea-ice withdraw, out-gassing from melting permafrost, Indian monsoon, oceans acidification;
social instability affecting economic well-being (social conflict, anarchy and mass people

movements).
Lack of understanding of systemic impact has driven to short term policies which failed in grasping

long term, systemic consequences and side effects:
    -   An example of this approach might be given by the sovereign debt issue. In fact it is
        relatively easy for governments under popular pressure to increase expenditure and public

        debt to cope with short term necessities, such as offsetting the negative impacts brought
        about by a regional or global crisis. On the other hand it is harder to take into account the
        long term effect determined by higher interest rates on private investments and
        consumption through crowding out and fiscal pressure

    -   Another example of short-termism are the financial policies pursued in south East Asia at the
        beginning of the 90s. Many countries, such as Thailand, liberalized their financial markets

        fostering the inflow of investments aimed at sustaining growth. Unfortunately those capitals
        triggered a real estate bubble which has been at the roots of the 1997-1998 crisis
    -   In 2008 the Central Bank of Iceland yielded liquidity loans for saving banks on the verge of

        default on the basis of newly-issued, uncovered bonds, i.e. effectively printing fiat money on
        demand, causing a significant rise in inflation. To cope with this rise in prices, the Iceland
        Central Bank had to keep very high interest rates thereby leading to an economic bubble



                                                                                          19 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

    -   According to a great number of economists the financial crisis was triggered by US

        government policies spanning across two administrations which were intended to ensure
        citizens’ right but instead determined an unprecedented high number of risky mortgages, as
        well as the decline in mortgage underwriting standards that ensued. According to the
        “Financial Crisis Inquiry Commission Report” those policies, together with the deregulation

        of the financial system, might have been catalyzed the crisis.
    -   Other examples can be the bail out of financial institutions: in the short run those actions

        maintain employment and economic standards, while in the long term they induce moral
        hazard, keep operating inefficient companies and decrease the trust of economic agents in
        regulation, which is the funding pillar of our economic system.



        3.3.1. Ensure long - term thinking

In traditional economics, decisions are utility-maximising. Agents rationally evaluate the
consequences of their actions, and take the decision that maximize their utility. However, it is well
known that this rationalistic view does not fully capture human nature. We tend to overestimate
short-term impact and underestimate the long term (see Saffo). In policy-making, short-termism is a

frequent issue. People are reluctant to accept short-term sacrifices for long-term benefits. Politicians
have elections typically every 5 years, and often their decisions are taken to maximize the impact
“before the elections”. There is also the perception that laypeople are less sensitive to long term
consequences, which are instead better understood by experts. Overall, long-term impact is less

visible and easier to hide, due to lack of evidence and data, as well as of models to simulate ex-ante
alternative policy options. As a result, decisions are too often taken looking at short-term benefits,
even though they might bring long term problems.

Climate change is a typical policy area where sub-optimal decisions were taken because the short-
term costs were considered to outweigh the long term consequences. The long term impact is not
visible, while the short term sacrifices were, even though ICT had an important role in stimulating

the debate and catalysing attention of the media on the issue.


        3.3.1. Encourage behavioural change and uptake

Once policies are adopted, a key challenge is to make sure that all stakeholders comply with
regulations or follow the recommendations. It is well known how the greatest resistance to a policy

is not active opposition, but lack of application.

For instance, several programmes to reduce alcohol dependency problems in the UK failed as they
excessively relied on positive and negative incentives such as prohibition and taxes, but did not take
into account peer-pressure and social relationships. They failed to leverage “the power of networks”
(Ormerod 2010). For instance, any policy related to reduction of alcohol consumption through
prohibitions and taxes is designed to fail as long as it does not take into account social networks, as

binge drinkers typically have friends who also have similar problems. In another classical example
(Christakis and Fowler 1997), a large scale longitudinal study showed that the chances of a person
becoming obese rose by 57 per cent if he or she had a friend who became obese.

The identification of social networks and the role of peer pressure in changing behaviour is not
considered in traditional policy-making tools.



9 http://fcic.law.stanford.edu/report



                                                                                            20 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




        3.3.2. Manage crisis and the “unknown unknown”

The job of policy-makers is increasingly one of crisis management. There is robust evidence that the
world is increasingly interconnected, and unstable (also because of climate change). Crises are by

definition sudden and unpredictable. Dealing with unpredictability is therefore a key requirement of
policy-making, but the present capacity to deal with crises is designed for a world where crises are
exceptional, rather than the rule. Donald Rumsfeld, former secretary of state, famously said during

the Iraq war that while the US government was capable of dealing with the “known unknown”, the
difficulty was the increasing recurrence of “unknown unknown”: those things that we don’t known
that we don’t know.

There is evidence that the instability and chaotic natures of our world is increasing, because of its

increasing connectedness. Every year, intense climate phenomena throw our cities in disarray,
because of snow, flooding, fires. Each crisis seems to find our decision-makers unprepared and
unable to deal with it promptly. As Taleb (2007) puts it, we live in the age of "Extremistan": a world

of "tipping points" (Schelling 1969) “cascades” and "power laws" (Barabasi 2003) where extreme
events are "the new normal". There are many indications of this extreme instability, not only in
negative episodes such as the financial crisis but also in positive development, such as the
continuous emergence of new players on the market epitomised by Google. The random

vulnerability of today’s world is well illustrated by this chart from the EC DG RESEARCH.





















                                    Figure 3: Total Disasters Reported

        3.3.3. Moving from conversations to action

The collaborative action of people is able to achieve seemingly unachievable goals: experiences such

as ZooGalaxy and Wikipedia show that mass collaboration can help achieve disruptive innovation.
Yet too often web-based collaboration is confined to complaints and discussions, rather than action.
As one blogger put it, paraphrasing Marx: “Philosophers have only interpreted the world: the point is
                      10
to complain about it” .





10  Quoted    in   Mick   Fealty,  The    wisdom    of   crowds,   The   Guardian    24   February   2007
http://www.guardian.co.uk/commentisfree/2007/feb/24/towardsadeliberativedemocra



                                                                                             21 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


For example, the recent Italian elections saw an explosion of activity in social media discussing about
the different candidates. This energy then failed to translate into concrete action in the aftermath of

the elections.



         3.3.4. Detect non-compliance and mis-spending through better transparency

In times of crisis, it is ever more important for governments to ensure that financial resources are
well spent and policies are duly implemented. But monitoring is a cost in itself, and a certain margin
of inefficiency in resources deployment is somehow “natural”. Yet the cost of this mismanagement is

stagg11ing: for instance, in 2010, 7.7% of all Structural Funds money was spent in er12r or against EU
rules . OECD estimates place the cost of corruption equals 5% of global GDP . Thereby it would be
crucially important to be able to avoid the mismanagement with anticipatory corrective actions.



         3.3.5. Understand the impact of policies

Measuring the impact of policies remains a challenge. Ideally, policy-makers would like to have real-
time clear evidence on the direct impact of their choice. Instead, the effects of a policy are often

delayed in time; the ultimate impact is affected by a multitude of factors in addition to the policy.
Timely and robust evaluation remains an unsolvable puzzle.

This is particularly true for research and innovation policy, where the results from investment are
naturally expected at years of distance. As Kuhlmann and Meyer-Krahmer (1994) puts it, “the results

of evaluations necessarily arrive too late to be incorporated into the policy-making process”.
































11http://www.europeanvoice.com/article/2011/november/commission-names-worst-managers-of-eu-money/72613.aspx

12http://www.oecd.org/dataoecd/51/5/49693613.pdf



                                                                                                 22 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



4. The Research Challenges

In this section, we illustrate in detail each research challenge, by describing:

     -  The definition,
     -  The potential opportunities for governance,

     -  The state of the art of market and research,

     -  The existing challenges and

     -  The recommended research themes.
The research challenges are organised in 2 groups: the first regroups 6 challenges on Policy
Modelling, while the second the 8 ones on Collaborative Governance.




    4.1. Policy Modelling

        4.1.1. Systems of Atomized Models
Introduction and definition

This research challenge seeks to find the way to model a system by using already existing models or
composing more comprehensive models by using smaller building blocks, sometimes also called

“atoms”, either by reusing existing objects/models or by generating/building them from the very
beginning. Therefore, the most important issue is the definition/identification of proper (or most
apt) modelling standards, procedures and methodologies by using existing ones or by defining new
ones. Further to that, the present sub-challenge calls for establishing the formal mechanisms by
which models might be integrated in order to build bigger models or to simply exchange data and
valuable information between the models. Finally, the issue of model interoperability as well as the

availability of interoperable modelling environments should be tackled.


Why it matters in governance

Using existing objects/models that are able to describe systems, sub-systems and interaction among
them, allows everyone to build his own insight on a specific problem/solution. So, in governance,
such opportunity give us the chance to:

        Release public data, linking them and producing visual representations able to reveal
        unanticipated insights.

        Use social computing to promote engagement and citizens’ inclusion in policy decision, and
        exploit the power of ICT in mining and understanding the opinions they express.

        Analyse policies and produce models that can be visualised and run to produce simulations
        able to show the effects and impacts from different perspectives such as political, economic,
        social, technological, environmental and legal facets.



Current Practice and Inspiring cases

In systems analysis, it is common to deal with the complexity of an entire system by considering it to
consist of interrelated sub-systems. This leads naturally to consider models as consisting of sub-
models. Such a (conceptual) model can be implemented as a computer model that consists of a


                                                                                           23 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


number of connected component models (or modules). Component-oriented designs actually
represent a natural choice for building scalable, robust, large-scale applications, and to maximize the
ease of maintenance in a variety of domains.

An implementation based on component models has at least two major advantages:

        First, new models can be constructed by connecting existing component models of known

        and guaranteed quality together with new component models. This has the potential to
        increase the speed of development.

        Secondly, the forecasting capabilities of two different component models can be compared,
        as opposed to compare whole simulation systems as the only option.

Further, common and frequently used functionalities, such as numerical integration services,
visualisation and statistical ex-post analyses tools, can be implemented as generic tools and
developed once for all and easily shared by model developers. By the way, the current practice in

composing and re-using models is still not sufficiently widespread. In relation to Model Reuse, this is
mainly due to the fact that little to any repository actually exists . Moreover, the publicly available
models are not “open” to modification or re-use. Some modelling environments (or modelling

suites) provide some examples and small libraries of ready-to-use models, but in most cases, they
are not completely open nor any explanation is provided on how to reproduce them (their structure,
parameters, etc.). As an inspiring case see the SEAMLESS project, which was funded by the EU

Framework Programme 6 (Global Change and Ecosystems), ran from 2005 till March 2009, and
developed a computerized framework for integrated assessment of agricultural systems and the
environment . During the project, a modular approach was chosen to develop a system named
“Agricultural Production and Externalities Simulator (APES)”, illustrated in figure (5). APES is a

modular simulation system targeted at estimating the biophysical behaviour of agricultural
production systems in response to the interaction of weather, soils and different options of agro-
technical management. Although a specific, limited set of components is available in the first

release, the system is being built to incorporate, at a later time, other modules which might be
needed to simulate processes not included in the first version. The processes are simulated in APES
with deterministic approaches which are mostly based on mechanistic representations of biophysical
processes. APES was used to compare alternative agricultural and environmental policy options,

facilitating the process of assessing key indicators that characterize interactions between agricultural
systems, natural and human resources, and society. The developed framework, named SEAMLESS-IF
in a finale stage, also enabled linkage of quantitative models, pan-European databases and

qualitative procedures to simulate the impact on society of biophysical, economic and behavioural
changes. SEAMLESS-IF now facilitates ex-ante assessments at the full range of scales from the global
to the field level to support policy and decision making for sustainable development. SEAMLESS-IF
nowadays can be used to investigate the effects of agricultural and environmental policies while

accounting for technical innovations. Further, the interactions of such policies with other major
trends such as climate change and increasing land used for bio-fuel crops can be studied efficiently
in the near future.







13This is true for most of the sectors, even though for instance most energy models are based on the MARKAL
family of models. Furthermore something to consider is that models need to be customized, so that having a

single framework readily applicable to different contexts and sectors may actually be counter productive
14http://www.seamless-ip.org



                                                                                           24 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


























                    Figure 4: Agricultural Production and Externalities Simulator (APES)



Analyses with SEAMLESS-IF can be done at multiple scales and with varying time horizons, whilst
focusing on the most important issues emerging at each scale. This is possible as the framework is
based on research innovations in linking models across scales allowing consistent “micro-macro”
analysis as well as linking models across disciplines allowing “economicbiophysical” analysis. The

linked models range from a bio-physical field model to a farm model and to an agricultural sector
model for the EU; in other words they ensure a consistent analysis of what effects EC policies may
have on agricultural markets, farming systems and the environment. In addition, the effectiveness of

a policy in its institutional context is assessed by applying qualitative procedures. The interlinked
pan-European database provides the relevant data needed at different scales.



Another inspiring example see the Insight Maker case at http://insightmaker.com. Insight Maker
allows to build simulation models ("Insights") for all scales: from the smallest cell, to the social
effects of product adoption, to global climate change. Once they are built one can share them with

others. The models are called “an Insight” as they will typically reveal one or more fascinating point
about the system under study. All the simulations built with Insight Maker can be shared via the
web. This means people can change the variables and see the results for themselves.

Vensim Molecules is software used for constructing system dynamics models from molecules of
system dynamics structure. Molecules are made of primitive stock and flow or auxiliary elements

and are, in turn, the building blocks of complete models, elements of substructure serving a
particular purpose. Molecules provide a framework for presenting important and commonly used
elements of model structure making faster and easier to develop system dynamics models.







15http://www.vensim.com/molecule.html



                                                                                             25 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

Project Anylogic , a multi-method simulation modelling tool capable of integrating and combining

the following modelling approaches: system dynamics, discrete event simulation and agent-based
modelling. Anylogic’s simulation language is composed by stock and flow diagrams (used for System
Dynamics modelling), statecharts, which define the agents’ behaviour in Agent Based modelling,
action charts (used to define algorithms), and finally process flowcharts which are the basic

constructions for defining processes in Discrete Event modelling.
Available Tools

A comprehensive review of the available tools is ongoing.

Key challenges and gaps

With regards to implementation architecture and use of modelling frameworks, there are two major
problems:

        The framework design and implementation must be optimized to balance carefully its
        flexibility and its usability to avoid incurring either a performance penalty or users having
        too steep a learning curve, and

        Developing components for a specific framework constrains their use to that framework.



The most immediate option to overcome such problems is developing inherently reusable
components (i.e. non framework specific), which can be used in a specific modelling framework by
encapsulating them using dedicated classes called “wrappers”; such classes act as bridges between

the framework and the component interface. The disadvantage of this solution is the creation of
another “layer” in the implementation, which adds to the already implemented machinery in the
framework. The appropriateness of this solution, both as ease of implementation and overall
performance, must be evaluated case by case.

Regardless of the choice of developing framework specific or intrinsically reusable components,
there is a basic choice which must be carefully evaluated prior to that and which is related, in

general terms, to the framework as a flexible modelling environment to build complex models
(model linking), but also to the framework as an efficient engine for simulation, calibration and
simulation of model components (model execution). Modern software technologies allow building
flexible, coherent and elegant constructs, but that comes at a performance cost. Without even

introducing specific references to Object Oriented Programming (OOP), it seems important to point
out that the use of object-oriented programming constructs, which actually enhance flexibility,
modularity and reuse of software, all nice things, require the compiler to use virtual methods calls,
dynamic dispatching, and so on. All these operations are resource intensive and in some cases, they

can heavily affect the code performance, and this becomes evident in applications in which such use
is done thousand times every simulation step.

By the way, the Model Composition horizon is even more clouded as the potential advantages
resulting from the possibility of composing bigger models from smaller ones have been shown only
recently. It is essentially due to the problem of interoperability and integration of different vendors’
(thus proprietary) model formats and to the lack of standards allowing performing composition

tasks. Another problem stems from the fact that many models are still too dependent on their
implementation methodology. Moreover, model integration is at present almost non-existing. Very
few modelling environments/suites provide the import/export functionalities and a standard



16
  http://www.xjtek.com/


                                                                                         26 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


language for model interoperability is not currently available. Most of the current practice for data
communication or information transfer is performed by means of third party solutions (e.g.:

interoperability in most cases is achieved by transferring data via electronic spread sheets or, only in
rare cases, by using Database Management Systems (DBMS) or Enterprise Resource Planning (ERP)
systems.



Current research
Current research, as well as previous research, has not yet worked on (with the exception of just a

few cases) the problem of different models integration. At present, due to the plethora of different
modelling/simulation environments/suites, as well as to differences at the scientific field level, many
competing file formats exist. It is possible that vendors perceive the modelling practice as a very
small market niche (as the users stem mainly from Academia and to a very small extent from private
companies where a Decision Support Systems is used, what is more the Public Administration share

is negligible) and therefore are reluctant to introduce interoperable features.
Also, current research, as well as previous research, has only recently begun to explore the following

issues:
        Open-source modelling and simulation environments (there are open environments that are

        rising in importance in the research community, albeit in most cases they only provide the
        possibility to implement and simulate a model according to the modelling methodology they
        refer to).

        Communication of data among models developed in different proprietary (or open)
        environments by depending on third party solutions (e.g.: interoperability is in most cases
        only achieved by transferring data by means of electronic spreadsheets or, only in rare
        cases, by using a DBMS or an organisation’s ERP).

        Open visualisation of results stemming from model simulation (e.g.: online visualisation of
        simulation results in a browser by interfacing - only in a few cases - the simulation engines,

        or - as it is more often the case - by connecting to a third party mean, as described in the
        previous bullet point).


Future research

Future research should therefore focus on:

        Definition of standard procedures for model composition/decomposition, e.g. how to
        deductively pass from a macro-description of models to the fine definition of its building-
        blocks or molecules (top-down approach), how to inductively conceive a progressive

        composition of bigger models by aggregating new parts as soon as they are needed (bottom-
        up approach) or by expanding already existing objects.

        Proposition of a minimum set of archetypical structures, building blocks or molecules that
        might be used according to the proper level of decomposition of the model (e.g. systemic
        archetypes, according to the Systems Thinking / System Dynamics approach, might be useful
        to describe the overall behaviour thanks to the main variables in the system to be modelled
        at a macro-to-middle level). The procedures to implement, validate and redistribute any

        further improvement of these “minimal” objects should be investigated.

        Definition of open modelling standards, as the basis for interoperability, that is defining
        common file formats and templates (i.e.: by means of XML), which would allow the models


                                                                                          27 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        described by means of these XML files to be opened, accessed and integrated into every
        (compliant) model-design and simulation environment.

        Interoperability, also intended in terms of Service Oriented Architectures (e.g.: certain stand-
        alone and always operative models might expose some “services” in order to make available

        either their endogenous data or bits of information, or some peculiar function or structural
        part, while some other may request to use those services when needed. In consequence, it
        creates a need for a definition of model repositories, a list of operative models and the
        functionalities that they might expose which finally, entails the definition of a SOA among

        interoperable models).
        Definition and implementation of model repositories (and procedures to add new objects to

        them), even if they are restricted to hosting models developed according to a specific
        methodology (Agent Based, System Dynamics, Event Oriented, Stochastic, etc...)

        Definition and implementation of new relationships that are created when two models are
        integrated.    All   possible   important     relationships   resulting   from     a   model
        integration/composition should be identified and eventually included in the new deriving
        integrated model.

        Input / Output definition / re-definition: the integration of modelling techniques is a
        pertinent issue in the scope of this challenge. The multi-modelling tools should be, in future,
        available not only to experts but also to lay users. Moreover, at present, only a few of the

        actually available modelling/simulation suites are able to provide the possibility to build a
        model by referring to a different modelling methodology.


        4.1.2. Collaborative Modelling

Introduction and definition

The English sayings “two heads are better than one” and “too many cooks spoil the broth” give an
idea of the expectations that arise from a collaboration of people. On the one hand, one would
expect that a group of people is able to better observe and perceive situations as well as to make

better decisions than a single person would be able to. On the other hand, it is also common
knowledge that the collaboration of several people entails the problem of group coordination,
which, if disregarded, can make group work inefficient, compared to the work of a single person.

There are three kinds of problems that are typically approached by groups:

        cognition problems, problems with a definite solution or a set of solutions that are certainly
        better than others;

        coordination problems, problems that require the group to figure out how to coordinate the
        behaviour of its members;

        cooperation problems, problems which feature the involvement of several self-interested,
        distrustful people who have to work together.


Collaborative modelling refers to a process where a number of people actively contribute to the

creation of a model. The weakest form of involvement is feedback to the session facilitator, similar
to the conventional way of modelling. Stronger forms are proposals for changes or (partial) model
proposals. In this particular approach the modelling process should be supported by a combination
of narrative scenarios, modelling rules, and e-Participation tools (all integrated via an ICT e-



                                                                                         28 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Governance platform): so the policy model for a given domain can be created iteratively using
cooperation of several stakeholder groups (decision makers, analysts, companies, civic society, and

the general public).
As a matter of fact groups require rules (or cultural norms) to maintain order and coherence, as well

as diversity and independence of its group members in order to create a kind of a collective
intelligence. Bringing together people with diverse perspectives and backgrounds for working
together in multi-disciplinary teams is expected to improve the overall group performance, so the
first issue on which the collaborative process should be based is the definition of a shared modelling
rules framework (the social norms), guiding the modelling team in determining whether a proposal

is accepted or rejected. Two usually adopted types of rules are:
        Rules of majority, where a certain number of group members had to support or oppose a

        proposal in order for the whole group to accept or reject it (e.g., more than half). A tie-break
        rule was sometimes specified (e.g., for the case of an equal number of supporters and
        opponents). The tie-break could involve seniority issues.

        Rules of seniority, where the weight of a group member’s support or opposition was related
        to his or her status within the group. This status could be acquired (e.g., by experience) or
        associated with a position to which the member was appointed. A frequent example of this

        was the case of a more experienced modeller who was considered as the leader by the
        group and took decisions on their behalf. The other members filled the role of consultants in
        such a case.


These rules were sometimes set up explicitly before the group began their work, or in an early phase

of this work. But in most cases they rather emerged as the result of each member’s behaviour.
Individuals making regular contributions of high quality were likely to acquire seniority. In
homogeneous teams majority rules were used more often.



Why it matters in governance
From a very high level of abstraction, collaborative modelling itself can be seen as a social

interaction between several people, while these people who together perform the modelling
process form a social entity. Thus, the process of collaboratively defining and implementing a model,
with a particular reference to the public policy modelling, is strictly connected with the public aspect
of every citizen’s life, starting from the communities bridged by the decision makers that
collaboratively define some policies, to an average citizen which interacts with other citizens within

the rules framework defined by the policies themselves.
Starting from the needs perceived by the citizens, the limitations of existing modelling techniques

adopted in policy making include the following issues:
        Changing models is too time-intensive and integrating to other diagrams is difficult. Also

        there are version control problems.
        It is not possible for more than one person to work on the same diagram at the same time.

        Modelling has to be done at the specific location where the modeller is present.

        Contribution to the model comes from those interviewed or at a group meeting, limiting the
        potential contribution from a larger group.





                                                                                          29 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        Low model acceptance: the model resulting from the modelling session is not supported by
        some of the stakeholders.

        Participants feel misunderstood: as a consequence of bad elicitation or a wrong

        understanding of the model.
        Low perceived model quality and limited model comprehension: Individuals do not fully

        understand the model or do not agree with it.


Reasons that argue for conducting policy modelling in a collaborative manner are:

        No person typically understands all requirements and understanding tends to be distributed
        across a number of individuals.

        A group is better capable of pointing out shortcomings than an individual.

        Individuals who participate during analysis and design are more likely to cooperate during
        implementation.


Collaborative modelling calls for the definition of the citizen’s role in the public policy modelling

process (e.g.: the mass participation issues and processes have been already researched in depth by
the e-Participation research programs). In order to guarantee participation there are some
prerequisites that should be fulfilled:

        all citizens who access ICT services in order to participate should represent the views of
        communities affected by the given policy;

        all citizens are able to take part in the modelling process via intuitive IT systems that enable
        them an effective and efficient contribution;

        all citizens possess proper skills (or are assisted) to purposely follow a process of group
        model-building in order to avoid/abate wrong mental models and thus ultimately reach a
        shared vision of the problem.



Current Practice and Inspiring cases

In current practice, collaborative modelling is mainly performed offline; still the rules and guidelines
for session processes are not yet sufficiently widespread. In fact, the abatement of wrong mental
models and the creation of knowledge from information usually imply the dialogue of people with
different views of the problem as well as the need for critical skills. Further to that, the information
that occurred in a discussion has to be grounded and definitively transferred to the formal model.

Thus, e-Participation might be of help in achieving a critical mass of data and information exchange
online but in itself does not solve the problem of mass cooperation and collaboration in a formal
modelling process. Even more, the participation in this process entails, at present, a thorough
knowledge on modelling processes or tools that an average citizen does not have. Therefore, there is
an urgent need for Intuitive Interfaces, Modelling Wizards and guided simplified approaches to

modelling. Starting from the relevance of collaborative modelling in policy making, as a very former
inspiring case Maarten Sierhuis and Albert M. Selvin, working at NYNEX Science & Technology Inc in
New York, presented in 1996 an applied research report on “Towards a Framework for Collaborative
Modelling and Simulation”, describing methodologies for modelling and simulation in a collaborative

analysis or design project, and describing a case study in which Conversational Modelling, a
software-supported technique for collaborative modelling, enabled participants to construct static


                                                                                          30 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


knowledge models in collaborative sessions. The sessions described in the report resulted in the

identification of 207 queries. Of these, 24 were chosen for detailed modelling. As a result of the
modelling, 44 resources, 29 knowledge items, 58 data items, and 8 organizational issues were
identified. The response from participants was positive. Many stated that they had learned more
about each other’s’ work in the conversational modelling sessions then they had been able to in the
course of their normal work activities. The development organization has been able to use the

output of the sessions to generate design requirements. A picture of the interface (figure 6) used
during the sessions follows.






















































                                                                                           31 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

































                                Figure 5: Conversational Modelling Interface



As more recent inspiring case, one can refer to the results of the FP7 projects OCPOMO or               17
          18
PADGETS . This last one, PADGETS, aims at bringing together two well established domains, the
mashup architectural approach of web 2.0 for creating web applications (gadgets) and the

methodology of system dynamics in analyzing complex system behaviour. The objective is to design,
develop and deploy a prototype toolset that will allow policy makers to graphically create web
applications that will be deployed in the environment of underlying knowledge in Web 2.0 media.

The project introduces the concept of Policy Gadget (PADGET) – similarly to the approach of gadget
applications in web 2.0 – to represent a micro web application that combines a policy message with

underlying group knowledge in social media (in the form of content and user activities) and interacts
with end users in popular locations (such as social networks, blogs, forums, news sites, etc) in order
to get and convey their input to policy makers.












                                     Figure 6: The PADGET Framework



17
  Open Collaboration in Policy Modelling, http://www.ocopomo.eu
18
  http://www.padgets.eu


                                                                                               32 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




A PADGET is composed of four main components:

        A message, that is a policy in any of its stages and forms

        A set of interaction services, that allows users to interact with the policy gadget (find it,
        access its content, comment its content, share it etc.). These interfaces may be provided by
        either the underlying social media platforms in which the PADGET Campaign is launched or
        by the PADGET itself when it takes the form of a micro application (i.e. in the case of the
        iGoogle gadget).

        The social context, that is the framework describing the social activity and content relating
        with the policy gadget in each individual social media platform where the policy gadget is

        present.
        The decision services, which are offered by two modules. The PADGETS analytics and the

        PADGETS simulation model. The decision services component is responsible for the
        generation of the information outputs to be presented to the PADGET initiator (usually a
        policy maker).



PADGETS will use publicly available APIs for interconnecting, publishing and retrieving content from
underlying social media platforms. The collected information and user activities that policy gadgets
invoke in the media platforms will be categorized using semantic tags as to their relation to the
policies in order to help the policy maker form an opinion about what the users think about relevant
issues and policies.



Available Tools

Research about collaborative software has been conducted since the mid 1980's, when computer-
human interaction, office automation, and support for group work became the focus of research
projects. The term computer-supported cooperative work (CSCW) was first used in 1984 and focused
on the support of small groups of people. Other terms are used as synonyms for CSCW, especially:

collaborative computing, computer mediated communication, and group decision support systems.
CSCW is defined as a “computer-assisted coordinated activity such as communication and problem
solving carried out by a group of collaborating individuals" or as a system, which “looks at how
groups work and seeks to discover how technology (especially computers) can help them work". The
term groupware also stems from the 1980's and is defined as “computer-based systems that support

groups of people engaged in a common task (or goal) and that provide an interface to a shared
environment". Interestingly, some authors see groupware as advanced software that has to provide
awareness support, while other authors also understand code management or emailing as
groupware systems. In contrast to groupware, CSCW does not only comprise technological aspects

of collaboration, but also incorporates psychological, social, and organizational effects.
Collaborative technologies, especially in the field of groupware and CSCW, are typically classified
using the time-space taxonomy which distinguishes between communication that occurs at the

same space or concurrently at different spaces, and communication that occurs in the same time
(synchronously) or in different times (asynchronously). This view was established in 1988 by R.
Johansen (“GroupWare: Computer Support for Business Teams”, The Free Press, New York) and
taken on in various related publications. The following figure depicts the typical time-space matrix as

presented in these publications.



                                                                                         33 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



























                                     Figure 7: The Time-Space Matrix


The matrix divides collaborative technologies into four possible constellations, while each of these

constellations can be supported better or worse by different communication media.

By the way the architecture of a collaborative modelling tool, i.e., a system that supports a group in
developing models, is still under investigation. Some authors have suggested groupware systems
that help teams in collective sense-making which is an important part of the modelling process.
Conklin, Selvin, Buckingham and Sierhuis in “Facilitated Hypertext for Collective Sensemaking: 15

Years on from gIBIS”, a paper presented in 2003 during the 8th International Working Conference on
the Language-Action Perspective on Communication Modelling (Tilburg, The Netherlands), reports
on an approach, Compendium, that is the result of 15 years of experience. Compendium combines
three different areas: meeting facilitation, graphical hypertext and conceptual frameworks. To make

them work, facilitation is viewed as essential to remove the cognitive overhead for the group
members. As groupware systems address the important issue of collective sense-making they can be
used as the core of a collaborative modelling tool. So far these systems are typically tailored for

specific modelling languages though. For a collaborative modelling tool they need to be more
modular so that any modelling language can be “plugged in” (e.g., other enterprise or information
systems modelling languages). In addition, there is also the need for a negotiation component that
facilitates structured arguments and decisions regarding modelling choices. Based on this reflections

and issues, recently two tools are emerging:
                                                              19
        The COllaborative Modelling Architecture, COMA , allows group modelling. Any group
        member can work on the models whenever it suits them. Any participant can contribute in
        the way they can: by just looking at proposals and commenting them, by making minor
        changes to them or maybe even by making their own proposals. The facilitator can see the

        status of the modelling process at any time and can decide whether a certain proposal



19www.coma.nu



                                                                                           34 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        should be adopted or needs improvement based on the comments by the other group

        members and his own judgment.

























                               Figure8 : COMA, Collaborative Modelling Architecture

        COMA's design has been inspired by theoretical insights from organizational semiotics and

        driven by observations of group modelling behavior. The tool is implemented in Vi20al C++
        2005 on Windows based on the UML Pad and with the wxWidgets GUI library .

        The OCOPOMO eParticipation platforms, deployed by Open Collaboration for Policy
        Modelling FP7 project, that will end in December 2012.





























20
  http://www.wxwidgets.org/



                                                                                             35 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling





















                                  Figure 9: OCOPOMO eParticipation Platform

        The platform is a suite of ICT tools for:

            o Iterative development of policies in a form of narrative scenarios;
            o Policy modelling, creation of agent-based formal policy models;

            o Open and transparent collaboration in the process of policy development;
            o Seamless, goal-oriented information exchange between all the stakeholders (policy

                analysts, operators, decision makers, wider interest groups, general public, etc.);
            o Simulation and visualisation of policy alternatives and their consequences;

        A First prototype was released in autumn 2011 and tested on a 1st round of pilot
        applications started on winter 2011. 2nd pilot applications and evaluation will start in
        autumn 2012, and the platform will definitely released in December 2012.



Key challenges and gaps

This research challenge is connected to the research on Web 2.0 and the next generation web. As far
as the Policy Modelling in Governance is concerned, this research challenge bridges the gap between
citizens and decision makers. It permits an early stage evaluation of the decision maker mental
models by opening a dialogue with citizens and allows for an exchange of perspectives. It finally

enables the collaboration in the public policy modelling process with the use of a rigorous and
formal scientific process.


Current research

According to current research, the following issues are being explored:

        Group model building and systems thinking, focusing on models when tackling a mix of
        interrelated strategic problems to enhance team learning, foster consensus, and create
        commitment; although people have different views of the situation and define problems

        differently, this current field of research shows that this can be very productive if and when
        people learn from each other in order to build a shared perspective.
        Web 2.0 tools for collaboration, as recently pointed out in the FP7 project OCOPOMO (Open

        Collaboration in Policy Modelling), which aim to implement collaborative scenario building


                                                                                          36 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        and policy modelling via an integrated ICT toolbox. OCOPOMO provides an innovative "off

        the mainstream" bottom-up approach to policy development, combined with advanced ICT
        tools and techniques supporting open collaboration. The project is developing an ICT-based
        environment integrating lessons and practical techniques from complexity science, agent
        based social simulation, foresight scenario analysis and stakeholder participation in order to
        formulate and monitor social policies to be adopted at several levels. The project is co-

        funded by the European Commission under the 7th Framework Program, Theme 7.3 (ICT for
        Governance and Policy Modelling).



Future research
Future research should therefore focus on:

        Collaborative Internet-based modelling tools, allowing more than one modeller to
        cooperate, at the same time, on a single model.

        Definition of frameworks allowing even “low-skilled” citizens to provide their contribution
        (even if in a discursive way) to the modelling process.

        Design of more intuitive and accessible Human-Computer Interfaces.









































                                                                                          37 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




        4.1.3. Easy Access to Information and Knowledge Creation

Introduction and definition

According to a cybernetic view of intelligent organisations knowledge supersedes 1. the facts, 2. data
(statements about facts) and 3. meaningful information (what changes us), the last also defined as
“the difference that makes the difference”. Knowledge most often defined as “whatever is known,
the body of truth, information and principles acquired” by a subject on a certain topic. Therefore
knowledge is always embodied in someone. It implies insight, which, in turn, enables orientation,

and thus may be also use as a potential for action (when we are able to use information in a certain
environment, then we start to learn, which is the process that helps developing and grounding
knowledge). Two more concepts come after knowledge on the same scale, and are Understanding
and Wisdom. Understanding is the ability to transform knowledge into effective action, i.e. in-depth

knowledge, involving both deep insights into patterns of relationships that generate the behaviour
of a system and the possibility to convey knowledge to others, whereby wisdom is a higher quality of
knowledge and understanding the ethical and aesthetic dimensions.

The research challenge is related to the elicitation of information which, in turn, during the overall
model building and use processes will help decision makers to learn how a certain system works and
ultimately to gain insights (knowledge) and understanding (apply the extracted knowledge from
those processes) in order to successfully implement a desired policy. It is important to note that

other research fields (in particular, ICT disciplines) tend to misuse the word “knowledge” and invert
it with ”information”.


Why it matters in governance

Proper information acquisition and knowledge development are the key aspect in all research fields,
so this research challenge has a horizontal importance for research in general. According to the
general need for policy assessment and evaluation, there are some specific issues stemming from

this research challenge, which are strongly related to governance:
        Public data use and thus public information elicitation (by citizens)

         Citizens’ behavioural data which are gradually becoming essential for any policy assessment
        process

        Interoperability of public IT systems
        Creation of a common understanding on a certain system’s behaviour (by means of learning)
        in order to develop a shared vision on the problems that a certain policy might want to

        overcome


Current Practice and Inspiring cases

In current practice, information is drawn from data stored in different types of media (mainly
DBMS/ERPs). Web 2.0 has further transformed the way we create data and elicit information from
data. Data availability ceased to pose problems as a result of:

        The Internet growth and its uptake

        User Generated Content in Social Networks





                                                                                         38 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        Cooperation of IT systems from different organisations thanks to the Service-Oriented
        Architectures (even among old legacy systems), which resulted also in private data

        availability
        Public Administration Transparency and Public Data use/reuse



Available Tools

A review of the available tools is ongoing.


Key challenges and gaps

The knowledge is still mostly created and passed on by formal methods of teaching, even though the
advents of the e-Learning field allow for an increased possibility to perform Distance Learning on the
Web. But, since knowledge is developed and grounded by the learning process through action in the
environment, the learning in real life comes from committing mistakes. In the field of real life

governance, it entails implementing a wrong policy and observing the positive and negative
consequences that this policy generates (for example due to a system’s “policy resistance”). At
present, thanks to the increasing data availability, information elicitation process is much easier,
either by tacitly bringing users (data generators) to provide data in a guided way (according to a pre-
set framework for data input) or with a help of a specific process (e.g.: consultations in e-

Participation tools).


Current research

According to current research, the main focus is put on the Knowledge Management field or also
(more properly, as in our case) to the Knowledge Elicitation field. The latter basically encompasses
the following steps:

        Data retrieval and extraction

        Data analysis and interpretation (which usually produces information)
        Data/information adaptation and integration (this is particularly the case where information
        needs to be used in a model)



Future research
There is still a large field to be explored – the methods of extraction of meaningful information from

unstructured sources of data, e.g. when analysing free texts, which applies to all sources of User-
Generated Content (forums, wikis, social networks, etc.), where the semantic dimension is essential
to derive meaningful information rather than just quantitatively analysing the syntax of text. In
general, a lot of data is generated by citizens and particularly by their behaviour online, so that the

available aggregated data sets contains information on what a citizen does, what s/he likes, how
s/he behaves in certain environments, and so on. This data is considered very valuable both for
private and public organisations (even though under privacy restrictions which have to be properly
addressed).

Also, according to the knowledge creation and development of understanding (regarding a specific
system), there is some research currently carried out on how to improve the learning process via the
use of e-Learning systems. Yet, what is still missing is the availability of micro-worlds, i.e. complex

virtual environments where reality is somehow reproduced and where a decision maker is trained in


                                                                                           39 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


order to implement his/her strategies and hypothesis and perform what-if analysis without the need
to necessarily learn from mistakes in real life.



Future research will thus have to focus on the following issues:

        Information elicitation by analysing and interpreting data, also taking into account the
        semantic point of view.

        Creation of proper micro-worlds (or ILEs, Interactive Learning Environments), where the
        acquired information on a certain system is used (by means of actions), and knowledge is
        developed by observation of the outcomes of the actions. Also, ILEs will have to be
        integrated into LMS (Learning Management Systems) in order to extend the potential of
        distance learning practices, eventually also in a cooperative way (mass learning).

        Interoperability of data sources in order to integrate/aggregate different types of data and
        be able to automatically infer information from more meaningful datasets.

        In view of the “Internet of Things”, the provision of “portable” models/tools for citizens in
        order to gather valuable data based on citizens’’ real behaviours. Moreover, these models
        and tools would enable citizens to check the results of their actions by analysing in real-time
        the response of the model to the information they are contributing to generate, and thus
        evaluating the eventual benefits they are receiving from their virtuous behaviour or harm

        they are creating either to their environment or to themselves (e-Cognocracy).


        4.1.4. Model Validation

Introduction and definition

Policy makers need and use information stemming from simulations in order to develop more
effective policies. As citizens, public administration and other stakeholders are affected by decisions
based on these models, the reliability of applied models is crucial. Model validation can be defined
as ”substantiation that a computerised model within its domain of applicability possesses a

satisfactory range of accuracy consistent with the intended application of the model” (Schlesinger,
1979). Therefore, a policy model should be developed for a specific purpose (or context) and its
validity determined with respect to that purpose (or context). If the purpose of such a model is to
answer a variety of questions, the validity of the model needs to be determined with respect to each

question. A model is considered valid for a set of experimental conditions if the model’s accuracy is
within its acceptable range, which is the amount of accuracy required for the model’s intended
purpose. The substantiation that a model is valid is generally considered to be a process and is
usually part of the (total) policy model development process (Sargent, 2008). For this purpose,
specific and integrated techniques and ICT tools are required to be developed for policy modelling.



Model validation is composed of two main phases:

        Conceptual model validation, i.e. determining that theories and assumptions underlying the
        conceptual model are correct and that the model’s representation of the problem entity and
        the model’s structure, logic, and mathematical and causal relationships are “reasonable” for
        the intended purpose of the model.

        Computerised model verification ensures that computer programming and implementation
        of the conceptual model are correct, as well as states that the overall behaviour of the
        model is in line with the available historical data.



                                                                                          40 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




Why it matters in governance

Model Validation is connected both to modelling and simulation. According to the general need for
policy assessment and evaluation, there are some specific issues stemming from the Model
Validation, which are strongly related to governance:

        Reliability of models: policy makers use simulation results to develop effective policies that
        have an important impact on citizens, public administration and other stakeholders. Model

        validation is fundamental to guarantee that the output (simulation results) for policy makers
        is reliable.
        Acceleration of policy modelling process: policy models must be developed in a timely

        manner and at minimum cost in order to efficiently and effectively support policy makers.
        Model validation is both cost and time consuming and should be automated and
        accelerated.

        Composable and re-usable models: a policy model developer deciding to re-use existing
        models or compose them, stumble across the issue of models’ reliability. Model validation
        can be used for certifying this reliability and creating a database of validated models.










































                                                                                           41 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Current Practice and Inspiring cases

In current practice the most frequently used is a decision of the development team based on the
results of the various tests and evaluations conducted as part of the model development process.
Another approach is to engage users in the validation process. When developing large-scale

simulation models, the validation of a model can be carried by an independent third-party. Needless
to say, that the third party needs to have a thorough understanding of the intended purpose of the
simulation model. Finally, the scoring model can be used for testing the model’s validity (e.g. see
Balci 1989; Gass 1983; Gass & Joel 1987). Scores (or weights) are determined subjectively when
conducting various aspects of the validation process and then combined to determine category

scores and an overall score for the simulation model. A simulation model is considered valid if its
overall and category scores are greater than some passing score.


Available Tools

A review of the available tools is ongoing.



Key challenges and gaps
Typically all above-mentioned approaches are applied after the simulation model has been

developed. Performing a complete validation effort after the simulation model has been finalised
requires both time and money. However, conducting model validation concurrently with the
development of the simulation model enables the model development team to receive inputs earlier
on each stage of model development. Therefore, ICT tools for speeding up, automating and
integrating model validation process into policy model development process are necessary to

guarantee the validity of models with an effective use of resources.


Current research

In Current research, there are a large number of subjective and objective validation techniques used
for verifying and validating the submodels and the overall model. Robert G. Sargent at the Syracuse
University in 2010 provided a relevant ones: Animation; Comparison to Other Model; Degenerate

Tests; Event Validity; Extreme Condition Tests; Face Validity; Historical Data Validation; Historical
Methods; Internal Validity; Multistage Validation; Operational Graphics; Parameter Variability /
Sensitivity Analysis; Predictive Validation; Traces; and Turing Tests. Furthermore, he described a new
statistical procedure for validating simulation and analytic stochastic models using hypothesis testing
when the amount of model accuracy is specified. This procedure provides for the model to be

accepted if the difference between the system and the model outputs are within the specified
ranges of accuracy. The system must be observable to allow data to be collected for validation.


Future research

Future research should explore the following issues:

        In order to speed up and reduce the cost of a model validation process, user-friendly and
        collaborative statistical software should be developed, possibly combined with expert
        systems and artificial intelligence.

        Due to the big gap between theory and practice, the considerable opportunity exists for the
        study and application of rigorous verification and validation techniques. In the current



                                                                                         42 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        practice, the comparison of the model and system performance measures is typically carried
        out in an informal manner.

        Complex simulation models are usually either not validated at all or are only subjectively
        validated; for example, animated output is eyeballed for a short while. Therefore,
        complexity issues in model validation may be better addressed through the development of

        more suitable methodologies and tools.
        Model validation is not a discrete step in the simulation process. It needs to be applied
        continuously from the formulation of the problem to the implementation of the study

        findings as a completely validated and verified model does not exist. Validation and
        verification process of a model is never completed.
        As the model developers are inevitably biased and may be concentrated on positive features

        of the given model, the third party approach (board of experts) seems to be a better solution
        in model validation.
        Considering the ranges that simulation studies cover (from small models to very large-scale
        simulation models), further research is needed to determine with respect to the size and

        type of simulation study
            o which model validation approach should be used,

            o how should model validation be managed,
            o what type of support system software for model validation is needed.

        Validating large-scale simulations that combine different simulation (sub-) models and use
        different types of computer hardware such as in currently being done in HLA (Higher Level
        Architecture). A number of these VV&A issues need research, e.g. how does one verify that
        the simulation clocks and event (message) times (timestamps) have the same representation

        (floating point, word size, etc.) and validate that events having time ties are handled
        properly.


        4.1.5. Immersive Simulation

Introduction and definition

As policy models grow in size and complexity, the process of analysing and visualising the resulting
large amounts of data becomes an increasingly difficult task. Traditionally, data analysis and
visualisation were performed as post-processing steps after a simulation had been completed. As

simulations increased in size, this task became increasingly difficult, often requiring significant
computation, high-performance machines, high capacity storage, and high bandwidth networks.
Computational steering is an emerging technology that addresses this problem by “closing the loop”
and providing a mechanism for integrating modelling, simulation, data analysis and visualisation.
This integration allows a researcher to interactively control simulations and perform data analysis

while avoiding many of the pitfalls associated with the traditional batch / post processing cycle. This
research challenge refers to the issue of the integration of visualisation techniques within an
integrated simulation environment. This integration plays a crucial role in making the policy
modelling process more extensive and, at the same time, comprehensible. In fact, the real aim of
interactive simulation is, on the one hand, to allow model developers to easily manage complex

models and their integration with data (e.g. real-time data or qualitative data integration) and, on
the other hand, to allow the other stakeholders not only to better understand the simulation results,
but also to understand the model and, eventually, to be involved in the modelling process.
Interactive simulation can dramatically increase the efficiency and effectiveness of the modelling




                                                                                         43 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


and simulation process, allowing the inclusion and automation of some phases (e.g. output and
feedback analysis) that were not managed in a structured way up to this point.



Why it matters in governance

Interactive simulation is a particular aspect of simulation. As far as the Policy Assessment in
Governance is concerned, this challenge may:

        Accelerate the simulation process: policy makers would be able to analyse simulation results,
        eventually run new scenarios and make decisions as soon as possible and at the minimum
        cost.

         Collaborative environment: the bigger is the number of stakeholders involved in policy
        modelling and simulation process, the greater is the necessity of an interactive simulation
        environment that allows non-experts to use the model and understand results as well as

        permit experts to easily understand new requirements and consequent modification.
        Citizen engagement: interactive simulation tools help to engage citizens in policy-making

        process and to display to them in a simple way the results.
        Data integration: interactive simulation tools allow better managing of a large number and

        different types of data and information, both for input and output/feedback analysis.


Current Practice and Inspiring cases

In current practice, data analysis and visualisation, albeit critical for the process, are often
performed as a post-processing step after batch jobs are run. For this reason, the errors invalidating
the results of the entire simulation may be discovered only during post-processing. What is more,

the decoupling of simulation and analysis/visualisation can present serious scientific obstacles to the
researcher in interpreting the answers to “what if” questions. Given the limitations of the batch /
post processing cycle, it might be advisable to break the cycle and improve the integration of
simulation and visualisation. Implementation of an interactive simulation and visualisation
environment requires a successful integration of the many aspects of scientific computing, including

performance analysis, geometric modelling, numerical analysis, and scientific visualisation. These
requirements need to be effectively coordinated within an efficient computing environment.
Recently, several tools and environments for computational steering have been developed. They
range from tools that modify performance characteristics of running applications, either by
automated means or by user interaction, to tools that modify the underlying computational

application, thereby allowing application steering of the computational process.


Available Tools

A review of the available tools is ongoing.


Key challenges and gaps

However, the development of immersive tools is still based on model developers needs and
therefore a gap still exists between requirements of policy makers and those of developers. In a

collaborative modelling environment, interaction is fundamental in order to speed up the process
and make ICT tools user-friendly for all the stakeholders involved in the policy model development
process.


                                                                                         44 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Current research

In the current research, interactive visualisation typically combines two main approaches: providing
efficient algorithms for the presentation of data and providing efficient access to the data. The first
advance is evident albeit challenging. Even though computers continually get faster, data sizes are

growing at an even more rapid rate. Therefore, the total time from data to picture is not decreasing
for many of the problem domains. Alternative algorithms, such as ray tracing (Nakayama, 2002) and
view dependent algorithms (Lessig, 2009) can restore a degree of interactivity for very large
datasets. Each of those algorithms has its trade-offs and is suitable for a different scenario. The
second advance is less evident but very powerful. Through the integration of visualisation tools with

simulation codes, a scientist can achieve a new degree of interactivity through the direct
visualisation and even manipulation of the data. The scientist does not necessarily wait for the
computation to finish before interacting with the data, but can interact with a running simulation.
While conceptually simple, this approach poses numerous technical challenges.



Future research

With regard to future research, interactive simulation plays a crucial role in a collaborative modelling
environment. The trade-off between the possibility of enlarging models and including several kinds
of data, and the number of people that can understand and modify the model should be deeply
analysed. For this purpose, some fundamental issues must be approached:

        Systems should be modular and easy to extend within the existing codes.

        Users of the systems should be able to add new capabilities easily without being experts in
        systems programming.

        Input / output systems should be easily integrated.

        Steering systems should be adaptable to hardware ranging from the largest of
        supercomputing systems to low-end workstations and PCs.


        4.1.6. Output Analysis and Knowledge Synthesis

Introduction and definition

Inputs driving a simulation are often random variables, and because of this randomness in the
components driving simulations, the output from a simulation is also random, so statistical
techniques must be used to analyse the results. In particular, the output processes are often non-

stationary and auto-correlated and classical statistical techniques based on independent identically
distributed observations are not directly applicable. In addition, by observing a simulation output, it
is possible to infer the general structure of a system, so ultimately gaining insights on that system
and being able to synthesise knowledge on it. There is also the possibility to review the initial
assumptions by observing the outcome and by comparing it to the expected response of a system,

i.e. performing a modelling feedback on the initial model. Finally, one of the most important uses of
simulation output analysis is the comparison of competing systems or alternative system
configurations.

Visualisation tools are essentials for the correct execution of this iterative step. The present research
challenge deals with the issue of output analysis of a policy model and, at the same time, of
feedback analysis in order to incrementally increase and synthesise the knowledge of the system.





                                                                                         45 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Why it matters in governance

Output analysis is a specific aspect of simulation. According to the general need for policy
assessment and evaluation, there are some specific issues stemming from the output analysis, which
are strongly related to governance:

        Acceleration of policy assessment process: automated output analysis tools would help
        policy makers to efficiently and effectively analyse the impacts of a policy even if the large

        number of simulation data must be taken into account
        Citizen engagement: user-friendly automated tools for output analysis can be offered to

        citizens in order to share the simulation results and better engage them in policy-making
        process.


Current Practice and Inspiring cases

In the current practice a large amount of time and financial resources are spent on model
development and programming, but little effort is allocated to analyse the simulation output data in

an appropriate manner. As a matter of fact, a very common way of operating is to make a single
simulation of somewhat arbitrary length run and then treat the resulting simulation estimates as
being the "true" characteristics of the model. Since random samples from probability distributions
are typically used to drive a simulation model through time, these estimates are realisations of
random variables that may have large variances. As a result, these estimates could, in a particular

simulation run, differ greatly from the corresponding true answers for the model. The net effect is
that there may be a significant probability of making erroneous inferences about the system under
study. Historically, there are several reasons why output data analysis was not conducted in an
appropriate manner. First, users often have the unfortunate impression that simulation is just an
exercise in computer programming. Consequently, many simulation studies begun with heuristic

model building and computer coding, and end with a single run of the program to produce "the
answers." In fact, however, a simulation is a computer-based statistical sampling experiment. Thus,
if the results of a simulation study are to have any meaning, appropriate statistical techniques must
be used to design and analyse the simulation experiments and ICT tools must be developed to make

the process more effective and efficient. In addition, there are some important issues of output
analysis that are not strictly connected to statistics. In particular, an evident gap in literature regards
the analysis and integration of feedbacks in modelling and simulation process. Actually, stakeholders
are involved, in a post-processing phase, in order to analysis the results (more often only the
elaboration of them) and understand something about the policy. Sometimes they are able to give a

feedback on the difference between their expectations and the result but the process is not
structured and effective tools are lacking. The development of tools for analysing and integrating
feedbacks should be explored in order to enlarge the number of stakeholders involved and, at the
same time, to allow efficient and effective modification at each phase of the process, incrementally
increasing the knowledge of the model and, consequently, of the given policy.



Available Tools

A review of the available tools is ongoing.








                                                                                          46 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Key challenges and gaps

A fundamental issue for statistical analysis is that the output processes of virtually all simulations are
non-stationary (the distributions of the successive observations change over time) and auto
correlated (the observations in the process are correlated with each other). Thus, classical statistical

techniques based on independent identically distributed observations are not directly applicable. At
present, there are still several output-analysis problems for which there is no commonly accepted
solution, and the solutions that are available are often too complicated to apply. Another
impediment to obtaining accurate estimates of a model's true parameters or characteristics is the
cost of the computer time needed to collect the necessary amount of simulation output data.

Indeed, there are situations where an appropriate statistical procedure is available, but the cost of
collecting the amount of data dictated by the procedure is prohibitive.


Current research

In current research, main references are Law (1983), Nakayama (2002) , Alexopoulos & Kim (2002),
Goldsman & Tokol (2000), Kelton (1997), Alexopoulos & Seila (1998), Goldsman & Nelson (1998),

Law (2006).
For output analysis, there are two types of simulations:

        Finite-horizon simulations. In this case, the simulation starts in a specific moment and runs
        until a terminating event occurs. The output process is not expected to achieve steady-state

        behaviour and any parameter estimated from the output will be transient in a sense that its
        value will depend upon the initial conditions (e.g. a simulation of a vehicle storage and
        distribution facility in a week time).

        Steady-state simulations. The purpose of a steady-state simulation is the study of the long-
        run behaviour of the system of interest. A performance measure of a system is called a
        steady-state parameter if it is a characteristic of the equilibrium distribution of an output
        stochastic process (e.g. simulation of a continuously operating communication system where

        the objective is the computation of the mean delay of a data packet).


Future research

Referring to previous cited works and in particular to Goldsman (2010), future research should
further explore following issues:

        ICT tools for supporting or automating output/feedback analysis

        Allowing an incremental understanding of the model (knowledge synthesis)

        Adapting Design Of Experiment (DOE) for policy model simulation
        Use and integration of more-sophisticated variance estimators

        Better ranking and selection techniques.











                                                                                           47 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



     4.2. Data-powered Collaborative Governance

         4.2.1. Big Data

Summary Overview


 Current free tools                    Top market tools                    Current and Future Research



 The freely available tools permit to  -Data storage platforms and other   -Technologies   for  collecting  cleaning,
 overcome data limitations, simplify   information         infrastructure  storing     and       managing       data:
 the analytical process and visualize
 results. The functionalities provided solutions                           datawarehouse;    pivotal  transformation;
 by these software are:                -MPP                                ETL;  I/O;  efficient  archiving, storing,
                                                                           indexing,   retrieving,  and     recovery;
 -massively parallel processing (MPP)  -Dataflow    engines,   software    streaming, filtering, compressed sensing
 database product for large-scale      interconnect technologies           sufficient  statistics; automatic    data
 analytics   and    next-gen    data
 warehousing                           -Data discovery and exploration     annotation; Large Database Management
 -data-parallel implementations of                                         Systems;   storage   architectures;  data
                                       tools                               validity, integrity, consistency, uncertainty
 statistical and machine learning      -Built-in text analytics, enterprise-anagement;         languages,      tools,
 methods                                                                   methodologies      and       programming
 -visual data mining modelling         grade security and administrative
                                       tools                               environments
                                                                           -Technologies for summarizing data and
                                       -RTAP
                                                                           extracting   some     meaning:    reports;
                                       - SaaS                              dashboard;    statistical  analysis   and
                                       -Visualization features supporting  inference;      Bayesian       techniques;
                                                                           information extraction from unstructured,
                                       exploratory     and     discovery   multimodal data; scalable and interactive
                                       analytics
                                                                           data    visualization;  extraction    and
                                       -OLAP                               integration of knowledge from massive,
                                       -BI/DW                              complex, multi-modal, or dynamic data;
                                                                           data mining; scalable machine learning;
                                       -EDW                                data-driven   high   fidelity simulations;

                                                                           scalable  machine    learning;  predictive
                                                                           modelling,  hypothesis   generation   and
                                                                           automated                        discovery




                                                                           -Technologies for using data a decision
                                                                           tool: Decision Trees, Pro-Con Analysis, Rule
                                                                           Based Systems, Neural Networks, Tradeoff

                                                                           based Decisions




Introduction and definition


Big Data refers to dataset that cannot be stored, captured, managed and analysed by the mean of
conventional database software. Thereby Big Data is a subjective rather than a technical definition,

because it does not involve a quantitative threshold (e.g. in terms of terabytes), but instead a
moving technological one. Keeping that in mind, the definition of Big Data in many sectors ranges
from a few terabytes to multiple petabytes . The definition of Big Data does not merely involve



21 1 terabyte is equal to 1 trillion bytes

22 1 petabyte is equal to 1000 terabytes



                                                                                                        48 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


the use of very large data sets, but concerns also a computational turn in thought and research

(Burkholder, L, ed. 199.)
As stated by Latour (2009) when the tool is changed, also the entire social theory going with it is

different. In this view Big Data has emerged a system of knowledge that is already changing the
objects of knowledge itself, as it has the capability to inform how we conceive human networks and
community. Big Data creates a radical shift in how we think research itself. As argued by Lazer et
al.(2009), not only we are offered the possibility to collect and analyze data at an unprecedented
depth and scale, but also there is a change in the processes of research, the constitution of

knowledge, the engagement with information and the nature and the categorization of reality. The
potential stemming from the availability of a massive amount of data is exemplified by Google. It is
widely believed that the success of the Mountain View company is due to its brilliant algorithms, e.g.
PageRank. In reality the main novelties introduced in 1998, which brought to second generation

search engines, involved the recognition that hyperlinks were an important measure of popularity
and the use of the text of hyperlinks (anchortext) in the web index, giving it a weight close to the
page title. This is because first generation search engines used only the text of the web pages, while
Google added two data set (hyperlinks and anchortext), so that even a less than perfect algorithm

exploiting this additional data would obtain roughly the same results as PageRank. Another example
is the Google’s AdWords keyword auction model. Overture had previously shown that ranking
advertisers for a given keyword based purely on their bids was an efficient mechanism. Google
improved the tool by adding the data on the clickthrough rate (CTR) on each advertiser's ad, so that
advertisers were ranked by their bid and their CTR.



Why it matters in governance

Big Data have a huge impact also in governance and policy making. In fact their benefits apply to a
wide variety of subjects:

        Health care: making care more preventive and personalized by relying on a home-based
        continuous monitoring, thereby reducing hospitalization costs while increasing quality.
        Detection of infectious disease outbreaks and epidemic development

        Education: by collecting all the data on students’ performance, it would be possible to design
        more effective approaches. The collection of these data is made possible thanks to massive

        Web deployment of educational activities
        Urban planning: huge high fidelity geographical datasets describing people and places are

        generated from administrative systems, cell phone networks, or other similar sources.
        Intelligent transportation based on the analysis and visualization of road network data, so as

        to implement congestion pricing systems and reduce traffic

        The use of ubiquitous data collection through sensors networks in order to improve
        environmental modelling

    Analysis and clarification of the energy pattern use through data analytics and smart meters,
    which can be useful for the adoption of energy saving policies avoiding blackouts

    Integrated analysis of contracts in order to find relations and dependencies among financial
    institutions in order to assess the financial systemic risk

    The analysis of conversation in social media and networks, as well as the analysis of financial
    transaction carried out by alleged terrorists, which can be used for homeland security




                                                                                            49 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    Assessment of computer security by the mean of the logged information analysis, i.e. Security

    Information and Event Management

    Better track of food and pharmaceutical production and distribution chain

    Collect data on water and sewer usage in order to reduce water consumption by detecting leaks

    Use of sensors, GPS, cameras and communication systems for crisis detection, management and

    response

    Use of sensors’ data for carbon footprint management



Policy Applications of Big Data Tools

There is a growing body of evidence highlighting the applications of Big Data not only in traditional
hard science and business, but also in policy making due to the predictive power of the data. Let us

see some applications:

         Predictability of human behaviour and social events. A research team from Northwestern
         University was able to predict people’s location based on mobile phone information
                                                                                    24
         generated from past movements. Moreover Pentland from MIT conducted a research
         showing that mobile phones can be used as sensors for predicting human behaviour, as they

         can quantify human movements in order to explain changes in commuting patterns given for
         example by unemployment. Recently another research team from Northeastern University

         was able to predict the voting outcome in the scope of a famous US television programme
         (American Idol) based on Twitter activity during the time span deﬁned by the TV show airing
                                              25
         and the voting period following it

         Public health. Online data can be used for syndromic surveillance, also called
         infodemiology . As an example Google Flu Trends is a tool based on the prevalence of
                                                                                          27
         Google queries for flu-like symptoms. As shown by Ginsberg et al. (2008) it is then possible
         to use search queries to detect influenza epidemics in areas with a large population of web
                                                                                                             28
         search users. In fact according to the US Center for Disease Control and Prevention (CDC) a
         great availability of data coming from online queries can help to detect epidemic outbursts

         before laboratory analysis. Another related tool is the Google Dengue Tren29 In this view the
         analysis of health related Tweets in US by Paul and Dredze (2011) found a high correlation
         between the modeled and the actual flu rate. In the same way Twitter’s data can be
                                                                                30
         analyzed to study the geographic spread of a virus or disease . Finally we can talk about
         Healthmap    31 in which data from online news, eyewitness reports, expert-curated



23http://online.wsj.com/article/SB10001424052748704547604576263261679848814.html

24http://www.nytimes.com/2011/04/24/business/24unboxed.html?_r=1&src=tptw>

25http://www.mobs-lab.org/uploads/6/7/8/7/6787877/american_idol_finale.pdf
26
  http://yi.com/home/EysenbachGunther/publications/2006/eysenbach2006c-infodemiologyamia-proc.pdf
27
  http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/p

apers/detecting-influenza-epidemics.pdf >
28http://www.cdc.gov/ehrmeaningfuluse/Syndromic.html

29http://www.cs.jhu.edu/%7Empaul/files/2011.icwsm.twitter_health.pdf

30http://www.ncbi.nlm.nih.gov/pubmed/21573238
31
  http://healthmap.org/en/


                                                                                                   50 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


         discussions, official reports, are used to get a thorough view of the current global state of

         infectious diseases which is visualised on a map

         Global food security. The Food and Agriculture Organization of the UN (FAO) is chartered
         with ensuring that the world’s knowledge of food and agriculture is available to those who
                                                                                          32
         need it when they need it and in a form which they can access and use . In fact human
         population will approach 9 billion by 2050, thereby it will be necessary to put in place
         policies aimed at ensuring a sufficient and fair distribution of resources. In fact the world

         food production will have to increase by 60% by increasing the agricultural production and
         fighting water scarcity. The online data portal to be launched by FAO will enhance planners’
         and decision makers’ capacity to estimate agricultural production potentials and variability

         under different climate and resources scenarios

         Environmental analysis. In the last United Nations conference on climate (i.e. COP 17) taking
         place in 2011, The European Environment Agency, the geospatial software company Esri and
                                                            33
         Microsoft presented the network Eye on Earth , which can be used to create an online site
         and group of services for scientists, researchers and policy makers in order to share and
         analyze environmental and geospatial data. Other three projects launched by these

         institutions at COP 17 include WaterWatch (using EEA’s water data); AirWatch, (about EEA’s
         air quality data); and finally NoiseWatch, which is a combination between environmental
         data with user-generated information provided by citizens. Moreover during 2010 United

         Nations climate meeting (COP 16) Google launched its own satellite and mapping service
         Google Earth Engine , which is a combination of a computing platform, an open API and

         satellite imagery along 25 years. All these tools will be available to scientists, researchers
         and governmental agencies for analyzing the environmental conditions in order to make
         sustainability decisions. In this way the government of Mexico created a map of the

         country’s forest incorporating 53,000 Landsat images, which can be used by the federal
         authority and the NGOs to make decisions about land use and sustainable agriculture.

         Crisis management and anticipation. In occasion of the Haiti earthquake : an European

         Commission’s Joint Resear36 Center team used the damage reports mapped on the
         Ushahidi-Haiti platform to show that this crowdsourced data can help predict the spatial
         distribution of structural damage in Port-au-Prince. Their model based on 1645 SMS reports

         crowdsourced data almost perfectly predicts the structural damage of most affected areas
         reported in the World Bank-UNOSAT-JRC damage assessment performed by 600 experts
         from 23 countries in 66 days based on high resolution aerial imagery of structural damage.
                                                           37
         As for future developments, some researches          highlight the fact that Big Data can be used
         for crisis management and anticipation by building up crisis observatories, i.e. laboratories
         devoted to the collecting and processing of enormous volumes of data on both natural

         systems and human techno-socio-economic systems, so as to gain early warnings of
         impending events. With those capacity would be possible to set up Crisis and Observatories





32http://data.fao.org/ and http://www.grdi2020.eu/Repository/FileScaricati/050e1e8a-3e69-4ba0-86a5-b8f7c8322ebe.pdf
33
  http://www.eyeonearth.org/
34
  http://earthengine.google.org/#intro
35http://publications.jrc.ec.europa.eu/repository/handle/111111111/15684

36http://haiti.ushahidi.com/
37
  http://arxiv.org/pdf/1012.0178v5.pdf


                                                                                                51 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



         for financial and economic, for armed conflicts, for crime and corruption, for social crisis, for
         health risks and disease spreading, for environmental changes.

         Global Development. An inspiring example is given by Global Pulse , which is a Big Data

         based innovation programme fostered by the UN Secretary-General and aimed at harnessing
         today's new world of digital data and real-time analytics in order foster international

         development, protect the world's most vulnerable populations, and strengthen resilience to
         global shocks. The programme is rooted on three main pillars: research on new data
         indicators providing real-time understanding of community’s welfare as well as real-time

         feedback on policies; creation of a toolkit of free open-source software for mining real-time
         data useful for shared evidence-based decisions; the establishment of country-level

         innovation centres (Pulse Lab) where real-time data are applied to development challenges.
         The programme encompasses 5 main projects carried out with several partners:

             o “Daily Tracking of Commodity Prices: the e-Bread Index” , which investigates how

                  scraping online prices could provide real-time insights on price dynamics
                                                                                   40
             o “Unemployment through the Lens of Social Media”                       , which relates the
                  unemployment statistics with unemployment-related conversation from open social
                  web

                                                                               41
             o “Twitter and the Perception of Crisis Related Stress” , which investigates what
                  indicators can help in understanding people’s concerns on food, fuel, finance,
                  housing

             o “Monitoring Food Security Issues through New Media” , which finds emerging

                  trends related to food security using text analysis, semantic clustering and networks
                  theory

             o “Global Snapshot of Wellbeing – Mobile Survey” , aimed at experimenting new

                  tools able of replicating the standards of traditional household surveys in real-time
                  on a global scale




















38
  http://www.unglobalpulse.org/about-new
39http://www.unglobalpulse.org/projects/comparing-global-prices-local-products-real-time-e-pricing-bread

40http://www.unglobalpulse.org/projects/can-social-media-mining-add-depth-unemployment-statistics
41
  http://www.unglobalpulse.org/projects/twitter-and-perceptions-crisis-related-stress
42
  http://www.unglobalpulse.org/projects/news-awareness-and-emergent-information-monitoring-system-food-security
43http://www.unglobalpulse.org/projects/global-snapshot-wellbeing-mobile-survey



                                                                                                 52 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        Intelligence and security. As examples of governments’ commitment to Big Data for national
         security we can present the Cyber-Insider Threat (CINDER) program, which aims at

         developing new ways for detecting cyber espionage activities in military computer networks
         as well as at increasing the accuracy, rate and speed with which cyber threats are detected.
         Another example is the Anomaly Detection at Multiple Scales (ADAMS) program led by the

         Defense Advanced Research Project Agency (DARPA), which addresses the problem of
         anomaly-detection and characterization in massive data sets. The program will be initially
         applied to insider-threat detection, in which individual actions are recognized as anomalous

         with comparison to a background of routine network activity. Finally the Center of
         Excellence on Visualization and Data Analytics (CVADA) of the Department for Homeland
         Security (DHS) is leading a research effort on data that can be used by first responders to

         tackle with natural disasters and terrorists attacks, by law enforcement to border security
         concerns, or to detect explosives and cyber threats.

An Interesting Application: Smart Cities

A Smart City is a public administration or authorities delivering services and infrastructure based on
ICT which are easy to use, efficient, responsive, open and sustainable for the environment. We can
                              46
identify six main dimensions :
        Smart economy, characterized by high standard of living and competitive elements:

         innovative and entrepreneurship, high productivity, flexibility of labour market,
         internationalism, ability to transform;

        Smart mobility, i.e. efficient public transportation system, local and international
         accessibility, availability of ICT-infrastructure, sustainability and safety;

        Smart environment (sustainability of natural resources): low pollution, protection of

         environment, natural attractiveness;

        Smart people, given by high level of human and intellectual capital, high level of
         qualification, lifelong learning, social and ethnic diversity, flexibility, creativity;

        Smart living (high quality of life); presence of cultural facilities, healthy environment
         conditions, individual safety, housing quality, education facilities, touristic attractiveness and

         social cohesion;

        Smart governance given by citizens’ participation in decision-making, the presence of public
         and social services and of transparent and open governance.















44
  http://www.darpa.mil/Our_Work/I2O/Programs/Cyber-Insider_Threat_%28CINDER%29.aspx
45http://www.darpa.mil/Our_Work/I2O/Programs/Anomaly_Detection_at_Multiple_Scales_%28ADAMS%29.aspx

46See also the project EuropeanSmartCities at http://www.smart-cities.eu/model.html



                                                                                               53 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


The combination of all the benefits stemming from Big Data in governance, make clear that the
integration of heterogeneous data from various domains bear high potential to provide insights on

cities. New technologies will unlock massive amounts of data about all the aspects of the city as well
as its citizens. For instance new systems involving energy use at fixed locations (like house and
office) are being implemented by the mean of smart metering as well as the integration of various
information systems used to record pricing and activity. Another possibility is given by the extraction

of positional and frequency data from social media such as Twitter, Facebook, Flickr and Foursquare.
All this data will be used for fulfilling the Smart Cities targets. Let us take into account for instance
the transportation system, where diagnosing and anticipating abnormal events such as traffic

congestions requires integration of various data like traffic data, weather data, road conditions, or
traffic light strategy. Another possibility will be given by e-inclusion technologies and open data for
governance. One important example of the development of the Smart City concept at large scale is
                                                               47
the New York City project “Roadmap for a Digital Future” , which outlines a path to build on New
York City's successes and establish it as the world's top-ranked Digital City, based on indices of
internet access, open government, citizen engagement, and digital industry growth.



Recent Trends

Big Data is a fast growing phenomenon: as the Google CEO Eric Schmidt pointed out in 2010,
currently in two days is created in the world as much information as it was from the appearance of
                           48
man till 2003. Nowadays it is possible to store all the world’s music in a $600 worth disk drive,
while Facebook content shared every month amounts to $30 billion. According to the forecast global
data will grow at a 40% rate next year while the total IT spending will grow just by 5%. By 2010 users
and companies will have store more than 13 exabytes of new data, which is over 50,000 times the

data in the Library of Congress.






























47http://www.nyc.gov/html/mome/digital/html/roadmap/theroadmap.shtml
48
  See McKinsey Global Institute (2011) “Big data: The next frontier for innovation, competition, and productivity”


                                                                                               54 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



Big Data is also a potential booster for the economy, bearing a $300 billion potential annual value to
US health care as well as a $600 billion potential annual consumers surplus from using personal
location data globally and a €250 potential annual value to European public administration. In fact

the European Commission is expected to adopt an Open Data Strategy, i.e. a set of measures aimed
at increasing government transparency and creating a €32 billion a year market for public data.
                                                                    49
Finally as reported last year by the McKinsey Global Institute , the United States will need 140,000
to 190,000 more workers with deep analytical expertise and 1.5 million more data-literate

managers. Always according to the McKinsey Global Institute the potential value of global personal
location data is estimated to be $700 billion to end users, and it can result in an up to 50% decrease
in product development and assembly costs. What’s the growth engine of big data? From one side

more “old world” data is produced through “open governance” and digitization. From the other side
“new world” data are created are continuously collected in domains such as “in silico” medicine, “in

silico engineering” and internet science. Brand new fields of science are being created:
computational chemistry, biology, economics, engineering, mechanics, neuroscience, geophysics,
etc. etc. This is true also in humanities, such as the birth of computational social science, based on

mobile phones and social network digital traces. A wide array of actors including humanities and
social science academics, marketers, governmental organizations, educational institutions, and

motivated individuals, are now engaged in producing, sharing, interacting with, and organizing data.
All these developments are allowed by the rise of new technologies for data collections: web logs;
RFID; sensor networks; social networks; social data (due to the Social data revolution), Internet text

and documents; Internet search indexing; call detail records; astronomy, atmospheric science,
genomics, biogeochemical, biological; military surveillance; medical records; photography archives;

video archives; large-scale eCommerce.



Inspiring cases
                TM             50
The Ion Proton     Sequencer .
                                        51
The NIH Human Connectome Project
                                                  52
The Models of Infectious Disease Agent Study
                 53
MyTransport.sg
                  54
UN Global Pulse

Tools on the market

Freely available tools

There are not many cases of freely available tools for Big Data analysis on the market.

The presence of freely available tools on the market bear many benefits, such as:




49
  http://www.mckinsey.com/Features/Big_Data
5http://www.lifetechnologies.com/global/en/home/about-us/news-gallery/press-releases/2012/life-techologies-
itroduces-the-bechtop-io-proto.html.html

51http://neuroscienceblueprint.nih.gov/connectome/index.htm

52http://www.nigms.nih.gov/Research/FeaturedPrograms/MIDAS/
53
  http://www.mytransport.sg/content/mytransport/home.html
54
  http://www.unglobalpulse.org/about-new


                                                                                                 55 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


         developers and analysts will use them to experiment with emerging types of data structure
         so as to develop new and different analytical procedures, he added


         developers and IT professionals contribute their findings and know-how back into the
         industry to drive knowledge exchange

The freely available tools permit to overcome data limitations, simplify the analytical process and

visualize results. The functionalities provided by these software are:

         massively parallel processing (MPP) database product for large-scale analytics and next-gen
         data warehousing

         data-parallel implementations of statistical and machine learning methods

         visual data mining modelling

In this view are very important the free Big Data tools developed by Greenplum for data scientists
                                                             55                                             56
and developers: MADlib and Alpine In-Database Miner and Greenplum HD Community Edition .
                                                                                                     57
Some other software partially for free with important Big Data applications: KNIME , Weka /
Pentaho , Rapid-I RapidAnalytics , Rapid-I RapidMiner . Finally there is R , which although was

not built for Big Data, it has interesting application in this realm.



Enterprise-level software

The enterprise-level software is adopted for the following functionalities:

     open source software based on Apache Hadoop

     data storage platforms and other information infrastructure solutions

     shared-nothing massively parallel processing (MPP) database architectures


     dataflow engines, software interconnect technologies

     data discovery and exploration tools

     built-in text analytics, enterprise-grade security and administrative tools

     real-time analytic processing (RTAP) platforms

     software-as-a-service (SaaS)

     visualization features supporting exploratory and discovery analytics

     on-line analytical processing (OLAP)


     BI/DW (business intelligence and data warehousing)



55
  http://www.greenplum.com/community/downloads/analytics-tools/
56http://www.greenplum.com/community/downloads/database-ce/

57http://www.knime.org/
58
  http://weka.pentaho.com/
59
  http://rapid-i.com/content/view/182/196/
60
  http://rapid-i.com/content/view/181/196/
61http://www.r-project.org/



                                                                                                    56 | P a g e
                                 0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



     EDW (enterprise data warehousing)Examples of these software include: Tableau BI platform ;                           62
                                          63                                            64                        65
     SAS Data Integration Studio ; SAS High Performance Analytics ; SAS On Demand ; SAND
                           66             67                       68                                                     69
     Analytic Platform ; SAP BEx ; SAP NetWeaver ; SAP In-Memory Appliance (SAP HANA) ;
     ParAccel Analytic Database (PADB) ; IBM Netezza ; IBM InfoSphere BigInsights ; IBM                            72
                             73                   74                   75                                   76
     InfoSphere Streams ; Kognitio WX2 ; Kognitio Pablo ; EMC Greenplum Database ; Greenplum
         77                                                        78                         79                          80
     HD ; EMC Greenplum Data Computing Appliance ; Greenplum Chorus ; Cloudera Enterprise ,
                           81
     StatSoft Statistica .

Some other software which have not been built specifically for Big Data applications, but
                                                                                82           83            84
nonetheless can be used for Big Data analytics are: Mathematica , MatLab and Stata .




Key challenges and gaps

In order to enjoy all the potential stemming from Big Data it would be necessary to remove the

technological barrier preventing the exchange of data, information and knowledge between,

disciplines, as well as to integrate activities which are based on different ontological foundations.
Even though Big Data have provided a lot of benefits, many challenges are still to be coped with. For








62
  http://www.tableausoftware.com/products/server
63
  http://support.sas.com/documentation/onlinedoc/etls/

64http://www.sas.com/software/high-performance-analytics/in-memory-analytics/analytics.html

65http://www.sas.com/solutions/ondemand/

66http://www.sand.com/analytics/architecture/

67
  http://scn.sap.com/community/business-explorer
68
  http://www.sap.com/platform/netweaver/index.epx
69
  http://www.sap.com/solutions/technology/in-memory-computing-platform/hana/overview/index.epx

70http://www.paraccel.com/

71http://www-01.ibm.com/software/data/netezza/

72
  http://www-01.ibm.com/software/data/infosphere/biginsights/
73
  http://www-01.ibm.com/software/data/infosphere/streams/
74
  http://www.kognitio.com/analyticalplatform
75
  http://www.kognitio.com/pablo

76http://www.greenplum.com/products/greenplum-database

77http://www.greenplum.com/products/greenplum-hd

78
  http://www.greenplum.com/products/greenplum-dca
79
  http://www.greenplum.com/products/chorus
80
  http://www.cloudera.com/products-services/enterprise/

81http://www.statsoft.com/

82http://www.wolfram.com/mathematica/

83
  http://www.mathworks.com.au/products/matlab/
84
  http://www.stata.com/



                                                                                                              57 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


instance Gartner (2011) argues that the challenges are not only given by the volume of data, but

also by the variety (heterogeneity of data types and representation, semantic interpretation) and
velocity (rate of data arrival and action timing). According to the recent research those
advancements include    86

         Data modelling challenges: data models coherent to the data representation needs; data

         models able to describe discipline specific aspects; data models for representation and
         query of data provenance and contextual information; data models and query languages
         representing and managing data uncertainty, and representing and querying data quality

         information

         Data management challenges: provide quality, cost-effective, reliable preservation and
         access to the data; protect property rights, privacy and security of sensible data; ensure
         data search and discovery across a wide variety of sources; connect data sets from different

         domains in order to create open linked data space data can be unstructured or semi-
         structured with no context; different data format; different data labels used for same data
         elements; different data entry conventions and vocabularies used; - data entry errors; data

         sets can be so large they cannot be effec87vely processed by a single machine; data
         parallelization and task parallelization .

         Data service/tools challenges: data tools for most scientific disciplines are inadequate to
         support research in all its phases so that scientists are less productive than what they might

         be. In fact there is the need of software able to “clean”, analyse and visualize huge amounts
         of data. Moreover are missing data tools and policies for the ensuring the cross
         collaboration and fertilization among different disciplines and scientific realms



As for other issues concerning Big Data, Boyd and Crawford (2011) highlight some of them:

         Relationship between automatic search and the definition of knowledge. at the beginning
                   th
         of the 20 century Ford introduced the mass production, automation and assembly line,
         reshaping not only the way things are produced, but also the general understanding of labor,
         the human relationship to work, and the society at large. Fordism consisted in breaking

         down holistic tasks into atomized and independent ones. In the same way Big Data is a new
         system of knowledge characterized by a computational turn in science leading to a change in
         the constitution of knowledge, the process of research and the categorization of reality. But

         as the Fordism had limits (indeed has been overcome by the Just in Time paradigm), also the
         specialized Big Data tools are not flawless. Big Data, as a new system of knowledge can
         change the very meaning of learning itself, with all the possibilities and limitations

         embedded in the systems of knowing




85
  http://my.gartner.com/portal/server.pt?open=512&objID=202&mode=2&PageID=5553&resId=1727219

2011. Available at http://www.gartner.com/it/page.jsp?id=1731916
86http://www.grdi2020.eu/Repository/FileScaricati/6bdc07fb-b21d-4b90-81d4-d909fdb96b87.pdf

87Some Big Data challenges are deeply related with policy making, such as the fact that many agencies pay a high premium
to both internal resources and external third parties to manage their data. Additionally, data management can sometimes
be redundant if not properly set up. Moreover regulations do not take into account the new, expanded capabilities that IT
offers as it takes time to issue a new law and bureaucrats are not so keen to novelty





                                                                                               58 | P a g e
                   0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Big Data may produce misleading claims of objectivity and accuracy. In the science there is
a deep cleavage between qualitative and quantitative scientists. Apparently qualitative

scientists would be engaged in creating and interpreting stories, while quantitative scientists
would be in the business of producing facts. Needless to say, that is not case as all the
objectivity claims come from subjects, who make subjective observation and choices.
Moreover data analysis is based on a tons of assumptions (see for instance the asymptotic

theory in statistics) and on the other hand even though a model may be mathematically or
an experiment may be scientifically valid, the final interpretation is subjective. Other
examples are the difficulty of integrating in a consistent way different datasets, the arbitrary
choices inherent data cleaning and finally the fact that internet databases may well be
affected by bias such as frictions and self selection. In this view, by increasing the

quantification space, especially in social sciences, Big Data might support objectivity and
accuracy claims which are not really grounded on good sense and reality.


A higher quantity of data does not always mean better data. In all sciences there is a

massive amount of literature (interpretation bias, design standardization, sampling
mechanism and question bias, statistical significance and diagnostics) aimed at ensuring the
consistency of data collection and analysis. Curiously, Big Data scientists sometimes assume
a priori quality of their data and completely neglects the methodological issues proper of
global sciences. A clear example is given by social media data, which are subject to self-

selection bias as people using social media is not representative of the society itself. Even
the definition of active user and account of a social media might not be innocuous: in fact it
is estimated that 40% of Twitter’s users are merely “listeners”, i.e. do not proactively take
part. Finally it has to be recognized that in my contexts high quality research is purposely

carried out with a limited amount of data, such as for instance in game theory experimental
analysis.


Big Data and Ethical Issues. The use for research purposes of “public” data on social media

websites opens the door to deontological issues. The problem is: can those data be used
without any ethical of privacy consideration? Obviously Big Data is an emerging field of
science, thereby ethical consideration are yet to be fully considered. How the researchers
can be sure that their activity is not harmful for some of their subjects? On one hand is
impossible to ask for data use permission from all the subjects present in a database. On the

other hand, the mere fact that the data are available does not justify their use.
Accountability to the field of research and accountability to the research subjects are the
ethical keys for Big Data. In all the traditional fields of science, researcher must follow a
series of professional standards aimed at protecting the rights and well being of human
subjects. On the other hand the ethical implications of Big Data research are not yet clear.



Digital divides created by Big Data. It is widely accepted that doing research on Big Data
automatically involves having a quick and easy access to databases. This is not the case, as
only social media companies have access to large datasets, and sell those data at a high

price, offering only small data sets to university based researchers. So researchers with a
considerable amount of founding or based inside those firms can have access to data that
the outsiders will not. Thereby their methodologies and claims cannot be verified. In this
view Big Data can create a new digital divide, between researchers belonging to the top
universities and working with the top companies, and scholar belonging to the periphery.



                                                                                  59 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

        But the digital divide can be also skills based: in fact only people with a strong computational

        background are able to wrangle through APIs and analyse massive quantities of data.
        Concluding there is a new digital divide between the Big Data reach, who are able to analyze
        and to buy datasets, and belong to top universities and companies, and the Big Data poor,
        who are outsiders



Finally according to the UN the Big Data challenges can be divided along two main dimension.

The data management:

        Privacy. The development of new technologies always raises privacy concerns for
        individuals, companies and societies. This is a very crucial issue as privacy, safety and
        diversity are important for defending the freedom of citizens, and obviously companies have
        the right to retain their confidential information. In the era of Big Data, the primary

        producers, who are the citizens using services and devices generating data, are seldom
        aware that they are doing so or how their data will be used. Sometimes it is also unclear to
        what extent users of social media such as Twitter consent to the analysis of their data. The
        pool of individual information shared by mobile phones and credit card companies, social

        media and research engines is simply astonishing. People must be conscious of that, as
        privacy is a freedom pillar.



        Access and sharing. A great amount of data is available online for the most disparate uses.
        On the other hand much data is retained by companies which are concerned about their
        reputation, the necessity to protect their competitiveness or simply lack the right incentive
        to do so. On the other hand there is a bunch of technical and regulatory arrangements which

        has to be put in place in order to ensure inter-comparability of data and interoperability of
        systems.


Data analysis:

        Summarising the data. Sometimes the data might be simply false or fabricated, especially
        with user-generated text-based data (blogs, news, social media messages). In addition

        sometimes data are derived from people’s perceptions, as in calls to health hotlines and
        online searches for symptoms. Another case is related to opinion mining and sentiment
        analysis, in which the true significance of the statements can be misled, so that the human
        factor is always crucial in the analysis. Another problem is that sometimes data are

        generated from expressed intentions in blogposts, online searches, mobile-phone systems
        for checking market price, which are not a sure indicator of actual intentions and final
        decisions. So there is a huge problem in summarizing facts from users’ generated text, as
        there might be a difficulty in distinguishing feeling from facts.

        Interpreting data. A very important concern is given by the sample selection bias, given by

        the fact that people generating data are not representative of the entire population. For
        instance younger generations use more internet and mobile devices. In this way the
        conclusions of the analysis are valid only for the sample at hand and cannot therefore be
        generalized. Sometimes dealing with huge amounts of data leads the researchers to focus on



88
  http://unglobalpulse.org/


                                                                                           60 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

        finding patterns or correlations without concentrating on the underlying dynamics. One

        thing is to find a correlation, another is to detect a causal relationship. Even more difficult is
        to identify the direction of the causal relationship without using a founding theory. A final
        issue is very much linked with using data from different sources, which can magnify the
        existing flaws in each database



Finally we have the challenges identified by the community white paper drafted with the
collaboration of a group of leading researchers across the United States :9

        Heterogeneity and incompleteness. Data must be structured prior to the analysis in an
        homogeneous way, as algorithms unlike humans are not able to grasp nuance. Most

        computer systems work better if multiple items are stored in an identical size and structure.
        But an efficient representation, access and analysis of semi-structured data is necessary
        because as a less structured design is more useful for certain analysis and purposes. Even
        after cleaning and error correction in the database, some errors and incompleteness will

        remain, challenging the precision of the analysis.
        Human collaboration. Even if analytical instruments gained tremendous advancements,

        there are still many realms in which the human factor is able to discover patterns algorithms
        cannot. An example can be found in the use of CAPTCHAs, which can discern human users
        from computer programmes. In this view a Big Data system cannot must involve a human
        presence. Given the complexity of today’s world, there is the necessity to harness human

        ingenuity from different domains through crowdsourcing. Thereby a Big Data system
        requires technologies able to support this kind of collaboration even in case of conflicting
        statements and judgments.



Current Big Data Techniques

Big datasets can be analysed by the mean of several techniques coming from statistics and
computers science.

A list of the principal categories is:
        Cluster analysis. Statistical technique consisting in splitting an heterogeneous group into

        smaller subsets of similar elements, whose characteristics of similarities are not known in
        advance. A typical example is to identify consumers with similar patterns of past purchases
        in order to tailor most accurately a given marketing strategy

        Crowdsourcing. Technique for the collection of data which have been drawn from a large
        group or community in response to an open call through a networked media such as the

        internet. This category bears a crucial importance in our case as it is a mass collaboration
        instance of using Web 2.0

        Data mining. Combination of database management, statistics and machine learning
        methods useful for extracting patterns from large datasets. Some examples include mining
        human resources data in order to assess some employee characteristics or consumer bundle
        analysis to model the behavior of customers





89
  http://imsc.usc.edu/research/bigdatawhitepaper.pdf


                                                                                           61 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        Machine learning. Subfield of computer science (in the scope of artificial intelligence)
        regarding the definition and the implementation of algorithms allowing computers to evolve

        their behaviour based on empirical evidence. An example of machine learning is the natural
        language processing.

        Natural language processing. Set of computer science and linguistic methods adopting
        algorithms to analyse natural human language. Basically this field, which began as a branch
        of artificial intelligence, deals with the interaction between computer and human language

        Neural networks. Computational models which are structured and work similarly to
        biological neural networks existing among brain cells, and that are used to find in particular
        non-linear patterns in the data. Some applications include game-playing and decision making

        (backgammon, chess, poker) and knowledge discovery in data bases
        Network analysis. Part of graph theory and network science which describes the

        relationships among discrete nodes in a graph or a network. In particular the social network
        analysis studies the structure of relationship among social entities. Some applications are
        include the role of trust in exchange relationships and the study of recruitment into political
        movements and social organizations

        Predictive modelling. Branch a mathematical model used to best predict the probability of
        an outcome. This technique is widely used in customer relationship management to produce
        customer-level models able to assess the probability that a customer would take a particular

        action, such as cross-sell, product deep-sell and churn
        Regression. Statistical method for assessing how the value of a dependent variable changes

        with one or more dependent variables. Examples of applications include the change in
        consumer’s behaviour due to manufacturing parameters or economic fundamentals

        Sentiment analysis. Natural language processing methods for extracting information such as
        polarity, degree and strength of the sentiment over a given feature, aspect of product. Many
        companies assess how different customers and stakeholders react to their products and
        action by applying this analysis to blogs, social networks and other social media

        Spatial analysis. Methods for assessing the geographical, geometric or topological
        characteristics of a data set. The spatial data are often drawn from geographical information

        systems (GIS) including addresses or latitude/longitude coordinates, to be incorporated into
        spatial regressions (correlation between commodity price and location) or simulations

        Simulation. Consists in modelling the behavior of a complex system for performing forecast
        and scenario analysis. As example we can mention Monte Carlo simulations, which are a
        class of computational algorithms that rely on repeated random sampling to compute their
        results

Current and Future Research

        Technologies for collecting cleaning, storing and managing data: datawarehouse; pivotal
        transformation; ETL; I/O; efficient archiving, storing, indexing, retrieving, and recovery;
        streaming, filtering, compressed sensing sufficient statistics; automatic data annotation;
        Large Database Management Systems; storage architectures; data validity, integrity,

        consistency, uncertainty management; languages, tools, methodologies and programming
        environments

        Technologies for summarizing data and extracting some meaning: reports; dashboard;
        statistical analysis and inference; Bayesian techniques; information extraction from
        unstructured, multimodal data; scalable and interactive data visualization; extraction and


                                                                                         62 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



         integration of knowledge from massive, complex, multi-modal, or dynamic data; data
         mining; scalable machine learning; data-driven high fidelity simulations; scalable machine
         learning;     predictive    modelling,      hypothesis     generation      and    automated       discovery



         Technologies for using data a decision tool: Decision Trees, Pro-Con Analysis, Rule Based
         Systems, Neural Networks, Tradeoff based Decisions (which incorporates Reporting,

         Statistics, Knowledge Based systems)





         4.2.2. Opinion Mining and Sentiment Analysis

Summary overview




 Current free tools           Top              Current research                   Short      term Long            term

                              market                                              future             future
                              tools                                               research           research



 filtering opinion based on   Machine     ·   Statistical + Semantic analys·is Visual            ·   Multilingual
 rating;           assessing  learning     +   through lexicon/corpus of words    representation     audiovisual
 sentiments     based    on   human            with   known     sentiment   for                      opinion mining
 keywords;    visual   word   analysis         sentiment classification      ·    Audiovisual
 counting                                                                         opinion mining·    Usable, peer-to-
                                          ·    Identification of policy                              peer opinion
 Argument    mapping    and                    opinionated material to be    ·    Real-time opinion  mining tools for
 VAA                                           analysed                           mining             citizens

                                          ·    Computer-generated reference  ·    Machine learnin·   Non-bipolar
                                               corpuses in political/governance   algorithms         assessment of
                                               field                         ·    Natural language   opinion

                                          ·    Visual mapping of bipolar          interfaces     ·   Automatic irony
                                               opinion                       ·    SNA applied to     detection

                                          ·    Identification of highly rated     opinion and
                                               experts                            expertise
                                                                             ·    Bipolar

                                                                                  assessment of
                                                                                  opinions

                                                                             ·    Multilingual
                                                                                  reference
                                                                                  corpora

                                                                             ·
                                                                                  Recommendatio
                                                                                  n algorithms




Introduction and definition

The explosion of social media has created unprecedented opportunities for citizens to publicly voice

their opinions, but has created serious bottlenecks when it comes to making sense of these opinions.
At the same time, the urgency to gain a real-time understanding of citizens concerns has grown:



                                                                                                        63 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


because of the viral nature of social media (where attention is very unevenly and fastly distributed)
some issues rapidly and unpredictably become important through word-of-mouth.

Policy-makers and citizens don’t yet have an effective way to make sense of this mass conversation
and interact meaningfully with thousands of others. As a result of this paradox, the public debate in

social media is characterized by short-termism and auto-referentiality. Many experts consider social
media as a missed opportunity for better policy debate.

At the same time, the sheer amount of raw data is also an opportunity to better make sense of
opinions. The key asset that Google exploited to reach dominance in the search market is not a
better algorithm, but the power of more data.

We are therefore at a crucial underpinning where the challenge of information overload can become
not a problem, but an opportunity for making sense of a thousand voices and identify problems as
soon as they arise.



Opinion mining can be defined as a sub-discipline of computational linguistics that focuses on
extracting people’s opinion from the web. The recent expansion of the web encourages users to
contribute and express themselves via blogs, videos, social networking sites, etc. All these platforms
provide a huge amount of valuable information that we are interested to analyse. Given a piece of

text, opinion-mining systems analyse:
    -   Which part is opinion expressing;

    -   Who wrote the opinion;

    -   What is being commented.


Sentiment analysis, on the other hand, is about determining the subjectivity, polarity (positive or

negative) and polarity strength (weakly positive, mildly positive, strongly positive, etc.) of a piece of
text – in other words:
    -   What is the opinion of the writer

Opinion mining and sentiment analysis cover a wide range of applications.

    1. Argument mapping software helps organising in a logical way these policy statements, by
        making explicit the logical links between them. Under the research field of Online
        Deliberation, tools like Compendium, Debatepedia, Cohere, Debategraph have been

        developed to give a logical structure to a number of policy statement, and to link arguments
        with the evidence to back it up.

    2. Voting Advise Applications help voters understanding which political party (or other voters)
        have closer positions to theirs. For instance, SmartVote.ch asks the voter to declare its
        degree of agreement with a number of policy statements, then matches its position with the
        political parties.

    3. Automated content analysis helps processing large amount of qualitative data. There are
        today on the market many tools that combine statistical algorithm with semantics and
        ontologies, as well as machine learning with human supervision. These solutions are able to

        identify relevant comments and assign positive or negative connotations to it (the so-called
        sentiment).





                                                                                             64 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


The first two point reflect mature application areas, while the third area is emerging and with
relevant research issues. We will therefore mainly focus on this area for the research issues.



Why it matters in governance

These applications are the basic infrastructure of large scale collaborative policy-making. They help
making sense of thousands of interventions. They help to detect early warning system of possible
disruption in a timely manner, by detecting early feedback from citizens. Traditionally, ad hoc
surveys are used to collect feedback in a structured manner. However, this kind of data collection is

expensive, as it deserves an investment in design and data collection; it is difficult, as people are not
interested in answering surveys; and ultimately it is not very valuable, as it detects “known
problems” through pre-defined questions and interviewees, but fails to detect the most important
problems, the famous “unknown unknown”. Opinion mining is helpful to identify problems by
listening, rather than by asking, thereby ensuring a more accurate reflection of reality.

Argument mapping software is then useful to ensure that policy debates are logical and evidence-
based, and do not repeat the same arguments again and again.

These tools would finally be helpful not only for policy-makers, but also for citizens who could more
easily understand the key points of a discussion and participate to the policy-making process.



Recent trends

Opinion mining is not in itself a new research theme. Automated methods for content analysis have
been increasingly used, and have increased at least 6 folds from 1980 to 2002 (Neuendorf, K. A.
2002. The Content Analysis Guidebook. Sage). The research theme is based in long established
computer science disciplines, such as Natural Language Processing, Text Mining, Machine Learning
and Artificial Intelligence, Automated Content Analysis, and Voting Advise Applications.

However, according to Pang and Lee (2008), since 2001 we see a growing awareness of the problems
and opportunities, and “subsequently there have been literally hundreds of papers published on the

subject.”
What is new today is the sheer increase in the quantity of unstructured data, mainly due to the

adoption of social media, that are available for machine learning algorithm to be trained on. Social
media content by nature reflects opinions and sentiments, while traditional content analysis tended
to focus on identifying topics ((Pang, Lee, and Vaithyanathan 2002). As such, it deals with more
complex natural language problems. Because of the combination of increase in the volume of data
available and more complex concepts to analyse, in recent years there has been a decrease in

interest on semantic-based application, and a move towards greater use of statistics and
visualisation. Just as any other scientific discipline, also automated content analysis is becoming a
data-intensive science.

Inspiring cases

Usage of DiscoverText in government
OpinionSpace

Tools on the market

The market of opinion mining tools is crowded with solution providers. Most of these applications
are geared towards analyzing customers feedback about products and services, and therefore




                                                                                         65 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

skewed towards sentiment analysis that detects positive/negative feelings by interpreting natural

language.

Freely available tools
Most of the state-of-the-art argument mapping and voter advise applications are freely available,

because they derive largely from academic community or NGOs. A comprehensive list of such tools
is available in http://groups.diigo.com/group/crossoverproject/content/tag/argumentmapping and
http://groups.diigo.com/group/crossoverproject/content/tag/VAA

There are currently freely available applications that simply analyse terms based on a pre-defined
glossary, and giver highly simplified and unreliable results. One example is http://twitrratr.com/



























                                          Figure 10: Twitrratr



Another stream of simple, free and popular solutions is the word visualisation. Wordclouds are
becoming more and more used to make sense of large quantities of information in a snapshot.
Obviously, such tools are also extremely simplified and only offer a visualisation of the most

common used terms, which is helpful to have an idea of what the document is about, but little more.
Tools such as wordle.com provide an appealing design solution that can serve as an entry level in the
opinion mining market. They are therefore important to involve a much wider public in this kind of
activities.













                                                                                           66 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

























                                        Figure11 : Wordclouds


Finally, another way of making sense of large amount of information is by relying on human effort,

by crowdsourcing and collective intelligence: people are not only submitting their opinions, but
actually filtering them by signalling the most important ones. Tools such as uservoice.com allow
customers to submit feedback and to rank other people ideas, thereby allowing the emergence of
the most popular ideas. These tools are available at very low cost, but research shows that they are

effective in gathering feedback but not in identifying good ideas, as voting tends to focus on easier
and most popular issues.




















                                         Figure12 : UserVoice


Enterprise-level software

Beside these simple and free applications, there is then a flourishing market of enterprise-level
software for opinion mining which much more advanced features. These tools are largely in use by

companies to monitor their reputation and the feedback about products on social media. In the


                                                                                         67 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


government context, opinion mining has long been in use as an intelligence tool, to detect hostile or
negative communications (Abbasi 2007). More recently, politics has become a key area of

applications, as politicians monitor public opinion on social media to understand public reaction to
their position.

Technically, these tools rely on machine learning with regard to identifying and classify relevant
comments, through a combination of latent semantic analysis, support vector machines, "bag of
words" and Semantic Orientation. This process requires significant human effort aided by machines:
all the tools on the market rely on a combination of machine and human analysis, typically using
machines to augment human capacity to classify , code and label comments.

Automated analysis is based on a combination of semantic and statistical analysis. Recently, because
of the sheer increase in the quantity of datasets available, statistical analysis is becoming more

important.
Key challenges and gaps

Current solutions for opinion mining and sentiment analysis are fastly evolving, typically by reducing
the amount of human effort needed to classify comments.

Among the challenges identified we can select:

    -    the detection of spam and fake reviews, mainly through the identification of duplicates, the
         comparison of qualitative with summary reviews, the detection of outliers, and the
         reputation of the reviewer (Liu 2008)

    -    the limits of collaborative filtering, which tends to identify most popular concepts and to
         overlook most innovative / out of the box thinking

    -    the risk of a filter bubble (Pariser 2011), where automated content analysis combined with
         behavioural analysis leads to a very effective but ultimately deviating selection of relevant
         opinions and content, so that the user is not aware of content which is somehow different

         from his expectations
    -    the asymmetry in availability of opinion mining software, which can currently be afforded

         only by organisations and government, but not by citizens. In other words, government have
         the means today to monitor public opinion in ways that are not available to the average
         citizens. While content production and publication has democratized, content analysis has
         not.

    -    the integration of opinion with behaviour and implicit data, in order to validate and provide
         further analysis into the data beyond opinion expressed

    -    the continuous need for better usability and user-friendliness of the tools, which are
         currently usable mainly by data analysts

Current research

Current research is focussing on:
    -    improving the accuracy of algorithm for opinion detection

    -    reduction of human effort needed to analyze content

    -    Semantic analysis through lexicon/corpus of words with known sentiment for sentiment
         classification

    -    Identification of policy opinionated material to be analysed

    -    Computer-generated reference corpuses in political/governance field


                                                                                               68 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


     -   Visual mapping of bipolar opinion

     -   Identification of highly rated experts

Future research: long term and short term issues
Short-term:

     -   Enhanced discoverability of content through Linked Data

     -   Visual representation

     -   Audiovisual opinion mining

     -   Real-time opinion mining
     -   Machine learning algorithms

     -   SNA applied to opinion and expertise

     -   Bipolar assessment of opinions

     -   Multilingual reference corpora
     -   Comment and opinion recommendation algorithm

     -   Cross-platform opinion mining

     -   Collaborative sharing of annotating/labelling resources


Long-term

     -   Autonomous machine learning and artificial intelligence

     -   Usable, peer-to-peer opinion mining tools for citizens

     -   Non-bipolar assessment of opinion
     -   Automatic irony detection



























                                                                                                      69 | P a g e
                                0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


          4.2.3. Visual Analytics


Summary Overview

 Market availability        Challenges and gaps                            Short    term   future Long     term    future
                                                    Current research
                                                                           research                research

 -Information  visualization-        Demographics -Close the loop of -Re-usable, mashable -Learning              adaptive
 requirements for businessvisualizations, allowing information selection, tools      for    visual algorithm   for  users

 intelligence and situationalstakeholders      and preparation        and analytics                intent
 awareness                  decision   makers    to visualisation          -Tighter    integration -Advanced        visual
                            have a clear picture of
 -enterprise     knowledge  the data and of their   -Simultaneous          between      automatic analytics interfaces
 visualization linking      trends over time        multiple visualisation computation        and
                                                                           interactive             -Intuitive  affordable
 -Online          analytical-   Legal   Arguments   -Integration        of visualisation           visual        analytics
 processing and data mining                         visualisation     with                         interface for citizens
 -Advanced social network   visualisation:     text comments / wiki / -Bias          identification
 analysis and visualization analysis,               blogs                  and     signalling   in -Development        of
                            argumentation                                                          novel      interaction
 -Data      mining      and mappings           and  -Collaborative         visualisation           algorithms
                            visualisation algorithms
 interactive   visualization                        platform display       -Perceptual, cognitive incorporating machine
 communication            of- Discussion Arguments  Interaction  between   and           graphical recognition   of   the
 location-based   statisticavisualisation,  making                         principles              actual user intent and
 data                                               visualisation     and                          appropriate
                            use   of   visualisationmodels                 -Efficiency   of   the  adaptation of    main
 -Information  visualizationtechniques          for                        visualisation
 tools for high dimensional visualizing           a -Mobile          visualtechniques to enable    display    parameters
 non-linear data            discussion’s flow       analytics tools        interactive exploration such as the level of
                                                                                                   detail, data selection,
 -Visual analysis of data in-Geographic             -Geo-visualisation  of interaction techniques  etc. by which the data
                            visualization tools     government data        such    as   focus   &  is presented
 spreadsheet format                                                        context
                            -Financial     markets  -Integration      with
                                                    opinion mining and     -Impact evaluation of
                            monitoring         and  participatory sensing  visual   analytics  on
                            visualizing in real time                       policy choices
                                                    -Evaluation framework
                            -Advanced applications  for       visualisation
                            for    security    and
                            defense                 effectiveness
                                                    -Visualisation

                                                    infrastructures    for
                                                    policy modelling issues






Introduction and definition

The explosion in computing techniques led to the generation of a tremendous amount of data which
are stored in the internet and processed in the IT infrastructures all over the world. Some examples
                                                  90
of new technologies for data collections are: web logs; RFID; sensor networks; social networks;
social data (due to the Social data revolution), Internet text and documents; Internet search

indexing; call detail records; astronomy, atmospheric science, genomics, biogeochemical, biological;
military surveillance; medical records; photography archives; video archives; large-scale eCommerce.

In managing this huge amount of data, when it comes to human-computer interaction there is a

need to distil the most important information to be presented it in a humanly understandable and




90 Source: wikipedia



                                                                                                          70 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


comprehensive way. Here it comes visualisation, which is a way to interpret and translate data from
computer understandable formats to human ones by employing graphical models, charts, graphs

and other images that are conventional for humans (Bederson and Shneiderman 2003). In a sense
we can define visualisation as any technique for creating images, diagrams, or animations to
communicate a message or an idea.

In contrast with visualisation traditionally seen as the output of the analytical process, visual
analytics considers visualisation as a dynamic tool that aims at integrating the outstanding

capabilities of humans in terms of visual information exploration and the enormous processing
power of computers to form a powerful knowledge discovery environment. In this view visual
analytics is useful for tackling the increasing amount of data available, and for using in the best way

the information contained in the data itself. Moreover visual analytics aims at present the data in
way suitable for informing the policy making process.

More in particular the interdisciplinary field of visual analytics aims at combining human perception
and computing power in order to solve the information overload problem. In Thomas and Cook’
(2003) definition, visual analytics is “the science of analytical reasoning supported by interactive

visual interfaces”. Precisely visual analytics is an iterative process that involves information
gathering, data preprocessing, knowledge representation, interaction and decision making. The
characteristic of this filed is that it entails the association of data-mining and text-mining
                                                                                                         91
technologies, used for preprocessing massive amounts of data, and information visualisation ,
which is useful for disentangling important from trivial and useless information. In a certain way
information visualisation becomes a tool in a semi-automated analytical process characterized by

the cooperation between humans and computers, in which is the user who decides the direction of
the analysis relating to a particular task, while the system works as an interaction tool. It is somehow
difficult to distinguish among information visualisation, scientific visualization and visual analytics.

In poor terms we can say that scientific visualisation deals with data having a natural geometric
structure, while information visualization handles abstract data structures such as trees or graphs,
and finally visual analytics deals properly with sense-making and reasoning. More in particular
information visualization is mostly applied to data not belonging to scientific inquiry, e.g. graphical

representations of data for business, government, news and social media. Visualization work does
not necessarily deal with an analysis task nor does it always use advanced data analysis algorithms.
On the other hand visual analytics can be seen as an integral approach to decision-making,

combining visualization, human factors and data analysis. It entails identifying the best algorithm for
a given analysis task, to be integrated with the best automated analysis algorithms with appropriate
visualization and interaction techniques.









91We can define Information visualisation as a way of making data easier to understand using direct sensory experience,

rather than linguistic or logical reasoning. Or in the words of Friendly, information visualization is the study of
"the visual representation of large-scale collections of non-numerical information, such as files and lines of code
in software systems, library and bibliographic databases, networks of relations on the internet, and so forth". (See Michael
Friendly 2008)


92According to Friendly (2008) scientific visualization is "primarily concerned with the visualization of three-
dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic
renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component"



                                                                                               71 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

Visualization and visual analytics should be considered in strict integration with other research areas,
                                   93
such as modelling and simulation , social network analysis, participatory sensing, open linked data,
visual computing.

The disciplines in the domain of visualization and visual analytics are: Human-Computer Interaction
(HCI), Usability Engineering, Cognitive and Perceptual Science, Decision Science, Information
Visualisation, Scientific Visualisation, Databases, Data Mining, Statistics, Knowledge Discovery, Data
Management & Knowledge Representation, Presentation, Production and Dissemination, Statistics,

Interaction, Geospatial Analytics, Graphics and Rendering, Cognition, Perception, and Interaction.

As far the visual analytics methodologies are concerned, in the CROSSOVER taxonomy we can
identify the following: visualisation of a single, static, embedded data set; visualisation of multiple
static data sets; visualisation of a single live data feed or updating data set; and finally visualisation
of multiple data points, including live feeds or updates.



Why it matters in governance

Today’s governments face the challenge of understanding an increasingly complex and
interdependent world, and the fast pace of change and increased instability in all the areas of
regulation requires rapid decision making able to draw on the wider amount of available evidence in
real-time. How can visual analytics help?

    -   Generate high involvement of citizens in policy-making. One of the main application of
        visualization is in making sense of large datasets and identifying key variables and causal

        relationships in a non-technical way. Similarly, it enables non-technical users to make sense
        of data and interact with them. For instance, the GapMinder software helps to understand
        the main global demographic changes and raise awareness on the implications of sound
        health policies in developing countries. Visualization is a fundamental part of simulation

        tools as it helps exploring and understanding better the data under different scenarios (Keim
        2011).

    -   Understand the impact of policies: visualisation is instrumental in making evaluation of
        policy impact more effective. For instance, farmsubsidy.org helps understanding who are the
        main beneficiaries of the common agricultural policy by geo-referencing the single
        beneficiary.

    -   Identify problems at an early stage, detect the “unknown unknown” and anticipate crisis:
        visual analytics are largely used in the intelligence community because they help exploiting

        the human capacity to detect unexpected patterns and connections between data. Thereby
        they help early detection of potential threats at an early stage. For instance, the VisAware
        project in the US provides situational awareness in situation of emergencies, helping the
        coordination of different resources involved in emergencies Livnat et al.(2005).



History and trends

Since from the beginning of human history, visualisation has been an effective way to communicate

both abstract and concrete ideas. The appearance of digital visualisation led to the development of
graphic hardware as well as to a wide array of technique used to visualize data in a number of ways


93
  The connections between simulation and visualization appears even more clear when dealing with user interfaces, which
enable the visualisation to take user commands


                                                                                             72 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


(van Wijk. 2005). Often visualisation is needed to enable interaction (Thomas and Ahrweiler Eds.

2005) and to demonstrate how an operation works and which results are generated and derivable.

In this view visualisation is massively used to provide the results of a simulation to users as well as to
receive feedback and promote interaction.

Traditionally the first examples of visualization date back to the 19 century with the drawings by        94

Charles Joseph Minard, who developed a format to show data tied to a timescale with a landscape

background. In 1869 Minard applied its drawings to show the march of Napoleon’s army towards
Moscow, starting with 422,000 and ending with 10,000 men, and Hannibal's crossing of the Alps,

starting with 97,000 and ending with 6,000 men. The modern visualization field, making use of
computer graphics, originated in the late 1980s with the studies on scientific visualization applied to

fluid dynamics, volume visualization, molecular modelling, imaging remote-sensing data, and
medical imaging (Rosenblum 1994). From scientific visualisation took place some more recent areas,

such as information visualization, mobile visualization, location-aware computing and visual

analytics. Information visualization arose when Robertson, Card and Mackinlay in the 1980s started
to use the work of Bertin (1967) and Tufte (1983) in interactive computer applications. Later

Shneiderman (1996) inter alia formalized the process of information visualization. Finally Ware
(2004) emphasized the important of human perception in information visualization. In parallel with

information visualization rose the field of data mining, aimed at discovering information hidden in
massive amounts of data. The problem with the field, is that it aimed at substituting the human

analysis with automatic computer operations, not supporting human perception with interactive

visualization. Visualization of data soon showed its limitations due to the complexity of required
analytical reasoning. In order to avoid that was developed the interdisciplinary field of visual

analytics, which combines human perception abilities with computers’ processing power in order to
tackle the information overload problem. Visual analytics can therefore be seen as the combination

between data mining and text-mining technologies on one side, and information visualisation on the
other side: “Visual analytics is more than only visualization. It can rather be seen as an integral

approach combining visualization, human factors and data analysis” (Keim 2008). Future

developments of visual analytics include the fields of enhanced collaboration capabilities, more
intuitive interaction, support of non-computing devices, as well as the integration of quantitative

and qualitative data. In fact visual analytics require particular technological advances, as traditional
data mining tools are unsuitable for some necessary functionalities such as the algorithm speed

required for iterative visualisation.

Inspiring cases

    -    GapMinder   97

    -    US Labour Force Visualization    98



94
  http://www.math.yorku.ca/SCS/Gallery/minbib/index.htm
95
  Bertin in 1967 described a framework for the design of the basic elements of diagrams
96Tufte developed a theory of data graphics

97http://www.gapminder.org
98
  http://flare.prefuse.org/launch/apps/job_voyager


                                                                                                 73 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


                                  99
     -   State Cancer Profiles
                        100
     -   Instant Atlas

Policy applications of visualization and visual analytics tools

With regard to the governance and policy making context, some visualization tools can be applicable

to a wide array of issues and situation (education, environment, public health, urban growth,
national defense, etc. etc.). In the public context, visual analytics of public data is an exploding field,

with particular relation to the open data movement, in order to monitor policy context and evaluate
government policies. Most basic mash-up tools are available to visualize government.


Let us see some other examples:

Demographics visualizations, allowing stakeholders and decision makers to have a clear picture of
the data and of their trends over time. Visualisation of demographic data make easier the design and

evaluation of various policies, as there is no need to dig through acres of numbers. In fact advanced
algorithms are able to create figures and illustrations easy to interpret. Typical examples are the
                                  101
aforementioned GapMinder             (which embeds visualizations of various demographic data at global
                                                    102             103               104                   105
level), as well as106namic Choropleth Maps , DataPlace , Hive Group , Name Voyager , State
Cancer Profiles

Legal Arguments visualisation: text analysis, argumentation mappings and visualisation algorithms

can be applied to legal documents in order to simplify legislation making it more accessible and
comprehensible to the general public, or in order to visually represent corroborative evidence (e.g.
                       107              108
the tools Carneades        and Deflog )

Discussion Arguments visualisation, making use of visualisation techniques for visualizing the flow of

a discussion that include various arguments, in order to instantly get awareness of the topics
discussed, as well as of the arguments and the support such arguments gain. In this view

visualisation supports all interested stakeholders to understand the flow of a discussion, which is
presented to them in a structured and interactive format, avoiding numerous discussion threads.
                                                                     109
Example of such visualisation tools include DebateGraph , which is intensively used for building
argumentation maps, as well as Araucaria , Compendium , Argublogging 111                 112and Rationale    113




99
  http://statecancerprofiles.cancer.gov/micromaps/
100
   http://www.instantatlas.com/CDC_story.xhtml
101
   http://www.gapminder.org
102http://www.turboperl.com/dcmaps.html

103http://www.knowledgeplex.org/dataplace.html

104http://www.hivegroup.com/gallery/worldpop/

105http://www.babynamewizard.com/voyager#

106http://statecancerprofiles.cancer.gov/micromaps/

107http://carneades.berlios.de/downloads/

108http://www.ai.rug.nl/~verheij/aaa/

109http://www.debategraph.org
110
   http://araucaria.computing.dundee.ac.uk/
111
   http://compendium.open.ac.uk/institute/
112
   http://www.arg.dundee.ac.uk/?p=624



                                                                                                         74 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Geovisualization, which is based on the provision of theory, tools and methods for visual analysis,

synthesis, exploration and representation of geographical data and information in order to derive
problem specific models and design task specific maps for incorporating geographical knowledge
                                                                                           114                  115
into planning and decision making. Some examples of such tools include ESTAT , GeoViz Toolkit ,
the geovisualization tools at the US National Cancer Institute , some applications of InstantAtlas             117


Advanced visualization applications used for security and national defense. In this fields, software
advances are being led both on the military and on the corporate front. In fact business

organizations also have urgent information visualization requirements that support their business
intelligence and situational awareness capability, data mining and reporting requirements. In this

view many of the software innovations are being targeted at financial and corporate requirements,
but are also applicable to the defense domain due to common data mining and information
                                                                                   118                119
visualization challenges. Examples of such tools are: DataMontage                     , HoneyComb        , Oculus
GeoTime   120 and Starlight . Other very interesting examples are Analyst’s Notebook                  122is Visual
                     123
Sentinel Visualizer , adopted by intelligence agencies such as the CIA

Visualization applications adopted for financial markets monitoring and visualizing in real time. An
example of such tool is SmartMoney        124




Tools on the market

There is a massive quantity of visualization tools in the market, both freely available and enterprise
level, critical for analysts and researchers, but also for common people, is now available online.

Freely available tools


First of all we have visualization websites useful for sharing and presenting data, provide clear
context on important cultural, environmental, social and economic issue, build chart and share

visualization and discoveries. Such examples are Data360, FlowingData, Hohli, IBM Many Eyes.

Then we have data visualization tools used for plotting data on maps, frameworks for creating
charts, graphs and diagrams and tools to simplify the handling of data transforming them into

spreadsheets, visual data mining and database exploration system, data visualization system for
high-dimensional data, visualization framework for animating data. Some examples of those tools






113http://rationale.austhink.com/

114http://www.geovista.psu.edu/ESTAT/
115
   http://www.geovista.psu.edu/geoviztoolkit/index.html
116
   http://gis.cancer.gov/nci/geovisualization.html
117
   http://www.instantatlas.com/clients.xhtml#government
118
   http://www.stottlerhenke.com/datamontage/examples/madcap/Air_force_wargame_simulation.htm
119
   http://www.hivegroup.com/solutions/demos/merit.html
120http://www.oculusinfo.com/papers/GeoTime_Brochure_06.pdf

121http://starlight.pnl.gov/

122http://www.i2group.com/us/products/analysis-product-line/ibm-i2-analysts-notebook

123http://www.fmsasg.com/LinkAnalysis/Government/Solutions.asp

124http://www.smartmoney.com/map-of-the-market/



                                                                                                     75 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


are: Data Wrangler, JavaScript InfoVis Toolkit, VisDB, Graphviz, IBM OpenDX, Gephi, GeoCommons,
Miso Dataset, Polymaps, Processing, Protovis, Raphael, Tableau Public.

Enterprise-level software

Apart from free visualization tools, there are also many more advanced software which are used by
firms in order to satisfy their information visualization requirements for business intelligence
support and situational awareness capability, as well as data mining and reporting requirements.
Other uses include enterprise knowledge visualization, linking knowledge to spatial data, online

analytical processing and data mining, advanced social network analysis and visualization, data
mining and interactive visualization, communication of location-based statistical data, on-line and
batch environment for business graphics, information visualization tools for high dimensional non-
linear data, visual analysis of data in spreadsheet format, analysis of high volumes of unstructured

text, analysis of high-dimensional data in large complex data sets and of multivariate time-oriented
data.

Some examples of such software are: CViz Cluster Visualization, IBM ILOG Visualization, Spotfire,
Survey Visualizer, Infoscope, Inspire, Sentinel Visualizer, Grapheur 2.0, InstantAtlas™, Miner3D,
VisuMap, Drillet, Eaagle, GraphInsight, Gsharp

Other examples of visualization software can be found in

http://groups.diigo.com/group/CROSSOVERproject/content/tag/visualization


Key Challenges and Gaps

New tools like the Word Tree (Wattenberg 2008), Treemap, Tag Cloud and Bubble Chart                125are

available but lack interactivity. What is also missing is a better interaction of visualization
approaches and analytical processes of text mining, as well as a better integration between new
opportunities for data collection, such as open data and participatory sensing, policy modelling and
visual analytics tools. Most applications related to visual analytics of public data remain at the level

of visualisation only, with limited analytical functionalities. Geo-visualisation is a fast growing
application area in the government context, but there is little integration with other related areas
such as participatory sensing.

Visualisation tools are still largely design for analyst and are not accessible to non-experts. Intuitive
interfaces and devices are needed to interact with data results through clear visualizations and
meaningful representations. User acceptability is a challenge in this sense, and clear comparisons

with previous systems to assess its adequacy and objective rules of thumbs to facilitate design
decisions would be a great contribution to the community.

Scalability of visualisation in face of big data availability is a permanent challenge, since visualisation
require additional performances with respect to traditional analytics in order to allow for real time
interaction and reduce latency.

Finally, visualisation is largely a demand- and design-driven research area. In this sense one of the
main challenge is to ensure the multidisciplinary collaboration of engineering, statistics, computer

science and graphic design.



125
   See http://manyeyes.alphaworks.ibm.com/manyeyes/, which can be also found in the project CROSSOVER Diigo
collection




                                                                                             76 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


A relevant challenge of visualization and visual analytics is to adapt existing techniques to policy
modelling:

RelaNet (Landesberger et al. 2008), which displays the network relations and thereby is able to show
the connections and co-variances of the different opinions overtime

CirVis3D (Landesberger et al. 2009), which can visualize clustered opinion snippets as well as display
time series in order to show the opinion trends over time



Current research

     -   Close the loop of information selection, preparation and visualisation

     -   Simultaneous multiple visualisation

     -   Integration of visualisation with comments / wiki / blogs

     -   Collaborative platform display

     -   Interaction between visualisation and models

     -   Mobile visual analytics tools

     -   Geo-visualisation of government data


     -   Integration with opinion mining and participatory sensing

     -   Evaluation framework for visualisation effectiveness

     -   Visualisation infrastructures for policy modelling issues

A list of EU funded projects in visual analytics include:

     -   VisMaster-Visual Analytics: Mastering the Information Age126

     -   VisSense- Visual Analytic Representation of Large Datasets for Enhancing Network
         Security127

     -   CUBIST- Combining and Uniting Business Intelligence and Semantic Technologies128

     -   WATTALIST – Modelling and Analysing Demand Response Systems129

     -   CODE – Commercially empowered Linked Open Data Ecosystems in Research130

     -   SemSeg-4D Space-Time Topology for Semantic Flow Segmentation131



126
   http://www.visual-analytics.eu/


127http://cordis.europa.eu/projects/rcn/94912_en.html


128http://cordis.europa.eu/projects/rcn/95904_en.html


129
   http://cordis.europa.eu/projects/rcn/100984_en.html
130http://cordis.europa.eu/projects/rcn/103419_en.html



                                                                                                        77 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Future research: long term and short term issues

Short-term research

    -    Reusability of mashup tools (mashup is a web application which combines data from one or
         more sources into a single integrated tool or application) for visual analytics

    -    Tighter integration between automatic computation and interactive visualisation, which

         consists in the availability of complex and powerful algorithms that allow for manipulating
         the data under analysis, transforming it in order to feed suitable visualizations

    -    Bias identification and signalling in visualisation

    -    Techniques and algorithms for creating effective visualization tools based on perceptual
         psychology (dealing with the process by which the physical energy received by sense organs

         forms the basis of perceptual experience), cognitive science (focusing on how information is
         represented, processed, and transformed) and graphical principles

    -    Visualisations enabling interactive exploration techniques such as focus & context, in order

         for the viewers to be able to see the object of primary interest presented in full detail while
         at the same time getting a overview–impression of all the surrounding information — or
         context — available

    -    Impact evaluation of visual analytics on policy choices

Long-term research

    -    Learning adaptive algorithm for users intent. Note that learning/adaptive algorithms are

         defined as being capable to automatically change behaviour based on its execution context
         (data handled by the algorithm, configuration parameters of the runtime environment,
         resources used) in order to obtain optimal performances

    -    Advanced visual analytics interfaces: visual interfaces in which neither the analytics nor the

         visualization needs to be advanced in itself but synergy between automation and
         visualization is in fact advanced

    -    Intuitive and affordable visual analytics interface for citizens

    -    Development of novel interaction algorithms incorporating machine recognition of the

         actual user intent and appropriate adaptation of main display parameters such as the level
         of detail, data selection, etc. by which the data is presented



         4.2.4. Serious Gaming for Behavioural Change

Introduction and definition
So far, collaborative ICTs have dramatically augmented the capacity of people to connect and

collaborate. Yet, less impact has been achieved in terms of actual change and action, as most
collaboration remain confined to an elite of highly-motivated individuals and faces the traditional
limits of human attention and motivation. As illustrated in other challenges, ICT can improve data


131
   http://www.semseg.eu/


                                                                                                78 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


collection and analysis, but if attention and motivation are not present, little impact can be
achieved. This challenge deals with the closing loop of collaboration and depicts ICT solutions that

enable behavioural change and action. Even when citizens and government are fully aware of
necessary policy choices, they might irrationally choose short-term benefits.

Simulation and serious gaming (also known as interactive learning environments) offer opportunities
to impact on personal incentives to action and showing long-term and systemic effects of individual
choices, thereby lowering the engagement barrier to collaborative governance and augmenting its
impact. In particular, serious games have been developed for educational purposes and raising
awareness on particular issues while not requiring high levels of engagement.

Simulation tools enable users to see the systemic and long-term impact of their action in a very
concrete and tangible form, thereby encouraging more responsible behaviour and long-term

thinking. Gaming engages users through the “fun” and “social” dimension, thereby providing
incentives towards action. Feedback and simulation systems include both individual and government
behaviour, thereby allowing policy-makers and citizens to detect the impact of both individual and
policy choices.

Engagement of domain experts is a crucial issue for building reliable games and simulation tools.
Toolkits and modules enable a wider audience of stakeholders to take a direct, active role in games
development, thereby enabling all relevant knowledge to be elicited and captured by the simulation

and gaming scenarios and models. Pre-built toolkit enables the creation directly by thematic experts
and not by technology experts.


Why it matters in governance

Most applications of simulation and gaming are developed into the context of education and
learning, while more interactive feedback producing systems have been applied to personal health

and energy conservation. The specific challenges of gaming for public policy awareness and action
are currently less researched, but are very specific because of their large-scale interaction and
systemic effects of individual behaviour, which characterised this field.

Furthermore, the availability of a simulation toolkit is necessary to empower a diverse and inclusive
simulation landscape, where the most diverse set of ideas can be influential and listened to.


Recent trends

Simulation and gaming have started to be applied in different policy contexts in order to engage
wider audiences. Games are developed “on purpose”, by highly skilled developers, in the public

sector and by civil society, therefore requiring significant investment and without the specific
thematic knowledge of the field. Furthermore, existing serious games lack flexibility to allow for
unpredictable developments and non-linear behaviours, where scenarios evolve and adapt to users
choices rather than being rigidly prescribed. Commercial solutions that turn long-term effects into
short-term feedback are available, but still lack usability as well as the fun dimension of games and

finally require high levels of engagement. They are designed for individual feedback and do not
cover the complexity of systemic interactions, which are typical of public governance issues.

To sum up, serious gaming is still requiring high level of engagement, and progress is needed in
terms of usability and appeal in order to reach “casual gamers” including, immersive and emotion
aware games.





                                                                                         79 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


Current practice

    -   Purpose-built gaming and simulation for understanding of policy issues and of individual
        behaviour



Public Policy Applications
Simulation and gaming can be useful to policy makers in the following terms (Mayer et al. 2004 ,

Bots and van Daalen 2007 ):
    -   Research and analyse a policy issue when it is not feasible to tackle the real system (due to

        time constraint or just because it does not exists) or to include human behaviour by way of a
        computer model (due to unrealistic assumptions such as perfect rationality). In this view the
        game becomes a laboratory which can produce a great deal of data which provide useful
        insights

    -   Design alternative solutions to a problem, analyze and assess the possible consequences of
        the alternative solutions in order to recommend a course of action for the policy-maker. In
        this view the game can be seen as a virtual design studio useful to boost out-of-the-box

        thinking about alternative solutions to a policy issue, and also to ponder recommendations’
        consequences

    -   A game can be used to provide strategic advice acting as a virtual practice ring in which the
        policy maker can rehearsal different strategies. A typical example of such kind is given by
        the war games, in which the other players act as sparring partners for the policy makers,
        playing the role of another stakeholder as opportunistically as possible

    -   Many policy issues require mediation so that it is necessary to seek for consensus among
        stakeholders. This can be done by putting the players around a virtual negotiation table by
        the mean of a mediation game. In this way the changes in attitude and the discovery of new

        opportunities for conflict resolution are eased by the interaction among stakeholders during
        the game

    -   Normally experts and elites are involved in the policy-making process, while citizens and
        ordinary people are completely neglected. However, by defining virtual consultation forums
        it is possible to allow equal access for all the actors carrying views and opinions which would
        have been otherwise disregarded. In this respect using games and simulations bears and
        advantage given by the fact that ordinary people can focus and express themselves more

        easily when playing a role
    -   Clearly ethical questions and opinions have a great influence on the policy making process.

        Games and simulations can be used to clarify the values and arguments behind a point of
        view. While in ordinary political debate values remain implicit, by creating a virtual
        parliament it is possible to make them explicit. Furthermore gaming and simulations can be
        used to magnify positions and opinions of stakeholders, so that the game can be designed to
        reward players for quality and clarity of argumentation

Moreover readapting the taxonomy of Sawyer and Smith simulation and serious gaming can be
useful in the following domains (cross-referenced with game objectives):

    -   Public sector and NGOs: public health education and mass casualty response (games for
        health); political games (advergames); employee training (games for training); provide info

        to the public (games for education); data collection/planning (games for research); strategic
        and policy planning, spatial planning (games for producing); diplomacy and opinion research
        (games as work)


                                                                                             80 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    -   Defence: rehabilitation and wellness (games for health); recruitment and propaganda
         (advergames); soldier/support training (games for training); school house education (games

         for education); wargames and planning (games for research); war planning and weapons
         research (games for producing); command and control (games as work)

    -   Healthcare: cyber therapy/exergaming (games for health); public health (advergames);
         policy and social awareness campaigns (games for training); training games for health
         professionals (games for education); games for patient education and disease management
         (games for research); visualization and epidemiology; biotech manufacturing and design
         (games for producing); public health response planning and logistics (games as work)

    -   Education: inform about diseases/risks (games for health); social issue games (advergames);
         train teachers/workforce skills (games for training); learning (games for education);

         computer science and recruitment           (games for research); P2P Learning (games for
         producing); distance learning (games as work)


Inspiring cases

Let’s some inspiring cases of serious games applied to policy making:

    -   SimHealth : The National Health Care Simulation is is a management simulation of the U.S.
         Healthcare system released during Congressional debates on the Clinton health care plan

    -   SimCity 2013 : is an upcoming city-building/urban planning simulation computer game
         allowing allows players to visualize data, such as pollution and water distribution, which will
         be realised in February 2013

    -   City One : the game teaches industry professionals and civil servants the real-world planning
         in fields such as optimization of banking, retail, energy and water solutions

    -   Democracy 2 : government simulation game in which the player acts as the president or
         prime minister of a democratic government introducing and altering policies in areas such as
         tax, economy, welfare, foreign policy, transport, law and order and public services

    -   Close Combat Marines : serious game for military training purposes, with particular
         reference to the United States Marine Corp

    -   Incident Commander™ NIMS-compliant training tool for Homeland Security: in this game
         the player mimics the role of incident commander in case of natural or manmade disaster,

         terrorist attack or hostage situation. Application: US Department of Justice officers’ training
    -   Virtual Battlespace Systems 2 : this is an interactive military simulator developed for the

         United States Marine Corp and the Australian Defence Force to meet the individual needs of
         military, law enforcement, homeland defence, loadmaster, and first responder training
         environments

    -   Pulse!! Virtual Clinical Learning Lab for Health Care Training: the game recreates a lifelike,
         interactive, virtual environment in which civilian and military heath care professionals
         practice clinical skills in case of catastrophe or terrorist attack

    -   Levee Patroller : immersive 3D game-based environment to train levee inspection
         knowledge and skills, in order to be prepared to cope with unexpected flooding

    -   Construct.it : game-based learning environment allowing players to experience and debrief
         some of the complexities involved in large-scale urban projects. Application: development
         plan for Scheveningen-Harbour of The Hague



                                                                                               81 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    -    Simport-Maasvlakte 2 : computer-supported multi-player simulation game that mimics the

         real processes involved in planning, equipping and exploiting the new area in the Port of
         Rotterdam

    -    Pro Rail : capacity optimization of a complex infrastructural network, in this case the Dutch
         railways. Applications: cargo capacity management, opening of the Vecht-bridge, increase
         traffic on the A2-corridor

    -    Win Win Manager: online multiplayer negotiation business game in which players conduct a
         sequence of bilateral negotiations pursued through private threads on the general game
         board

    -    Management Business Game: business game focussed on the simulation of a company’s
         management in a competitive market, which can be played both online and offline

    -    Management Utilities Euroshop: management of a chain of retail stores selling electronic
         products, through which players identify the relationships between management issues and

         competitive market factors
    -    Shadow Government: serious game based on the gamification of real countries, systems,

         and worldwide events. Based on System Dynamics, customized at the country level, it allows
         players to test several policy interventions and evaluate their impacts132


Key challenges and gaps

Following Mayer (2009) we can identify the following challenges and gaps:

    -    Cultural changes concerning the interaction between science in politics, democracy. Changes
         in the role of elites, activism and citizens’ participation, as well as the recent emergence of
         game cultures

    -    Changes in public policy making perception, i.e. from rational comprehensive to political and
         incremental

    -    How natural and human-caused events can influence the political agenda (climate warming,
         pollution, depletion of natural resources, terrorism)

    -    Institutional changes and the emergence of new industrial or institutional actors

    -    Technological innovation in computing and simulation modelling, such as agent-based
         models, cellular automata, or virtual game worlds.



Moreover following IDATE we can display the following key challenges regarding in particular
serious gaming:

    -    Restructure the game in order to cope with specific purposes and broaden the audience
    -    Innovate the existing business models

    -    Automating a portion of the production process, such as for example the integration of
         sector-specific elements





132
   To be released in summer 2012


                                                                                                 82 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    -    Try to persuade reluctant users and create sector-targeting serious gamins and persuading
         reluctant users

    -    Investing in all connected platforms



Current research
    -    Kit-based serious games

    -    Integration between policy models and simulation

    -    Design of appealing, adaptive and context-aware interfaces. Impact of simulation and
         gaming on individual behaviour

    -    Unconscious impact of feedback systems


Research disciplines: human-computer interaction, sensors, information visualisation, sensor design,

psychology, pedagogy, public policy


Future research: long term and short term issues

Short-term research

    -    Citizens- and experts-generated gaming
    -    Immersive interfaces

    -    Large-scale collaboration in development

    -    Casual serious gaming

    -    Ethical issues in serious gaming
    -    User-controlled simulation and gaming

    -    Non-linear and adaptive scenarios for gaming in policy context

    -    Integrated analysis of information and behavioural change

    -    Impact of simulation and gaming on systemic behaviour


Long-term research

    -    Augmented reality citizens-generated gaming and simulation

    -    Ubiquitous feedback systems on public governance
    -    Model and display long-term systemic effects of individual choices on public policy topics

    -    Interplay between different feedback systems and other information outputs



         4.2.5. Open Government Data

Introduction and definition
The current emergence of open data portals in the government context is opening great

opportunities for collaborative government, but it is happening in a scattered way leading to sub-


                                                                                                   83 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


optimal data reuse and impact. Open data publication needs to meet requirements for timely
publication but at the same time to ensure sufficient data quality. At a more advanced level,

publication of linked data requires significant effort and has encountered unequal success, but the
benefits are high in terms of data interoperability and deriving reuse and data quality. Linked data
refers to a set of best practices for exposing, sharing, and connecting structured pieces of data,
information, and knowledge on the Web. Simplifying and lowering costs of open data publication is
indeed a key area of research.

Curating tools (selecting, aggregating and presenting) and on-the-fly data quality agreements will
reduce the cost and time of data quality assurance. Finer grained data privacy solutions will also

contribute to increase the amount of public data being published, as well as enable real-time
publication of data.


Why it matters in governance

Data openness has resulted in some application in the commercial field, but by far the most relevant
applications are created in the context of government data repositories. With regard to linked data

in particular, most research is being undertaken in other application domains such as medicine.
Government starts to play a leading role towards a web of data. However, current research in the
field of open linked data for government is limited.



Recent trends
Current emerging practice focuses on the publication of open government data in machine-readable
format, possibly through open standards. More innovative implementation includes publication of

linked data but little research is carried out in the governance context. Other lighter-weight forms of
interoperability focus on RESTful interfaces. Furthermore, because of privacy concerns, the only data
provided are aggregated and anonymised.

Existing research focuses mainly on publishing tools for linked data, converting data in RDF, linking
RDF data sources and creating semantic vocabularies for linked data.

Current practice

    -   Data.gov repositories
    -   Linked data in data.gov repositories

    -   Machine-readable formats

    -   RESTful interfaces


Advantages of Application in Policy Making

The openness in government data is important for the following reasons:

    -   Promotion of transparency concerning the destination and use of public expenditure

    -   Improvement in the quality of policy making, which becomes more evidence based
    -   Display the full economic and social impact of information, create services based on

         government data
    -   Increase in the collaboration across government bodies, as well as between government and

         citizens


                                                                                              84 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



     -   Permits new added-values services to come into existence

     -   Increase the awareness of citizens on specific issues, as well as their information about
         government policies

     -   Promote accountability of public officials



Inspiring cases

Very important examples are given within the scope of the Open Government Initiative                      133carried

out by the Obama Administration for promoting government transparency on a global scale:
                    134
     -   Data.gov : platform which increases the ability of the public to easily find, download, and
         use datasets that are generated and held by the Federal Government. In the scope of

         Data.gov US and India have developed an open source version called the Open Government
         Platform   135(OGPL), which can be downloaded and evaluated by any national Government or

         state or local entity as a path toward making their data open and transparent
                              136
     -   USAspending.gov : it is a searchable website displaying for each Federal award the name of
         the entity receiving the award, the amount of the award, information on the award, and the

         location of the entity receiving the award
                                137
     -   FederalRegister.gov : HTML edition of the Federal Register to make it easier for citizens
         and communities to understand and get informed about the regulatory process

     -   Recovery.gov : website aimed at showing the American public how Recovery funds are
         being spent by recipients of contracts, grants, and loans, and the distribution of Recovery

         entitlements and tax benefits
                             139
     -   Performance.gov : website providing a window of US Government Administration effort to
         improve performance and accountability, in order to create a government that is more

         effective, efficient, innovative, and responsive
                         140
     -   IT Dashboard : website enabling federal agencies, industry, the general public and other
         stakeholders to view details of federal information technology investments

     -   Reginfo.gov : website displaying information about regulations under development in

         order to ensure public availability of regulatory information, effective regulatory planning,
         Sound economic and scientific data for regulatory action






133
   http://www.whitehouse.gov/open
134
   http://www.data.gov/
135http://www.opengovplatform.org/

136http://www.usaspending.gov/
137
   https://www.federalregister.gov/
138
   http://www.recovery.gov/Pages/default.aspx
139
    http://www.performance.gov/
140
   http://www.itdashboard.gov/
141http://www.reginfo.gov/public/



                                                                                                       85 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


At the European level we have the repository of applications making use of open data:
publicdata.eu 142



At the European national level the initiatives include:

    -    United Kingdom: Data.gov.uk    143 , which collects data from 5,400 datasets available, from all

         central government departments and a number of other public sector bodies and local
         authorities. Another interesting case is “Where Does My Money Go?”        144  which shows how
         daily taxes are allocated among the different functions of the government.

    -    Italy: Dati.gov.i145 , which is an open data portal allowing citizens, developers, firms and

         public administrations to make use of the public administration information stock

    -    Denmark: DigitalisérDK  146  , a formal central repository of information on data interchange
         standards established and maintained by the Danish National IT and Telecom Agency

    -    France: data.gouv.fr 147  . Moreover we have the meta data portal of the APIE       148  (Agence

         du Patrimoine Immatériel de l’Etat), responsible for open data policy making and selling

    -    Spain: datos.gob.es 149  , the national portal for managing and organizing the Catalogue of
         Public Information of the General State Administration

    -    Ireland: StatCentral.ie150  , providing standard documentation on recurring official statistics
         and links to where they can be found

    -    Netherlands: Overheid.nl  151  , the central access point to all information about government

         organisations of the Netherlands

    -    Norway: data.norge.no   152   national portal containing a data catalogue and a platform to
         discuss OGD questions

    -    Sweden: Opengov.se    153  , an initiative to highlight available public datasets in Sweden

         containing a commentable catalogue of government datasets, their formats and usage
         restrictions





142
   http://publicdata.eu/app?page=1
143http://data.gov.uk/

144http://www.wheredoesmymoneygo.org/
145
   http://www.dati.gov.it/content/datigovit-il-portale-dei-dati-aperti-della-pa
146http://digitaliser.dk/

147http://www.data.gouv.fr/
148
   http://www.economie.gouv.fr/apie
149http://datos.gob.es/datos/

150http://www.statcentral.ie/
151
   http://www.overheid.nl/
152http://data.norge.no/

153http://www.opengov.se/sidor/english/



                                                                                              86 | P a g e
                                0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



At European regional level we have:


     -    Spain: regional data portals in the Asturias       154  , Basque Country     155   and Catalonia   156

     -    Italy: data portal of the Regione Piemonte        157


     -    France: data portal of the Rennes metropolitan region            158

Some other open data initiatives are present in countries such as Saudi Arabia , Australia ,           159            160
         161                 162           163               164         165             166             167          168
Canada , South Korea , Estonia , Hong Kong , Kenya , Moldova , Morocco , Mexico ,
New Zealand , Peru , Singapore , Tunisia , Portugal , Brazil , Chile , Uruguay , Russia . 175            176         177


Key challenges and gaps

     -    Language barriers preventing from finding datasets on the websites of public bodies from

          different countries

     -    Obsolete systems still in use delay the process of opening

     -    Unclear or ambiguous licensing of government data


     -    Difficult dialogue between the public institution which produces data and the final user



154www.asturias.es/

155http://opendata.euskadi.net/w79-prehome/es/

156dadesobertes.gencat.cat/

157www.dati.piemonte.it/

158www.data.rennes-metropole.fr/

159 http://www.saudi.gov.sa/wps/portal/!ut/p/c4/04_SB8K8xLLM9MSSzPy8xBz9CP0os3iTMGenYE8TIwODUEsLA89QU69g1

1A_YwMTQ_3g1Dz9gmxHRQCO1nwy
160
   http://data.gov.au/
161
   http://www.data.gc.ca/default.asp?lang=En&n=F9B7A1E3-1
162
   http://www.data.go.kr/Main.do
163
   http://pub.stat.ee/px-web.2001/Dialog/statfile1.asp
164
   http://www.gov.hk/en/theme/psi/welcome/
165
   https://opendata.go.ke/
166
   http://data.gov.md/en
167
   http://data.gov.ma/Pages/Home.aspx
168
   http://mapas.gob.mx/home.do

169http://www.data.govt.nz/

170http://www.datosperu.org/

171http://data.gov.sg/

172http://www.opendata.tn/

173dados.gov.pt

174dados.gov.br

175datos.gob.cl

176datos.gub.uy

177opengovdata.ru




                                                                                                           87 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


    -    Some material not downloadable in bulk but available only via a web interface

    -    Difficulty in aggregating/comparing data from different public bodies sources

    -    Insufficient reuse data skills from the users’ side
    -    Many data catalogues do not have any mechanism to capture value added to the datasets

         (or metadata about datasets) by users, or the metadata provided are insufficient
    -    Link and combine a large number of datasets from a large number of different sources

    -    Limitations in the data formats which are accepted

    -    Privacy concerns


Current research

    -    Government data

    -    Catalogues vocabularies

    -    Open data interoperability
    -    Capturing and publishing linked data

    -    Curating tools for data quality

Disciplines of research: technological research: semantics, interoperability, privacy-enhancing-
technologies

Possible research instruments: Testbeds and living labs, STREPs


Future research: long term and short term issues

Short-term research

    -    On-the-fly data quality agreements

    -    Easy and privacy-compliant data publication tools
    -    Open dataset integration and quality check

Long-term research

    -    Web of data

    -    Privacy-by-design compliancy in open data

    -    Real-time publication of linked data


         4.2.1. Collaborative Governance

Introduction and definition

While all challenges provide opportunities for a more effective large-scale collaboration in public
action, the relevant institutional design is far from being introduced. The formal inclusion of citizens
input in the policy-making process, the deriving institutional rules, the legitimacy and accountability
framework are all issues that have so far been little explored. Instant, open governance implies a

substantial increase in feedback loops that are of a different scale with respect to the present



                                                                                                   88 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


context. Any system stability is affected by the number, speed and intensity of feedback loops, and
the institutional context has been designed for less and slower loops.

The definition and design of public sector role is being directly affected by the radical increase in
bottom-up collaboration, deriving from the lower cost of self-organisation. There are also important

questions to be answered – where does the legitimacy come from, how to gain and maintain the
trust of users, how to identify the users online. There is also a very important issue of how to take
into the account the diversity of the standpoints, i.e. how to achieve a consensual answer to
controversial social issues, especially when we do not offer alternatives (ready-made options) but
start from an open question and work throughout different options proposed by participants.

Furthermore, the trade-off between direct or representative model of democracy will have to be
analysed in this context. It is far from being proved that the open and collaborative governance is
really inclusive and representative of all the social groups, including the disadvantaged and of all
standpoints. There is a visible risk that online collaboration increases the divide, rather than reduces

it.
The management of institutional bodies is changing: innovative ideas and insight coming from
employees and citizens are key resources to be exploited, and meritocracy and transparency are

entering an once stable and conservative workforce. Enhanced collaboration with citizens and
private third parties should be accompanied by adequate legal and accountability frameworks,
mapping incentives to participation and enabling business models for different stakeholders.

The privacy paradigm is changing and appropriate, more dynamic frameworks have to be designed,
taking into account the willingness of citizens to share information and at the same time ensuring
their full awareness of the implications and their control over the data usage.



Recent trends

The current status is characterized by practice-driven implementation, accompanied by little
scientific reflection. Guidelines and soft regulation are being created from scratch and by building on
other institutions examples. The development of collaborative governance is growing rapidly
without an appropriate reference framework.



Current practice
    -    Made-up guidelines and behaviour



Public Policy Application

As it is widely recognized, policy issues of our age can be addressed only through the collaboration
of all the components of the society, including the private sector and individual citizens. In this view
the advantages in collaborative governance are given by:

    -    Effectiveness and efficiency in the delivery of programs

    -    Professional development / capacity building
    -    Better needs assessment and use of available resources

    -    Boost communication among citizens and stakeholders

    -    Increase transparency and accountability, as well as equity and inclusiveness

    -    Avoiding duplication in policy making


                                                                                               89 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


     -   Increasing responsiveness, access and build relationship

     -   Improving public image

     -   Improve the quality of information

     -   Consensus based decision-making

     -   Increased acceptance of results


Collaborative governance can be applied to virtually all the policy making fields. The following areas

constitute a mere example:

     -   Infrastructure management: building new infrastructures often entails the necessity to
         balance conflicting interests, especially for what concerns the case of huge externalities

     -   Digital inclusion: the increase in the use of ICT has to be fostered by the collaboration at
         every level

     -   Energy: delivering affordable and efficient energy, collaboration on the definition of energy
         regulations

     -   Environment: definition of new regulations on environmental safeguard, mediation
         concerning the use of environmental resources, collaboration in assessing public projects

         with environmental impact

     -   Health care: collaboration in health care reform, awareness and education campaigns,
         disease prevention

     -   Transportation: collaboration in the definition of a transportation plan, negotiation of
         transportation rules, settlement of disputes on the construction of a transportation facility



Inspiring cases of ICT applications to Collaborative Governance
                                        178
The Open Government Initiative              carried out by the Obama Administration, for promoting
government transparency and citizen engagement on a global scale:

     -   Partner4Solutions : the website for the Partnership Fund for Program Integrity Innovation
         at the Office of Management and Budget. By using this tool, the Partnership Fund aims at

         gathering ideas from citizens for improving the Federal assistance programme
                           180
     -   Regulations.gov : in this website it is possible to comment on proposed regulations and
         related documents published by the U.S. Federal government, as well as to search and
         review original regulatory documents as well as comments submitted by others

     -   Challenge.gov : online challenge platform allowing the public to bring the best ideas and
         top talent to bear on our nation’s most pressing challenges, which can range from simple

         ideas and suggestions to proofs of concept, designs, or finished products that solve the
         grand challenges of the 21st century



178http://www.whitehouse.gov/open
179
   http://www.partner4solutions.gov/
180
   http://www.regulations.gov/#!home
181http://challenge.gov/



                                                                                                     90 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

     -   We the People : allowing citizens to create and launch a petition in order to engage the

         government


Key challenges and gaps

The key challenges and gaps in collaborative governance are:

     -   Coping with accelerating changes in policy making

     -   Overlapping in institutions and jurisdictions

     -   Increasing complexity in the issues to be tackled
     -   Ability to choose the appropriate tool for tackling the problem at hand

     -   The need to integrate policies and resources

     -   Managing expectations

     -   Public involvement processes can be disconnected from real decision making

     -   Tackle conflicting interests among participants
     -   Using tools appropriate for the scale (small-scale or large scale) of the problem/solution

     -   Calibrate the level of citizens’ participation required with respect to the nature of the
         problem/solution

     -   Define the appropriate levels of accountability

     -   Avoid instability in preferences



Current research
     -   Analysing the compatibility of new collaborative behaviour with existing institutional

         framework


Research disciplines: political sciences, public administration, law, sociology, and other social
sciences in general (including institutional economics for example), as well as organisational,

network, innovation theories, etc.
Possible research instruments: thematic networks, Support Action



Future research: long term and short term issues

Short-term research

     -   Updated institutional framework


Long-term research

     -   New models of governance and service provision



182
   https://wwws.whitehouse.gov/petitions


                                                                                                    91 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




        4.2.2. Participatory Sensing

Introduction and definition

Participatory sensing refers to the usage of sensors, usually embedded in personal devices such as
smartphones to allow citizens to feed data of public interest. This could include anything from
photos to passive monitoring of movement in the traffic. Participatory sensing involves higher

commitment from citizens, contrary to opportunistic sensing where user may not be aware of active
applications. The diffusion of mobile phones significantly lowers the barriers of participation and

data input by citizens, with automated geo-tagging and time-stamping: given the right architecture,
they could act as sensor nodes and location-aware data collection instruments. While traditional
sensor nodes are centralised, these sensors are under the owners’ control. This would give way to

data availability at an unprecedented scale.



Why it matters in governance

Participatory sensing radically improves the data availability for evaluating the effect of public
policies and how individual behaviour is changing, provided adequate privacy provisions are in place.
Devices should assure enhanced users’ control over data, i.e. which data is being sent, when and

how it is treated, as well as possibility for enhanced data anonymisation.

Furthermore, design of participatory sensing should be placed in the framework of policy contexts,
allowing inference of policy impact from data. Future platforms should combine participatory
sensing, mass moderation, personalised feedback and social network analysis to assess the interplay

between perception, data and social interaction.

Participatory sensing is already used in “public sphere” activities such as environment and health.
However the specific issue of evaluating public policies has been so far little researched, with

particular regard to the implications for privacy, large-scale deployment and bias management on
citizens sensing.



Recent trends

Small-scale experiments are being carried out in different domains, mainly dealing with
environmental and health data. Applications in the field of urban planning are particularly promising,

yet there is no structure link between participatory sensing and policy models. Larger scale
deployment would require more granular privacy compliance and user-control, adequate incentives
to participation and deriving business models. There is no formalisation of the requirements and the

design of opportunistic versus participatory sensing, including sampling design for participants
recruitment.



Advantage of Application in Public Policy

Participatory sensing can be used to gather and collect the following kinds of information:



                                                                                          92 | P a g e
                                0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



     -    Civic data: neighborhood maintenance issues, power outage documentation

     -    Environmental data: data providing hints on pollution levels, climate-change related data

     -    Transportation: commutation habits, location and movement data, condition of the road,
          connections to public transportation, incidence of traffic, accidents occurrence

     -    Health: vital signs, info providing early warnings of diminishing health, info on epidemic
          spread, self-administered diagnostic tests



The advantages for policy making are:

     -    Possibility to collect data at an otherwise unachievable scale and geographic range

     -    Virtually costless data collection

     -    Reveal and highlight behavioural patterns and routines which can be accordingly changed

     -    Engage common citizens in sensitive issues

     -    More pervasive monitoring capacity in fields such as environment and health



Inspiring cases of policy making related applications          183

               184
     -    PEIR     (Personal Environmental Impact Report), which is a system allowing users to
          determine their exposure to environmental pollution by using a sensor in their mobile phone

          able to determine the location and the mean of transport
     -    eHealthSense , automatically detects health related events which are not directly observed

          by current sensor technology, like pain, tow conditions, depression
                  186
     -    SenSay , which is able to alert the medical staff when the user falls or in case of suspect
          behaviour
                        187
     -    MobAsthma , which monitors the exposure to pollution affecting asthma. Both the volume
          of air inhaled and the pollution rate are collected by sensors interfaced to the mobile phone

     -    Haze Watch , in which the concentration of carbon monoxide, ozone, sulphur dioxide, and

          nitrogen dioxide is measured by embedding pollution sensors in mobile phones
     -    NoiseTube , which registers noise levels used to monitor noise pollution, which can affect

          human hearing and behaviour
     -    EpySurveyor , used by the Red Cross to evaluate anti-malarial bednet distribution and use

          throughout sub-Saharan Africa, as well as the coverage achieved by vaccination campaigns



183To the best of our knowledge there are not yet government applications in the realm of participatory sensing

184http://peir.cens.ucla.edu
185
   http://dl.acm.org/citation.cfm?doid=1411759.1411761
186
   See Siewiorek et al. (2003)
187http://crystal.uta.edu/~kumar/CSE4340_5349MSE/mobsense.pdf

188http://www.pollution.ee.unsw.edu.au/
189
   http://noisetube.net/
190
   http://www.episurveyor.org/user/index


                                                                                                          93 | P a g e
                               0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling

                 191
     -   CarTel , which is a mobile sensing and computing system making use of mobile phones
         carried in vehicles to collect information about traffic or WIFI access points
                    192
     -   NoiseSpy , which is a sound-sensing system able to log data for monitoring environmental
         noise. Users can explore a city area while at the same time visualize noise levels in real time


Key challenges and gaps

     -   Preserve the privacy of the users which are required to provide extremely personal data

     -   Create new mobile device interfaces which are engaging and efficient and can be used
     -   Ensure security, as the current and past citizen’s position might be spotted
     -   Provide new sensors capable of increasing the range of information that individuals can

         track and use
     -   Create network infrastructures aimed at supporting participatory sensing services

     -   Provide incentive for participation to data collection
     -   Develop analytical techniques to carry out more accurate inference with mobile phone

         supplied data such as geo-data and images
     -   Develop visual analytics and data analysis techniques which provide relevant and easy to

         interpret information for the general public
     -   Create engaging and efficient mobile device interfaces to support effective, real-time user

         interaction
     -   Provide quality data, temporal and geo-graphical availability, and ability to cover the

         phenomena


Current research

     -   Aggregating and validating citizens generated and government data resource discovery,

     -   Selective sharing, and context verification mechanisms, as well as application-level support
         for data gathering campaigns,

     -   Incentives for participatory sensing,

     -   Evaluation of human agents as sensors

















191
   http://cartel.csail.mit.edu/doku.php
192www.cl.cam.ac.uk/mobilesensing/downloads.htm



                                                                                                      94 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




Disciplines of research: sensor networks, location services; psychology, economics of participation;

privacy
Research instruments: testbeds and living labs, STREPs



Future research: long term and short term issues

Short-term research

     -   Sensing coverage, sensor calibration and sensor context for opportunistic sensing.

     -   Quality verification for participatory sensing

     -   Privacy-compliant sensing and sharing

     -   Business models for participatory sensing

     -   Intelligently recruiting collaborators and deploying data collection protocols.

     -   Anonymous, transparent use of human-carried sensing devices

     -   Evaluating behavioural change through participatory sensing



Long-term research

     -   Enhanced analytical techniques to make more accurate inferences from mobile phone-
         supplied data such as location and images and to automatically detect and respond to subtle
         events;

     -   New personal-scale sensors to expand the range of information that individuals can track

         and use

     -   Privacy by design in participatory sensing























                                                                                                    95 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


        4.2.1. Identity Management

Introduction and definition

Digital identity management has long been a policy priority in the EU Member States, and large-scale
investments have been deployed. In the context of collaborative governance, digital identity
constitutes a fundamental pillar of trustworthy cooperation. Identity management systems include
control and management of credentials used to authenticate one entity to another, and authorise an

entity to adopt a specific role or perform a specific task. Global in nature, they should support non-
repudiation mechanisms and policies; dynamic management of identities, roles, and permissions;
privacy protection mechanisms and revocation of permissions, roles, and identity credentials.
Furthermore, all the identities and associated assertions and credentials must be machine

processable and human understandable.
At the EU level, the goal is to provide an interoperable privacy protecting infrastructure for eID that

is federated across countries, with multiple levels of security for different services, relying on
authentic sources, and usable in a private sector context.
Alongside this, a flexible, context-dependent and interoperable identity management system is

required for large-scale deployment. In particular, federated identity management systems that
ensure flexible deployment and seamless integration of users’ preferred identities, including
commercial (such as Facebook connect) and open source solutions (such as OpenID) are needed.

Particular focus should be put on usable delegation of privileges, which is very important for
workflows and integrating services.

Electronic identity management should identify non-humans (devices, sensors) as well as humans, in
order to ensure validated identity in the context of participatory sensing and the Internet of Things.

At the same time, eIdentity management should take into account the risks of information
centralization in terms of data privacy and security. Cost-benefit considerations of centralised versus
federated systems remains a key issue. Identity federation can be accomplished in any number of
ways, some of which involve the use of Internet standards, such as the OASIS Security Assertion

Markup Language (SAML) specifications, with the use of open source technologies and/or other
openly published specifications.


Why it matters in governance

Identity certification is one of the core tasks of government, and therefore pertains specifically to
the governance context. This is reinforced by Meta Group (2002), who views the implementation of

identity management “not as a differentiator but as mandatory security consideration, a business
imperative and a non-negotiable user expectation”.


Recent trends

The role of Identity Management is vital in the context of ICT for Governance and Policy Modelling.
The importance of addressing eIdentity-related issues for secure public service provision, citizen
record management and law enforcement has made Identity management a strategic issue for

governments at both a local and international level. Research for the design and implementation of
privacy preserving digital identity, as well as for its supporting management infrastructures, and
delegation of authority, has reached a satisfactory level. Nevertheless, one of the greatest problems
in Identity Management is lack of interoperability of digital identities and identity management




                                                                                         96 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


systems between proprietary systems and standards-based ones, and between organisations and
governments.



Current practice

    -    Electronic ID creation at national level
    -    Pilots in cross-border interoperability of field in EU (STORK project)



Public Policy Applications

The development of a Federated Identity Management would be to the following benefits at
governmental level:

    -    Avoid replicated efforts: reduction in the number of sign-ons and passwords needed for
         accessing multiple systems and databases, thereby decreasing cost and time-waist

    -    It would be possible to define a mechanism of sharing and managing identity information as
         it moves between discrete legal, policy and organizational domains which would be based
         on standards

    -    Institutions would not have to establish separate relationships and procedures with one
         another

    -    It is possible to grant ad revoke user access to info more easily

    -    Reduce the number of passwords accumulated: citizens either forget them or choose simple
         ones thereby increasing insecurity and fraud possibility

    -    Increase in security regarding the user access to information and the digital resources, as it
         eliminates the need to replicate databases of user credentials for separate applications and
         systems, which are potential weak points

    -    Increase in sensitive information shared across government and organizational boundaries in
         case of crisis

    -    Allows to focus on users of information and services rather than on entities that house those
         resources


Key challenges and gaps

    -    Fragmentation of research in identity along disciplinary lines

    -    Need for new identity proof processes

    -    Privacy issues: use limitation principles, avoid pervasive surveillance
    -    Capability to efficiently integrate services throughout the chain

    -    Time saving identification

    -    Specifications and nature of a Digital Identity dictated by the social and political
         environment of the country of issuance

    -    Increasing number of electronic identity-related crimes (identity fraud, identity theft,
         impersonation), which makes it difficult to guarantee the legitimacy of identities




                                                                                                   97 | P a g e
                              0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling




Current research

     -   Cultural-dependent identity systems

     -   Mobile and biometrics in eIdentity
     -   Privacy protecting identity management systems

     -   User-centric identity, delegation of authority



Disciplines of research: legal, technological, social, economic

Possible research instruments: testbeds and living labs, STREPs
Future research: long term and short term issues

Short-term research

     -   Quantitative research on cost-benefit analysis of interoperable identity

     -   Dynamic user-controlled identity disclosure
     -   Formal verification of identity management systems

     -   Governance and legal issues, levels of assurance

Long-term research

     -   Context-dependent identity management


































                                                                                                    98 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



5. Conclusions: Closing the Loop of Policy-Making 2.0

The research challenges identified so far are not just a simple collection of research issues, but an
integrated bundle of innovative solutions that together can lead to a paradigm shift in policy-making.

The CROSSROAD roadmap already identified an integrated approach based on a technology layer
model with three layers: data, analysis and decision/action:

    -    The data layer provides new information that was previously not available.

    -    The analysis layer provides a new perspective and understanding of data.

    -    The behavioural change layer acts on the incentives and barriers to action and behaviour.



















                                    Figure 13: CROSSROAD’s Approach

The CROSSOVER project refines this approach by linking each of the research challenge to a specific
challenge in the framework, as described in section 3 and illustrated in the figure below.

To do so, we will go back in this section to the challenges and tasks of policy-making, as illustrated in
section 3.

As we can see, the described research challenges capture all the main needs of policy-makers, and in
particular the capacity to detect problems early and to leverage the collective intelligence in policy-
making.



















                                                                                              99 | P a g e
                            0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


























                 Figure 14: Relation Between Policy-Making Needs and Research Challenges
In view of this analysis, the next step is to relate each of the research challenges in the policy-making

cycle. Each research challenge is in fact relevant for one or more of the specific tasks, not for all.
The figure below illustrates this relationship. In each of the phases of the cycle, for each of the tasks,

we can identify the potential impact of the research challenges described.























The policy cycle starts with the agenda setting phase, where the problem is identified and analyzed.
In this section, visualization and opinion mining can help to identify the problems at an early stage.

Advanced modelling techniques are then used to untangle the casual relationships behind the
problem, understanding the causal roots that need to be addressed by policy.

Once the problem is clearly spelled out, we move to the policy design phase, where collaborative
solutions are useful to identify the widest range of options, by leveraging collective intelligence. In


                                                                                             100 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


order to facilitate the choice of the most effective option, immersive simulations support decision-
makers by taking into account unexpected impacts and relationships. Collaborative governance

enables then to develop further and fine-tune the most effective option, for example through
commentable documents.

Once the option is developed and adopted, we enter into policy implementation. In this phase, it is
crucial to ensure awareness, buy-in and collaboration from the widest range of stakeholders: social
network analysis, crowdsourcing and serious gaming are useful to deliver this.

Already during this implementation, we move into the monitoring and evaluation. Open data allow
stakeholders and decision makers to better monitor execution; together with sentiment analysis,
they can be used to evaluate the impact of the policy, also through advanced visualization
techniques.

In summary, our vision for 2030 embodies a radically different context for policy-making 2.0.

On policy modelling and simulation, thanks to standardisation and reusability of models and tools,
system thinking and modelling applied to policy impact assessment has become pervasive
throughout government activities, and is no longer limited to high-profile regulation. Model building

and simulation is carried out directly by the responsible civil servants, collaborating with different
domain experts and colleagues from other departments. Visual dynamic interfaces allow users to
directly manipulate the simulation parameters and the underlying model.

Policy modelling software becomes productized and engineered, and is delivered as-a-service,
through the cloud, bundled with added-value services and multidisciplinary support including
mathematical, physics, economic, social, policy and domain-specific scientific support.

Cloud-based interoperability standards ensure full reusability and composability of models across
platforms and software.

System policy models are dynamically built, validated and adjusted taking into account massive
dataset of heterogeneous data with different degrees of validity, including sensor-based structured
data and citizens-generated unstructured opinions and comments. By integrating top-down and

bottom-up agent based approaches, the models are able to better explain human behaviour and to
anticipate possible tipping points and domino effects

On collaborative governance, policy-making leverages collective intelligence and collective action. It
accounts for the greater policentricity of our governance system. While traditional tools are
designed for the public decision-makers, these research challenges are more symmetric by nature,
in order to engage stakeholders all through the phases of the policy-making cycle. Thanks to
visualisation and design, it is able to reach out to new stakeholders and lower the barriers to entry in

the policy discussions. Policy-making 2.0 is not only designed to be more effective, but also more
participatory.

Next steps
 The present roadmap represents the starting point for collaboration. It will be published in
commentable format for practitioners and researchers to comment and revise.

In particular, the present roadmap aiming to become a living platform, comments will be sought:

    -   From all stakeholders, on the relevance, clarity and completeness of the proposed research
        challenges

    -   From researchers, on the actual research carried out on these challenges

    -   From policy-makers, on the actual usage of these tools and methodologies, lessons learnt
        and challenges encountered


                                                                                         101 | P a g e
                             0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


In addition, a dedicated survey of policy-makers on their needs and challenges will be carried out, in
order to fine-tune the final roadmap to their needs.
































































                                                                                               102 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling



6. References

1. Alexopoulos, C. & Kim, S.H. (2002) Output Data Analysis for Simulations, Proceedings of the 2002
    Winter Simulation Conference, San Diego (CA), December 8-11

2. Alexopoulos, C. & Seila, A.F. (1998) Output Data Analysis, in: Banks, J., ed., Handbook of
    Simulation: Principles, Methodology, Advances, Applications and Practice, John Wiley, New York.

3. Balci, O. (1989) How to assess the acceptability and credibility of simulation results, Proceedings
    of the 1989 Winter Simulation Conference, Washington (DC), December 4-6.

4. Barabasi, A. L., 2003. Linked: How Everything is Connected to Everything Else and What it Means
    for Business and Everyday Life, Plume Books.

5. Bederson, B and Shneiderman, B. (2003). The Craft of Information Visualization: Readings and
    Reflections,

6. Bertin, J (1967) Sémiologie Graphique. Les diagrammes, les réseaux, les cartes.

7. Boyd, D., Crawford, K, (2011) Six Provocations for Big Data. A Decade in Internet Time:
    Symposium on the Dynamics of the Internet and Society, September 2011.

8. Burkholder, L, ed. (1992) Philosophy and the Computer, Boulder, San Francisco, and Oxford:
    Westview Press.

9. Christakis N and Fowler J.“The spread of obesity in a large social network over 32 years,” in New
    England Journal of Medicine, 357, 370–379, 2007

10. Colecchia, A. and P. Schreyer (2002), ICT Investment and Economic Growth in the 1990s: Is the
    United States a Unique Case? A Comparative Study of Nine OECD Countries, Review of Economic
    Dynamics, Vol. 5, No. 2, April, pp. 408–442.

11. European Commission (2009), Impact Assessment Guidelines, SEC (2009) 92
12. Friendly, M. (2008) "Milestones in the history of thematic cartography, statistical graphics, and

    data visualization"
13. Gass, S.I. (1983) Decision-aiding models: validation, assessment and related issues for policy

    analysis, Operations Research, 31(4), pp. 601-663
14. Gass, S.I. & Joel., L. (1987) Concepts of model confidence, Computers and Operations Research,

    8 (4), pp. 341-346.
15. Goldsman, D. & Tokol, G. (2000) Output Analysis procedures for computer simulations,

    Proceedings of the 2000 Winter Simulation Conference, Orlando (FL), 10-13 December 2000.
16. Goldsman, D. & Nelson, B.L. (1998) Comparing systems via simulation, In: Banks, J., ed.,

    Handbook of Simulation: Principles, Methodology, Advances, Applications and Practice, John
    Wiley, New York

17. Howard, C. (2005), The Policy Cycle: A Model of Post-Machiavellian Policy Making?. Australian
    Journal of Public Administration, 64: 3–13. doi: 10.1111/j.1467-8500.2005.00447.x

18. Jorgenson, D. W., Ho, M. S. and K. J. Stiroh (2008), A Retrospective look at the US Productivity
    Growth Resurgence, Journal of Economic Perspectives, Volume 22, Number 1, 2008, Pages 3-24.
19. Keim, D., 2011. Solving Problems with Visual Analytics : Challenges and Applications. Society, 65,

    p.1.




                                                                                       103 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


20. Kelton, W.D. (1997) Statistical Analysis of Simulation Input, Proceedings of the 1997 Winter
    Simulation Conference, Atlanta (GA), 7-10 December 1997.

21. Kuhlmann, S. & Meyer-Krahmer, F., 1994. Practice of Technology Policy Evaluation in Germany:
    Introduction and Overview. In G. Becher et al., eds. Evaluation of Technology Policy Programmes

    in Germany. Springer Netherlands, pp. 3-29
22. Landesberger, T. von, Goerner, M., Schreck, T. (2009) Visual Analysis of Graphs with Multiple

    Connected Components. IEEE Symposium on Visual Analytics Science and Technology
23. Landesberger, T. von, Knuth, M., Schreck, T., Kohlhammer, J. (2008) Data Quality Visualization

    for Multivariate Hierarchic Data, IEEE Information Visualization Conference (INFOVIS), Columbus,
    OH, USA (2008)

24. Latour, B. (2009). ‘Tarde’s idea of quantification’, in The Social After Gabriel Tarde: Debates and
    Assessments, ed M. Candea, London: Routledge, pp. 145-162.
25. Law, A.M. (1983) Statistical Analysis of Simulation Output Data, Operations Research, 31(6), pp.

    983-1029.
26. Law, A.M. (2006), Simulation Modelling and Analysis, 4th Ed., McGraw-Hill, New York.

27. Lazer, D., Pentland, A., Adamic, L., Aral, S., Barabási, A., Brewer, D.,Christakis, N., Contractor, N.,
    Fowler, J.,Gutmann, M., Jebara, T., King, G., Macy, M., Roy, D., & Van Alstyne, M. (2009).

    ‘Computational Social Science’. Science vol. 323, pp. 721-3.
28. Lessig, L. (2009). The New Republic.

29. Livnat, Y. et al. (2005), Visual correlation for situational awareness. IEEE Symposium on
    Information Visualization. INFOVIS, 1, p.95-102.

30. Nakayama, M.K. (2002) Simulation Output Analysis, Proceedings of the 2002 Winter Simulation
    Conference, San Diego (CA), 8-11 December 2002.

31. OECD. (2005). Modernising government: the way forward. Paris, France, OECD.

32. Oliner, S. D. and D. Sichel (2000), The Resurgence of Growth in the Late 1990s: Is Information
    Technology the Story? Journal of Economic Perspectives, 14: 3-22.

33. Ormerod, P., (2010). N Squared: public policy and the power of networks, RSA Pamphlets

34. Pang, B., & Lee, L. (2008). Foundations and Trends® in Information Retrieval, 2(1–2), 1-135. doi:
    10.1561/1500000011.

35. Phelps ed. (1970), Microeconomic Foundations of Employment and Inflation Theory. New York,
    Norton and Co. ISBN 0-393-09326-3
36. Rosenblum (1994). "Research issues in scientific visualization". In: Computer Graphics and

    Applications, IEEE. Volume 14, Issue 2, March 1994 Page(s):61 - 63.
37. Saffo, Paul, (1997), Looking Ahead: Implications Of The Present, Are You Machine Wise. Harvard

    Business Review
38. Schelling, T.C., (1969). Models of segregation. The American Economic Review, 59(2), pp.488-

    493.
39. Sargent, R. (2009). Verification and Validation of Simulation Models. Proceedings of the 2009

    Winter Simulation Conference.
40. Schlesinger, M. (1979). Terminology for model credibility. Simulation, 32(3), 103-104.




                                                                                        104 | P a g e
                           0204F01_International Research Roadmap on ICT tools for Governance and Policy Modelling


41. Shneiderman, B. (1996) The Eyes Have It: A Task by Data Type Taxonomy for Information

    Visualizations, IEEE Symposium on Visual Languages
42. Siewiorek, A. Smailagic, J. Furukawa, A. Krause, N. Moraveji, K. Reiger, J. Shaffer, and F. L. Wong,

    (2003) “SenSay: A Context-aware Mobile Phone,” in Proceedings of the 7th IEEE International
    Symposium on Wearable Computers (ISWC), pp. 248–249

43. Taleb, N. (2008). The Black Swan: The Impact of the Highly Improbable. Penguin.
44. Thomas and Ahrweiler Eds. (2005). Internationale partizipatorische Kommunikationspolitik -

    Strukturen und Visionen. Münster: LIT.
45. Thomas, J.J. and K. Cook (2003), Illuminating the Path: The R&D Agenda for Visual Analytics.

    2005: IEEE
46. Tufte, E. R (1983) The Visual Display of Quantitative Information, Graphics Press

47. van Wijk. (2005). The Value of Visualization. In IEEE Visualization Conference. pp. 79-86

48. Ware, C. (2004) Information Visualization - Perception for Design, Second Edition, Academic
    Press

49. Wattenberg, M. (2008) The Word Tree, an Interactive Visual Concordance, IEEE Transactions on
    Visualization and Computer Graphics, Vol. 14, No. 6








































                                                                                         105 | P a g e