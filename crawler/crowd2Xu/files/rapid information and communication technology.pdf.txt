▯A▯u▯t▯h▯o▯r▯ ▯Be▯e)▯s▯o▯n▯,▯ ▯R▯.▯ ▯T▯r▯a▯v▯i▯s



▯T▯i▯t▯l▯e    ▯R▯a▯p▯i▯d▯ ▯I▯n▯f▯o▯r▯m▯a▯t▯i▯o▯n▯ ▯a▯n▯d▯ ▯C▯o▯m▯m▯u▯n▯i▯c▯a▯t▯i▯o▯n▯ ▯T▯e▯c▯h▯n▯o▯l▯o▯g▯y▯ ▯A▯s▯s▯e▯s▯s▯m▯e▯n▯t▯ ▯T▯e▯a▯m▯ ▯(▯R▯T▯A▯T▯)▯:
              ▯e▯n▯a▯b▯l▯i▯n▯g▯ ▯t▯h▯e▯ ▯"▯h▯a▯n▯d▯s▯ ▯a▯n▯d▯ ▯f▯e▯e▯t▯"▯ ▯t▯o▯ ▯w▯i▯n▯ ▯t▯h▯e▯ ▯"▯h▯e▯a▯r▯t▯s▯ ▯a▯n▯d▯ ▯m▯i▯n▯d▯s▯"


▯P▯u▯b▯l▯i▯s▯ h▯e▯r▯t▯e▯r▯e▯y▯,▯ ▯C▯a▯l▯i▯f▯o▯r▯n▯i▯a▯:▯ ▯N▯a▯v▯a▯l▯ ▯P▯o▯s▯t▯g▯r▯a▯d▯u▯a▯t▯e▯ ▯S▯c▯h▯o▯o▯l



▯I▯s▯s▯u▯e▯ ▯ D2▯0▯1e4 ▯▯0▯9



▯U▯R▯L        ▯h▯t▯t▯p▯:▯/▯/▯h▯d▯l▯.▯h▯a▯n▯d▯l▯e▯.▯n▯e▯t▯/▯1▯0▯9▯4▯5▯/▯4▯3▯8▯7▯6


            ▯T▯h▯i▯s▯ ▯d▯o▯c▯u▯m▯e▯n▯t▯ ▯w▯a▯s▯ ▯d▯o▯w▯n▯l▯o▯a▯d▯e▯d▯ ▯o▯n▯ ▯M▯a▯r▯c▯h▯ ▯2▯9▯,▯ ▯2▯0▯1▯5▯ ▯a▯t▯ ▯2▯2▯:▯1▯2▯:▯2▯3
              NAVAL
       POSTGRADUATE

             SCHOOL

        MONTEREY, CALIFORNIA





              THESIS



   RAPID INFORMATION AND COMMUNICATION
    TECHNOLOGY ASSESSMENT TEAM (RTAT):
  ENABLING “HEARTS AND MINDS”T” TO WIN THE

                  by

              R. Travis Beeson

              September 2014

Thesis Advisor:               Brian Steckler
Second Reader:               Bryan Hudgens

    Approved for public release; distribution is unlimited
THIS PAGE INTENTIONALLY LEFT BLANK
                 REPORT DOCUMENTATION PAGE                                  Form Approved OMB No. 0704-0188

 Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instruction,
 searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send
 comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to
 Washington headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA
 22202-4302, and to the Office of Management and Budget, Paperwork Reduction Project (0704-0188) Washington DC 20503.
 1. AGENCY USE ONLY (Leave blank)            2. REPORT DATE        3. REPORT TYPE AND DATES COVERED
                                                September 2014                   Master’s Thesis

 4. TITLE AND SUBTITLE                                                   5. FUNDING NUMBERS
 RAPID INFORMATION AND COMMUNICATION TECHNOLOGY
 ASSESSMENT TEAM (RTAT): ENABLING THE “HANDS AND FEET” TO WIN
 THE “HEARTS AND MINDS”

 6. AUTHOR(S) R. Travis Beeson
 7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)                      8. PERFORMING ORGANIZATION
     Naval Postgraduate School                                           REPORT NUMBER
     Monterey, CA 93943-5000

 9. SPONSORING /MONITORING AGENCY NAME(S) AND ADDRESS(ES)                10. SPONSORING/MONITORING
     N/A                                                                  AGENCY REPORT NUMBER

 11. SUPPLEMENTARY NOTES The views expressed in this thesis are those of the author and do not reflect the official policy
 or position of the Department of Defense or the U.S. Government. IRB protocol number ____N/A____.

 12a. DISTRIBUTION / AVAILABILITY STATEMENT                              12b. DISTRIBUTION CODE
 Approved for public release; distribution is unlimited                               A

 13. ABSTRACT (maximum 200 words)
 Large-scale disasters severely damage local information and communication technology (ICT) infrastructure. This
 negatively impacts responders’ ability to communicate and collaborate with one another. As a result, humanitarian
 assistance (HA) response organizations cannot maintain situational awareness and efforts remain disjointed and

 inefficient.
     Out of the rubble of the Haiti earthquake, a cross-organizational collection of first responders created the Rapid
 ICT Assessment Team (RTAT) to conduct and share a holistic assessment of the ICT environment. However, RTAT

 has yet to solve the problem of efficiently and effectively collecting the ICT data and creating a shareable, common,
 ICT operational picture. Employing a campaign of experimentation (COE), this thesis analyzes RTAT with an
 Enterprise Architecture framework and Savvion process modeler and employs the Android based, mobile, spatial data
 collection applications Lighthouse and Open Data Kit (ODK) Collect to exploit the open source form builder ODK.

 RTAT founders, along with Bicol University and local volunteers, field tested the ODK forms with crowd sourcing
 techniques and when Typhoon Haiyan struck; they validated the organizational RTAT model and integrated
 assessments into the Pacific Disaster Center’s (PDC) DisasterAWARE collaborative website.

     This thesis highlights the disjointed rapid response ICT assessment community which lacks standard forms and
 unifying data standards. The COE validates using open source, spatial data collection tools and crowdsourcing
 techniques for even highly technical needs. However, the COE revealed programming logic limits of the ODK forms,
 and the imperfect back-end integration between RTAT and the PDC. Debates remain over the validity of qualitative,

 crowdsourced ICT assessments. Going forward, RTAT must refine its forms and lead the movement to harmonize HA
 community assessment data sets. Furthermore, future data collection tools must become operating system
 independent.

 14. SUBJECT TERMS disaster, information and communication technology (ICT), informatio15. NUMBER OF
 communication, infrastructure, mobile, data collection, UN, emergency telecommunicatioPAGESster
 (ETC), Philippines, Typhoon, Haiyan, Android, Lighthouse, Bicol University, Open Data Kit (ODK)323
 campaign of experimentation, Savvion, Interagency Standing Committee (IASC)
                                                                                       16. PRICE CODE
 17. SECURITY                 18. SECURITY                     19. SECURITY            20. LIMITATION OF

 CLASSIFICATION OF            CLASSIFICATION OF THIS           CLASSIFICATION OF       ABSTRACT
 REPORT                       PAGE                             ABSTRACT
          Unclassified                   Unclassified                Unclassified               UU
NSN 7540-01-280-5500                                                              Standard Form 298 (Rev. 2-89)
Prescribed by ANSI Std. 239-18
THIS PAGE INTENTIONALLY LEFT BLANK





























                       ii
                 Approved for public release; distribution is unlimited


      RAPID INFORMATION AND COMMUNICATION TECHNOLOGY
ASSESSMENT TEAM (RTAT): ENABLING THE “HANDS AND FEET” TO WIN

                         THE “HEARTS AND MINDS”


                                R. Travis Beeson
                        Major, United States Marine Corps
                       B.S., The Ohio State University, 2000
                        M.S.B.A., Boston University, 2009



                       Submitted in partial fulfillment of the
                          requirements for the degree of



 MASTER OF SCIENCE IN INFORMATION TECHNOLOGY MANAGEMENT


                                    from the


                     NAVAL POSTGRADUATE SCHOOL

                                September 2014


Author:             R. Travis Beeson




Approved by:        Brian Steckler
                    Thesis Advisor



                    Bryan Hudgens

                    Second Reader



                    Dan Boger
                    Chairman, Information SystemsDepartment



                                       iii
THIS PAGE INTENTIONALLY LEFT BLANK





























                    iv
                                   ABSTRACT



Large-scale disasters severely damage local information and communication technology
(ICT) infrastructure. This negatively impacts responders’ ability to communicate and

collaborate with one another. As a result, humanitarian assistance (HA) response

organizations cannot maintain situational awareness and efforts remain disjointed and
inefficient.


       Out of the rubble of the Haiti earthquake, a cross-organizational collection of first
responders created the Rapid ICT Assessment Team (RTAT) to conduct and share a

holistic assessment of the ICT environment. However, RTAT has yet to solve the

problem of efficiently and effectively collecting the ICT data and creating a shareable,
common, ICT operational picture. Employing a campaign of experimentation (COE), this

thesis analyzes RTAT with an Enterprise Architecture framework and Savvion process

modeler and employs the Android based, mobile, spatial data collection applications
Lighthouse and Open Data Kit (ODK) Collect to exploit the open source form builder

ODK. RTAT founders, along with Bicol University and local volunteers, field tested the

ODK forms with crowd sourcing techniques and when Typhoon Haiyan struck; they

validated the organizational RTAT model and integrated assessments into the Pacific
Disaster Center’s (PDC) DisasterAWARE collaborative website.

       This thesis highlights the disjointed rapid response ICT assessment community

which lacks standard forms and unifying data standards. The COE validates using open

source, spatial data collection tools and crowdsourcing techniques for even highly

technical needs. However, the COE revealed programming logic limits of the ODK
forms, and the imperfect back-end integration between RTAT and the PDC. Debates

remain over the validity of qualitative, crowdsourced ICT assessments. Going forward,

RTAT must refine its forms and lead the movement to harmonize HA community
assessment data sets. Furthermore, future data collection tools must become operating

system independent.




                                         v
THIS PAGE INTENTIONALLY LEFT BLANK





























                    vi
                               TABLE OF CONTENTS



I.      INTRODUCTION........................................................................................................1
        A.      PROBLEM STATEMENT.............................................................................2
        B.      RESEARCH QUESTIONS.............................................................................2
        C.      PURPOSE STATEMENT...............................................................................3
        D.      CONTRIBUTION............................................................................................3

II.     LITERATURE REVIEW ...........................................................................................5
        A.      INFORMATION..............................................................................................5
                1.      Raw Data to Understanding................................................................6
                        a.      Data............................................................................................6

                        b.      Processed Data (Information)..................................................7
                        c.      Knowledge.................................................................................
                        d.      Situational Understanding..................................................9
                2.      Relational Database.............................................................................9
                3.      Spatial Data........................................................................................10
                4.      Data Collection...................................................................................10

                5.      Microsoft Excel...................................................................................10
        B.      U.S. DISASTER RESPONSE.......................................................................11
                1.      U.S. Federal Government Response to Disasters Within the
                        U.S. and its Territories ......................................................................12
                2.      U.S. Response to International Disasters.........................................14
                        a.      U.S. Department of State Response to Foreign Disasters.....14
                        b.      U.S. Department of Defense Response to International

                                Disasters ..................................................................................15
                        c.      The Interagency Coordination for Foreign Humanitarian
                                Assistance................................................................................
        C.      UNITED NATIONS DISASTER RESPONSE............................................17
                1.      United Nations Humanitarian Events..............................................18
                        a.      Natural Disaster......................................................................18
                        b.      Complex Emergency...........................................................18

                        c.      United Nations Level of Crises...............................................19
                2.      UN Resident Coordinator..................................................................19
                3.      United Nations Emergency Relief Coordinator..............................20
                4.      United Nations Office for the Coordination of Humanitarian
                        Affairs..................................................................................................20
                        a.      United Nations Disaster Assessment Coordination Team.....        20

                        b.      Inter-Agency Standing Committee.........................................21
                5.      UN Emergency Telecommunication Cluster...................................23
                        a.      Fast Information Technology and Telecommunications
                                Emergency and Support Team...............................................
                        b.      Emergency Preparedness Integration Centre........................24
        D.      OTHER           INFORMATION                AND         COMMUNICATION
                TECHNOLOGY NON-GOVERNMENTAL ORGANIZATIONS...........25

                                               vii
        1.      The International Federation of the Red Cross/Red Crescent......25
                a.      National Red Cross or Red Crescent Societies ......................25
                b.      Red    Cross/Red      Crescent    First    Assessment      and
                        Coordination Team.................................................................26

        2.      NetHope ..............................................................................................26
        3.      TÉLÉCOMS SANS FRONTIÈRES.................................................27
                a.      Humanitarian Calling Operations.........................................27
                b.      First Responder Emergency Telecommunications Centers..27
                c.      Information       and      Communication          Technology
                        Assessments.............................................................................28
        4.      Humanity Road..................................................................................28

        5.      Humanitarian Data Exchange..........................................................28
                a.      Repository................................................................................29
                b.      Analytics..................................................................................29
                c.      Standards.................................................................................29
E.      RAPID          INFORMATION                AND         COMMUNICATION
        TECHNOLOGY ASSESSMENT TEAM....................................................29
F.      FOUR CATEGORIES OF POST DISASTER ASSESSMENT DATA

        SETS................................................................................................................30
        1.      Coordinated Assessments..................................................................31
        2.      Harmonized Assessment....................................................................31
        3.      Joint Assessment ................................................................................31
        4.      Uncoordinated Assessments..............................................................32
G.      THREE CURRENT POST DISASTER ASSESSMENT FORMS ...........32

        1.      United States Agency for International Development....................32
                a.      Communication.......................................................................
                b.      Electric Power.........................................................................35
        2.      United Nations Assessments..............................................................36
                a.      United Nations Assessment Phases........................................36
                b.      UN Situational Analysis (Phases 1–2)...................................38
                c.      Multi-Cluster Initial Rapid Assessment.................................38

                d.      United Nations Disaster Assessment Coordination Team
                        Assessments.............................................................................
        3.      World     Food    Programme       Emergency       Field   Operations
                Pocketbook..........................................................................................41
H.      ENTERPRISE ARCHITECTURE ANALYSIS.........................................45
        1.      Enterprise Architecture.....................................................................45
        2.      Operating Model................................................................................46

                a.      Standardization.......................................................................
                b.      Integration...............................................................................47
                a.      Agility.................................................................47
I.      SAVVION BUSINESS PROCESS MANAGEMENT................................48
J.      HASTILY FORMED NETWORK ..............................................................50
        1.      Wireless Mesh Network.....................................................................50

        2.      Hastily Formed Network...................................................................51

                                      viii
                          a.      Hastily Formed Network Organization..................................51
                          b.      Hastily Formed Network Equipment .....................................52
         K.      MOBILE DEVICE OPERATING SYSTEMS............................................53
                 1.       iPhone Operating System..................................................................53

                 2.       Android Operating System...............................................................54
         L.      MOBILE DATA COLLECTION METHODS...........................................55
                 1.       Electronic Form Interface.................................................................57
                 2.       Short Messaging Service + Cue Card ..............................................58
                 3.       Voice Interface ...................................................................................58
         M.      AVAILABLE MOBILE ELECTRONIC FORM INTERFACES ............58
                 1.       The HumanitariaN Operations Mobile Acquisition of Data..........59

                 2.       Android Options.................................................................................60
                          a.      Open Data Kit..........................................................................60
                          b.      Lighthouse Application...........................................................61
                          c.      Open Data Kit Collect.............................................................62
                          d.      Field Information Support Tool.............................................63
                          e.      CyberTracker...........................................................................63
                          f.      Humanitarian Data Toolkit....................................................64

                 3.       iPhone Options...................................................................................65
                          a.      Fulcrum...................................................................................65
                          b.      GeoChat...................................................................................
                          c.      Majella.....................................................................................66
                          d.      ViewWorld...............................................................................66
                          e.      DataKeep.................................................................................67

III.     RESEARCH METHOD............................................................................................69
         A.      CAMPAIGN OF EXPERIMENTATION...................................................69
                 1.       Discovery Experiments......................................................................70
                 2.       Hypothesis Testing.............................................................................70

                 3.       Demonstration Experiments.............................................................70
         B.      RAPID INFORMATION COMMUNICATION TECHNOLOGY
                 ASSESSMENT TEAM CAMPAIGN OF EXPERIMENTATION...........71
                 1.       The Discovery Stage...........................................................................71
                 2.       Investigative Stage .............................................................................71
                 3.       Demonstration Stage..........................................................................71

         C.      THE DISCOVERY STAGE .........................................................................72
                 1.       Research and Literature Review......................................................72
                 2.       Enterprise Architecture Assessment................................................72
                          a.      Organization Mission..............................................................73
                          b.      Stakeholder Analysis...........................................................73
                          c.      Operating Model Analysis ......................................................73
                          d.      Enterprise Architecture Assessment Conclusions and

                                  Recommendations...................................................................
                 3.       Mobile Data Collection Tool Prototyping........................................74
                 4.       Mobile Data Collection Tool Analysis of Alternatives....................76
                          a.      Requirements...........................................................................

                                                  ix
                         b.      Selected Operating System: Android......................................78
                         c.      Selecting the Data Collection Application: Lighthouse
                                 and ODK Collect.....................................................................78
                5.       RTAT Assessment Form Development............................................79

                         a.      Beginning Through Legazpi City Field Experiment.............82
                         b.      Post Legazpi City Through Typhoon Haiyan.........................85
                         c.      Post Typhoon Haiyan to Today..............................................89
        D.      HYPOTHESIS TESTING.............................................................................91
                1.       Legazpi City Field Experiment.........................................................91
                         a.      Format.....................................................................................92
                         b.      Training...................................................................................93

                         c.      Execution.................................................................................93
                         d.      Results......................................................................................94
                2.       Savvion Process Model......................................................................97
                         a.      The Hypothesis for the Savvion Experiments: Utilizing
                                 Crowdsourcing Techniques Will Reduce RTAT Costs
                                 While     Decreasing      the   Time     to    Complete      ICT
                                 assessments.Experiment Format............................................98

                         b.      Process Changes .....................................................................99
                         c.      Conclusion and Results ........................................................101
        E.      DEMONSTRATION STAGE.....................................................................
                1.       Typhoon Haiyan Deployment.........................................................103
                         a.      Concept of Operations ..........................................................105
                         b.      Execution...............................................................................105

                         c.      Results....................................................................................107
                2.       Joint Interagency Field Exercise 2014–4 .......................................112
                         a.      Concept of Operation............................................................113
                         b.      Results and Recommendations.............................................

IV.     SUMMARY, CONCLUSIONS, AND LIMITATIONS........................................119
        A.      SUMMARY..................................................................................................119
        B.      CONCLUSIONS..........................................................................................121
        C.      LIMITATIONS............................................................................................121
                1.       Lack of Budget.................................................................................121
                2.       Displaying Typhoon Haiyan Data on DisasterAWARE...............121

                3.       Integration       with      Other      Humanitarian          Assistance
                         Organizations...................................................................................122
                4.       ODK Form Development Limitations............................................122
                5.       Satellite Communication.................................................................122
                6.       Local Language................................................................................123
                7.       Crowdsourcing.................................................................................123

V.      FOR FURTHER RESEARCH ...............................................................................
                1.       Integration      with     Humanitarian        Assistance      Response
                         Community.......................................................................................126
                         a.      Hyper Text Markup Language 5..........................................126

                         b.      Data Base Development........................................................126
                                                x
                    c.    Humanitarian Disaster Exchange Compliance...................127
             2.     Pacific Disaster Center Integration................................................127
                    a.    Data Exchange......................................................................127
                    b.    DisasterAWARE View...........................................................127

                    c.    RTAT Assessment Reports....................................................129
APPENDIX A.         COMMAND       & CONTROL        CASE    STUDY OF THE
       RESPONSE TO HURRICANE KATRINA..........................................................131

APPENDIX B.         RAPID     INFORMATION         AND     COMMUNICATION
       TECHNOLOGY ENTERPRISE ARCHITECTURE ASSESSMENT...............145

APPENDIX C.         RTAT SAVVION BUSINESS PROCESS MODELING .............161
APPENDIX D.         TERRESTRIAL SYSTEMS FORM..............................................175

APPENDIX E.         CELLULAR WIRELESS FORM..................................................177

APPENDIX F.         SATELLITE SYSTEMS FORM....................................................181
APPENDIX G.         RADIO WITH POWER FORM.....................................................187

APPENDIX H.         RTAT V6 TRAINING POWERPOINT ........................................203

APPENDIX I.         RTAT QUICK LOOK REPUBLIC OF THE PHILIPPINES
       SEPTEMBER 2013 V2..................................................................................241
APPENDIX J. TYPHOON HAIYAN AFTER ACTION REPORTS.............................257

APPENDIX K. JOINT INTER-AGENCY FIELD EXERCISE 2014–04 AFTER
       ACTION REPORT..................................................................................................275

LIST OF REFERENCES...........................................................................................289

WORKS CONSULTED......................................................................................................297
INITIAL DISTRIBUTION      LIST.......................................................................................301




















                                      xi
THIS PAGE INTENTIONALLY LEFT BLANK





























                     xii
                                   LIST OF FIGURES



Figure 1.       Process from Raw Data to Situational Understanding (after Headquarters,
                U.S. Marine Corps, 2002)..................................................................................6
Figure 2.       Plato’s Knowledge is a Subset of What is Both True and Believed..................8
Figure 3.       Interagency Coordination for Foreign Humanitarian Assistance (from
                DOD, 2011)......................................................................................................17
Figure 4.       IASC Members and Standing Invitees (from Inter-Agency Standing

                Committee, 2011a)...........................................................................................22
Figure 5.       Inter-Agency Standing Cluster (from Inter-Agency Standing Committee
                2012b)..............................................................................................................23
Figure 6.       Four Stages of Enterprise Architecture Maturity (from Ross et al., 2006, p.
                72)....................................................................................................................45
Figure 7.       Four Operating Models with Description (from Ross et al., 2006, p. 39).......     46

Figure 8.       Shift from Diversification to Replication Operating Model (after Ross et
                al., 2006, p. 39) ...........................................................................47
Figure 9.       Savvion RTAT Old “As Is” Business Process (from Beeson, Gladem, &
                Gonzalez, 2014)...............................................................................................49
Figure 10.      Hastily Formed Network (HFN) Puzzle Pieces (from Steckler, 2009) ...........53
Figure 11.      Mobile Device Operating System Market Share (from Reed, 2014)...............      55
Figure 12.      Mobile Phone Data Collection Methods (from Patnaik et al., 2013)...............57

Figure 13.      CyberTracker in use (from CyberTracker, 2014)............................................64
Figure 14.      Humanitarian Data Toolkit (Humanitarian Data Toolkit, 2014).....................65
Figure 15.      RTAT Recommended Change in Operating Model (after Ross et al.,
                2006, p. 29).............................................................................................74
Figure 16.      Spiral Development Diagram (from Osmundson, 2014).................................76
Figure 17.      RTAT Assessment Status Option Screenshot..................................................
Figure 18.      Lighthouse ICT Services Being Assessed at a Location .................................86

Figure 19.      Terrestrial Sub-Services...................................................................................88
Figure 20.      Screenshot of DisasterAWARE with RTAT Data (from Pacific Disaster
                Center, n.d.)......................................................................................................89
Figure 21.      Legazpi City Hot Wash Feedback Session (from Chang, 2013).....................    93
Figure 22.      ODK Aggregate Map Visualization for Surveys Conducted September 24,
                2013 (from Chang, 2013).................................................................................95

Figure 23.      Data clutter in ODK Aggregate Map View .....................................................96
Figure 24.      RTAT “As Is” Process Model (from Beeson et al., 2014)...............................99
Figure 25.      Savvion RTAT Process Changes (after Beeson et al., 2014) ..........................99
Figure 26.      Savvion “To Be” Process Model (from Beeson et al., 2014) ........................101
Figure 27.      Savvion As Is-To Be Comparison (from Beeson et al., 2014)......................102
Figure 28.      Broken Broadcast Tower Assessed by RTAT in the Aftermath of Typhoon
                Haiyan............................................................................................................

Figure 29.      USAID Typhoon Haiyan Effected Area (from USAID 2014b) ....................104
Figure 30.      Author Conducting RTAT Training in Cebu City.........................................106
Figure 31.      First Wave Confers with Local Officials.................................................106

                                               xiii
Figure 32.      RTAT Conducting an Assessment in Tacloban City.....................................107
Figure 33.      Typhoon Haiyan Mission ODK Aggregate Results.......................................108
Figure 34.      DisasterAWARE Screenshot with RTAT Data.............................................110
Figure 35.      UN-ETC Current Operational Picture (from WFP, 2013c)...........................111

Figure 36.      RTAT Member at a Charging Station in Tacloban City (From Appendix
                K)...................................................................................................................112
Figure 37.      Two Cisco Rapid Response Kits (from Bharania, 2014)...............................114
Figure 38.      Goal Zero Yeti 1250 (from Goal Zero, n.d.)..................................................115
Figure 39.      Sample of JIFX 14–4 RTAT Assessments ....................................................115
Figure 40.      Antennae Hill Assessment Photos, Author is Shown Testing the Goal Zero
                and Rapid Response Kit (from Appendix L).................................................116

Figure 41.      UN-ETC Typhoon Haiyan Current Operational Picture November 27,
                2014 (from WFP, 2013c)...............................................................................125
Figure 42.      Current DisasterAWARE view......................................................................128
Figure 43.      Potential DisasterAWARE view (after B. King, personal communication,
                April 22, 2014)...............................................................................................128
Figure 44.      Proposed    RTAT     Assessment     Report    (after   B.   King,   personal
                communication, April 22, 2014)....................................................................

































                                             xiv
                               LIST OF TABLES



Table 1.      IASC    Phase   Assessment    Reports  (from   Inter-Agency   Standing
              Committee, 2011b) ..........................................................................................37
Table 2.      World Food Programme Communication Requirements by Phase (from
              WFP, 2002, pp. 282–283)................................................................................44
Table 3.      Service and Sub-Service Table (after Appendix D-G)....................................81











































                                         xv
THIS PAGE INTENTIONALLY LEFT BLANK





























                   xvi
           LIST OF ACRONYMS AND ABBREVIATIONS



APAN             All Partners Network Access
ASEAN            Association of Southeast Asian Nations

BU               Bicol University

C2               command and control
CMMI             capability maturity model integration

COD              common operational dataset

COM              chief of mission
COP              common operational picture

DART             Disaster Assistance Response Team

DCHA             U.S. Bureau for Democracy, Conflict, and Humanitarian
                 Assistance
DHS              Department of Homeland Security

DOD              Department of Defense

DOS              Department of State
DRC              disaster relief coordinator

EPIC             Emergency Preparedness Integration Centre

ERC              emergency relief coordinator
FEMA             Federal Emergency Managementgency

FHA              foreign humanitarian assistance

GA               General Assembly
GPS              global positioning system

HA               humanitarian assistance

HADR             humanitarian assistance disaster relief
HC               humanitarian coordinator

HE               humanitarian event
HF               high frequency

IASC             Inter-Agency Standing Committee

ICRC             International Committee of the Red Cross
ICT              Information Communicationd Telecommunication

JP               joint publication

                                 xvii
KML              Keyhole Mark-up Language

MIRA             multi-cluster/sector initial rapid assessment
MySQL            My Structured Query Language

NATF             Needs Assessment Task Force

NGO              Nongovernmental organizations
NOMAD            HumanitariaN Operations Mobile Acquisition Of Data

NRF              National Response Framework
NRP              National Response Plan

NSC              National Security Council

OCHA             Office for the Coordination of Humanitarian Affairs
ODK              Open Data Kit

OFDA             Office of U.S. Foreign Disaster Assistance

OSA              Online Selection Assistance
PCC              Policy Coordination Council

PDC              Pacific Disaster Center

ROP              Republic of the Philippines
RC               regional coordinator

RTAT             Rapid Information Communication Telecommunication (ICT)
                 Assessment Team

SMS              short messaging service
SQL              Structured Query Language

TSF              Télécoms Sans Frontières

UN               United Nations
UNDAC            United Nations Disaster Assessment and Coordination

UNCT             United Nations country team

UN-ETC           United Nations Emergency Telecommunication Cluster
UHF              ultra-high frequency

VHF              very high frequency

WFP              World Food Programme






                                 xviii
                            ACKNOWLEDGMENTS



       God. For all that I have and all that I am.

       Wife—my great confidante and woman of Proverbs 31. You are a true woman of
worth. Without your support this project would never have left my infamous whiteboard

of ideas.

       Mom—Whose love gave me the confidence to dare greatly.


       Dad—You taught me what’s actually important in life and what it takes to
succeed as a man. You taught me patience and perseverance both of which were tested

during this thesis.

       NPS HFN Center—gave me the tools, motivation, and funding to get involved.

       La’arni—You are my soul sister who organizes chaos into managed pieces. May

God bless you and all that you do.

       Bicol University—The local knowledge assistance during testing and Typhoon

Haiyan was invaluable. Sir Boy, Mr. Jim, and Leilani to name but a few, you are all

inspirations.

       Roddenberry Foundation—Eric, Alex and Nico… I’d lock shields and march with

you to the gates of fire any time. James Tiberius Kirk would be proud.

       Team Patola—You are the secret ingredient that makes the dish!

       Brian—you opened my eyes to the world of HADR and your tireless efforts save

lives.

       Bryan—You are the epistemological brains and organization behind the thesis, I’d
never have gotten the thoughts out of my head and on paper without your help!


       Coach Miller, Coach Hetrick and Coach Roll—You taught me that it was ok to be
a Christian leader, the Lord’s Prayer, and literally taught me the word “adversity.” You

taught me discipline and instilled in me leadership skills. Your trust, faith and confidence

in me through my rough spots greatly impacted my life and has been subsequently

                                          xix
promulgated through every Marine and person I meet. Just as “iron sharpens iron.” it

takes a group of men to make a man.

       People of the Philippines—Your unyielding faith in God, each other and

resiliency as a people, despite the constant threat of disaster, is humbling and awe
inspiring in every aspect. I pray the Lord bless you for your continued faith.













































                                          xx
                             I.     INTRODUCTION



       Cataclysmic events such as the Indonesian tsunami in 2004 and hurricane Katrina
in 2005 leave a wide and devastating wake of destruction. The horrific human suffering

in these events is exacerbated by the inability of governmental and relief organizations to

operate in a chaotic environment (Donahue & Tuohy, 2006). Specifically, their inability
to collectively assess the situation, prioritize efforts, and effectively allocate/direct scarce

resources (Donahue & Tuohy, 2006). Office for the Coordination of Humanitarian

Affairs (Office for the Coordination of Humanitarian Affairs) (2014a) stated that
unfortunately,


        The humanitarian community’s existing data-sharing practices usually involve
one-to-one exchange of non-standardized spreadsheets or individual figures over email at

irregular intervals. These ad-hoc methods can cause a significant delay between the

collection of data and the formulation of that data into a common operational picture. In
the worst case, information is simply not shared at all, leaving gaps in the understanding

of the field situation.

       This lack of shared information is due, in large part, to an ineffective or missing

overarching collaboration organization and further compounded by severely damaged

host nation communication infrastructure (Donahue & Tuohy, 2006; and Steckler, 2009).
Most organizations “don’t know what they don’t know” when they arrive and, as a result,

incorrectly equip themselves for the information and communication technology (ICT)

environment (Steckler, 2009). Further, with the commoditization of smart devices and

sensors, every organization is haphazardly collecting and sharing raw data in an
unstructured manner. Kennerly & Mason (2008) described a growing concern that

organizations are now “drowning in data, whilst thirsting for information.”

       Several organizations assess various aspects of the ICT infrastructure, but none

collate the information into an ICT current operational picture or a complete

understanding for decision making (Steckler, 2009).To fill the gaps and help make sense
of the data noise, the Rapid ICT Assessment Team (RTAT) was created to conduct a


                                           1
holistic assessment of the ICT environment and share this information in a coherent

manner with other responding organizations. The problem of efficiently and effectively
collecting the data, creating an ICT common operational picture and getting this

information into the right hands, however, has yet to be solved.


A.     PROBLEM STATEMENT

       Understanding    the  information   and   communication    technology   (ICT)

environment in a post disaster environment is difficult. Compounding this problem is the
potential lack of host nation ICT infrastructure that could facilitate information

collaboration between relief organizations. The Rapid ICT Assessment Team (RTAT)

program was created to assess the ICT environment and give ICT prioritization

recommendations. The current assessment form that uses Microsoft Excel spreadsheet,
however, is not optimal for sharing ICT team findings in an efficient and expeditious

manner to the rest of the humanitarian assistance (HA) response community (HARC).

       Without an adequate assessment platform, RTATs will not be able to efficiently

or effectively communicate the current disaster area ICT situation. As a result, HA

response organizations may not have the required ICT tools and supporting infrastructure
to respond adequately to the situation. Worse, they will not know what to bring to the

disaster area to facilitate required collaboration and communication between the affected

population, host nation officials and responders, international HA relief organizations,

and US entities (DOD/DOS) further exasperating the dynamic and difficult problem of
large scale disaster response.


       This research will explore the use of mobile software applications on established
existing networks, wireless meshed networks (WMNs), and exercise/real world hastily

formed networks (HFNs) by RTAT members in an effort to bridge the gap and provide a

more useful optimal ICT assessment tool for end users.

B.     RESEARCH QUESTIONS


       1.     What are the limitations of the current Microsoft Excel based assessment
              form?


                                          2
       2.     What are the costs and benefits of moving RTAT forms from a laptop
              based Microsoft Excel to a mobile data collection tool?

       3.     What are the best mobile data collection tool/ electronic form interface
              options available to RTAT?

C.     PURPOSE STATEMENT

       The purpose of this analysis is to research and create a mobile data collection tool

and assessment form that can be used by the Rapid Information and Communication

Technology (ICT) Assessment Teams (RTATs) in the field. This research/project will
result in a more efficient and effective working mobile data collection platform that will

have significantly greater capability than the current Microsoft (MS) Excel spreadsheet

model. This improvement includes a working mobile spatial-data collection tool and

backend aggregate server with links to an online collaboration website.

       This research can be transferred to any DOD entity that has a need to transmit
standardized reports from an off-the-grid, austere environment and allow those reports to

be importable into a useful database that feeds a current operational picture (COP)

website to enable a cohesive overview and, thereby, better decisions.


D.     CONTRIBUTION

       This project contributes to the understanding of how organizations make sense of

chaotic environments by exploring possible improvements to rapidly collect and share
accurate information to develop a current operational picture. This current operational

picture leads to a shared understanding among participants and improved collaborative

decisions.

       Without an adequate assessment platform, RTATs will not be able to efficiently,

or effectively, communicate the current ICT situation in the disaster area. In turn,
Humanitarian Assistance Disaster Response (HADR) organizations may not have the

expected ICT tools and supporting infrastructure to adequately respond to the situation.

Worse, they will not know what to bring to the disaster area to facilitate required

collaboration and communication between the affected population, host nation officials



                                          3
and responders, international HA relief organizations and U.S. entities (DOD/DOS)

further exasperating the dynamic and difficult problem of large scale disaster response.


















































                                         4
                       II.    LITERATURE REVIEW



       The major topics for discussion in the literature review include: A. Information,
B. U.S. Disaster Response, C. UN Disaster Response, D. Other Information and

Communication Technology Non-Governmental Organizations, E. Rapid Information

and Communication Technology (ICT) Assessment Team (RTAT), F. Categories of Post
Disaster Assessment Data Sets, G. Current Post Disaster Assessment Forms, H.

Enterprise Architecture Analysis, I. Savvion Business Process Management, J. Hastily

Formed Network, K. Mobile Device Operating Systems, L. Mobile Data Collection
Methods, and M. Available Mobile Electronic Form Interfaces.


A.     INFORMATION

       DOD (2006) stated, “There are two basic uses for information. The first is to help

create situational awareness (SA) as the basis for a decision. The second is to direct and

coordinate actions in the execution of the decision.”

       Kennerly & Mason (2008) reiterated this view stating the purpose of information

is to make better decisions further positing that, “research evidence suggesting that better
use of information can improve decision making.” Unfortunately, Davenport, Harris, De

Long & Jacobson (2000) brought to light that “one of the most enduring traits of the

information age is that we have focused too much on mastering transaction data and not
enough on turning it into information and knowledge that can lead to business results.”

Reaffirming this, Chopoorian, Witherell, Khalil and Ahmed (2001) found that

“businesses currently analyze less than 7 percent of the data that they collect.” Therefore,
a focus on data collection is not enough; information must be processed into an

actionable or usable form, and this process must add value to the organization within its

market for it to thrive (Kennerly & Mason, 2008). Figure 1 shows the process of

processing raw sensor data into actionable situational understanding for decisions.






                                         5
Figure 1.   Process from Raw Data to Situational Understanding (after Headquarters, U.S.
                                         Marine Corps, 2002)



         While the problem of collecting data is being solved through the proliferation of
 smart devices and sensors, the ability to turn this data into actionable information and

 situational awareness in a disaster zone is an on-going issue (Office for the Coordination

 of Humanitarian Affairs, 2014a). Asterisked within Figure 1, the Rapid Information and
 Communication Technology (ICT) Assessment Team (RTAT) seeks to interject at key

 points in the information process to enable and facilitate better decision makingSteckler,

 2009). RTAT processes the raw data reports by conducting assessments and collating and

 promulgating this processed knowledge, along with recommended courses of action, via
 a shared current operational picture (COP) (Steckler, 2009).


         1.     Raw Data to Understanding

         This section briefly outlines the process of taking raw data (Figure 1) and turning

 it into understandable data that adds value to HADR organizations. This thesis will use

 the collection of weather data to illustrate each stage.

         a.     Data


         Merriam Webster (2014) defined data as:



                                             6
       1: factual information (as measurements or statistics) used as a basis for
       reasoning, discussion, or calculation. 2: information output by a sensing
       device or organ that includes both useful and irrelevant or redundant
       information and must be processed to be meaningful. 3: information in

       numerical form that can be digitally transmitted or processed.

       Kennerly & Mason (2008) furthers the definition:

       The word data is the plural of Latin datum, past participle of dare, “to
       give,” hence “something given.” Thus in general, data consists of
       propositions that reflect reality. A large class of practically important
       propositions is measurements or observations of a variable. Such
       propositions may comprise numbers, words, or images.


       Raw data in this context is a reported ICT outage or the omission of electronic
reports in a disaster indicating an outage. Once raw data has been processed into a shared

understanding, humanitarian Assistance (HA) workers, assuming they have the ability to

communicate, can begin to deconflict and collaboratively organize and prioritize their

efforts.

       For illustration purposes, raw data are the numbers coming out of a sensor; in the
weather example this would be the temperature, humidity, barometric pressure, and

precipitation at a location at a specific time.


       b.      Processed Data (Information)

       Kennerly and Mason (2008) described information in the following terms:

       The way the word information is used can refer to both “facts” in
       themselves and the transmission of the facts. The double notions of

       information as both facts and communication are also inherent in one of
       the foundations of information theory: cybernetics introduced by Norbert
       Wiener (1948).

       Processed data would be the collection of weather data (such as temperature) for a

period.


       c.      Knowledge

       Knowledge communicated concerning some particular fact, subject or
       event; of which one is apprised or told; intelligence, news.


                                            7
        Information is the result of processing, manipulating and organizing data
        in a way that adds to the knowledge of the receiver. In other words, it is
        the context in which data is taken.

        There are many epistemology and cognitive definitions for “knowledge;” simply

put “knowledge is what is known” (Kennerly & Mason, 2008). Plato stated that

knowledge is a subset of what is true and what is believed, Figure 2 (Kennerly & Mason

2008). For the purposes of this thesis, knowledge will be will be t defined as a refined set
of facts that are pertinent to a narrow subset of the overall situation and can be acted upon

in and of themselves.















     Figure 2.    Plato’s Knowledge is a Subset of What is Both True and Believed


        Knowledge is organized into two categories: Explicit and tacit.


        (1)    Explicit knowledge

        Explicit knowledge is easily codified and shared with others (Dienes & Perner,
1999). How to use a particular tool or do a specific process within an organization should

be explicit knowledge.


        (2)    Tacit Knowledge

        Tacit knowledge can be described as “head knowledge,” that is knowledge that is

trapped in one’s head and not easily transferred to another (Headquarters, Department of

the Army, 2012). This knowledge is gained through years of experience and gives rise to
gut feelings and intuition. Tacit knowledge is knowledge that one may not know that they


                                             8
know it (Dienes & Perner, 1999). Further, tacit knowledge “representations merely reflect

the property of objects or events without predicating them of any particular entity”
(Dienes & Perner, 1999). Making it difficult for an organization to identify the existence

of tacit knowledge because the person who has it may not know they have it and it is

difficult to associate this knowledge to an entity/object to codify it explicitly (Dienes &

Perner, 1999). Unfortunately, a significant amount of knowledge for disaster response is
currently tacit and difficult to transfer between people (Donahue & Tuohy, 2006). Worse,

turnover and unpredictability of disaster location, i.e. rarely in the same local area twice,

leaves much of the tacit knowledge gained and “lessons learned” to be relearned by the
next group of responders at the next event (Donahue & Tuohy, 2006). Tacit knowledge

examples may include cultural awareness, financial market understanding, and expert

salesmanship.

        Using the weather construct, knowing the average rainfall for an area or the fact

that it is raining is tacit knowledge. Knowing the streets sometimes flood in Manila when

it rains is tacit, until someone misses their flight or thinks to share the information.

        d.     Situational Understanding

        Situational understanding combines both types of knowledge (tacit and explicit)

to develop a complete mental picture of the situation allowing the development of a plan

furthering decision making (Headquarters, U.S. Marine Corps, 2002).

        Knowing to leave three hours early to get to the airport because the streets often

flood, for example, shows situational understanding on a micro scale.


        2.     Relational Database

        In order to organize, retain, and share explicit information, companies often look
to information technology solutions. One such common solution is the use of relational

databases.

        According to Oracle, a leading database company,


        A database is a means of storing information in such a way that
        information can be retrieved from it. In simplest terms, a relational

                                             9
        database is one that presents information in tables with rows and columns.
        A table is referred to as a relation in the sense that it is a collection of
        objects of the same type (rows). Data in a table can be related according to
        common keys or concepts, and the ability to retrieve related data from a

        table is the basis for the term relational database. (Oracle, 2014)

        Telvent (2014) stated, “A database that contains only one table is called a flat
database.” Excel is an example of a flat database. To realize the true power of a relational

database, however, one must learn how to tie multiple tables together to represent

potentially complex relationships between the items stored therein (Yank, 2009, p. 71).


        3.     Spatial Data

        A world leading mapping non-governmental organization (NGO), MapAction
(2011), stated:


        Spatial data is any data that has a “where” component that can be recorded
        and mapped. Attributes can be any data about the specified place. So, by
        adding the coordinate data to an existing data set, you have created a
        spatial database—data that can be mapped.

        Spatial-relational databases can be accessed and queried via entity attributes or

location, as well as posted to a responder shared current operational picture map.


        4.     Data Collection

        The United Nations (UN) Office for the Coordination of Humanitarian Affairs

(Office for the Coordination of Humanitarian Affairs) (2013b) defined data collection “as
the ongoing systematic collection of data (quantitative and/or qualitative) necessary for

identifying and prioritizing needs for disaster relief assistance.”

        Jung (2011) defined mobile data collection (MDC) as “the targeted gathering of

structured information using devices such as smartphones, PDAs, or tablets.”


        5.     Microsoft Excel

        Currently, the Rapid Information and Communication Technology (ICT)

Assessment Team (RTAT) utilizes a Microsoft (MS) Excel spreadsheet to conduct its



                                            10
assessment. To understand why they are seeking an alternative data collection method,

one must understand the capabilities and limitations of their current MS Excel based
solution.


       Microsoft’s (MS) Excel spreadsheet is prevalent throughout the HA response
community and is widely adopted to track HA efforts (Office for the Coordination of

Humanitarian Affairs, 2014a). Additional benefits of Excel include: wide adoption and

use, pivot table functions to perform complex analysis, solver functions for optimization
problems, auto fill features based on previous input, data visualization with various charts

and graphs, exportability as a text file (tab or comma separated), Extensible Markup

Language (XML) data and comma separated value (CSV) ((Microsoft [MS], 2014).

       Unfortunately Excel has some serious drawbacks: has no skip logic, i.e. it can’t

walk a user through an assessment, requires MS Excel compatible program, and is

limited on the number of fields. Additionally, a limitation exists as to the size and number
of relations, views and their intermediate results, imposed by the maximal available

number of worksheets, columns and rows in the spreadsheet system if one attempts to

create a stand-alone Excel database (Tyszkiewicz, 2010). Finally, “the size of the data

values (integers, strings, etc.) is also limited. The variety of data types in spreadsheets is
also restricted when compared to database systems” (Tyszkiewicz, 2010).


B.     U.S. DISASTER RESPONSE

       Between 1980 and 2010, 640 disasters have occurred within the U.S. which

accrued a staggering $18 trillion cost from damages and the loss of over 12,000 lives

(PreventionWeb, n.d.). This section outlines how local, state, territorial, regional and
federal governmental entities respond to disasters within the U.S. and its territories, with

primary focus on the U.S. federal government.

       The first analysis focuses on the U.S. federal government’s response to disasters.








                                           11
       1.     U.S. Federal Government Response to Disasters Within the U.S. and
              its Territories

       The outdated United States (U.S.) National Response Plan (NRP), resulting from

Presidential Policy Directive Number 5 (PPD-5) in 2004, recognizes that planning,

preparing for, and responding to natural and other disasters are primarily responsibilities
of the individual states. This reflects the U.S. Constitutional perspective and results in a

pull response assumption. Local authorities have the lead at the start, escalating to state

level and then to federal level, if necessary and if requested, in the event of a disaster

(Moffat, 2008).

       The Stafford Act outlines the process by which state governors request this

assistance from the federal government when the event becomes one of national
significance. The President of the United States (POTUS) then has to decide

whether the event of national significance merits designation as an emergency

(releasing limited resources to the states), a major disaster (releasing much greater
resources to the states) or a catastrophe. The first two of these result in a pull

response; the states requesting and drawing down (pulling) from these federal

resources as they see the event unfolding. The third category of catastrophe would have

resulted in a proactive push of resources to the region, states and local level,
irrespective of the states’ requests (Moffat, 2008). The Stafford Act attempts to

organize and capture all federal costs associated with the significant event. Its processes,

however, can be cumbersome, slow and ill-suited to a dynamic situation where a rapid
response, vice monetary accountability, is the gauge of success (Cannon, Beeson,

Mitchell, Spencer, & Liguori, 2012).

       Under the NRP, a comprehensive framework of response to significant event is

established. At the federal level, the Homeland Security Operations Centre, the Federal

Emergency Management Agency (FEMA),             National Response Centre and the

Interagency Incident Management Group jointly coordinate the response across
government departments. The federal coordinating officer (FCO), a representative of the

Secretary for Homeland Security, is authorized to lead a joint field office (JFO). This is a

temporary federal facility established locally at the time of a disaster to coordinate the

                                          12
local, state, and federal response. It consists of senior representatives from all agencies

and responders involved and development of objectives, strategies, plans, and priorities
(Moffat, 2008). The membership of this office is envisaged as growing and adapting over

time as the incident escalates or diminishes (Moffat, 2008).

       In summary, at pre-hurricane Katrina landfall, the NRP and Stafford Act

delineated that states have the lead in handling natural disasters within their state and,

with the exception of “catastrophic events,” were required to request assistance from the
federal government as necessary. FEMA is the lead federal command and control (C2)

agency for handling “nationally significant” events (Meeds, 2006). Unfortunately as

shown in Katrina, the entire system was set up in a strict, regimented, hierarchical

system, which involved local, state, regional and federal entities respectively and was
shown to be ill-suited and deficient for the dynamic task at hand (Cannon et al, 2012).


       In response to the shortfalls experienced during hurricane Katrina, President
Barack Obama signed Presidential Policy Directive 8 (PPD-8): The National

Preparedness System (NPS) in 2011 (U.S. Department of Homeland Security, 2011).

PPD-8 outlines the U.S. current approach to national preparedness (U.S. Department of

Homeland Security, 2011). Hence, National Response Program (NRP) was changed to
the National Response Framework (NRF) to reflect this change. The Department of

Homeland Security (2013) further elaborated on this change:

       The National Response Framework (NRF) is an essential component of
       the National Preparedness: System mandated. PPD-8 is aimed at
       strengthening the security and resilience of the United States through

       systematic preparation for the threats that pose the greatest risk to the
       security of the Nation. PPD-8 defines five mission areas—Prevention,
       Protection, Mitigation, Response, and Recovery—and mandates the
       development of a series of policy and planning documents to explain and
       guide the Nation’s collective approach to ensuring and enhancing national
       preparedness. The NRF sets the doctrine for how the Nation builds,

       sustains, and delivers the response core capabilities identified in the
       National Preparedness Goal (the Goal). The Goal establishes the
       capabilities and outcomes the nation must accomplish across all five
       mission areas tobe secure and resilient.

       This thesis will concentrate on the response mission.


                                          13
       Added to the federal response, the U.S. also has an established response to

international disasters which is comprised of three parts: the U.S. Department of State
response, the U.S. Department of Defense response, and the Interagency Coordination for

Foreign Humanitarian Assistance. These will be examined in the following section.


       2.      U.S. Response to International Disasters

       The Department of State (DOS) leads the U.S. response for international disasters,

with USAID as its lead agency. Upon disaster striking, the ambassador or the

       Chief of Mission (COM) may send a disaster declaration cable outlining
       the extent of the damage, possible needs, and may recommend assistance
       in the form of funding, material, or technical assistance. When the
       President, Secretary of Defense (SecDef), and the Secretary of State have

       determined that a U.S. humanitarian response to a foreign disaster or crisis
       is required, the National Security Council (NSC) normally directs the
       Special Coordinator for International Disaster Assistance to convene an
       International Development and Humanitarian Assistance NSC Policy
       Coordination Committee (PCC) to review all pertinent information and
       recommend policy and specific actions. The PCC; which consists of senior
       DOS and DOD representatives, the COM, USAID representatives, and

       heads    of   other  concerned    agencies;   concurrently   develops    a
       comprehensive strategy for emergency response and develops tasks for
       each key participant.(Department of Defense, 2014).

       a.      U.S. Department of State Response to Foreign Disasters

       As stated, the U.S. Department of State (DOS) is the lead department for the U.S.

response to foreign disasters (Department of Defense, 2011). The DOS has many

agencies that work in concert to achieve the strategic goals or disaster preparedness and
response. Two of these agencies are outlined below.


       (1)     Office of U.S.Foreign Disaster Assistance

       The Office of U.S. Foreign Disaster Assistance (OFDA) is the lead office of

USAID for immediate disaster relief. OFDA lies within the USAID Bureau for

Democracy, Conflict, and Humanitarian Assistance (DCHA) (Department of Defense,
2011). OFDA is delegated the responsibility to “provide international disaster and

humanitarian assistance and coordinate the USG response to declared disasters in foreign


                                           14
countries” (Department of Defense, 2011). OFDA gets its authority from the Foreign

Assistance Act of 1961, as amended, § 491-493 and from delegated Presidential
Authority. In the performance of such tasks, OFDA “maintains stocks of emergency

relief supplies in warehouses worldwide and has the logistical and operational

capabilities to deliver them quickly” (Department of Defense, 2011).


       (2)    Disaster Assistance Response Team

       OFDA has a responsibility to respond to disasters quickly and it uses to the
Disaster Assistance Response Team (DART) to quickly assess the situation. DOD (2011)

aptly summed DART,

       When disaster strikes, OFDA sends regional and technical experts to the

       affected country to identify and prioritize humanitarian needs. In the wake
       of a large-scale disaster, OFDA can deploy a Disaster Assistance
       Response Team (DART) to coordinate and manage an optimal U.S.
       Governmen   t response, while working closely with local officials, the
       international community, and relief agencies.

       DART teams are the first responder eyes and ears of the OFDA.

       The second tier of U.S. international disaster response is the Department of

Defense.


       b.     U.S. Department of Defense Response to International Disasters

       The U.S. government (USG) responds to “approximately 70–     80 natural disasters

across the globe each year. In approximately 10–15 percent of these disaster responses,

the Department of Defense (DOD) lends support to the overall U.S. effort” (Department
of Defense, 2011). In these instances, DOD acts in support to the Department of State

(DOS) in concert with USAID efforts and in close coordination with the effected country

and the international humanitarian assistance (HA) response community organizations
(HARC) (Department of Defense, 2011). These HARC entities include other donor

countries  and    participating international  organizations   and   non-governmental

organizations (NGOS), key NGOs will be discussed in greater detail later. The U.S. DOD

role in foreign disasters is governed by the Foreign Assistance Act of 1961 under U.S.
Code Title 22. DOD Directive 5100.46, Foreign Disaster Relief, “establishes policy and

                                          15
provides for component participation in foreign disaster relief operations only after a

determination is made by DOS that foreign disaster relief shall be provided” (Department
of Defense, 2014). U.S. DOD Joint Publication (JP) JP 3–29 calls these types of

activities—Foreign Humanitarian Assistance (FHA) operations (Department of Defense,

2011). According to the DOD (2014),

       FHA consists of DOD activities, normally in support of the United States
       Agency for International Development (USAID) or the Department of
       State (DOS), conducted outside the United States, its territories, and
       possessions to relieve or reduce human suffering, disease, hunger, or

       privation. While, U.S. military forces are not the primary U.S.
       Government (USG) means of providing FHA, the foreign assistance they
       are tasked to provide is designed to supplement or complement the efforts
       of the host nation (HN).

       Typical DOD FHA operation missions include:

              Relief missions: Missions that include prompt aid that can be used to
               alleviate the suffering of disaster victimspartment of Defense, 2014).

              Dislocated civilian support missions: Provide assistance and protection for
               dislocated civiliansDepartment of Defense, 2014).

              Security missions: “Establish and maintain conditions for the provision of
               FHA by organizations of the world relief community to include secure
               areas for storage of relief material, provide protection and armed escorts
               for convoys and personnel delivering emergency aid, protection of shelters
               for dislocated civilians, and security for multinational forces” (Department

               of Defense, 2014).
              Technical assistance and support functions: “Advice and selected training,
               assessments, manpower, and equipment” (Department of Defense, 2014).

              Foreign consequence management (FCM): DOD assistance to a “HN to
               mitigate the effects of a deliberate or inadvertent chemical, biological,
               radiological, nuclear, and high-yield explosives attack or event and to

               restor essential government services” (Department of Defense, 2014).
       Coordination between the numerous FHA responding agencies can be

overwhelming for new entrants. The third and final tier of U.S. international disaster

response is the Interagency Coordination for Foreign Humanitarian Assistance; the
diagram shown in the next section is helpful and outlines the organizational relationships

between the U.S. DOS and U.S. DOD agencies.



                                           16
       c.     The Interagency Coordination for Foreign Humanitarian Assistance

       Under the President of the United States lay the various departments and agencies

shown in Figure 3. During a disaster response, the affected country’s Chief of Mission is

the focal point of the U.S. effort. The Chief of Mission falls under the Department of
State but has direct contact with the President of the United States. One should note that

while lines of coordination to the Chief of Mission exist from the responding DOD Joint

Task Force commander and the United States Agency for International Development
(USAID), those organizations do not work for the Chief of Mission but for their parent

organization, a minor detail that can lead to major consequences.


























Figure 3.   Interagency Coordination for Foreign Humanitarian Assistance (from DOD,
                                              2011)


C.     UNITED NATIONS DISASTER RESPONSE


       The United States is but one player in a much larger humanitarian assistance (HA)
response community (HARC). The United Nations (UN) has taken on a large role for

coordinating international humanitarian event response and relief efforts. This section

                                          17
outlines how the UN responds to large scale disasters and is comprised of four sections:

UN Humanitarian events, UN resident coordinator, United Nations Emergency Relief
Coordinator, United Nations Office for the Coordination of Humanitarian Affairs, and

UN Emergency Telecommunication Cluster.


       1.      United Nations Humanitarian Events

       UN characterizes its responses to human suffering and disasters as humanitarian

events (Office for the Coordination of Humanitarian Affairs, 2013b). Humanitarian
events are characterized into two broad categories of natural disaster and complex

emergency.


       a.      Natural Disaster

       Natural disaster occurs when a “disaster-affected country requests international

assistance in coping with a natural disaster and requires additional international
coordination resources” (Office for the Coordination of Humanitarian Affairs, 2013b).


       b.      Complex Emergency

       Complex emergency is defined as “a humanitarian crisis in a country, region or

society where there is total or considerable breakdown of authority resulting from internal

or external conflict and which requires an international response that goes beyond the

mandate or capacity of any single agency and/or the ongoing United Nations’ country
program” (Department of Defense, 2011). DOD (2011) further described complex

emergencies as involving:

              Extensive violence or loss of life.
              Massive displacements of people.

              Widespread damage to societies and economies.

              The need for large-scale, multi-faceted humanitarian assistance.

              Hindrance or prevention of humanitarian assistance by political and
               military constraints.

              Security risks for humanitarian relief workers in some areas.



                                           18
       c.     United Nations Level of Crises.

       The UN further categorizes humanitarian events by level of crisis.

             Level 1 (L1) Emergency is “an emergency where the national and
              international resources in-country can handle the response and no outside
              assistance is needed” (Office for the Coordination of Humanitarian
              Affairs, 2013b).

             Level 2 (L2) Emergencies require “some support from neighboring
              countries, regional entities and possibly agency headquarters will be
              needed” (Office for the Coordination of Humanitarian Affairs, 2013b).

             Level 3 (L3) Emergency is “a major sudden-onset humanitarian crisis
              triggered by natural disasters or conflict that requires (UN) system-wide
              mobilization” (Office for the Coordination of Humanitarian Affairs,
              2013b).

       The affected country’s resident coordinator or designated humanitarian

coordinator (HC), with the permission of the effected nation and the help of a

humanitarian Country Team (HCT), will request event tailored support from the UN
Emergency Relief Coordinator (ERC) via the UN Regional Coordinator (RC) (Office for

the Coordination of Humanitarian Affairs, 2013b). Normally requests are reserved for L3

Emergencies.


       2.     UN Resident Coordinator

        The UN Office for the Coordination of Humanitarian Affairs (Office for the
Coordination of Humanitarian Affairs) stated (2011),


       The overall coordination of United Nations activities falls primarily to the
       United Nations’ (effected country’s) Resident Coordinator (RC) in
       consultation with relevant United Nations agencies. The position equals
       the same rank as an Ambassador of a foreign state and is the designated
       Representative of the Secretary-General. The RC also leads the United
       Nations Country Team (UNCT). Office for the Coordination of
       Humanitarian Affairs, 2013b)


       The RC is designated as or will designate a humanitarian coordinator (HC) to
oversee a humanitarian event (natural disaster or complex emergency) (Office for the

Coordination of Humanitarian Affairs, 2013b).




                                          19
       3.     United Nations Emergency Relief Coordinator

       The UN, in providing emergency assistance, is guided by General Assembly (GA)

resolution 46/182, Strengthening of the Coordination of Humanitarian Emergency

Assistance of the United Nations. The resolution, adopted 19 December 1991,
strengthened the then existing position of the Disaster Relief Coordinator (DRC) to

include both natural disasters and complex emergencies and renamed the position

Emergency Relief Coordinator (ERC) (Office for the Coordination of Humanitarian
Affairs, 2013). The ERC is an Undersecretary position within UN and reports directly to

the Secretary of the UN on matters of emergency response (Office for the Coordination

of Humanitarian Affairs, 2013b).


       4.     United Nations Office for the Coordination of Humanitarian Affairs

       ERC utilizes the UN Office for the Coordination of Humanitarian Affairs
(OCHA) to respond operationally to RC/HC and the affected nation’s requests. DOD

(2011) stated, “OCHA is the arm of the UN Secretariat that is responsible for bringing

together humanitarian response participants to ensure a coherent response to disasters.”

       In 1998, as part of the Secretary-General’s program of reform, Department
       of Humanitarian Affairs was reorganized into the Office for the
       Coordination of Humanitarian Affairs (OCHA). Its mandate was expanded

       to include the coordination of humanitarian response, policy development
       and humanitarian advocacy. (Office for the Coordination of Humanitarian
       Affairs, 2014b).

       a.     United Nations Disaster Assessment Coordination Team

       In 1993,   the UN OCHA created the United Nations Disaster Assessment

Coordination (UNDAC) organization to improve response to humanitarian events (Office

for the Coordination of Humanitarian Affairs, 2014b). UNDAC has the primary mission
to “help the United Nations and governments of disaster-affected countries during the

first phase of a sudden-onset emergency” (Office for the Coordination of Humanitarian

Affairs, 2014b). To clarify, the UNDAC lies within OCHA and has teams that can deploy

within 12–48 hours of disaster striking in order to help the RC/HC coordinate the initial



                                          20
disaster response (Office for the Coordination of Humanitarian Affairs, 2014b). The

UNDAC can be likened to the USAID DART teams.

       b.     Inter-Agency Standing Committee


       In an effort to delineate organizational responsibilities, coordinate international
NGOs, and improve the operational responses to humanitarian events, the UN OCHA

created the Inter-Agency Standing Committee (IASC) (Office for the Coordination of

Humanitarian Affairs, 2013a).

       OCHA carries out its coordination function primarily through the IASC,
       which is chaired by the ERC. Participants include all humanitarian
       partners, from United Nations agencies, funds and programs, to the Red
       Cross movement and NGOs. The IASC ensures inter-agency decision-

       making in response to complex emergencies. These responses include
       needs assessments, consolidated appeals, field coordination arrangements
       and the development of humanitarian policies. (Office for the
       Coordination of Humanitarian Affairs, 2013a)

       The IASC was established in June 1992 under the UN General Assembly (GA)

Resolution 46/182 and affirmed in GA resolution 48/57 as the “primary mechanism for

inter-agency   coordination  of   humanitarian   assistance”  (Inter-Agency   Standing
Committee, 2011a). OCHA (2011) stated,


       IASC brings together international organizations working to provide
       humanitarian assistance to people in need as a result of natural disasters,
       conflict-related emergencies, global food crises and pandemics.

       The IASC is made of both members and    standing invitees (see Figure 4).
















                                          21
    Figure 4.    IASC Members and Standing Invitees (from Inter-Agency Standing
                                        Committee, 2011a)


       The IASC is organized along functional lines called “clusters” (Office for the

Coordination of Humanitarian Affairs, 2013a). Following the recommendations of an
independent Humanitarian Response Review in 2005,


       The cluster approach was proposed as one way of addressing gaps and
       strengthening the effectiveness of humanitarian response through building
       partnerships. The cluster approach ensures clear leadership, predictability
       and accountability in international responses to humanitarian emergencies
       by clarifying the division of labor among organizations and better defining
       their roles and responsibilities within the different sectors of the response.
       It aims to make the international humanitarian community better

       organized and more accountable and professional, so that it can be a better
       partner for the affected people, host governments, local authorities, local
       civil society and resourcing partners. (Inter-Agency Standing Committee,
       2012b  )

       Each cluster is headed by a cluster lead agency, see Figure 5. Intra-cluster

information management (IM) is the responsibility of the Cluster Lead agency, inter     -

cluster IM is the responsibility of OCHA (Office for the Coordination of Humanitarian
Affairs, 2011). This thesis will be primarily concerned with the UN Emergency

Telecommunication Cluster (UN-ETC).

                                           22
  Figure 5.   Inter-Agency Standing Cluster (from Inter-Agency Standing Committee
                                            2012b)


       5.     UN Emergency Telecommunication Cluster

       The cluster lead agency for the UN-ETC is the World Food Programme (WFP)

(World Food Programme, 2013b). The WFP (2013b) stated, The UN “Emergency

Telecommunications Cluster (UN-ETC) provides humanitarian workers with the

communications services they need to operate effectively and efficiently, and to save
lives.” WFP (2013b) further stated, “The ETC is a network of organizations that work

together to provide shared communications services in humanitarian emergencies.”

       The UN-ETC provides vital security communications’ services and voice and

Internet connectivity to assist humanitarian workers in their life-saving operations (World

Food Programme, 2013b). The UN-ETC advertises that its first responders can deploy
within 48 hours of a disaster to provide basic emergency services with service expansion

for continued emergency relief within four weeks (World Food Programme, 2013b).



                                         23
Specifically, the ETC provides      humanitarian workers with      information  and

communication technology (ICT) services to meet three broad goals (World Food
Programme, 2013b):

             Enhance response and coordination among humanitarian organizations
              (World Food Programme, 2013b).

             Improve operational security environment for staff and assets (World
              Food Programme, 2013b).

             Facilitate decision making through timely access to critical information
              (World Food Programme, 2013b).

       In order to help meet the immediate ICT needs of responders (within 48 hours),
the UN-ETC utilizes the Fast Information Technology and Telecommunications

Emergency and Support Team (FITTEST) , comprised of two elements.


       a.     Fast Information Technology and Telecommunications Emergency and
              Support Team

       The Fast Information Technology and Telecommunications Emergency and

Support Team (FITTEST) establishes information and communication systems and

services where they have been disrupted by disasters (World Food Programme, 2013a).

       b.     Emergency Preparedness Integration Centre


       In order to create a simple and consistent ICT solution, the UN-ETC has created
Emergency Preparedness Integration Centre (EPIC) suite of information systems and

applications. WFP (2011) stated,

       EPIC is an inter-agency innovation program to support improved disaster
       preparedness and enable faster, more cohesive emergency response.
       Initiated and led by the World Food Programme (WFP), EPIC is being

       developed for the humanitarian community, by the humanitarian
       community.

       EPIC consists of the EPIC Portal information management platform, humanitarian

assistance specific EPIC Apps, EPIC Unified Comms that enable field communication,
EPIC Interaction that enables users to have the ability to interact with collected

information, and EPIC Situation Room that allows for collective interaction and the



                                        24
display of near real time information updates (World Food Programme, 2011). EPIC is

the UN-ETC attempt at a single ICT solution to manage data information flow.

D.     OTHER INFORMATION AND COMMUNICATION TECHNOLOGY

       NON-GOVERNMENTAL ORGANIZATIONS

       There are countless international organizations committed to reducing the human
suffering brought on by natural disasters and complex emergencies. Outlined below are

five of the key players (and their subsets) that have information and communication

technology (ICT) responsibilities pertinent to this thesis.


       1.     The International Federation of the Red Cross/Red Crescent

       The International Federation of the Red Cross/Red Crescent (IFRC) consists of
both the International Committee of Red Cross (Red Cross) and the International

Committee of Red Crescent (Red Crescent). Most Westerners are more familiar with the

Red Cross and most Middle Easterners are familiar with the Red Crescent. The

International Federation of Red Cross and Red Crescent Societies (IFRCS) are
headquartered in Geneva, Switzerland and “act as a secretariat for the national Red Cross

and Red Crescent Societies, and assist in disaster management and response.” The IFRC

supports national societies in disaster situations (Department of Defense, 2011). The

ICRC is a world-wide organization that was established in 1863 (Red Cross, n.d.). The
mission of the ICRC to “provide humanitarian help for people affected by conflict and

armed violence and to promote the laws that protect victims of war” (Red Cross, n.d.).

The Red Cross is an independent and neutral organization; its mandates stem from the
law of armed conflict Geneva conventions of 1949 (Red Cross, n.d.).


       a.     National Red Cross or Red Crescent Societies

       DOD (2011) stated, “The 186 recognized national Red Cross or Red Crescent

Societies are auxiliaries of their governments; national societies assist in both disasters

and conflict situations.” Members of these societies can be seen at refugee camps and
disaster relief centers.




                                         25
       b.     Red Cross/Red Crescent First Assessment and Coordination Team

       First Assessment and Coordination Team (FACT) “members have technical

expertise in relief, logistics, health, nutrition, public health and epidemiology,

psychological support, water and sanitation, finance and administration, as well as
language capabilities” (IFRC, 2014).

       FACT can deploy within 12–24 hours, for 2 to 4 weeks, this allows operations to

begin while longer-term support is mobilized (IFRC, 2014). FACT is similar to the

DART or FITTEST teams but without the same capability.


       2.     NetHope

       A NGO consortium, NetHope’s mission is to “act as a catalyst for collaboration,

bringing together the knowledge and power of 41 leading international humanitarian
organizations so that the best information communication technology and practices can

be used to serve people in the developing world” (NetHope, 2014a). NetHope enables

standard ICT and help desk capabilities as well as economies of scale through its Shared
Services program to the aforementioned humanitarian organizations under six

fundamental values (NetHope, 2014a):

             Technology matters.

             Benefiting all benefits one.
             Learning through collaboration.

             Build for the field.

             Bias for action.

             Trust above all else.
       NetHope works with the UN-ETC to collaborate and provide ICT services for its

member organizations (NetHope, 2014a). NetHope works with its member NGO’s in a

manner similar to how World Food Programme (UN-ETC lead agency) provides ICT
services for the other UN cluster agencies. Additionally, NetHope has its own

“FITTEST” team that immediately responds to disasters to establish ICT services for its

member organizations known as the Emergency Response Working Group (ERWG)

(NetHope, 2014b).

                                          26
       3.     TÉLÉCOMS SANS FRONTIÈRES

       Telecoms Sans Frontieres (TSF) was formed in the crucible of both the Balkans

crisis and in Kurdistan in the aftermath of the first Gulf War in the 1990’s (Telecoms

Sans Frontieres, 2013). TSF’s mission is to provide information and communication
technology (ICT) support to both emergency humanitarian events (HE) (natural disaster

and complex emergency) and non-emergency support to affected people and HE

responders. Non-emergency support comes in the form of general ICT support to the
United Nations (UN) and more specifically telecoms assessments support to the UN

Disaster Assessment and Coordination (UNDAC) (Telecoms Sans Frontieres, 2013). TSF

was named a “first responder” of the Emergency Telecoms Cluster in 2006 (Telecoms

Sans Frontieres, 2014). During such missions, TSF provides emergency ICT services in
three broad categories:


       a.     Humanitarian Calling Operations

       TSF provides three minute, satellite based, phone calls free of charge to any HE

affected family. This allows people to pass critical information status and location to

loved ones during a time of crisis (Telecoms Sans Frontieres, 2013).

       b.     First Responder Emergency Telecommunications Centers


       Simultaneously, TSF specialists,

       Establish emergency telecom centers for emergency responders. The
       centers   offer—at   no   charge—broadband     Internet  access,   voice
       communications, fax lines and all the IT equipment needed for a field
       office. These centers enable emergency NGOs, the UN agencies, and local
       authorities to communicate right at the heart of a crisis. They also
       facilitate the coordination of aid efforts. First responders use TSF’s

       telecommunications services to communicate vital information, stay
       connected with headquarters and other emergency responders in the
       country who are often spread across a wide geographic area. Information
       management and sharing has become critical for an effective humanitarian
       response. (Telecoms Sans Frontieres, 2013)






                                          27
       c.      Information and Communication Technology Assessments

       Finally, the TSF rapid response teams also assist local governments and

emergency response coordinators to perform ICT assessments of damaged areas. TSF

uses its ICT experience to assist organizations and the effected nation to reestablish
commercial networks or planning to build the ICT support infrastructure needed for the

recovery stage following an emergency (Telecoms Sans Frontieres, 2013).


       4.      Humanity Road

       Founded in 2010, Humanity Road uses Internet volunteers and tools to monitor

social media to help save lives and reduce suffering.

       Humanity Road delivers disaster preparedness and response information to
       the global mobile public before, during, and after a disaster. Humanity
       Road is a leader in the field of online disaster response, providing social
       media disaster training and participating in both civilian and military
       communications exercises worldwide. Humanity Road support aid

       agencies and first responders during natural disaster and relay urgent
       needs to those who can response. (Humanity Road, 2014)

       Humanity Road concentrates on unstructured social media posts and tweets to

conduct social network analysis (SNA) to obtain actionable information (Humanity Road,
2014). While SNA is outside this thesis’s concentration, Humanity Road is technically a

mobile data collection organization when users use their mobile devices to Tweet or post

to their Facebook and are worth mentioning.

       5.      Humanitarian Data Exchange


       One hindrance to data sharing, aggregation, and processing to situational
awareness (see Figure 1) is the inability to fuse incompatible raw data sets. Incompatible

data sets will be defined and further discussed in the next section. The Humanitarian Data

Exchange (HDX) is an initiative within OCHA that seeks to solve this problem. The goal

of HDX is “to make humanitarian data easy to find and use for analysis” (Office for the
Coordination of Humanitarian Affairs, 2014a). This initiative attempts to link collected

raw data from various entities and sensors into one collective humanitarian assistance



                                           28
repository (Office for the Coordination of Humanitarian Affairs, 2014a). While still in the

pilot stage, the HDX effort continues along three lines:

       a.     Repository


       Sensors and data providers can upload their “raw data spreadsheets for others to
find and use” into the HDX repository (Office for the Coordination of Humanitarian

Affairs, 2014a).


       b.     Analytics

       HDX analytics is “a database of high-value data that can be compared across

countries and crises, with tools for analysis and visualization” (Office for the
Coordination of Humanitarian Affairs, 2014).


       c.     Standards

       HDX has created an open source data standard called the Humanitarian Exchange

Language (HXL) (Office for the Coordination of Humanitarian Affairs, 2014a). The goal

of HXL is to create standards to help share humanitarian data through the use of the
above HDX Repository and HDX Analytic tools (Office for the Coordination of

Humanitarian Affairs, 2014a).


E.     RAPID INFORMATION AND COMMUNICATION TECHNOLOGY
       ASSESSMENT TEAM

       The Rapid Information and Communication Technology (ICT) Assessment Team

(RTAT) concept was started out of the rubble of the 2010 Haiti earthquake as a way to

improve disaster response (Steckler, 2009). The need to quickly assess the ICT
environment post disaster has been recognized in nearly every major disaster event where

immediate efforts were less than optimal due to the inability of respondents to

communicate, much less collaborate, with each other (Donahue & Tuohy, 2006). All of
the ICT organizations listed in paragraphs B, C, and D of Chapter II Literature Review

have some ICT assessment expertise and responsibilities; none are independent, however,

of the competing responsibility to set up their parent organization’s ICT infrastructure

(Steckler, 2009). RTAT has the sole mission to assess the post disaster ICT environment
                                          29
and share those assessment results on an ongoing basis to the rest of the humanitarian

assistance (HA) response community. RTAT’s independent lens allows them to make
unbiased ICT recovery prioritization recommendations to the HA response community on

a continual basis (Steckler 2009). However, RTAT is a fledgling organization that has not

perfected its processes and is not fully integrated into the rest of the HA response

community (Cannon et al., 2012). This thesis will become a first step towards that end.

       “The Rapid Technology Assessment Teams (RTAT) concept seeks to provide a
pool of multi-disciplinary experts who will rapidly deploy to the disaster zone to provide”

a holistic assessment of the ICT environment (Steckler, 2012). “While there are existing

disaster assessment teams from major organizations that deploy to such events, these

teams primarily focus on sector specialty areas other than ICT and Information Sharing”
(Steckler, 2012).


       Additionally, RTAT is not looking to add more data noise but to enhance and
complete the information process shown in Figure 1. That is, an end-to-end solution that

automatively processes its assessment data into a shared collective understanding of the

ICT environment for better decision (Steckler, 2012).


F.     FOUR CATEGORIES OF POST DISASTER ASSESSMENT DATA SETS

       Nearly all of the aforementioned organizations have some ICT assessment form to
accompany their rapid response teams (DART, FITTEST, FACT, ERWG, etc.).

Information on the ICT environment is vital in the initial phases to enable better

communication and collaboration. The existence of the assessment is not enough, “the

timeliness and quality of assessments help determine an effective humanitarian response”
(Office for the Coordination of Humanitarian Affairs, 2014a). Additionally, “experience

has shown that there are significant benefits to coordinating needs assessments and that

doing so can help save more lives and restore more people’s livelihoods” (Office for the
Coordination of Humanitarian Affairs, 2014a). Finally, this information must be shared

with the people who can make decisions and enact change for the better (Office for the

Coordination of Humanitarian Affairs, 2014a). Unfortunately not all assessments, and

their corresponding data sets, are compatible. The UN categorizes assessments by the

                                           30
degree to which they can be integrated with one another and hence aggregated into a

collective common operational picture or understanding.

       1.      Coordinated Assessments


       The UN describes coordinated assessments as,

       Those which are planned and carried out in partnership with other
       humanitarian actors, with the results shared for the benefit of the broader
       humanitarian community to identify the needs of the affected population
       of a humanitarian crisis. Such assessments range from inter and intra
       cluster/sector joint assessments to single agency assessments that are
       harmonized. (Inter-Agency Standing Committee 2011b)


       Unless RTAT is absorbed by the UN-ETC, its assessments cannot be
characterized as coordinated.


       2.      Harmonized Assessment

       Harmonized assessments are conducted by external UN entities, but their

assessment data set structures allow for their integration. The Humanitarian Data

Exchange (HDX) initiative discussed earlier strives to put more data sets into this
category.


       Data collection processing and analysis is undertaken separately, however
       the data is sufficiently comparable (due to the use of common operational
       datasets, key indicators, and geographical and temporal synchronization)
       to be compiled into a single database, and to serve as the subject of a
       shared analysis. (Inter-Agency Standing Committee, 2011b).

        RTAT is striving to ensure data collected falls within this category.


       3.      Joint Assessment

       Joint assessment is the aggregation or combining of multiple cluster reports into a

single assessment.

       The IASC (2011b) defined joint assessments as the “data collection, processing

and analysis form one single process among agencies within and between

clusters/sectors. This leads to a single report. This is sometimes also referred to as a


                                           31
‘common assessment’” (Inter-Agency Standing Committee, 2011b). The Multi-

Cluster/Sector Initial Rapid Assessment report, discussed in detail later, is an example of
a joint assessment.


       RTAT wants its collected data to be included in the UN-ETC ICT common
operational picture.


       4.     Uncoordinated Assessments

       In contrast to the above assessments, uncoordinated assessments are those in

which “data sets are not interoperable, and the results cannot be used to inform the

overall analysis” (Inter-Agency Standing Committee, 2011b). RTAT’s previous Excel
assessment form fell into this category.


G.     THREE CURRENT POST DISASTER ASSESSMENT FORMS

       The literature reviewed revealed that nearly all of the aforementioned

organizations have some sort of ICT assessment form or questions that pertain to the

ability to communicate information. Unfortunately, these forms are not harmonized
(integrated) with one another (Office for the Coordination of Humanitarian Affairs,

2014a). The current forms discussed below, however, were an important starting point for

the RTAT ICT assessment form development and, as such, should be discussed.


       1.     United States Agency for International Development

       United States Agency for International Development’s (USAID’s) Field
Operations Guide for Disaster Assessment and Response Appendix II Assessments

includes Section 12. Infrastructure, Subsection a. Communication with an itemized

assessment checklist that has some similar items to Appendices G-J to include (United

States Agency for International Development, 2005, p. II-54–II-57). Its areas include
communication and electric power.








                                         32
a.     Communication

      Describe where the system’s facilities are located.

      Determine the broadcast/reception area or zone of influence (e.g., towns
       serviced by the system).

      Identify the organization/firm responsible for operation and maintenance
       of the system.

      Is there a disaster response plan with identification of priority facilities,
       material supply, and priority screening of messages?
      Obtain technical information, such as:

             Broadcast power.

             Operating frequencies, call signs.

             Relay/transmission points.
             Hours of operation.

             Standby power sources.

             Mobile capability.

             Repair/maintenance     facilities,  including   capabilities   of
              manufacturer’s local agent.
      Language of transmission.

      Identify  key    personnel   (owners,   management,    operations,   and
       maintenance).

      Determine    the  degree   of  integration  of   military  and   civilian
       communications networks.

      Note the source(s) of the above information.

      Determine which communications facilities exist that are operable or
       easily repaired and could be used to pass on assessment information and
       assist in coordination of lifesaving responses.

      Identify the type of system assessed:
             Radio.

             Private ownership.

             Commercial.
             Broadcast.

             Two-way.

             Amateur.

                                   33
             Citizens band.

             Public systems.
             Police.

             Armed forces.

             Government agencies. (Which ministries have communications
              facilities?)

             Telephone.
             Cable and wireless.

             Television.

             Newspaper.

             Other.
      Describe specific reasons why a system is not operating.

      Unavailability of:

             Personnel.

             Power.
             Fuel.

             Access to facilities.

      Damage to system:
             Broadcast/transmission equipment.

             Antennae.

             Buildings.

             Transmission lines.
             Relay facilities.

             Power source.

             Other.
      Note source(s) of the above information.

      Outline options for restoring minimum essential services.

      Identify local/regional suppliers of communications equipment and
       materials available for repair. Check cost and availability.

      Determine the local/regional availability of technical services available for
       repair.


                                   34
b.     Electric Power

      Describe the power system, including:

              Base load facility.

              Peaking facility.
              Number of units.

              Fuel source.

              Plant controls.
              Output capability (specify voltage and cycle).

              Mobile plants.

              Other standby capability.

              Switching facilities.
              Transmission facilities.

              Distribution facilities (number of substations).

              Interconnections.

      Inventory auxiliary equipment that may be available locally (e.g., from
       construction companies).
      Determine why power is not available (i.e., at what point the system has

       been damaged).
      Ascertain the condition of generating units.

      Check the integrity of the fuel system.

      Determine whether towers, lines, and/or grounding lines are down.
      Assess the condition of substations.

      Outline the impact of power loss on key facilities, such as hospitals and
       water pumping stations.

      Describe the options for restoring minimum essential services.

      Ascertain whether load shedding and/or switching to another grid can
       restore minimal services.

      Identify local/regional suppliers of equipment and materials.
      Check the cost and availability.

      Determine the local/regional availability of technical services available for
       repair.



                                   35
       2.      United Nations Assessments

       This section includes four subsections: a. Assessment Phases, b. Situational

Analysis, c. MIRA, and d. Disaster Assessment Coordination.


       a.      United Nations Assessment Phases

              Phase 1, the initial 72 hours: Initial assessments—Situational Analysis
              Phase 2, weeks 1–2: Rapid assessments.

              Phase 3, weeks 3–4: In-depth assessments.

              Phase 4, week 5 onwards: In-depth assessments, including recovery needs
               (Inter-Agency Standing Committee, 2011b).

       Table 1 outlines the UN assessment reports due within each phase. RTAT works
within phases zero (coordination and baseline) through phase two (weeks one and two

post disaster).
































                                           36
Table 1.   IASC Phase Assessment Reports (from Inter-Agency Standing Committee, 2011b)



                                          37
       b.      UN Situational Analysis (Phases 1–2)

       A Situational Analysis (SA) report provides,

       An initial overview of the situation, priority humanitarian needs and
       information gaps. It informs the Strategic Statement and the initial

       decision-making about scale and resource allocation. The SA should be
       updated regularly, until the next phase of the assessment is complete.
       (Office for the Coordination of Humanitarian Affairs, 2013b).

       This report is typically conducted by the earliest responders such as the UNDAC

teams and FITTEST within the UN-ETC sector (Office for the Coordination of
Humanitarian AffairsOffice for the Coordination of Humanitarian Affairs, 2013b).

Situational Analysis assessment includes the following (Office for the Coordination of

Humanitarian AffairsOffice for the Coordination of Humanitarian Affairs, 2013b):

              Drivers of the crisis and underlying factors
              Scope of the crisis and humanitarian profile

              Status of populations living in affected areas

              National capacities and response

              International capacities and response
              Humanitarian access

              Coverage and gaps

              Priority needs

       c.      Multi-Cluster Initial Rapid Assessment

       The UN Multi-Cluster Initial Rapid Assessment (MIRA) is an assessment product

of the UN ’s Inter-Agency Standing Committee (IASC) and is completed by the IASC’s

Needs Assessment Task Force (NATF) (Inter-Agency Standing Committee, 2012).
MIRA is joint assessment of the IASC clusters and is designed to,

       Identify strategic humanitarian priorities during the first weeks following

       an emergency. The main benefit of the MIRA is the elaboration, from the
       onset of the crisis, of a concerted operational picture based on the best
       information available from primary and secondary sources. This picture is
       expressed through two key products: a preliminary scenario definition,
       issued 72 hours after the disaster’s onset, and a MIRA Report, released
       after 2 weeks.(Inter-Agency Standing Committee, 2012).

                                           38
       MIRA assesses crises along eight axes (Office for the Coordination of

Humanitarian AffairsOffice for the Coordination of Humanitarian Affairs, 2013b):
              Drivers of the crisis and underlying factors.

              Scope of the crisis and humanitarian profile.

              Status of populations living in affected areas.

              National capacities and response.
              International capacities and response.

              Humanitarian access.

              Coverage and gaps.
              Priority needs.

       Unfortunately, MIRA is a broad overarching assessment that does not yield the

specific detail to meet all the needs of the RTAT stakeholders.


       d.      United Nations Disaster Assessment Coordination Team Assessments

       UN Disaster Assessment Coordination (UNDAC) teams deploy within 48 hours
of disaster and they contain sector experts from each of the clusters. Therefore, UNDAC

teams are a primary means for Phase 1 Situational Analysis assessments and have a large

supporting role in future MIRA reports (Office for the Coordination of Humanitarian

Affairss, 2013b). Additionally, UNDAC teams have a “primary responsibility to assist
the government of an affected country in its decision making through the identification

and prioritization of needs for international disaster relief assistance” (Office for the

Coordination of Humanitarian Affairs, 2013b). “UNDAC teams must develop an

adequate and efficient structure and flow of information to disseminate the analysis of
emergency needs to national authorities and other disaster responders in a timely manner”

(Office for the Coordination of Humanitarian Affairs, 2013b). RTAT looks to help

facilitate the ICT portion of this information flow.

       UNDAC reports include the following (Office for the Coordination of

Humanitarian Affairs, 2013b):
              Summary (highlights and key issues).

              Situation (general description of the situation, response and recent
               incidents).

                                           39
      Coordination overview:

             Overall coordination mechanisms in place, both national and
              international.

             Summary of meeting times and frequency.
             Constraints in coordination.

      Operational considerations:

             Relief entry point.
             Logistical constraints in relief delivery.

             Relief delivery issues (e.g., customs information).

             Special administrative concerns.

             Security issues.
      Urban search and rescue (USAR) activities (only applicable in USAR
       phase):

             Number of teams, name and sending area/country.

             Differentiation between national and international response.

             Areas covered/not covered.
      Cluster overview:

             List of operation clusters.

             Overview of coordination within clusters.

             Specific details for each operational cluster under new headings.
             Clearly identified national response in each cluster.

             List of relief provided or in the pipeline for each cluster.

      National response:
             Other national response not covered in the cluster section.

      Bi-lateral response:

             In-kind contributions.

             Cash contributions.
      Other:

             Other issues of interest not applicable in the above mentioned
              headings.




                                   40
                    General context of the situation.

                    Attached maps and lists where applicable, or included in the report
                     itself.

       The FITTEST teams reside within the UN-Emergency Telecommunication
Cluster (UN-ETC). FITTEST teams are the primary means of assessing ICT

infrastructure in the early phases. FITTEST teams have the added responsibility to

establish infrastructure to facilitate emergency inter and intra UN agency communication.
RTAT looks to assist FITTEST teams by conducting ICT assessment while they

concentrate on establishing communication means (Steckler, 2009).

       No explicit ICT assessment checklist was found in either the Situational Analysis

or MIRA report. Nor could a standalone ICT checklist for either the UNDAC or

FITTEST teams be located. One question was located within the Logistics checklist of

Annex Z of UNDAC Handbook (2013b) in regards to logistical communication.

       No explicit ICT assessment checklist was found in either the Situational Analysis
or MIRA report. Nor could a standalone ICT checklist for either the UNDAC or

FITTEST teams be located. However, one question was found within the Logistics

checklist of Annex Z of UNDAC Handbook (2013b) in regards to logistical

communication:

       “Communications: Do telephone and/or radio systems exist? What is their
       reliability/usefulness? Is there cell phone coverage? If yes; what system,
       i.e., roaming or procurement of scratch cards?” (Office for the
       Coordination of Humanitarian Affairs, 2013b).

       The World Food Programme, UN-ETC lead agency, also has a section on ICT

assessment within their Field Operations Pocketbook.


       3.     World Food Programme Emergency Field Operations Pocketbook

       Section 11.4 Information and Communication Technology (ICT) of World Food

Programme’s (WFP’s) Emergency Field Operations Pocketbook stated that responders

should monitor various items, to include (World Food Programme, 2002, p. 280–281):




                                          41
      Telephones:

             Normal phones: coverage and reliability of the network; whether
              all offices are connected.

             Mobile phones: coverage and reliability of the network(s); which
              offices/staff are using them
             Satellite phones: whether each office has one; as of security phase

              2, the security focal point and the CD should each have a satellite
              phone
             in their residence

             Faxes: which offices have fax machines and are able to
              send/receive

      Radios:

             WFP/UN radio room(s): the location(s) and hours of operation.

             HF/VHF radios: which offices, vehicles and individuals have
              them?
             VHF base stations and repeaters: locations.

             Lightning protection: whether all radio equipment with external
              antennas is protected.

             Local technical support: availability and quality of service;
              availability of spare parts.

      IT Environment:

             The numbers of functioning desktops, laptops, printers; whether
              there is a local area network (LAN).
             E-mail services: whether offices have ‘Notes’ connectivity and

              deepfield mailing (DFMS)
             Anti-virus software; Whether installed on all computers; the
              regularity of updating

             Local technical support: availability and quality of service;
              availability of spare parts

      Electric power:

             The local power grid: hours that power is available; its reliability
              and stability

             Whether UPS stabilizers are installed to protect equipment
             Back-up capacity: generators, solar panels, batteries



                                   42
       Table 2 is a synopsis of WFP’s minimum communication requirements based on

phases, as indicated above, based on the phases of relief above.


















































                                         43
Table 2.   World Food Programme Communication Requirements by Phase (from WFP, 2002, pp. 282–283)


                                                 44
 H.     ENTERPRISE ARCHITECTURE ANALYSIS

        According to the capability maturity model integration (CMMI) model, system

 requirements analysis must be done before designing a solution (Huang & Lien, 2012).

 The Enterprise Architecture Analysis marks the starting point for the discovery stage of
 the COE. Appendix B includes a detailed enterprise architecture assessment for RTAT;

 however, definitions and highlights are included in this section.


        1.     Enterprise Architecture

        Ross, Weill and Robertson (2006) defined Enterprise Architecture as,


        The organizing logic for business processes and IT infrastructure,
        reflecting the integration and standardization requirements of the
        company’s operating model. The enterprise architecture provides a long-
        term view of a company’s processes, systems, and technologies so that
        individual projects canbuild capabilities—not just fulfill immediate needs.
        Companies go through four stages in learning how to take an enterprise

        architecture approach to designing business processes: Business Silos,
        Standardized Technology, Optimized Core, and Business Modularity [see
        Figure 6 ]. As a company advances through the stages, its foundation for
        execution takes on increased strategic importance. (p.9)






















Figure 6.   Four Stages of Enterprise Architecture Maturity (from Ross et al., 2006, p. 72)

                                           45
        Summarily, the RTAT “enterprise” is defined as the people, equipment and

processes associated with the collection, storage and promulgation of pre and post
disaster ICT data collection (Beeson, 2013).


        2.     Operating Model

        Ross et al. (2006) stated, “The operating model is the necessary level of business

process integration and standardization for delivering goods and services to customers”

(p. 8). Figure 7 breaks up the different operating models to be discussed in a 2 X 2 matrix
based on the levels of business process integration and process standardization between

units in an organization.























   Figure 7.   Four Operating Models with Description (from Ross et al.,   2006, p. 39)


        RTAT is trying to improve standardization within the organization and within the

forms, as well as become more integrated with other HA organizations thereby increasing

its organizational agility, see Figure for this shift (Beeson, 2013).






                                            46
        a.     Standardization

        With the use of standardized capabilities (gear), the developed mobile data

collection tool (standard data forms) and standardized training (how qualitative questions

within the forms are assessed), RTAT hopes to improve, standardize, and add value to its
assessments with the HA community.


        b.     Integration

        Integration is defined as the ability of stakeholders (Pacific Disaster Center) to

seamlessly import and display RTAT collected assessment data in a value added,

intuitive, easily understood manner to the end-use customer (UN-ETC and other NGOs).

        RTAT seeks to increase the standardization of its processes from a diversified to a

replication organization, see Figure 8 for this movement.






















Figure 8.    Shift from Diversification to Replication Operating Model   (after Ross et al.,
                                             2006, p. 39)


               a.      Agility

        Agility is the ability to get assessment information in the hands of those who need

it and the ability to re-task as required within the ICT assessment realm. Agility includes

                                             47
time training assessors, speed and accuracy of assessment, speed and ability to send

assessment information to end users, and the ability to modify forms quickly as need
arises.


       RTAT currently uses a Microsoft (MS) Excel spreadsheet that is manually
updated by subject matter expert (SME) assessors and emailed to other team members

and organizations. RTAT has agreed upon the questions to ask and the items to assess,

but they have not agreed to their associated standard answer/assessment metrics. The
current MS Excel spreadsheet method does not expeditiously collate the raw data and

export processed information. To obtain actionable information from RTAT, an

individual must collect all of the spreadsheets and individually review each one. This is a

painstakingly, time consuming process that does not work when time equates to lives
lost.


       In response, RTAT is looking into alternatives that will better meet the current
and future data collection, processing, and sense-making needs. In the process, RTAT has

discovered that it needs to change its operating model to remain relevant (Beeson, 2013).

To enable this operating model change, RTAT is seeking an IT solution that will foster

standardization while enabling integration.

       This developed IT solution must enable RTAT to efficiently collect and distribute
baseline/post-disaster ICT information in (near) real time. (Near) real time is situational

dependent but should be less than 1 hour from collection by RTAT to receipt by the end-

use customer (UN-ETC/NGO). Potential IT solutions will be discussed later; all available

options, however, require a data connection to work. This is problematic because the ICT
infrastructure is typically in ill-repair in a post disaster scenario. HA organizations have

been experimenting with and developing Hastily Formed Network (HFN) concepts and

technologies to cope with this problem. Before exploring the technologies, RTAT wanted

to examine its processes. Savvion process modeler was chosen to do this.

I.     SAVVION BUSINESS PROCESS MANAGEMENT

       Savvion is a comprehensive, model-based business process management (BPM)

system that empowers a user to build and simulate business process alternatives (Aurea,

                                           48
n.d.). Savvion (2006) has identified that, “The first step in improving a business

processes is to articulate precisely where bottlenecks exist, and the most efficient way to
do that is by simulating existing processes using business process modeling software.”


       Based on input, Savvion calculates the cost (time and or monetary) of the created
business process map (Figure 6). This enables a quantitative comparison of alternatives.

Savvion delivers real-time, context-relevant insight into critical business operations and

tools to change business rules and logic (Aurea, 2014). Savvion was used to simulate and
improve the RTAT assessment business processes; detailed results are included in

Chapter III and in Appendix C. Figure 9 shows a screen capture of the RTAT old “as is”

assessment business process using Savvion.
























  Figure 9.   Savvion RTAT Old “As Is” Business Process (from Beeson, Gladem, &

                                          Gonzalez, 2014)


       Starting in the top left portion of Figure 9, a member of a responding NGO
needing information requests the UN-ETC to conduct an ICT assessment at a particular

site or region of interest. UN-ETC requests support from RTAT who assigns one of three

available teams to support. The team conducts the assessment using the outdated Excel

                                           49
spreadsheet and manually inputs GPS coordinates via an external device. Completed

assessments are emailed to the UN-ETC for quality assurance and map plotting. Errors
and ambiguity are sent back to the RTAT for correction and or clarification. Once

reviewed and plotted by the UN-ETC, results are shared with the requesting NGO via

email dissemination. Review and rework are the largest sources of delay, outside of

RTAT travel to the assessment site. Manual, point-to-point email is the least desirable
form of information sharing; if collaboration, and a collective shared understanding of the

operational environ is desired (Office for the Coordination of Humanitarian Affairs,

2014a).

J.     HASTILY FORMED NETWORK


       All RTAT means of forwarding the ICT assessment require an Internet
connection. This section will discuss how RTAT can leverage hastily formed network

organizations, gear and procedures to obtain an Internet connection within a disaster

zone.

       1.     Wireless Mesh Network


       Wireless mesh networks is the term used to describe the ability of devices to
automatically connect to other networks within a network. Akyildiz, Wang & Wang

(2005) explained,

       Wireless Mesh Networks (WMN) offers an inexpensive, quickly
       deployable, stable and fault tolerant solution for wireless coverage,

       requiring zero maintenance. The WMN based on (IEEE standard)
       802.11(s) is most popular due to easily available and inexpensive radios.
       There are currently many implementations of wireless mesh networks
       based on 802.11(b,g,n) hardware. All of these (solutions) run some
       proprietary mesh protocol at the networkayer.

       Furthermore, WMNs provide:


       [n]etwork access for both mesh and conventional clients. The integration
       of WMNs with other networks such as the Internet, cellular, IEEE
       802.11(b,g,n), IEEE 802.15 (WIMAX), IEEE 802.16, sensor networks,
       etc., can be accomplished through the gateway and bridging functions in



                                         50
       the mesh routers. Mesh clients can be either stationary or mobile, and can
       form a client mesh network among themselves and with mesh routers.
       (Akyildiz et al., 2005).

       Any individual or organization with a Wi-Fi device (laptop, phone, etc.) within

range, on the move or stationary, can access the WMN and the support and/or reach back

capability that it offers. WMNs can quickly and cheaply replace failed host nation

infrastructure and are a key enabling component to hastily formed networks.

       2.      Hastily Formed Network

       Hastily formed network (HFN) is a type of hyper-network that is


       (a) put together quickly in response to an emergency, crisis, or urgent
       situation (b) from a collection of entities who have expertise or local
       responsibility to help but have not worked together before (c) and who
       accept no higher decision-making authority. (Denning & Hayes-Roth,
       2006).

        The lack of a recognized, over-arching authority may further compound the ad

hoc/complex nature of disaster relief. Like “networks” and “hyper networks” defined

above, HFNs are a composition of both the organizational make up and their equipment.


       a.      Hastily Formed Network Organization

       HFN’s are neither a hierarchy n    or flat organization, but an ecosystem or a
federation of organizations with a shared purpose (Denning & Hayes-Roth, 2006). HFNs

may be made up of a loose federation of hierarchical organizations, but success depends

on their ability to create a collective “network” that shares more in common with ‘edge’

organizations, as defined later by Alberts and Hayes (2003). Denning (2000) further
described a HFN as “the newest form of a hyper-network, having the special

characteristic that the participants have little time to learn and adapt before producing

results” (Denning, 2000). According to Denning (2000),

       An HFN has five elements: it is (1) a network of people established
       rapidly (2) from different communities (3) working together in a shared
       conversation space (4) in which they plan, commit to, and execute actions,
       to (5) fulfill a large, urgent mission.



                                           51
       Furthermore, “HFNs must employ organizational forms compatible with the

nature of the organizational challenges at that level. Decentralized decision making and a
focused sharing of high value situation information should be base principles” (Denning,

2000). Successful HFN organizational forms are more similar to “Edge” organizations as

described in Alberts and Hayes’s (2003) classic book, Power to the Edge than

hierarchical organization typical of the military or most host nation governments. Alberts
and Hayes (2003) described entities on an organizations edge as those on the “tip of the

spear,” on the edge of the empire, and or in direct contact with the customer. People on

the edge have the most up to date, local, situational awareness and should be empowered
with the right information, tools, equipment and authority to enact rapid decisions and

change. Alberts and Hayes (2003) described this process in Power to the Edge. Edge

organizations are empowered with the ability to smart pull of resources and information

(Alberts & Hayes, 2005, p. 119). Suppliers in edge organizations must post information
and available assets and resources for the consumers to pull (Alberts & Hayes, 2003, p.

119). This is similar to a Craig’s List or eBay posting format that efficiently links

resources to needs and suppliers with consumers.


       b.      Hastily Formed Network Equipment

       Denning (2000) asserted, “The heart of the (HFN) network is the communication
system… and the ways they (participants) interact within it. We call this the

‘conversation space’ of the HFN.” Denning (2000) continues that conversation space is

made of three key aspects: “(1) a medium of communication among (2) a set of players

(3) who have agreed on a set of interaction rules.” The physical equipment systems vary
greatly from humanitarian event (HE) to event. Some are brought by nongovernmental

organizations for specific tasks, but, by definition, the HFN quickly becomes a

compendium of ad hoc meshed networks brought to bear by the collective. Figure 10
shows the various puzzle pieces (systems, procedures, etc.) required for an HFN and

includes power systems, as well as cellular, Internet, radio, and communication means

(Steckler, 2009).

       The RTAT collection tool will leverage HFN technologies.


                                           52
   Figure 10.  Hastily Formed Network (HFN) Puzzle Pieces  (from Steckler, 2009)



K.     MOBILE DEVICE OPERATING SYSTEMS

       Several competing operating systems in the mobile (smart) device arena currently
exist. Competitors include: Microsoft Windows Mobile, Symbian Operating System

(OS), Research In Motion Blackberry OS, Apple iPhone OS (iOS) and the Android™

OS. Currently, iOS and Android make up 94 percent of this market, justifying further

study.

       1.     iPhone Operating System


       iPhone Operating System (iOS) was created for Apple’s iPhone in 2007 in
response to Windows Mobile, Palm OS, Symbian, and BlackBerry (Verge Staff, 2014).

Apple revolutionized the smart phone market through its innovative use of the capacitive

touch screen. Apple removed all but five physical buttons and perfected its touch screen
to allow multi-touch commands such as “pinch-to-zoom and inertial scrolling to make

apps feel more natural and immediate” (Verge Staff, 2014). Additionally, Apple

integrated iOS into its Apple ecosystem that included iTunes and the Apple Appstore;

this combined with its ultra-usability propelled iOS to the forefront of the smart phone
                                         53
market (Verge Staff, 2014). Unfortunately, iOS is available only on Apple products and

the Appstore has a more rigorous vetting process for the creation of third party
applications than other competing operating systems and has inadvertently limited the

mobile data collection applications as a result.


       2.     Android Operating System

       The Open Handset Alliance (OHA) Android™ initiative was created as a way to

cost effectively develop a mobile device operating system. While Google is the lead
entity of the OHA, there are a total of 84 technology and mobile companies contributing.

These entities include: Acer, ASUS, HTC, Huawei, LG, Kyocera, Motorola, Samsung,

Sony and ZTE to name but a few (Open Handset Alliance, 2014). OHA created Android

as an open source operating system to promote the smart phone market and get the most
out of emerging cell phone technologies. According to Reed (2014), Android makes up

approximately 46 percent of smart device operating systems; however, research has

shown that market share estimates vary slightly by source and time frame (Figure 11).
What is clear is that Android has been increasing its market share over the years,

especially in developing nations and is poised to take over the smart phone market in

2014 (Levine 2014). Currently, Android phone shipments make up approximately 85

percent of smart phone market, but that could change with the scheduled release of the
iPhone 6 in fall 2014 (Levine, 2014).




















                                          54
    Figure 11.  Mobile Device Operating System Market Share (from Reed, 2014)


       Andr oid allows a user to download applications (app(s)) that interface with

certain capabilities organic to the device. These capabilities include: Camera (video and

still), microphone, GPS, network (cellular and Wi-Fi), touch screen capacitance,

accelerometer, memory, and the device’s computing power. Any user can create their
own Android compatible application (Lighthouse is one such example) using the OHA

standards, or the user can simply download the appropriate application from an app store

such as GooglePlay or Amazon (Open Handset Alliance, 2014). The One Platform

Foundation (OnePF) tracks over 30 popular app stores and found that GooglePlay makes
up about 1/3 of the market with approximately 2.5 billion downloads/month (One

Platform Foundation, 2014). In comparison, the next largest competitor is Tencent at 300

million/month and Amazon accounts for comparatively low 25 million downloads (One
Platform Foundation, 2014). Comparatively, iOS users download approximately 1.56

billion applications from Apple’s App Store every month (Levine, 2014).


L.     MOBILE DATA COLLECTION METHODS

       Historically, data collection methods in undeveloped areas or disaster zones have

been lacking and reliant upon paper forms in interview or checklist assessment format.
Unfortunately, the paper forms require additional processing to collate their associated

information into a data base for further refinement that enables logical and cogent


                                         55
delineation of needs and decisions. With the advent of laptops and Personal Digital

Assistants (PDA), some forms have been migrated to electronic format. The formats can
be integrated into data base formats, but their forms require extensive programming or

understanding of the software to do so. Additionally, these forms typically could not

update information in (near) real-time. With the “proliferation of smartphones, low cost

mobile connectivity with good coverage and availability of several data collection
applications that can work around the connectivity concerns, pen-paper surveys are now

being replaced by mobile based data collection applications” (Gupta, Thapar, Singh,

Srinivasan & Vardhan, 2013).

       Mobile phone data collection options fall into three main methodologies,

Electronic Form Interface, Short Messaging Service (SMS) + Cue Card Interface, and
Voice Interface (Patnaik, Brunskill, & Thies, 2013). The strengths and weaknesses for

each are outlined in Figure 12.































                                           56
    Figure 12.   Mobile Phone Data Collection Methods (from Patnaik et al., 2013)


        1.     Electronic Form Interface

        The term “electronic forms” denotes “any external application that can be placed

on a phone and that automatically guides the user how to enter data, through the use of

text, menus or other tools” (Patnaik et al., 2013). The term “developed electronic form

interface” is interchangeable with mobile data collection tool/platform, RTAT tool, or by
its associated application Lighthouse or Open Data Kit (ODK) Collect, to be discussed

later.



                                            57
       2.     Short Messaging Service + Cue Card

       Patnaik et al. (2013) referred to Short Messaging Service (SMS) as “data

collection systems that involve information entered by a structured text message: in

particular we assume that the information is entered by following a small cue sheet with a
flowchart that directs the collector how to enter the data.” This method assumes a SMS

(text) service works in the area being assessed.


       3.     Voice Interface

       Voice interface relies upon a voice connection and an interviewer to collect

information. Gupta et al. (2013) found that voice interface is the most accurate of the
three methods when answers are relayed to a live person or by voice mail. They require

only a basic phone. Questions and answers can be modified or explored further and the

education level and training of the interviewer is the lowest of the methods (Gupta et al.,
2013). Unfortunately, the method relies upon a working phone network that may not be

available in a post disaster environment. Further, voice takes up more bandwidth than

SMS, and small data transmissions leading to interruptions in calls and voice services are

typically more expensive than SMS services in developing regions. For these reasons,
Voice Interface was not further considered for RTAT.


M.     AVAILABLE MOBILE ELECTRONIC FORM INTERFACES

       NOMAD (n.d.-a and b) and MobileActive (2013) have extensive listings of

available Mobile Data Collection Platforms; below is an overview of pertinent platforms.

       Chapter III includes an extensive discussion as to how the final platform was

selected. The basic mobile data collection tool requirements include:

             Small form factor.
             Ruggedized.

             Android compatible.

             Free to develop and use.

             Ability to make and tailor forms quickly and easily.
             KML file exportable (importable into a database).


                                          58
             GPS location enabled

             Able to take and include photos.
             Skip logic pattern supportable (able to ask/skip questions based previous

              answers).
             Not restricted to SMS (text message) only.

             Forms must be stored on the device until they can be opportunistically
              uploaded when a data connection is available.

       1.     The HumanitariaN Operations Mobile Acquisition of Data

       The HumanitariaN Operations Mobile Acquisition of Data (NOMAD) is a non-

governmental organization (NGO) project that “links organizations with the latest

information management tools to more easily collect, analyze and manage data”

(HumanitariaN Operations Mobile Acquisition Of Data, 2012). NOMAD utilizes its
Online Selection Assistant (OSA) to connect organizations with one of 39 established

mobile data collection (MDC) solutions (HumanitariaN Operations Mobile Acquisition

Of Data, 2014). Participants are asked a series of questions regarding their requirements

and the OSA returns viable options for further research, test and evaluation. According to
NOMAD (2014), established MDC tools include:

       Acquee,    COMMANDmobile,        CommCare,     CommTrack,      CSPro,

       CyberTracker, DevInfo, do Forms, droidSURVEY, Enketo Smart Paper,
       EpiCollect, FrontlineSMS, Fulcrum, GeoChat, GeoPoll, Humanitarian
       Data Toolkit, Imogene, iSURVEY, KoBo, Last Mile Mobile Solution,
       Magpi, Majella Insight, Mobenzi Researcher, Nokia Data Gathering
       system, Oasis Mobile, Open Data Kit, openXdata, Pendragon, Poimapper,
       PSI Mobile—   Fusion, RapidSMS, RDMS, Smap, SoukTel, Telerivet,

       ViewWorld, Voxiva, and Wepi.

       One notable popular disaster information management solution is Ushahidi’s
CrowdMap. Ushahidi is a Kenyan based initiative and its CrowdMap differs from the

listed mobile data collection tools in that it uses a “crowd-sourced data aggregation

paradigm... Data aggregators collect unstructured data found as posts to services such as

Twitter, Facebook, email, and SMS, and they mine this data for information (Jung, 2011).
In contrast,




                                        59
       Mobile data collection systems run designed surveys which collect
       specific information from a target audience. The audience can be either
       organizational staff trained to conduct surveys/assessment or the target
       population being studied can be surveyed directly via their personal

       mobile devices. In either case, the specific questions and structured
       responses can be important to rapidly collecting information deemed
       essential to an emergency response. (Jung, 2011).

       Developers of the aforementioned established tools are allowed to update their

tool’s information within the OSA to ensure the latest information. The results for free,
Android operating systems, with Keyhole Markup Language (KML) output options are

outlined in the next sections.


       2.      Android Options

       Android is a product of the Open Handset Alliance (OHA), led by Google (Open

Handset Alliance, 2014). Android currently makes up approximately 46 percent of smart
device operating systems and has been slowly increasing its market share (Reed, 2014).

The following sections briefly discuss the Android compatible mobile data collection

(MDC) tool applications that HumanitariaN Operations Mobile Acquisition Of Data

(NOMAD) suggested for further research by the NOMAD Online Selection Assistant
(OSA) program. Full OSA results are available from the author upon request.


       a.      Open Data Kit

       According to Brunette, Sundt, Dell, Chaudhri, Breit, and Borriello (2013),

       Open Data Kit (ODK) is an open-source, modular toolkit that enables

       organizations to build application specific information services for use in
       resource-constrained environments. ODK is one of the leading data
       collection solutions available and has been deployed by a wide variety of
       organizations in dozens of countries around the world.

       ODK has a robust community of practice and Google group forum located at

https://groups.google.com/forum/#!forum/opendatakit-developers.        In       addition,

Developmental support is available for a fee if required. ODK is a suite of systems that
include the following tools:




                                           60
             Build: ODK Build enables users to generate forms using a drag-and-drop
              form designer. Build is implemented as an HTML5 web-based application
              and targets the common use case of a simple form” (ODK, 2014).

             Collect: ODK Collect is a “powerful phone-based replacement for your
              paper forms. Collect is built on the Android platform and can collect a
              variety of form data types: text, location, photos, video, audio, and
              barcodes” (ODK, 2014).

             Aggregate: ODK Aggregate “provides a ready to deploy online repository
              to store, view and export collected data. Aggregate can run on Google’s
              reliable and free infrastructure as well as on local servers backed by

              MySQL and PostgreSQL” (ODK, 2014).
             Form Uploader: ODK Form Uploader facilitates the uploading of a blank
              form and its media files to ODK Aggregate (ODK, 2014).

             Briefcase: ODK Briefcase is “the best way to transfer data from Collect
              and Aggregate” (ODK, 2014).

             Validate: ODK Validate “ensures that you have an OpenRosa compliant
              XForm—one that will also work with all the ODK tools” (ODK, 2014).

             XLS2XForm: ODK XLS2XForm allows XForms to be designed using
              MS Excel (ODK, 2014).

       b.     Lighthouse Application

       The Naval Postgraduate School’s (NPS) Common Operational Research

Environment (CORE) lab developed Lighthouse. Lighthouse leverages the above ODK

technologies (ODK Build forms and ODK Aggregate) to collect, aggregate and display

spatial data on a common operational picture (COP) display. This data can then be further
analyzed using social network analysis (SNA) tools to develop a better understanding of

the commonalities and groupings of events and data sets. In the context of the DOD,

Morganthaler and Summers (2011) defined SNA as “a type of applied art where social
science and mathematics are integrated to flesh out the strategic options within both the

kinetic and non-kinetic approaches of a counterinsurgency campaign” (p. 10). These tools

have been used by the CORE lab with success in Iraq, Afghanistan, and Thailand to

combat improvised explosive device (IED) threats and to better understand insurgent
cells (Bumatay, & Graeber, 2014). While SNA is out the scope of this thesis, these tools

can be used to link cellular providers with tower locations, determine hardest hit disaster



                                          61
areas, and even piece together the structures/areas that remain the most resilient for later

research. Ushahidi, mentioned above, is a SNA tool.

       Unfortunately, Lighthouse only works for Android operating system based

devices and thus will not work for 54 percent of smart device users (Reed, 2014). Further,
Lighthouse is not available in the GooglePlay store making distribution and installation

of the application difficult. Lighthouse, however, is supported by the NPS CORE lab staff

and they were instrumental in the development of the early RTAT forms. Unfortunately,
due to budget restraints and operational commitments, updates to the Lighthouse

application have not kept pace with feature developments of the ODK Forms. Further

Lighthouse developments have been halted in favor of the development of a Hypertext

Mark Up Language 5 (HTML5) based solution that is operating system independent and
will work on both Google’s Android and Apple’s iOS.


       A direct competitor to Lighthouse is Open Data Kit (ODK Collect).

       c.      Open Data Kit Collect

       Like Lighthouse, “ODK Collect is a mobile platform that renders complex

application logic and supports the manipulation of data types that include text, location,

images, audio, video, and bar-codes” (Hartung, Anokwa, Brunette, Lerer, Tseng, &

Borriello, 2010).

       ODK Collect is available on the GooglePlay store and works with the created

ODK Build forms and can be synched to the CORE lab’s ODK aggregate server, just like
Lighthouse. ODK Collect solves the distribution and update problems of Lighthouse.

ODK has an extensive wiki and online community of practice, but their level of support

does not match that of the CORE lab’s walk-in face to face help when it comes to form

building.

       Like Lighthouse,

       ODK Collect renders forms into a sequence of input prompts that apply
       form logic, entry constraints, and repeating sub-structures. Users work
       through the prompts and can save the submission at any point. Finalized
       submissions can be sent to (and new forms downloaded from) a server.

       Currently, ODK Collect uses the Android platform, supports a wide
                                           62
       variety of prompts (text, number, location, multimedia, barcodes), and
       works well without network connectivity. (Open Data Kit Collect, 2014)

       d.     Field Information Support Tool

       As Longley (2010) explained,


       The Field Information Support Tool (FIST) is a field data-collection
       system using commercial-off-the-shelf (COTS) smartphones, customized
       software, and a robust information management backend known as
       FusionPortal with a deployable sensor fusion system known as
       FusionView that enables information to flow from the point of capture to
       an analyst in near real-time regardless of location or physical proximity.

        FIST is free for a Naval Postgraduate student to develop, RTAT would be

expected to pay, however, for ongoing future support. FIST became a pay for service

option when it expanded the Lighthouse data collection capability into a more robust

social network analysis platform.

       e.     CyberTracker


       According to NOMAD (n.d.-a),

       CyberTracker is a downloadable solution for mobile data collection that
       can be implemented on PalmOS, PocketPC, Windows Mobile or Android.
       The CyberTracker designer enables the creation of graphical collection
       forms, originally targeted at non-literate animal trackers. No coding is
       required and it automatically generates the required database schema in

       MS (Microsoft) Access. CyberTracker exports data in 14 formats
       including ESRI Shape file. CyberTracker can send data from mobile
       device or smartphone to remote FTP (File Transfer Protocol) site.

       CyberTracker Consists of a Desktop Windows Application to design sequences to

use in the mobile application, a Mobile Application (see Figure 13) to capture data and
another Desktop Windows application to query, visualize, and export the data using

Microsoft (MS) Access (CyberTracker, 2014).









                                          63
             Figure 13.   CyberTracker in use (from CyberTracker, 2014)


       Forms are customizable, but there is no ability to use skip logic (CyberTracker,

2014). Photos can be manually added to the database only after the form has been

uploaded thus necessitating a two-step processes (CyberTracker, 2014). The requirement

to manually attach a photo after upload in the database and physically attach the device to
a laptop/desktop to manually download a assessment file to the computer for export gives

the tool low marks for usability. Additionally, Cybertracker has no way of synching

downloaded assessments or resuming a download should the link between the computer

and the server be disconnected. The supportability of CyberTracker is minimal with no
established community of practice or wiki.


       f.      Humanitarian Data Toolkit

       NOMAD (n.d.-a) stated,

       The Humanitarian Data Toolkit (HDT), developed by Internews and Modi
       Research Group at Columbia University, is a ruggedized, self-contained

       data collection toolkit that makes it possible to conduct rapid mobile and
       paper based data collection and analysis in an off-line and off       -grid
       environment. The HDT consists of a laptop running a local instance of the
       Formhub data collection software, a scanner, Wi-Fi network and phones
       that fit in a carry-on sized Pelican case and an additional portable solar
       panel / battery that are able to reliably power the toolkit when electricity is

                                            64
       not possible. Powering the HDT is Formhub, an open source mobile data
       collection platform, bamboo, a data analytics service, both developed by
       the Modi Research Group, and Captricity service that rapidly converts
       paper forms into structured data. In the HDT, these tools are integrated

       together make it possible to author a survey offline in Excel, collect data
       using Android phones/tablets, offline enabled webforms and paper forms,
       with all data managed in a central place where they can be quickly
       analyzed in almost a real-time basis allowing responders to make quick,
       evidence-based decisions on how best to intervene.

       Unfortunately, this system does not meet the small form factor (laptop, scanner,

solar panel, etc.) requirements of the on-the-go assessor model and was not considered

further (Figure 14).















      Figure 14.   Humanitarian Data Toolkit (Humanitarian Data Toolkit, 2014)



       3.      iPhone Options

       The NOMAD OSA program was used to narrow the Apple iOS compliant mobile
data collection tool options, with the following results: Fulcrum, GeoChat, Majella and

ViewWorld. All three were iOS compliant, but all three required payment for their use in

the scale required by RTAT (HumanitariaN Operations Mobile Acquisition Of Data, n.d.-

b). Full OSA results are available upon request.

       a.      Fulcrum

       Fulcrum allows a user to “create, deploy, and manage field data collection apps

for iPhone, iPad, and Android  ” (MobileActive, 2010). Fulcrum includes a web-based


                                           65
drag-and-drop app designer for creating customized survey forms to control the data

captured from the field. Fulcrum has offline mobile support but there is a charge for its
services (MobileActive, 2010). As of August 6, 2014, plans go from $29/month for one

user to $749/month for 50 users (Fulcrum, n.d.).


       b.      GeoChat

       GeoChat is,

       A collaboration tool that allows users to chat, report, and get alerts on their
       phone that can be represented on a map. It facilitates a slightly different
       communication paradigm based on collaboration rather than one way data

       collection. Geochat is an open source solution that supports GeoRSS,
       KML and http API’s on any mobile device.       (HumanitariaN Operations
       Mobile Acquisition Of Data, n.d.-b )

       GeoChat is an interesting way to track personnel and simple data points; the

system, however, was not supported like the Open Data Kit community, and it is better

suited to track personnel with simple data messages such as: “I’m here,” “I’m going to
__,” or “Send Help!”


       c.      Majella

       “Majella Insight is a complete Mobile Data Collection and Integration System”

(HumanitariaN Operations Mobile Acquisition Of Data, n.d.-b). Majella provides a

“secure cloud application and the ability to collect and integrate data on both a web and
mobile mapping application” CSV, XML, KML and PDF export formats (HumanitariaN

Operations Mobile Acquisition of Data, n.d.-b).


       d.      ViewWorld

       ViewWorld is a plug and play mobile data collection platform hosted in the cloud,

designed for organizations collection data in harsh conditions”HumanitariaN Operations
Mobile Acquisition Of Data, n.d.-b). ViewWorld is an end-to-end solution that allows a

user to create a form, collect data, and view the data on a web console (HumanitariaN

Operations Mobile Acquisition Of Data, n.d.-b   ). Results can be exported to the web,

social media, or on line map (HumanitariaN Operations Mobile Acquisition Of Data,

                                           66
n.d.-b). Data can also be manipulated on ViewWorld’s dashboard with simple visual

analytics such as graphs and pie charts (HumanitariaN Operations Mobile Acquisition Of
Data, n.d.-b). ViewWorld’s API facilitates data exportation into various data formats

(HumanitariaN Operations Mobile Acquisition Of Data, n.d.-b).


       e.     DataKeep

       A Formhub/Open Data Kit based application is available in the Apple appstore

called DataKeep. Released in March 2014, DataKeep version 1.01 is a free XForm
format compatible application. Like ODK Collect, DataKeep allows forms to be retrieved

from and returned to any setup server, Formhub or ODK Aggregate for example.

Unfortunately, this application does not support all XForm features like ODK Collect and

does not work with developed RTAT forms (iTunes App Store). The application,
therefore, would not work with established servers. This application may become a viable

option once all ODK Form features are incorporated into DataKeep.






























                                          67
THIS PAGE INTENTIONALLY LEFT BLANK





























                  68
                         III.   RESEARCH METHOD



       This chapter documents a campaign of experimentation to develop an improved
mobile data collection tool for the Rapid Information and Communication Technology

(ICT) Assessment Team (RTAT) organization and mission. This chapter will explain

what a campaign of experimentation is and what was done at each stage of the campaign
to research, develop, test, analyze and ultimately demonstrate a viable RTAT solution.


A.     CAMPAIGN OF EXPERIMENTATION

       Alberts and Hayes (2002) stated that the objectives of experimentation are “to

develop and refine innovative concepts of operation and to coevolve mission capability

packages to turn these concepts into real operational capabilities. One experiment cannot
possibly achieve this objective. Rather, it will take a well-orchestrated experimentation

campaign consisting of a series of related activities to accomplish this” (p. 16). Alberts

and Hayes (2002) further described the linking of several related experiment, discovery,
hypothesis and /or demonstration experiments, in a systematic and coherent manner to

achieve a much larger end state goal as an “experimentation campaign” (p. 25). Alberts

and Hayes (2002) explained that an experimentation campaign is,

       A series of related activities that explore and mature knowledge about a
       concept of interest…experimentation campaigns use the different types of
       experiments in a logical way to move from an idea or concept to some
       demonstrated military capability. Hence, experimentation campaigns are
       organized ways of testing innovations that allow refinement and support

       increased understanding over time. (p. 25).

       Simply put, “Campaigns of experimentation explore and mature knowledge about
a subject” (Hudgens & Bordetsky, 2009). Elaborating further on campaigns of

experimentations, Alberts and Hayes (2002) stated,

       Campaigns (of experimentation) are designed to provide comprehensive
       insight across a set of related issues. The focus of campaign planning is to

       ensure that each important aspect of force capability is addressed and that
       no critical issues are overlooked. As a result, the various axes of the
       experimentation campaign employ a range of conditions and methods for
       investigating different types of issues. The fundamental planning question

                                         69
        for an experimentation campaign is: ‘Are we addressing all of the
        important aspects of the problem?’ (p.45)

        Alberts and Hayes (2005) characterized COE experimentation activities within

three categories: discovery, hypothesis and demonstration (pp. 72–76).


        1.     Discovery Experiments

        Alberts and Hayes (2002) defined discovery experiments as those experiments

that  “involve    introducing   novel   systems,   concepts,   organizational   structures,
technologies, or other elements to a setting where their use can be observed and

catalogued” (p. 9). Further, “discovery experiments are designed to generate new ideas or

ways of doing things. They seek to create opportunities for individuals and organizations
to ‘think outside the box’ and thus to stimulate creativity” (Alberts & Hayes, 2005, p.

73). The result, product, or output of a discovery experiment “is a promising idea or

approach (Alberts & Hayes, 2005, p. 73).


        2.     Hypothesis Testing

        Alberts and Hayes (2002) stated, “hypothesis testing experiments are the classic
type used by scholars to advance knowledge by seeking to falsify specific hypotheses

(specifically if then statements) or discover their limiting conditions” (p. 22). Alberts and

Hayes (2005) continue that,

        Depending on the nature of the hypotheses tested, this type of experiment
        provides ‘proof’ that a theory, idea, or approach is valid; establishes its
        value under specific conditions; establishes the exceptions and limits of its

        application or utility; and establishes a degree of credibility. (p.

        3.     Demonstration Experiments

        Alberts and Hayes (2005) explained demonstration experiments as a, “venue in

which known truth is recreated…They are used to show potential customers that some
innovation   can,   under   carefully   orchestrated   conditions,   improve    efficiency,

effectiveness, or speed” (p. 75).





                                            70
B.     RAPID      INFORMATION           COMMUNICATION            TECHNOLOGY
       ASSESSMENT TEAM CAMPAIGN OF EXPERIMENTATION

       Following Alberts and Hayes’s (2002, 2005) campaign of experimentation model

outlined above, this section describes a campaign of experimentation to research, explore

and mature mobile data collection technology and methodology. The specific goal for
this COE is a working mobile data collection tool for the RTAT organization and

mission.

       Alberts and Hayes (2005) stated, “campaigns of experimentation (COE) should

generally move along an axis that takes them from discovery experiments to preliminary

hypotheses experiments, to refined hypotheses experiments, and finally, when the state of

knowledge is mature enough to support serious policy and acquisition decisions, to
demonstration experiments” (p. 77). Naturally, COE flows in three stages from the

discovery stage and investigation phase to the demonstration stage. Below is a listing of

the actions taken during the RTAT COE within each stage. Details for each action follow
in sections within each stage heading.


       1.     The Discovery Stage

             Research and literature review

             Enterprise architecture analysis

             Mobile data collection tool prototyping

             Mobile data collection tool analysis of alternatives

       2.     Investigative Stage

             Savvion process modeling

             Legazpi City field experiment

       3.     Demonstration Stage

             Typhoon Haiyan field deployment

             Joint Interagency Field Exercise (JIFX) 2014–4





                                        71
C.     THE DISCOVERY STAGE

       The discovery stage is marked by research and discovery experiments which “are

meant to provide the inspirational spark that gives life to a new piece of knowledge or a

disruptive innovation—a spark that would otherwise not occur or occur at some unknown
time in the future” (Alberts & Hayes 2005, p. 78).

       The research purpose for the discovery stage was to assess current RTAT

data collection methods, organization and processes and to explore possible

alternatives that might better serve the RTAT organization.

       It was discovered early in this stage that the current MS Excel assessment solution

needed to be replaced (Beeson, 2013). At the conclusion of the discovery stage, RTAT

selected Android based smart phones and Open Data Kit (ODK) for form development
and integration and RTAT had a working prototype assessment form for experimentation

in the Investigative Stage. Actions taken during the Discovery Stage included:


       1.     Research and Literature Review

       Research related to the humanitarian assistance (HA) response community

(HARC), formal and informal organizations, as well as various related topics are included
in the literature review.


       2.     Enterprise Architecture Assessment

       The author conducted an enterprise architecture analysis of RTAT to understand

the organization, what it does, and what it wants to accomplish; this is outlined in greater

detail in Appendix B. Strategic outputs of the assessment include: RTAT mission
statement, stakeholder analysis, operating model, and operating model change

recommendations; the COE and its developed RTAT solution must meet the strategic

vision of this assessment. Stakeholders judge the success and failure of the RTAT

solution.






                                          72
       a.      Organization Mission

       The mission of RTAT is: “Conduct and promulgate baseline and post-disaster

Information Communication Technology (ICT) infrastructure assessments, in order to

facilitate host nation and international disaster relief efforts” (Steckler, 2012). “Facilitate”
includes the management and dissemination of a shared common operational picture and

ICT recovery prioritization recommendations (Beeson, 2013).


       b.      Stakeholder Analysis

       Several stakeholder organizations are associated with RTAT which include:

United Nations (UN) Emergency Telecommunication Cluster (UN-ETC), Association of
Southeast Asian Nations (ASEAN), Pacific Disaster Center (PDC), U.S. Pacific

Command, the Naval Postgraduate School, as well as other HA governmental and non-

governmental organizations (NGO) (Steckler, 2012).

       c.      Operating Model Analysis


       As discussed in the literature review, RTAT must move from a Diversification
model with low process integration between teams and with stakeholders and low

standardization of processes to a Replication operating model with integration minimally

with Pacific Disaster Center and standard processes enabled by an enterprise wide mobile

data collection tool solution. Figure 15 is an adaption from Ross, Weill, & Robertson’s
(2006) Enterprise Architecture as a Strategy “characteristics of four operating models”

and graphical depicts this proposed change (p. 29). Further explanation can be found in

Appendix B.














                                           73
            Figure 15.   RTAT Recommended Change in Operating Model
                                   (after Ross et al., 2006, p. 29)



       d.      Enterprise Architecture Assessment Conclusions and Recommendations

       Standardizing and refining the assessment forms in a query-able format is the top
priority of the RTAT architects.   This enables the forms to be programmed into an

envisioned mobile data collection tool solution (Beeson, 2013).

       Concurrently, RTAT must integrate their data with Pacific Disaster Center to take

advantage of their DisasterAWARE website information dissemination capabilities.

       Team training, and standardization of “go-kits” is a low priority recommendation,

but some training must be conducted to test the various versions of the RTAT assessment

tool for validity and refinement (Beeson, 2013).

       3.      Mobile Data Collection Tool Prototyping


       It was concluded early in the project that concluded that a MS Excel form
modification would not meet the requirements of the enterprise architecture assessment

recommendations nor leverage available technologies, see Appendix B for greater


                                           74
explanation. As a consequence, the author created individual ODK Collect prototype

assessment forms for each ICT category, based on Appendices G–J. The author used an
iterative, spiral development strategy to prototype and refine these assessment forms. The

author used the spiral development strategy because not all requirements for the form and

mobile data collection tool were established from the outset; and new requirements

emerged through the course of the COE (Hawthorne & Lush, 2002). Further, a spiral
development strategy provides,

       [t]he opportunity for interaction between the user, tester, and developer. In

       this process, the requirements are refined through experimentation and risk
       management, there is continuous feedback, and the user is provided the
       best possible capability within the increment. Each increment may include
       a number of spirals. (Hawthorne & Lush, 2002)

       This strategy forces interaction feedback early and often from RTAT teams. The

multiple prototype tests also lines up the multiple experiments required in the COE thesis

strategy. See Figure 16 for a spiral development diagram, note the use of multiple
prototypes and tests to flesh out the requirements and develop the product. Prototype 1, 2,

etc. of Figure 16 could be replaced by the various experiments (Legazpi City, Typhoon

Haiyan, etc.) for RTAT tool development, or the various RTAT form versions (e.g.,

rtat_mobile_assessment_philippines_130924, v4, v5) found in the “RTAT Assessment
Form Development” section below.





















                                           75
          Figure 16.  Spiral Development Diagram (from Osmundson, 2014)


       The “Mobile Data Collection Tool Analysis of Alternatives” and “RTAT

Assessment Form Development” sections found below give more information on form

development. The lack of RTAT development funding was the single most significant
factor for tool development (B. Steckler, personal communication, September 3, 2014).

According to Brian Steckler, RTAT plans to charge zero dollars for its services hence any

solution must be free to develop and operate (personal communication, September 3,
2014).


       4.     Mobile Data Collection Tool Analysis of Alternatives

       The author attempted to modify the then current MS Excel RTAT assessment

form spreadsheets found in Appendices G-J to improve their practicality.    Practicality

                                          76
here means a intuitive, user friendly interface and data is integrated easily into the larger

humanitarian assistance community. All initial attempts failed to convert the Appendices
G-J into a relational database.


       This initial failure suggested that current processes were outdated, and that
the campaign of experimentation should move to the hypothesis testing phase, to

investigate whether a mobile device (i.e., a smart phone) solution might prove better.

       In response, the author spoke informally with several people working with various

aspects of disaster response, researched via the various academic papers, Internet search

engines related to disaster response and smart device application (app) stores for potential
alternatives to MS Excel. Various applications were downloaded from the app stores for

first impressions. The analysis of alternatives included the use of the HumanitariaN

Operations Mobile Acquisition of Data (NOMAD) Online Selection Assistant (OSA) to

both expand and narrow the field of potential mobile data collection tool options. More
information on NOMAD and the OSA can be found in Appendices E, F and Jung’s

(2011) Mobile Data Collection Systems a review of the current state of the field research

report. NOMAD and the analysis of application alternatives will be explored later.


       a.      Requirements

       Rapid Information and Communication Technology (ICT) Assessment Team
(RTAT) mobile data collection requirements include: Small form factor, rugged, free to

develop and use (no formal budget), ability to make and tailor forms quickly and easily,

KML file exportable (importable into a database), GPS location enabled, ability to

include photos, skip logic pattern supportable (able to ask/skip questions based previous
answers), not restricted to SMS (text message) only, and ability for forms to be stored on

the device until they can be opportunistically (when data connection is available)

uploaded.








                                           77
       b.     Selected Operating System: Android

       After analyzing the capabilities and limitations of the various operating systems,

Google’s Android OS was down-selected during the analysis of alternatives for

development for several reasons.
                     Compatible device availability: Availability of free Android
                      based Samsung S2/S3 and Google Nexus devices within Naval

                      Postgraduate School’s Hastily Formed Network (HFN) Lab and
                      Common Operational Research Environment (CORE) lab enabled
                      form development and testing. This was the single largest factor in
                      this decision.

                     Support: Recent research, development and use of the CORE
                      Lab’s Android based data collection tool Lighthouse enabled better
                      direct development support.

                     Android market share: Research in the literature review found
                      that Android’s market share has been steadily increasing over iOS
                      in recent years especially in developing countries where the tool
                      would most likely be employed.

                     Free data collection platforms: Availability of free mobile data
                      collection platforms such as ODK Collect and Lighthouse enabled
                      form development without financial cost. Cost was the single
                      largest deciding factor.


       c.     Selecting the Data Collection Application: Lighthouse and ODK Collect

       The researcher utilized the HumanitariaN Operations Mobile Acquisition of Data
(NOMAD) OSA feature to help narrow the field of options. NOMAD (n.d.-a and b)

contained a detailed list of questions asked, results, and detailed comparison between

valid application options. Three RTAT requirements narrowed the field to the final three

contenders: Open Data Kit, CyberTracker and Humanitarian Data Toolkit (HDT).
                     Android operating system: The operating system requirement

                      narrowed the field to 33 options.This requirement stems from the
                      availability of Android operating systems available and the lack of
                      project funding to purchase unlocked iPhones.

                     Keyhole Markup Language (KML) export file format: This
                      requirement narrowed the field to 10 options. KML is one of the
                      file formats supported by the Pacific Disaster Center (PDC) to
                      import spatial data sets for display on its DisasterAWARE website
                      (personal communication with T. Bosse August 11, 2014). This
                      website link is critical for the attainment of RTAT’s end state goal

                                          78
                      of a shared ICT environment situational understanding via a
                      current operational picture (COP) visual display.

                     Cost (free): As discussed in the previous section, this requirement
                      narrowed the field to the final three (B. Steckler, personal
                      communication, September 3, 2014).

       As discussed in the literature review, the Humanitarian Data Toolkit was not
selected due to its form factor size, which includes a laptop, scanner and printer, as well

as a handheld device, see Figure 14.

       CyberTracker did not have the community support comparable to ODK Collect

and the forms that could be created could not incorporate the complex skip logic of “If

any ___ selected then ___” or “If any but ___ selected then___.” CyberTracker has form

development support available for purchase, but this violated the third requirement
above.


       As a consequence the Open Data Kit suite of tools was down selected for RTAT
form and mobile data collection tool development and will be further discussed below.


       The discovery stage hypothesis became: an ODK based solution would better
meet the needs of RTAT and would enable desired organizational operating model

changes.


       5.      RTAT Assessment Form Development

       To test the above hypothesis, ODK forms needed to be created.

       RTAT forms are intended to be the principle data collection drivers of a data to

situational understanding processing chain. The aim of this RTAT assessment interjection
chain is to help the humanitarian aid leaders make better decisions. As Kennerly &

Mason (2008) aptly stated,

       If decision making is to be informed by information (the developed RTAT

       forms) then clearly it is important what data is available (collected). Not
       only does the availability of data enable a decision to be made, but in
       many circumstances data can indicate when a decision needs to be made.

       Therefore it is vitally important to get the RTAT assessment/data collection forms

correct.

                                           79
       As stated earlier, RTAT originally used an excel spreadsheet to conduct its ICT

assessments. This method relied heavily upon assessor’s professional judgment to assess
non-standardized markings within the MS Excel spreadsheet (Steckler, 2009 and 2012).

Email was the original method to disseminate findings (Steckler, 2009 and 2012). These

spreadsheets were not set up with established entity or attribute relationships and

therefore could not be exported into a query-able, relational database. According to
OCHA (2014a), unstructured Excel documents are poor means of communicating time

critical humanitarian assistance data due to their poor ability to be imported into shared

databases. Further, email is a poor means of data transfer due to its singular point-to-point
characteristic in an environment with frequent personnel turnover (Office for the

Coordination of Humanitarian Affairs 2014a). This research supports the original and

refined discovery stage hypotheses.

       Additionally, RTAT looks to add spatial data attribute information to the

assessment so that it can be imported into an easily understood current operational picture

(COP) map that could then be rapidly promulgated to HA responders (Beeson, 2013).
ODK collected data can be exported into a myriad of formats (to include KML) and

could therefore be importable into Pacific Disaster Center (Open Data Kit Collect, 2014).

       A quick re-examination of currently available ICT centered disaster response

surveys in the Literature Review found that the various reputable,           established

humanitarian assistance organizations, specifically USAID, UNDAC, UN-ETC,
FITTEST and WFP, all lack the level of fidelity desired by RTAT and its customers

(Steckler 2009, 2012). This confirms RTAT’s pre-thesis decision to develop its own ICT

centric disaster assessment forms based on what forms were available at the time and

what the collective RTAT member disaster experience deemed necessary (Steckler,
2012). The original RTAT MS Excel forms can be found in Appendices G-J.


       As stated earlier, the author iteratively refined the RTAT forms were during each
campaign stage. For simplicity, a discussion on all form changes are consolidated in this

section. More information on the various stage events can be found in their respective

sections.


                                           80
       The RTAT forms were built using the Open Data Kit (ODK) XLS2XForm, the

resultant EXtensible Markup Language (XML) forms were then compiled and checked

for errors with ODK Validate. Forms were uploaded to the Naval Postgraduate School’s

(NPS) Common Operational Research Environment (CORE) lab managed ODK

Aggregate server via ODK Form Uploader. Once on the CORE lab’s ODK Aggregate
server, the forms could be accessed (downloaded), filled out and submitted via ODK

Collect or the CORE Lab’s Lighthouse application on an Android based smart device.


       One initial thought by the author was to make a separate form for each of the
services outlined in Appendices G-J, but the researcher quickly realized that one form

could be created that included all of the services with the use of ODK’s skip logic

feature. Table 1 is the breakdown of “service” and “sub-service” as discussed throughout

the rest of this section. They were taken directly from Appendices G-J.



                    Service                   Sub-Service
               Electrical Power  Generator

                                 Copper Line
                  Terrestrial    Fiber Optic
                                 DSL
                   Services      Cable

                                 T-1
                                 Voice
               Cellular Services Data-2g-4g Long-TermEvolution (LTE)

                                 Text/Short Messaging Service (SMS)
                Wi-Fi (Wireless Voice

                   Fidelity)     Data
                                 Voice
                   Satellite
                                 Data
                                 UHF

                    Radio        VHF
                                 HF
                                 Television
                  Broadcast
                                 Radio

            Table 3.    Service and Sub-Service Table(after Appendix D-G)



                                         81
       The major form capabilities/changes are outlined below using the following time

periods: beginning through Legazpi City field experiment, post Legazpi City through
Typhoon Haiyan (October-December 2013), post Typhoon Haiyan to today (December

2013-September 2014). Names of the forms are taken verbatim from the ODKAggregate

website. All forms were tested using Samsung S2/S4 and the Google Nexus smart phones

using Lighthouse exclusively until RTAT Assessment v4. Thereafter forms were tested
with both Lighthouse and ODK Collect.


       a.      Beginning Through Legazpi City Field Experiment

       This period of form development starts at the author’s involvement in the RTAT

mobile data collection tool development and ends at the conclusion of the Legazpi City

field experiment. This occurred from the beginning of June 2013 through the first week
of October 2013.


       (1)     rtat_mobile_assessment_philippines_130924

       This represents the first ODK form tested within the campaign of

experimentation. This version was used to show the art of the possible to RTAT members

at the Legazpi City experiment. Capabilities included: GPS location, video and audio
recording as well as standard form questions and simple skip logic. Below is a more

detailed review of the form version. User feedback was taken from RTAT conducted

after action discussions and a nightly “hot wash” meeting attended by the author

(author’s notes, available upon request).

       rtat_mobile_assessment_philippines_130924 had one overall status marked for
the location (working, operational with some degradation, highly degraded, broken, or

disconnected from the infrastructure/electrical grid etc.) (see Figure 17). Unfortunately,

only one service could be assessed per form, i.e. a tower with both cellular and broadcast

television service antennae would require two separate assessments.







                                           82
               Figure 17.   RTAT Assessment Status Option Screenshot


       Other specific questions included: type of assessment (training, test, baseline, post

disaster), GPS or fill in location (Latitude / Longitude or Military Grid Reference

System), lengthy address and point of contact telephone number input. Lengthy status
drop down menus were included throughout the assessment. Examples include required,

primary, secondary, and tertiary “Cause of the issue?,” “What’s the issue?,” and “What’s

needed to fix” the issue options. There were very detailed power questions (voltage,

cycle, phase, Hz, etc.), to include what power assets are on location (generator, inverter,
uninterrupted power supply, etc.). Towards the end of the form there was the option to

take a photo or record a video audio message.

       The following issues/recommended fixes were taken from the author’s after

action notes generated during the various hot washes and can be furnished upon request.




                                           83
       All questions in the form were required; the assessor had to mark an input before

moving on to the next question. So if volcano ash were the only cause of the damage at
the location, the assessor still had to pause to mark secondary and tertiary damage causes.

While minor, this issue was multiplied throughout the form and led to user irritation

especially if conducting the assessment in a rain storm.

       Another take away was that too many unknowable or impertinent questions to the

environment were required. For example, the phase of alternating electrical power (1, 2
or 3) would need to be answered for an assessment on a broken UHF antenna. Many of

the first responder volunteers had no idea what phase meant and guessed or left it as

unknown. The RTAT subject matter experts (SME) dropped this and many other

irrelevant questions in later form versions.

       While the lengthy drop down menus worked well for a relational database (i.e.

Tell me all the locations assessed with “2 phase power”), they did not work well for
speed if the question did not fit the situation. One could argue that many of these

irrelevant questions did not add value to the assessment or meet the RTAT assessment

intent. Additionally, many of the questions were deemed redundant, such as the primary,

secondary, and tertiary causes of damage.

       The point of contact telephone number stopped short of the required number of
digits for a foreign telephone number, i.e. the integer field was too small.     Finally,

assessments with video recordings could not be uploaded to the NPS servers. It was

concluded at the time that the size of the assessment file was too big to be accepted by the

CORE Lab’s ODK Aggregate server. However, this was not investigated further. The
option to take multiple photos was deemed acceptable by the RTAT SMEs.


       The overall impression is that the assessment was too slow and cumbersome.
Additionally, users wanted the ability to “slew” their GPS location on a map to an

“unreachable” location; unfortunately, this is not possible with ODK. Users wanted the

ability to mark up the taken picture; unfortunately, this was not possible using
Lighthouse. Finally, users wanted the ability to assess more than one service at the

location.


                                           84
       (2)     rtat_mobile_assessment_philippines_130925

       This is the same as rtat_mobile_assessment_philippines_130924, except the

number of required fields was reduced. Secondary and tertiary causes of issues and a

number of electrical data questions were made optional to speed up the process. Address
field was pre-filled to some extent for Legazpi City; telephone number was shortened and

prefilled for the Republic of the Philippines; the ability to record a video was dropped

and replaced with the ability to take multiple photos at a location. Users reemphasized
their desire to assess multiple services at one location. Testing of this form was

conducted largely in the rain with flash flooding throughout the area. As a consequence,

teams wanted the GPS and photo at the very beginning of the form so that the assessor

could take refuge while filling out the rest of the form or move on to the next assessment
location.


       b.      Post Legazpi City Through Typhoon Haiyan

       The goal of this stage was to create a form that could be crowdsourced by novice

users and assess multiple (sub-) services differently at one location. The time period

covers roughly the second week in November 2013 to the second week in December
2013 and is punctuated by the deployment of two RTAT waves to Tacloban City, the

Republic of the Philippines (ROP) in response to Typhoon Haiyan relief efforts.


       (1)     rtat_mobile_assessment_philippines_131110

       The assessor could only give one overall assessment for the location (Figure 17),

but the assessor could check multiple services and sub-services to be included in the
assessment (Table 1). There remained no ability to assess sub-services individually and

the one overarching assessment inferred the same status for the other services checked.

For example, this form could not state that cellular text was operational but that the radio

broadcast service was dead lined (see Figure 12). It could only state that something at the
site was assessed dead lined (see Figure 12) and that cellular text and broadcast radio

services were assessed. This leads to ambiguity and forces the assessor to make multiple

assessments at the location if more than one status exists at that location (see Figure 18
for a screenshot from the Lighthouse application). Electrical power was assessed

                                            85
separately from the rest of the form, however, and operational status of power could

differ from the services being assessed. This form marks a turning point in assessing
multiple services at a given location.































          Figure 18.   Lighthouse ICT Services Being Assessed at a Location


       Telephone number was changed from an integer variable input to a text variable

to skirt the input size limitation.

       This form replaced the primary, secondary, and tertiary causes of damage with

what’s the issue? and what’s needed to fix the issue? open text fields. While less
searchable via a data base program, this vastly simplified the form and reduced user

survey fatigue.





                                            86
       Unfortunately the photo remained at the end of the form, and, as stated, the ability

to give different operational status marks to the various sub-services was still not
programmed into this version.


       (2)     haiyan_rtat

       This    version    cleaned    up    some    typographical    errors   found    in

rtat_mobile_assessment_philippines_131110 and was used during initial testing in the

Republic of the Philippines (ROP) during the first RTAT wave while waiting for team
members to assemble in Manila. This form was not intended to be utilized by RTAT

volunteers; however, it was accidentally left on some of the phones during the turnover

between the first and second RTAT waves and was subsequently used by some of the

volunteers in the second wave to conduct assessments. This led to integration issues
during the data consolidation and submission to Pacific Disaster Center. (See section on

Typhoon Haiyan below for more detail.)


       (3)     Haiyan Post Disaster

       This change incorporated some of the requested changes that had not been

incorporated into previous versions. For example, the photo was finally placed at the very
beginning of the survey immediately followed by the location input (GPS).


       This form was created with the intention of using crowdsourcing techniques, and
the assessor name and point of contact info dropped due to privacy concerns.


       Figure 18 question remained with some notable improvements. Skip logic was
introduced into the form. The assessor was taken only to the services being assessed and

asked what is the operational status of the assessed services checked in the Figure 18

question. Subsequent radial menus were shown asking which subservices are working
and which  sub-services are not working.


       This change gave much better fidelity to the data collected. An end user could
now, for example, get the overall operational status of the cellular service and know that

voice and text sub-services were working, but that 2g, 3g, and 4g data sub-services were

not, see Figure 19.

                                           87
                         Figure 19.   Terrestrial Sub-Services


       The form was a little disjointed, reflective of the rapid revision done on location
at the Typhoon Haiyan relief effort staging area, but an assessor could now assess

multiple (sub-) services differently at a given location. The form was a success during

Typhoon Haiyan’s demonstration testing and validated that RTAT could crowdsource a

simplified RTAT assessment form utilizing ODK tools and the CORE lab’s Lighthouse
application. Further, this form was the first to be integrated into PDC’s DisasterAWARE

situational awareness web portal. (See Figure 20 for results.)





















                                           88
             Figure 20.  Screenshot of DisasterAWARE with RTAT Data

                                (from Pacific Disaster Center, n.d.)


       c.      Post Typhoon Haiyan to Today

       Unfortunately, getting Haiyan Post Disaster assessment form data onto the

DisasterAWARE web portal was an arduous and manually intensive task that took more
than one week to accomplish. Further, the results on the DisasterAWARE web portal do

not intuitively convey assessment information with the default turquoise assessment icons

(see Figure 20). Therefore, the goal during this stage was to finalize the form so PDC
could integrate data seamlessly into DisasterAWARE. Additionally, subsequent forms

were needed to create a single data field that PDC could use in order to match an

appropriate icon to the assessment data (i.e. green icon for a working ICT service). This

would intuitively convey information and situational understanding to stakeholders and
end-customers visiting the DisasterAWARE web portal.


       (1)     RTAT Assessment v4

       This form smoothed out the “disjointedness” of the Haiyan Post Disaster form

and marked a naming convention turning point for the RTAT forms.



                                           89
       Unfortunately, an assessor must go through each subservice regardless of if there

is an issue or not. For example, radio ICT service is marked operational, but the user has
to manually check which radio sub-services are working and which subservices are not

working.


       (2)    RTAT Assessment v5

       RTAT Assessment v5 incorporated logic to avoid non-assessed subservices and

fixed issues of a single attribute for each service versus an attribute assigned for each
sub-service. This effort required an extensive XML code rewrite and surpassed the

capabilities of ODK XLS_Form builder.


       (3)    RTAT Assessment v6

       This version of the form (v6) is the current RTAT ICT assessment form at the

time of thesis completion. It includes a type of location added for PDC and other
cosmetic fixes from v5. V6 is the last form anticipated to be built on the ODK platform.

Subsequent forms will be built using HTML5 so that they can be utilized on any smart

device regardless of operating system. Below is an outline of the form. A full training

brief is included in Appendix I.
             Admin info

                     Assessor info

                     Location point of contact info

                     Location info (GPS)
             Photo (multiple)

             What services are you assessing?

             Electrical, Terrestrial, Cellular, Satellite, and or Radio service:
                     “What’s working?”

                     “What’s broken?”

                     “What’s needed to fix the issue?”

             Final remarks




                                          90
D.     HYPOTHESIS TESTING

       This stage in the campaign is marked by hypothesis testing. Hypothesis testing

experiments “build on explanatory knowledge to create predictive knowledge” (Alberts

& Hayes 2005, p. 81).

       1.     Legazpi City Field Experiment

       From above, the hypothesis for the Legazpi City field experiment was that:

an ODK based solution would better meet the needs of RTAT and would enable

desired organizational operating model changes. Further, people with first

responder or ICT backgrounds could be quickly trained to conduct RTAT
assessments using the ODK/Lighthouse solution.


       RTAT conducted baseline ICT infrastructure assessments in Legazpi City,
Republic of the Philippines (ROP) from 21–26 September 2014 (Chang, 2013), ironically

just a couple of months before Typhoon Haiyan. Utilizing the created mobile data

collection tool from the Discovery Stage, RTAT members from around the world met
with local leaders, volunteers, and first responders in an effort to conduct baseline

assessments throughout the Albay Province. As a note taker in attendance at the “hot

wash” feedback sessions at the end of each day, the author was able to rapidly prototype

form improvements for the next day’s use. According to DHS (n.d.) a hot wash is an
informal conversation where participants,


       [s]hare their perspectives on key strengths and areas for improvement. Hot
       washes are important because they mark the transition from actual
       exercise play to the evaluation phase where lessons learned and corrective
       actions are documented. It is important to conduct the hot wash at the end
       of the exercise while all participants are still present and the day’s
       discussions are still fresh in their minds.

       The result from the Legazpi City experiment was a validated mobile data

collection tool that consisted of a vastly improved ODK based assessment form,

Lighthouse electronic form interface application and Android powered Samsung S3 and

S4 phones.



                                         91
       This field experiment confirmed that an ODK based mobile data collection

solution would better meet the needs of RTAT and was a step forward in the desired
organizational operating model changes. See Chapter IV for the analysis supporting this

conclusion.

       According to the official Marine Forces Pacific official Quick Look after action

report (Appndix L), the Legazpi City experiment effort consisted of,

       81 participants coming from 42 different entities including those from
       academia, industry, UN, NGO, U.S. government, military, and law

       enforcement as well as Philippines national, regional and local leadership
       and other government agencies. U.S. participants made up about 20
       percent of the people for the overall event. Defense related entities (U.S.
       and Philippines) made up about 15 percent of the total entities.

       Fewer than 10 of the participants were RTAT members, most of whom had not

seen the created RTAT assessment forms or the Lighthouse application prior to the event.


       a.      Format

       Formal training with practical application was followed by real world use of the
tool and a feedback session at the conclusion of each day.


       Iterative changes were made to the form and tested the following day. This pattern
was repeated for September 24, 25, and 26. Feedback from the 26 was included in the

Typhoon Haiyan version.



















                                           92
      Figure 21.   Legazpi City Hot Wash Feedback Session (from Chang, 2013)


       b.      Training

       The author trained the RTAT members and volunteers in a “hands on” classroom

setting using a brief similar to Appendix I followed by a practical application session.

Participants were able to download the Lighthouse application on their own Android
compatible device or use a RTAT team provided Samsung S3 phone for the hands on

practical application session.


       c.      Execution

       Participants were divided into 6 vans and assigned to a specific Barangay (local

governance  district) to meet the local leadership (Barangay Captain) and conduct baseline
ICT assessments in the area. Barangay is “the smallest administrative division in the

Philippines and is the native Filipino term for a village, district or ward...the term often

refers to an inner city neighborhood, a suburb or a suburban neighborhood” (Barangay,
2014). Teams taught the Barangay Captains how to use the tool, and the teams along with

the local leadership conducted as many ICT assessments of the local areas as time

permitted.

       Teams were given a Broadband Global Area Network (BGAN) satellite system to

get access to the live Internet to upload the forms to the ODK Aggregate server in real

time. Team leads were given unlocked Samsung S4 phones and furnished with local
                                           93
subscriber identity module (SIM) cards with unlimited data for the duration of the

exercise and other team members could alternatively upload the forms by tethering a
connection to the Internet via the S4 device.


       d.      Results

       The following is a synopsis of what worked and what did not work during the

exercise. The results were taken from hot wash after action discussions and from the after

action report found in Appendix J (Chang, 2013).

       (1)     What Worked?


       This experiment validated the RTAT mobile data collection tool which consisted
of an Android device, ODK forms, and Lighthouse application. Specific form questions

were debated, but the utility and convenience of the Lighthouse applications were

undeniable.

       The experiment validated a train-the-trainer model. Participants were able to teach

local volunteers how to use the device with less than two hours of training. Users were
able to easily and intuitively navigate the form. Even Barangay Captains with no

experience with touch screen devices were able to quickly pick up on how to use the tool

with just one use of the device. It was discovered that RTAT Subject Matter Experts

(SMEs) could train others to conduct the assessments, thus multiplying their capability.

       RTAT teams were able to use the forms to successfully conduct scores of RTAT

ICT assessments in various field conditions. These forms could be uploaded to the NPS
CORE Lab ODK Aggregate server in Monterey, CA via BGAN satellite communication

or via the local Smart Communications (cellular service provider). Figure 22 is a display

of one of the day’s data collection results.










                                           94
Figure 22.   ODK Aggregate Map Visualization for Surveys Conducted September 24,
                                     2013 (from Chang, 2013)


       (2)     What Did Not Work?

       While the ODK Aggregate map function did a good job of indicating an

assessment was conducted, the end user could not easily read the assessment values in

this view. Further variable names and values were not always intuitively understandable.

Care was taken in the creation of variable names; questions often remained, however, if
only looking at the ODK Aggregate output. A database should be created that can import

the data and export standardized reports. Figure 23 is a screen shot from ODK Aggregate

and is a typical example of the clutter created when attempting to read the data in the map

view.

       About half of the participants that had smart phones were utilizing the Android

operating system. Those with Android phones preferred using the provided Samsung SIIs
versus downloading the Lighthouse application. Participants had concerns with

downloading an application that was created by the U.S. government or military, i.e. the

Naval Postgraduate School, given the then recent U.S. National Security Agency scandal
that had come to light that summer (The Guardian, 2014).








                                           95
                Figure 23.   Data clutter in ODK Aggregate Map View


       Assessments with videos could not be uploaded to the CORE lab’s ODK

Aggregate server. The option to take a video was dropped in favor of adding the ability to

take multiple pictures.

       Due to the modifications to the forms between each day of testing, the collected

data could not be integrated together easily and efforts to create a master spreadsheet

were abandoned early. This brought to light a shortcoming in the ODK Aggregate server:
even a simple typographical correction in a form required a new form to be uploaded to

the server. This required a name change to the new form or the deletion of the corrected

form and all of its associated data. One can download the data into a spreadsheet or
import the data into a database and manually join the data sets.


       Users wanted the ability to draw on the photo once taken. ODK supports such a
function, but, due to circumstances discussed in the literature review, the CORE Lab had

not updated its Lighthouse application to take advantage of the newer capabilities.

       Users wanted the ability to “slew” their location on a map, that is, move their

position electronically vice accepting the current GPS position. Rationale: What if the

assessor cannot be at the actual location to use the device’s GPS due to safety or security

issues. This cannot be done currently in ODK and may require the use of the Internet for


                                            96
mapping or third party offline map that can be preloaded on the phone before

deployment. There is no budget to fund this requirement and was deemed a “nice to
have” by the group.


       Some questions were deemed non-value added and dropped to shorten and
simplify the form. One example of this simplification was in the electrical power

assessment. Initial questions taken directly from Appendix G such as 110 volts or 220

volts, Direct versus Alternating current, 2 versus 3 phase and 60Hz versus XX cycle were
not understood by many of the first responder volunteers, were unknown by many of the

points of contacts we met on location, and were dropped by consensus of the RTAT

members. Even staunch supporters of the original questions conceded that what was

important was the status of the power (working, not working or intermittent) and do they
have a generator with fuel on hand.


       The feedback trend was a desire to get at what was really important for each
service being assessed. Did it work? If not, what did you need to fix it?


       Additionally, users wanted one form to assess multiple services at one location.
The tested form required the user to create a separate assessment form for each service at

the location. Unfortunately, this form was too complicated and could not be built/tested

during the experiment.

       2.      Savvion Process Model


       By standardizing the forms and assessment procedures during the Legazpi City
field experiment RTAT demonstrated that it could teach others to conduct RTAT

assessments, thus multiplying its reach and speed. Savvion was utilized to quantify how

much utility there was in the train-the-trainer/crowdsource concept of operations.

Specifically, Savvion was utilized to test a crowdsourcing model that was modestly
demonstrated during the Legazpi City field experiment.

       This experiment attempts to quantify the value of the change in the operating

model proposed in the enterprise architecture assessment (Beeson, 2013), specifically the




                                           97
need to increase organizational agility as enabled by an increase in process

standardization and process integration.

       a.     The Hypothesis for the Savvion Experiments: Utilizing Crowdsourcing

              Techniques Will Reduce RTAT Costs While Decreasing the Time to
              Complete ICT assessments.Experiment Format

       Three Naval Postgraduate School students collaborated to create a model of

RTAT’s business processes; see Figure 24 for a visual representation and Appendix C for
complete details. These processes were then modified and tested through Savvion

simulations.

       The team analyzed the processes from a pre-Legazpi City experiment

organizational structure (Excel, email and subject matter expert assessor only) to a post-

Legazpi City model that leverages the mobile data collection tool, satellite
communication, and automated/integrated backend servers for aggregation and

dissemination as well as incorporating a crowdsource train-the-trainer assessor model.

       Figure 24 shows the “As Is” model that utilizes a hierarchical process that utilizes

three deployed subject matter expert assessors, the MS Excel spreadsheet that relies upon

manual inputs, and manual dissemination via email to the UN-ETC for manual review,

processing and dissemination. This model only utilizes subject matter expert assessors
(pre-Legazpi City experiment model).

       The “As Is” model, Figure 24, requires the RTAT assessment (supply) to go

through an intermediary (UN-ETC) before dissemination to the end customer (NGO

organization). This intermediary must conduct quality assurance and locate the

assessment on a map. This assumes higher man hours in assessment form processing,
quality assurance, location services and rework than an electronically automated and

integrated process.









                                          98
  Figure 24.  RTAT “As Is” Process Model (from Beeson et al., 2014)


b.     Process Changes

Figure 25 shows the highlighted process changes tested with Savvion.






















Figure 25.   Savvion RTAT Process Changes (after Beeson et al., 2014)


                                   99
       The front end cost of crowdsourcing is shown with the training of volunteers. The

RTAT tool’s integrated GPS ensures valid location, thus cutting out the UN mapper. The
nature of its electronic form interface ensures compliant spatial data values and facilitates

the link with Pacific Disaster Center (PDC). Display/link on PDC’s DisasterAWARE

(Figure 20) closes the data to situation awareness (current operational picture) process

and ensures that assessments can be delivered in a timely manner to those who need it.

       In the “To Be” model the information management exchange model changes from
a push or pull model reliant upon point-to-point social contacts (email) in the “As Is”

process to a bulletin board posting or, in this case, current operational picture format

model. This is in line with literature review information flow improvement

recommendations (Donahue & Tuohy, 2006) (Kennerly & Mason, 2005), and (Office for
the Coordination of Humanitarian Affairs, 2014a).


       Figure 26 shows the “To Be” model that was tested with Savvion. The reader will
note the expansive number of assessment teams that the crowdsourced model enables.

The Legazpi City experiment showed that both SME and non-SME volunteers could

conduct the assessment if given a proper mobile data collection tool and two hours of

training.























                                          100
        Figure 26.   Savvion “To Be” Process Model (from Beeson et al., 2014)


       c.      Conclusion and Results

       Despite doubling the number of assessments from 100 to 200, the “To Be” model

expenses dropped 87 percent, wait time decreased by 81 percent,      and it took only 20

percent of the time to complete the 200 surveys (Beeson et al., 2014). Unfortunately,

Assessor utilization remained high (89 percent) but is unlikely to be significantly reduced
due to the nature of the disaster scenario (Beeson et al., 2014).


       When stretched further, the “To Be” model shows over 500 assessments are
possible in approximately 1/3 the time required in the original “As Is” model, while still

reducing costs by nearly 70 percent (Beeson et al., 2014). Figure 27 shows a detailed

metric comparison of the “As Is” and “To Be” models.










                                           101
       Figure 27.   Savvion As Is-To Be Comparison (from Beeson et al., 2014)


       Critical changes included nine satellite BGAN terminals and the train-the-
trainer/crowdsourcing model, which both greatly improved speed and efficiency. The

addition of the user-friendly but highly (data) structured mobile data collection tool

application eliminated the redundant internal mapping and quality assurance cycles and

enabled the use of crowdsourcing/volunteers; both of which drastically improved the
RTAT performance and capability.


       Quality of assessments was addressed in this experiment through the reduction of
rework of GPS coordinates and incorrect data values. The quality of the qualitative

assessment remarks from a crowdsourced novice versus a seasoned information

technology (IT) professional, however, could not be gauged. A “good enough”
philosophy was incorporated in this model in that even a novice can tell if something is

broken, even if all they can say is “a service call” or “new tower is needed” to fix the

issue, see Figure 27 for an example.











                                          102
Figure 28.   Broken Broadcast Tower Assessed by RTAT in the Aftermath of Typhoon
                                             Haiyan


       Savvion team concluded that RTAT should switch to a train-the-trainer model and

crowdsource assessments to significantly reduce costs and wait times.e crowdsourcing
solution, however, must be simple enough for the layperson to use while providing the

fidelity and quality required of stakeholders and end customers.


E.     DEMONSTRATION STAGE

       RTAT proved that the ODK/Lighthouse mobile data collection tool was a better

solution in Legazpi City, and Savvion lab tests validated the crowdsourcing model.
RTAT needed to demonstrate the train-the-trainer model and prove that its data could be

displayed on PDC’s DisasterAWARE current operational web portal in a field exerciseor

real world humanitarian event.


       1.     Typhoon Haiyan Deployment

       On November 8, 2013 Typhoon Haiyan (known locally as Yolanda) made landfall

in the central Philippines (see Figure 29) at nearly 200 miles per hour, making it one of
the strongest storms ever recorded (United States Agency for International Development,



                                         103
2014a). As a result, 6,300 people lost their lives, 1.1 million homes were damaged or

destroyed and 4.1 million people were internally displaced (United States Agency for
International Development, 2014b).






























      Figure 29.   USAID Typhoon Haiyan Effected Area (from USAID 2014b)


       In response, RTAT members deployed to the Republic of the Philippines (ROP)

to conduct information and communication technology assessments to aid in the relief

efforts and validate the RTAT concept (Steckler, 2013).

       Specifically, RTAT tested and validated the hypothesis that RTAT assessments

could be crowdsourced using locally acquired volunteers in or near a disaster zone and
those assessments could be posted to the DisasterAWARE web portal.






                                          104
       a.     Concept of Operations

       RTAT sent two waves of teams to conduct assessments within the

Tacloban/Borongan City areas in the Leyte Province (Steckler, 2013). The first wave was

led by the Roddenberry Foundation with the assistance of the author (Steckler, 2013).
The second wave was led by this thesis’s advisor, and program development leader, Mr.

Brian Steckler. Both waves were greatly assisted by Bicol University, Team Patola non-

governmental organization (NGO) and various local volunteers as well as the support of
both the Armed Forces of the Philippines (AFP) and Philippine National Police (PNP)

(Steckler, 2013).

       The first wave consisted of three NGOs from the Roddenberry foundation, four

faculty members from Bicol University that helped with the Legazpi City experiment, as

well as the author. Volunteers were obtained through previously established contacts

within the Philippine Armed Forces, Philippine National Police and the Team Patola
NGO.

       The second wave consisted of the aforementioned Mr. Steckler, one other student

from the Naval Postgraduate School, three faculty members from Bicol University as

well as several local volunteers (Steckler, 2013).


       b.     Execution

       The author trained the Roddenberry members and volunteers in Cebu City (Figure

30), and the Philippine Armed forces provided logistical support to/from Tacloban City
the following day. Team Patola provided a volunteer and logistical support within

Tacloban City and the greater Leyte Province.













                                          105
             Figure 30.  Author Conducting RTAT Training in Cebu City


       In Tacloban, the teams contacted the UN established relief center and provided

ICT assessment support as requested, see Figure 31.



















                 Figure 31.   First Wave Confers with Local Officials


       The first wave then broke up into three teams before heading out to conduct

RTAT assessments in the local area, see Figure 32.








                                          106
           Figure 32.   RTAT Conducting an Assessment in Tacloban City


       c.     Results

       RTAT successfully conducted 40     assessments using the Lighthouse application

and the ODK forms. Assessments were successfully uploaded in the disaster zone to the
NPS CORE Lab ODK Aggregate servers located in Monterey, CA. Assessments were

then manually transmitted and displayed on the DisasterAWARE web portal.

       RTAT was able to obtain over 40 volunteers for the first wave alone and had to

turn volunteers away due to logistical constraints. The sheer numbers of volunteers

obtained within a 24 hour period validated Savvion process assumptions for the number
of RTAT teams that could be created in a disaster zone. Further, the ability of volunteers

to conduct and upload assessments validated the hypothesis that RTAT could

crowdsource the RTAT assessments with the Lighthouse application and ODK forms.

       Results from the two waves were taken from the author’s notes during the

operation’s hot wash and are available upon request.






                                          107
       (1)    What Worked

       The two waves conducted 40 assessments in the Leyte Province in support of

Typhoon Haiyan “real-world” relief operations. Figure 33 shows the results of the effort

on the ODK Aggregate mapping view.




















            Figure 33.  Typhoon Haiyan Mission ODK Aggregate Results


       Local knowledge and language skills were a must. The local volunteers were

invaluable for both the safety of team members and for communicating with local points
of contact while conducting assessments.

       Crowdsourcing works. Simplifying the forms enables volunteers with no IT

experience to contribute to the RTAT effort; volunteers had to be turned away due to

logistical constraints. RTAT was able to field more teams using simple Crowdseeding

techniques discussed in the literature review.

       The developed operating model from Savvion was validated, albeit at a smaller

scale due to logistical constraints. The author obtained and trained 40+ volunteers within
24 hours of arrival in Cebu City. Three RTAT subject matter experts and four Bicol

University faculty members, who trained during the Legazpi City experiment, deployed

along with seven local volunteers to Tacloban City. RTAT was able to deploy three

separate teams on the first wave; only one team would have been supportable without the
                                          108
volunteers. Additionally the teams could have been made into four teams if more

transportation assets within the disaster area could have been obtained.

       (2)    What Did Not Work


       Two conflicting RTAT forms were utilized during the second wave. The form
was improved between the first and second RTAT waves; unfortunately, an in person

turnover could not be accomplished due to length of flights to/from the United States. As

a result, some of the devices had the older version of the form and some were utilizing
the new form. This became problematic when it came time to integrate the data with

PDC. PDC needed one set of variables, thus the two forms needed to be manually

collated into one data set. This process was delayed for days due to the communication

issues and competing priorities of RTAT and PDC.

       Integration with Pacific Disaster Center had to be accomplished manually. Late

form changes precluded the use of any automated import functions developed during
earlier PDC/RTAT team interactions. As a result, data was manually exported from ODK

Aggregate into an MS Excel spreadsheet and emailed to a point of contact at PDC for

import and display on their DisasterAWARE website (see Figure 34). Timeliness of data
posting was not an acceptable (near) real time posting for decision making use during the

disaster, but it did prove that the hypothesis was correct. Crowdsourced assessments

could be displayed on the PDC DisasterAWARE web portal.



















                                          109
              Figure 34.   DisasterAWARE Screenshot with RTAT Data


       This operation’s success ( Figure 34 germane) brought to light some shortfalls
with the current RTAT assessment display. The icon for an assessment is set to a default

value on DisasterAWARE. The aqua blue box on DisasterAWARE that denotes an

assessment does not intuitively convey any information to a viewer. According to

personal communications with T. Bosse from PDC (December 12, 2013), icons can be
established and color coded based on a specific variable within the form. Unfortunately,

the change that fixed the Legazpi City requirement and enabled the assessment of several

sub-services versus a single assessment for the location did not support the requirements
of PDC. For example, under Cellular services, cellular data, text and voice were assessed

but there was not a single assessment value for the Cellular service itself nor one for that

assessment overall, i.e.something is “not working” at this location. The logic to allow a

single overarching assessment for the location while allowing for the independent
assessment of all (sub) services without creating a lengthy form was discovered to be a

challenge for ODK XLS2Form and required further experimentation to fix. More

information on this can be found within the form development section. Further discussion

on DisasterAware display improvements can be found in Chapter V.





                                          110
       While outside the scope of the Typhoon Haiyan demonstration experiment, it is

worth noting: RTAT did not make any attempt to integrate RTAT assessment data into
the larger UN-ETC current operational picture, Figure 35.




























         Figure 35.  UN-ETC Current Operational Picture (from WFP, 2013c)


       One other notable challenge during the experiment was that many of the

volunteers had no information technology background and did not know what to look for
to start conducting an assessment. An ad hoc class was conducted on various antennas

types and how electrical power is distributed. A small, weather resistant, quick reference

card should be developed to help volunteers identify ICT related infrastructure.

        Many of the participants, as in the Legazpi City experiment, had hesitation

downloading an application (Lighthouse) that was created by the U.S. government, albeit
the Naval Postgraduate School. Fortunately, the gravity of the situation and volunteer

desires to help prevailed.



                                          111
       Team member life support was an issue. Water purification methods were

brought, but no fresh water was available within the disaster area. Energy was also an
issue; charging stations were available and teams brought alternate power sources, but

time spent charging a phone was time spent not assessing. See Figure 36 for an example

of a charging station. Teams need a quick small form factor method of recharging RTAT

devices




















Figure 36.   RTAT Member at a Charging Station in Tacloban City (From Appendix K)


       2.      Joint Interagency Field Exercise 2014–4

       The purpose of the Joint Interagency Field Exercise (JIFX) is to,


       [p]rovide a field experimentation resource for the Unified Combatant
       Commands (COCOMs) and other federal agencies. In addition, State,
       local and international emergency management, disaster response and
       humanitarian assistance organizations are most welcome to help create an
       innovative cooperative learning environment. (Naval Postgraduate School,
       n.d.)


       JIFX events are held quarterly,  and elements of RTAT participated with other
organizations from 10–14 August 2014 (Goolsby, & Steckler, 2014). The author

participatedin this event as an RTAT assessment tool subject matter expert.



                                          112
       The RTAT goals for JIFX 2014–4 were: To test and evaluate the RTAT

assessment form version 6 (v6) for finalization, and to test the ODKCollect application to
ensure that it works as well as the Lighthouse application in a field environment, but with

the benefit of the Google Play store for application dissemination (Goolsby, & Steckler,

2014).

       RTAT v6 included the logic to independently assess all of the (sub) services,

while skipping non-assessed sub-services at the location. RTAT v6 marks the final form
expected to be developed on the ODK suite of systems. Future RTAT development

efforts will focus on an operating system agnostic solution; see Chapter V for more

details.


       a.      Concept of Operation

       Building upon the Typhoon Haiyan success, the author trained two teams in the

same manner: classroom instruction using Appendix I, followed by practical application
utilizing the required equipment. Teams then deployed in a simulated disaster scenario in

the Camp Roberts, CA training areas (Chang, 2013).

       The teams consisted of three military officers and five civilians. The mission of

the RTAT teams was to conduct RTAT assessments throughout the Camp Roberts

training area in support of other collaborative experiments (Goolsby, & Steckler, 2014).
None of the team members, save the author, had used the RTAT tool prior to training,

and all of them had at least a bachelor’s degree. About half had never used an Android

based smart device before and one had just received his first smart phone (Android) that

week. All were able to grasp the use of the phone and the RTAT mobile data collection
tool (Lighthouse and ODKCollect) after just one full assessment use. Participants with

Android smart phones downloaded the ODKCollect application from the Google Play

application store.

       b.      Results and Recommendations


       Fourteen assessments were conducted during the exercise and no bugs were found
within the form. Approximately two thirds (2/3) of the assessments were conducted using


                                           113
the ODKCollect application with no issues being recorded. As a side note, participants

preferred downloading ODKCollect from the Google Play application store versus using
an “untrusted” application transferred from the author. RTAT v6 was validated at the

conclusion as ready for use in the next disaster (Goolsby, & Steckler, 2014).

       The team utilized available cellular data networks (AT&T and Verizon), the Cisco

Rapid Response Kit (RRK)’s BGAN satellite modem or cellular connection, along with

the Goal Zero Yeti 1250 power system to submit RTAT assessment forms on location.

       The RRK is a lightweight low electrical power networking solution in an austere

environment (Bharania, 2014). The RRK can connect to the Internet via a satellite
broadband global area network (BGAN), or via a cellular data connection (Bharania,

2014). Figure 37 shows the RRK. Note the cases can fit in the overhead compartment of

most major airlines (Bharania, 2014).


















          Figure 37.   Two Cisco Rapid Response Kits (from Bharania, 2014)


       The Goal Zero Yeti 1250 is a 1,250 watt power system that can be recharged via
solar panel or by plugging in some other power source (Goal Zero, n.d.). The system

fully deployed includes solar panels and the base shown in Figure 38 (Goal Zero, n.d.).

The base system includes a large marine battery, a built-in inverter, a charge controller,

alternating and direct current outlets, as well as charging input (Goal Zero, n.d.).



                                           114
               Figure 38.  Goal Zero Yeti 1250 (from Goal Zero, n.d.)


       (1)    What Worked?

       RTAT form v6 was validated. A sample of the RTAT Assessments is included in

Figures 39 and 40.























                Figure 39.  Sample of JIFX 14–4 RTAT Assessments







                                         115
Figure 40.   Antennae Hill Assessment Photos, Author is Shown Testing the Goal Zero
                            and Rapid Response Kit (from Appendix L)


       The availability of the ODKCollect application on the Google Play store greatly

aided in RTAT mobile data collection tool adoption and use, albeit it on a micro scale,

and should be the preferred method of disseminating and promoting RTAT. Lighthouse’s
ability to be stored on a computer and locally available in an Internet challenged

environment should not be discounted and should remain as a back-up to ODKCollect.

       The RRK worked as advertised. The Goal Zero easily provided all of the power

requirements for the RRK and for charging the utilized phones.


       (2)    What Didn’t Work?

       RTAT v6 is awaiting official RTAT organizational blessing before attempts to

integrate v6 data into PDC’s DisasterAWARE web portal.

       The Goal Zero Yeti 1250 system was more than the mission required and was

extremely heavy to move. Note the use of two people carrying the base in Figure 38.

       (3)    Recommendations.

       Consideration should be given to utilizing a much smaller Goal Zero or

comparable product.



                                          116
       RTAT needs to officially approve the RTAT v6 form and continue integration

discussions with Pacific Disaster Center.


















































                                        117
THIS PAGE INTENTIONALLY LEFT BLANK





























                  118
       IV.    SUMMARY, CONCLUSIONS, AND LIMITATIONS



A.     SUMMARY

       Coordination within the humanitarian assistance response community to
efficiently and effectively respond to international large-scale disasters is hard.

Compounding this problem is the widespread destruction of the affected nation’s critical

information and communication technology (ICT) infrastructure, and the lack of ICT

operational status situational awareness. As a result, most responding organizations don’t
bring the right communication equipment, have difficulty communicating with one

another and therefore cannot collaborate to affect a coordinated response. Many

organizations have been organized to assess the ICT environment and combat this
problem. However, all of these organizations have a narrow focus pertinent to their field

and have the added responsibility of providing communication capabilities to their parent

organization. The Rapid ICT Assessment Team (RTAT) was created to provide a holistic
assessment of the ICT environment, pass on all pertinent ICT assessment findings and

provide recommendations for recovery to the humanitarian assistance response

community. RTAT is a fledgling, ad hoc, unfunded, volunteer organization looking to
improve and integrate their processes into the larger response community. This campaign

of experimentation as a first step towards that strategic aim

       During the course of this thesis, RTAT discovered that their current means of

conducting assessments and disseminating their findings were outdated and inefficient.

Due to constraints, restraints, and available support, the open source Open Data Kit
(ODK) suite of tools were down selected for further testing and developing to solve this

problem.

       The ODK developed assessment forms were successfully tested on both the Naval

Postgraduate School (NPS) Common Operational Research Environment (CORE) lab’s

Lighthouse, and on the Google Play available ODKCollect applications. Unfortunately,
Lighthouse and ODKCollect are currently limited to Android based smart devices only.




                                       119
       Investigative experiments were conducted in Legazpi City, in the Republic of the

Philippines (ROP), where form refinements were made and it was discovered that
crowdsourcing the assessments may be possible.


       Investigative experiments with the Savvion process modeler compared the “As Is”
process to the “To Be” model. The “As Is” process utilized only RTAT subject matter

experts, the old Excel spreadsheets assessment forms, and email to communicate data to

points of contact within the humanitarian assistance response community. The “To Be”
model was comprised of crowdsourced assessments on the developed mobile data

collection tool with automated backend server integration with the Pacific Disaster

Center’s (PDC’s) DisasterAWARE web portal. This experiment revealed the “To Be”

model to be superior in every regard. Using the “To Be” model, the number of
assessments could be doubled from 100 to 200, expenses dropped 87 percent, wait time

decreased by 81 percent and it took only one fifth (1/5) of the time to complete the 200

surveys. Further, the number of surveys in the “To Be” model could be increased five-

fold from 100 to 500 and expenses and time would still be lower than the “As Is” model.

       RTAT’s demonstration experiment and deployment in response to Typhoon

Haiyan in Tacloban City, ROP tested the findings of the Savvion experiment in a real-
world environment and proved that the RTAT assessments could be crowdsourced given

the caveats of the “Limitations” section below. Typhoon Haiyan deployment further

validated that the crowdsourced assessments could be viewed on, and its data
disseminated through, the Pacific Disaster Center’s DisasterAWARE web portal given

the boundaries outlined in the “Limitations” section below.

       Finally, the demonstration experiment during Joint Inter-Agency Field Exercise

(JIFX) 2014–4 at Camp Roberts finalized the RTAT v6 assessment with plans to further

integrate the forms into and improve their display on DisasterAWARE. See Chapter V

for further details.

       The campaign of experimentation ended successfully with a usable mobile data
collection tool and processes that could integrate the process of data collection to

situational awareness dissemination in near real time.


                                          120
B.     CONCLUSIONS

       The NPS CORE lab’s Lighthouse application, as well as the ODKCollect

application, were successfully used in real-world testing conditions as the electronic form

interface to download, fill out, and upload completed RTAT assessment forms to the NPS
ODKAggregate servers located in Monterey, CA.

       Demonstration experiments further validated that ODK data could be exported

and assessments displayed on PDC’s DisasterAWARE website with conditions outlined

below.

       RTAT v6 meets current needs to integrate with Pacific Disaster Center. No

research was conducted into compliance with the Humanitarian Data Exchange initiative

which could influence whether v6 will meet RTAT needs going into the future.

C.     LIMITATIONS

       There are many caveats and limitations that are discussed in this section that

affect the conclusions outlined above.


       1.     Lack of Budget

       Given the lack of budget, available Samsung S2 and S3 phones at no charge, the

prevalence of experience and support within the Common Operational Research

Environment (CORE) lab at the Naval Postgraduate School, and its ability to meet initial
mission requirements, the Open Data Kit (ODK) suite of tools was selected for further

RTAT development and testing. According to research, there were no free ODK

compatible Apple iOS applications available that met the requirements of RTAT. This is
an ongoing limitation within the adoption realm and a solution will be discussed in

Chapter V. Many RTAT design and selection decisions were based primarily on cost.

Removing this limitation may invalidate the ODK selection.


       2.     Displaying Typhoon Haiyan Data on DisasterAWARE

       The Typhoon Haiyan successful demonstration experiment to display RTAT data
on PDC’s DisasterAWARE was conducted after returning to the United States. Further,


                                        121
this display experiment was a non-automated process that included the manual

exportation of the data, transmission via email, and then manual importation of the data
by PDC. This was due to the lack of formal organizational letters of agreement between

the NPS, CORE lab and PDC; as well as form changes that took place after PDC points

of contact left the Philippines for other operational commitments. This limitation will be

further addressed in Chapter V.

       3.     Integration with Other Humanitarian Assistance Organizations

       Talks have begun, but there have been no serious efforts to link RTAT assessment

data into other partner or stakeholder organizations such as USAID, the UN-Emergency

Telecommunication Cluster, the All Partners Area Network (APAN) web portal, or the

Association of the Southeast Asian Nations (ASEAN). Nor has there been an effort to
ensure data collected is compliant or harmonized with recent open source Humanitarian

Data Exchange (HDX) developments. Any future database builds and HTML5

developments must be in line with partner initiatives to ensure widespread use and
adoption of the RTAT mobile data collection tool.


       4.     ODK Form Development Limitations

       The ODK compliant forms were developed initially on the ODK XLS2XFORM

program until logic within the XLS2XFORM could no longer be supported.

XLS2XFORM developed Extensible Markup Language (XML) form file lines of code

were then edited manually to meet the needs of RTAT. This severely limits the simplicity
of, and time to create, form edits and changes.


       5.     Satellite Communication

       Internet connectivity in the form of surviving cellular data service, NGO provided

Internet café, or BGAN is required within the disaster zone to meet the needs of RTAT

and validate the Savvion findings.






                                          122
       6.     Local Language

       Ability to speak the local language is a must in interviewing local points of

contact in the conduct of RTAT assessments. It should be noted that ODK does support

numerous languages. However, Filipino (Tagalog) is not one of them.

       7.     Crowdsourcing

       There were crowdsourcing limitations during Typhoon Haiyan. All but two

volunteers had either military, first responder or an ICT background. More research

should be conducted into the validity of assessments by volunteers with no military, ICT

or first responder experience (or combinations thereof). Further, no comparative analysis
was done between those that had prior experience at the Legazpi City demonstration

experiment and those that were using the RTAT assessment tool for the first time during

Typhoon Haiyan.

       Vast improvements in the Savvion process costs hinge upon obtaining 10–20

local volunteers. This is tied to the crowdsourcing limitation above. RTAT members must
train those crowdsourced volunteers before they can be utilized. Legazpi City and

Typhoon Haiyan demonstrated that large numbers of volunteers could be obtained in a

short period of time given the right motivation.






















                                          123
THIS PAGE INTENTIONALLY LEFT BLANK





























                  124
                    V.     FOR FURTHER RESEARCH


       The intent of RTAT is to deploy scores of crowdsourced evaluators that could

conduct ICT assessments and for those collected data points to automatically work its

way up the information chain shown in Figure 1. The real purpose of this data collection
effort is the integration of RTAT data into the situational awareness that can be found

directly in or gleaned from and overarching ICT current operational picture (Figure 41).

Holistic situational awareness, regardless of cluster, is the means by which better
collaborative decisions can be made by humanitarian assistance (HA) responders.


























Figure 41.  UN-ETC Typhoon Haiyan Current Operational Picture November 27, 2014
                                     (from WFP, 2013c)


       This chapter outlines efforts that were ongoing at the conclusion of the thesis,

potential future research areas, as well as some solutions to the limitations discussed in

Chapter IV.


                                       125
       1.     Integration with Humanitarian Assistance Response Community

       RTAT should explore collaborating more with the USAID, UNDAC and

FITTEST teams to align assessment efforts and future assessment developments

(collection data, form, and tool). As discussed in the Limitations section of Chapter IV,
efforts must be harmonized with partner initiatives to ensure widespread use and adoption

of the RTAT mobile data collection tool specific areas for further research and develop

are outlined and discussed below.

       a.     Hyper Text Markup Language 5


       As previously discussed, the RTAT mobile data collection tool is not compatible
with Apple’s iOS operating system. This is a problem because nearly half of all smart

phones run on iOS. One potential solution is to create a Hyper Text Mark-up Language 5

(HTML5) program that would be operating system agnostic while meeting the
requirements of RTAT. Project requirement discussions are underway to explore this

avenue with Humanitarian Tool Box (HTBox), an organization that volunteers computer

programming to HA organizations. Discussions are in the requirements development

stage and could be accelerated with an upcoming “hack-a-thon.” If, however, the Naval
Postgraduate School (NPS) Common Operational Research Environment (CORE) Lab’s

Lighthouse application HTML 5 project is any indication, these efforts will take six

months to a year to complete. Therefore the ODK RTAT v6 solution must remain viable

until a follow-on operating system agnostic solution is brought on-line and discussions
with PDC to integrate v6 forms should continue.


       b.     Data Base Development

       ODKAggregate can export a flat file (spreadsheet) that can be imported into a

database such as MySQL or MS Access. ODK does have some rudimentary query

capabilities that have thus far met the needs of RTAT. Unfortunately, ODKAggregate’s
flat file interface will not be able to keep pace as RTAT grows and integration with

Pacific Disaster Center and the rest of the HA response community continues. Constant

revisions of RTAT assessment forms have thus far hampered this effort and should be
addressed as soon as RTAT v6 is officially adopted.

                                          126
       c.      Humanitarian Disaster Exchange Compliance

       Database efforts should align with current and anticipated Humanitarian Disaster

Exchange (HDX) requirements to facilitate sharing of collected data to the rest of the HA

response community. This will ensure assessment harmonization with other UN
assessment reports.


       2.      Pacific Disaster Center Integration

       The primary development partner, in regards to assessment display, has been the

Pacific Disaster Center (PDC) and its DisasterAWARE web portal. Late form finalization

has effected both the database development discussed above and with RTAT assessment
data integration with PDC and will be discussed in greater detail below.


       a.      Data Exchange

       RTAT Assessment v6’s successful demonstration test at the Joint Interagency

Field Exercise (JIFX) in August 2014 marks the unofficial finalization of the RTAT

assessment form. However, RTAT as a whole still needs to officially bless the form
before efforts to fully automate and integrate a data exchange can begin. JIFX assessment

data was sent to PDC on September 9, 2014 with an initial conference call to discuss the

data and server letters of agreement pending at the time of thesis submission.


       b.      DisasterAWARE View

       The current view of a RTAT assessment on DisasterAWARE yields little intuitive
information, Figure 42. Research is needed to develop the best operational view that

easily conveys pertinent information to the end user, see Figure 43 for latest proposal.













                                           127
                     Figure 42.   Current DisasterAWARE view



























 Figure 43.  Potential DisasterAWARE view (after B. King, personal communication,
                                         April 22, 2014)


       In the Figure 43 example, the red lightning bolt indicates that electricity is “Not

Working” at that assessment location. A quick “Mouse Over” (Figure 43) of the

                                         128
assessment location reveals additional issues with Land Lines/Fiber (sub-) services,

Cellular (sub-) services, and Broadcasting (sub-) services. Double clicking on the icon
would return the RTAT assessment report discussed next.


       c.     RTAT Assessment Reports

       Tied to the database and HA response community integration, RTAT needs to

develop a standardized report that can be generated from collected data. Figure 44 is an

example of one such proposed report. Research into intuitive report design and
harmonization with common HA response community assessment report practices should

be conducted before report finalization. Further, integration (coordination or

harmonization) into USAID reports and the UN’s Situational Analysis and MIRA reports

are a must going forward if RTAT’s contributions are to remain valid within the larger
HA response community.
































                                         129
Figure 44.  Proposed RTAT Assessment Report (after B. King, personal communication, April 22, 2014)

                                                 130
 APPENDIX A.         COMMAND & CONTROL CASE STUDY OF THE
                RESPONSE TO HURRICANE KATRINA



       Appendix A is an unpublished paper written as a final project by five officers at
the Naval Postgraduate School for a command and control class (Canon et al., 2012).

Quotes and ideas were taken from this paper. This document could not be found by any

other means other than by inclusion with the thesis.



             Command & Control Case Study of the Response to Hurricane Katrina

                                   Christopher Cannon


                                     R. Travis Beeson

                                    Keystella Mitchell

                                     Patrick Spencer

                                     Michael Liguori

                                Naval Postgraduate School


                                    15 November 2012





      Command & Control Case Study of the Response to Hurricane Katrina

Introduction
       Hurricane Katrina made landfall as a Category 4 storm on August 29, 2005 near

New Orleans, Louisiana. Katrina’s devastation was exacerbated by the subsequent failure

of the levee system that protected New Orleans from Lake Pontchartrain, 24 hours after
land-fall. The levee failure resulted in wide spread flooding of New Orleans causing

extensive damage to its infrastructure that in turn hampered the command and control of

rescue, relief and recovery efforts. The official death toll surpassed 1,200, over 1 million
people were displaced and damages exceeded $200 billion. Hurricane Katrina created a

                                       131
humanitarian crisis on a scale unseen in the history of the U.S. and is to date the most

destructive and costliest natural disaster in the history of the United States (Striedl,
Crosson, & Farr, 2006).


       This paper will explore the Command and Control (C2) inter/intra relationships
between the involved local, state, federal government entities, as well as, non-

governmental and regional partnership organizations and discuss what worked and what

should be improved upon. Additionally, this paper will glean from the Katrina lessons
learned a set of C2 principles that are both Katrina specific and generalizable to other

complex endeavors.

Background


       The U.S. National Response Plan (NRP), resulting from Presidential Directive
No. 5 in 2004, recognizes that planning, preparing for and responding to natural and other

disasters are primarily responsibilities of the individual states. This reflects the U.S.

constitutional perspective, and results in a pull response assumption, with local

authorities having the lead at the start, escalating to state level and then to federal level, if
necessary and if requested (Moffat, 2008).


       The Stafford Act also outlines the process by which state governors request this
assistance from the federal government when the event becomes one of “National

Significance.” The U.S. President then has to decide whether this merits designation as an

Emergency (releasing limited resources to the states), a Major Disaster (releasing much
greater resource to the states) or a Catastrophe. The first two of these result in a pull

response: the states requesting and drawing down from these federal resources as they see

the event unfolding. The third category of Catastrophe would have resulted in a proactive

push of resources to the region, states and local level, irrespective of the states’ requests
(Moffat, 2008). The Stafford act attempts to organize and capture all Federal costs

associated with the significant event. However, its processes can be cumbersome, slow

and ill-suited to a dynamic situation where a rapid response, vice monetary

accountability, is the gauge of success.



                                           132
       Under the NRP, a comprehensive framework of response to significant event is

set up. At the Federal level, the Homeland Security Operations Centre, the Federal
Emergency Management Agency (FEMA) National Response Centre and the Interagency

Incident Management Group jointly coordinate the response across government

departments. The federal coordinating officer (FCO), a representative of the Secretary for

Homeland Security, is authorized to lead a Joint Field Office (JFO). This is a temporary
federal facility established locally at the time of a disaster to coordinate the local, state,

and federal response. It consists of senior representatives from all of the agencies and

responders involved, and develop objectives, strategies, plans, and priorities. The
membership of this office is envisaged as growing and adapting over time as the incident

escalates or diminishes (Moffat, 2008).

       In summary at pre-Katrina landfall, the NRP and Stafford Act clearly delineate

that states have the lead on handling natural disasters within their states and with the

exception of “catastrophic” events are required to request assistance from the federal

government if necessary. FEMA is the lead federal C2 agency for handling “nationally
significant” events (Meeds, 2006). The entire system is set up in a strict, regimented,

hierarchical system, local, state, regional and or federal respectively, that will be shown

to be ill-suited and deficient for the dynamic task at hand.
                    Analysis of Principles with Alternate Decisions


Principle: Fit


       A key C2 principle relevant to the Hurricane Katrina response involves the
concept of fit. Fit is the match between the organization structure and contingency

factors that has a positive effect on performance (Nissen & Burton, 2011). Regardless

of the mission, successful C2 systems “fit” within the constraints of the environment and
successfully match organizational structure and methodology to the mission.

Organizations that fail to appropriately design their C2 system to the operational

environment and mission achieve a “misfit,” which significantly degrades organizational

performance.



                                          133
       The organizational structure and methodology employed by local, sand federal

agencies during the response to Hurricane Katrina was based off the National Response
Plan (NRP). The NRP is the federal government’s baseline plan to coordinate disaster

response, and is designed to facilitate coordination of federal resources in response to a

catastrophic event. The NRP is based off a structured C2 configuration closely

resembling a Machine Bureaucracy. Thus, the C2 organization predicated in the NRP is
hierarchical and utilizes centralized command structure, high degrees of specialization,

highly formalized vertical communications pipelines, high decision thresholds and

standardization of work processes for coordination. It is designed around the assumption
that the environment is stable and simple (i.e. predictable), and seeks to optimize

responses based off repeatable cause-and-effect relationships. The strength of the

Machine Bureaucracy resides in its stability; however, this stability also makes it slow

and inflexible.

       Unfortunately, the bureaucratic machine designed to respond to Katrina was too

slow and inflexible to handle the chaotic situation in Louisiana, Mississippi and
Alabama—which “misfit” the situation. The high decision thresholds and “red tape,”

which accompanied each major decision, slowed recovery efforts. Highly formal and

vertically oriented communications pipelines slowed information flow that led to poor
situational awareness. Highly centralized command functions and the lack of self-

contained units exercising initiative in distributed areas resulted in duplicated recovery

efforts in some areas and total neglect in other locations. Together, these results indicate

that the recovery effort (particularly during the initial stages of the response) was largely
conflicted and unsynchronized which caused unnecessary suffering and additional loss of

life.

       A C2 configuration offering a better “fit” to the highly chaotic and unpredictable

post-Katrina environment is the Adhocracy. A C2 system organized along these lines is

the polar opposite of a Machine Bureaucracy. The Adhocracy utilizes decentralized and
informal command structures, low degrees of specialization, informal communications

pipelines (particularly horizontal), low decision thresholds and mutual adjustment for

coordination. It performs best in highly dynamic and unstable environments, by stressing

                                           134
fully distributed patterns of interaction, broad dissemination of information and peer-to-

peer allocation of decision rights (low decision thresholds). The Adhocracy assumes that
the environment is unpredictable and favors agility to respond to unknown circumstances

over optimization of responses to predictable scenarios (Machine Bureaucracy approach).

       The strength of the Adhocracy lies in its speed and maneuverability, which comes

at the cost of accuracy and stability. In catastrophic response scenarios however, the

critical factor is time not accuracy; therefore, speed of response vice accuracy of response
should have been the key factor dictating the federal government’s response to Katrina.

Consequently, the C2 configuration representing the best “fit” to these operational

requirements should have been identified as an Adhocracy vice a Machine

Bureaucracy—the improved performance from employing the better fitting C2 system
would likely have eased suffering and saved lives.

Principle: Unity of Command


       Another principal that stood out due to not having the right “fit” as stated above

was the concept of unity of command. C2 is largely about organizing people with aligned
goals, who coordinate efforts via procedures and leverage capabilities through

technologies (Van Creveld, 1985, 10). Van Creveld’s biggest learning point was that

command systems cannot be understood in isolation. Movement towards labeling

command as a “system” vice a hierarchical chain produces a deeper understanding of
relationships in a complex environment. Unity of command is paramount in C2.

Command as a process vice an individual, effectively uses information in a more

powerful way to coordinate people and equipment. Great leader    s understand that the
organization does not exist to serve them rather that they exist to serve the organization,

to work with others to help create conditions necessary for success (Alberts and Hayes,

2003).

       C2 of all support forces was a serious issue during recent disaster relief
       operations... the answer to “Who is in charge?” depended on to whom you
       posed the question. Lack of unity of effort led to overloaded support in

       some areas and not enough in others (Center for Army Lessons Learned
       06 –11: Hurricane Katrina, 2006).


                                          135
       The above statement from the Army lessons learned amplifies the point that given

a complex disruption (catastrophe), people often look to a stated person in charge. When
the “commander” does not appear, chaos and lack of cohesion exponentially rise. Some

argue that the focus should be on unity of effort vice unity of command. By focusing on

unity of effort, the cure to a symptom is sought rather than the root cause in that

command is still a function of the commander vice a process. The principle views
command as a process emphasizing unity of command that produces unity of effort as a

bi-product.

       The lack the unity of command unfortunately points out many of the failures from

the Katrina response. Specifically, the lack of coordination to align goals produced

duplicate efforts, confusion, frustration and misappropriated assets. By not organizing
people to coordinate efforts, the system from the start became conflicted. The Final

Report of the Select Bipartisan Committee to Investigate the Preparation for and

Response to Hurricane Katrina (2006) listed an overwhelming lack of unity of command

that spawned dismal coordination. Below are a few key identifiers from the report that
conveyed a lack of unity and coordination (pp. 3, 4, 299, 315):

              The C2 of the National Guard units and the federal level could not
               exchange information.

              No unified C2 system was put in place during the search and rescue,
               evacuation, and supply delivery missions. The effect was that of having
               multiple rescue teams operating in the same area while other areas were
               left uncovered.

              DOD, FEMA, and the State of Louisiana had difficulty coordinating with
               each other, which slowed the response.

              DOD-DHS coordination was not effective during Hurricane Katrina.

              Government did not effectively coordinate private air transport capabilities
               for the evacuation of medical patients.

              Lack of coordination led to delays in recovering dead bodies.
              State officials feel there has been a lack of coordination within the
               interagency community causing delay in relocating and housing people.

       The large organizations (local, state, DOD, DHS, FEMA, etc.) may have had

great intentions to help; however, the leaders decisions broke down in the dynamic, less
predictable environment that conflicted the entire system. Commanders and top-level

                                          136
officials were making decisions with the understanding they are the C2 of their

organization vice looking at C2 as a process. The sheer overwhelming event of Katrina
exposed the vulnerability in the system that gave too much importance to some

individuals. Fortunately, due to the tireless work of people at the tactical level,

coordination began to produce direction. As the maturity level grew due to unflagging

tactical initiatives, the efforts matured and the system evolved to deconflicted with signs
of coordination at the very end. Specifically, what evolved were multiple organizations

communicating with liaisons. The steep learning curve came at a very high cost.

       Unfortunately, many official definitions continue to be focused on the authorities
associated with command, not on the what and the how of what needs to be accomplished

(Alberts and Hayes, 1995). To better employ and maximize the stated principle, the

process must be the focus. By focusing on the process, coordination will prosper.

Specifically, to increase maturity and coordination rapidly (key in a Katrina like event),
the frequency of interactions among the entities must be emphasized. These interactions

shift the focus from the Information domain (from sparse to rich exchange of

information) to the Cognitive domain (from low to high degrees of shared awareness) and

to the Social domain (from low to high sharing of resources) (Moffat and Alberts, 2006).
Concrete examples to employ are to emphasize liaisons and establish coordination

centers for fusion cells to coordinate efforts. Other key aspects to improve C2 came from

the White House’s Lessons Learned, 2006. These initiatives began to acknowledge
coordination and C2 as a process to empower unity of command:

      Ensure that for events preceded by warning, we are prepared to pre-position an
       interagency federal joint field office (JFO) to coordinate and, if necessary, direct
       federal support to the disaster.

      Ensure that relevant federal, state, and local decision makers, including leaders of
       the State National Guard, are working together and in close proximity to one
       another in the event of another disaster.
      Embed DOD points of contact at the JFO and FEMA regional offices to enhance
       coordination of military resources supporting the response (liaisons).


Principle: Communications must be adaptive

       Poor   unity   of   command     exacerbated   the  third  principle   concerning
communications. Communication is broken into two facets to support C2. The need to


                                          137
maintain an informal, as well as a formal network of communications inside the

organization; as well as the need for a regular reporting and information
transmission system working both from the top down and from the bottom up (Van

Creveld, 1985, 270). Communication is a vital aspect to Command and Control (C2)

whether one is looking at it from a much defined warfighting organizational structure or a

fairly loose coordinated structure such as a disaster relief effort. Communication must be
adaptive to provide needed information to the right place at the right time.

       Communication can be viewed as a system with multiple roles to support the C2

process (JP-6, 2006). One role of the system is to ensure connectivity thus to provide the

capability to effectively plan, conduct, and sustain operations. Another role is to provide

the essential tools necessary to collect, transport, process, protect, and disseminate
information. Finally, it serves a role to provide processes and procedures in which to aid

in ensuring information availability to facilitate the need for distributed operations in a

nonlinear process. A communication system that is effective in each of these roles as well

as being agile, interoperable, trusted and shared forms a network that is linked and
synchronized in time and purpose to allow a C2 process to successfully implement to

achieve the mission.

       Now taking a look at the disaster relief efforts of Hurricane Katrina in 2005 from

a national, regional and local perspective on how well their communication system

support the operational efforts to accomplish the mission. There were inherent failures at
all levels of this disaster relief effort from a communication perspective. Starting from the

top of the U.S. Emergency Structure that was obviously not in place or operating properly

to effectively deal with the devastation that Hurricane Katrina left behind. The lines of

communication with respect to the Stafford Act were not at all effective statistically there
were 1,833 fatalities, winds gusting at 175mph and an estimated 108 billion dollars in

damages however the response from the federal government was to wait to determine if

whether the state and local government could handle to destruction that was caused by
the hurricane. The timeline of responsive action is the biggest indicator of an ineffective

communication system it took what must have seemed like a lifetime to those affected to

get the necessary assistance required. As the devastation and destruction played out in the

                                           138
national media civilian and military decision makers throughout the government

estimated that the inflow of National Guard troops was sufficient to handle the situation.
On 31 August after being given a “blank cheese” for any DOD resources General Honore

still “did not believe that federal ground forces were needed.” This proved the breakdown

or lack of communication from a national level.

       On a regional level the federal government and the governor of Louisiana

required 24 hours to agree on a structure of separate active-duty and National Guard task
forces. However, the final agreement was not reached until six days after the landfall of

Hurricane Katrina. From a local level the coordination of the local law enforcement did

not take place until eight days after the landfall of the hurricane. Failure to establish an

effective communication system with identified lines of communication resulted in the
situation experienced during Hurricane Katrina one of mass chaos and confusion.

Specifically, multiple units searched for survivors covering the same ground while other

areas go unsearched.

       Establishing an effective communication networks which allows the necessary

communication between and among national, regional and local agencies that are agile,

interoperable, trusted and shared would alleviate issues experienced by Hurricane
Katrina. Linking and synchronizing the C2 process through communications would

greatly improve the probability of success in a dynamic environment. Formal and

informal communications that are interoperable and synced would also increase the unity
of command as a process with in a coordinated effort against a complex situation.

Principle: Agility


       The concept of C2 agility and maturity surfaced a fourth principle tied to Van

Creveld’s idea in that an organization that will make such low-decision thresholds
possible must provide self-contained units at a fairly low level (Van Creveld, 1985, 270).

The more uncertain and dynamic an adversary and or the environment are, the more agile

a C2 organization must be or become (Alberts and Hayes, 2003, 124).

       Alberts and Hayes (2003, 127     -128) went on to define agility by six key

dimensions: robustness, resilience, responsiveness, flexibility, innovation and adaptation.

                                          139
The aforementioned dimensions will be the lens used to analyze the Hurricane Katrina

C2 organization with respect to the stated principle.

       Robustness is the ability to maintain effectiveness across a range of tasks,
situations, and conditions (Alberts and Hayes, 2003, p.128). Mobilized state, National

Guard, regional partners, and local assets were not capable of handling the C2

requirements of Hurricane Katrina and the subsequent levee failures that flooded New
Orleans (Meeds, 2006). As federal DOD assets became available C2 of the relief efforts

became an attainable objective.

       Resilience is the ability to recover from or adjust to misfortune, damage, or a

destabilizing perturbation in the environment (Alberts and Hayes, 2003, p.128). The C2

system was not resilient to a hurricane, Dourandish, Zumel, and Manno (2007) found that
“severe damage to the communications infrastructure created significant difficulties and

hampered rescue efforts due to the resultant lack of situational awareness by civilian and

military officials.”

       Responsiveness is the ability to react to a change in the environment in a timely

manner (Alberts & Hayes, 2003, p.128). The entire mobilization effort was not
responsive, of the 50,000 National Guard and 20,000 federal military personnel

eventually deployed to the affected areas, only about 11,000, or about 16 percent were on

the ground within the first three days of the event, including approximately 9,000

prestaged National Guard personnel (Dourandish, Zumel, and Manno, 2007).

       Flexibility is the ability to employ multiple ways to succeed and the capacity to
move seamlessly between them; innovation is the ability to do new things and the ability

to do old things in new ways (Alberts & Hayes, 2003, p.128). FEMA provided funding

for a Southeast Louisiana Catastrophic Hurricane Planning Project, and two planning

conferences were held in the 6-8 weeks leading up to hurricane Katrina for federal, state
and local entities. Shortfalls were identified, but not corrected, and more importantly no

one exercised the plan drafted. As a result, the entities involved could not exercise the

plan created nor adapt as the events did not go as planned (Townsend, 2006). This gives

rise two applicable combat principles: the plan is nothing, planning is everything

                                           140
(conversations with Colonel D. Crall, USMC, 2007), tempered with, no plan ever

survives first contact (derived from quotes from Carl von Clausewitz and Helmoth von
Moltke).


       The final dimension to agility is Adaptation. Alberts and Hayes (2003, p.128)
defined adaptation as the ability to change work processes and the ability to change the

organization. The first week post Katrina land fall was chocked full of examples of ad

hoc and adaptive processes and changes but their efforts were sporadic and overall
inefficient and ineffective. With that said, the Katrina C2 system matured from non-

existent or conflicted operations to de-conflicted operations after the first week as more

entities and assets arrived in theater and working relationships were developed (Meeds,

2006).

       Recommended agility improvements. Scalable, modular capability package

organizations should be regionally developed/based that can provide Emergency Support
Functions capabilities to the federal coordinating officer. Specific DOD and National

Guard units should be tasked with setting up and manning Civil Military Operations

Centers and those units should participate in annual FEMA exercises with their state and

local counterparts to exercise C2 capabilities. These organizations should be nimble,
maneuverable/mobile, self-sufficient and capable of sustained operations for 10 days.

These entities will be the forerunner to larger more hierarchical capabilities.

Summary


       By using a number of lenses to analyze C2 during Hurricane Katrina, we

concluded that the organizational structure and methodology employed by local, state and
federal agencies were less than adequate. By understanding the guiding documents used

during the response to Hurricane Katrina (National Response Plan, and Stafford Act) we

assemble some necessary principles:

       1.      Unity of command
       2.      Agility

       3.      Adaptive communication

       4.      Organizational fit

                                          141
       The lack of unity of command unfortunately points out many of the failures from

the Katrina response; having individuals, organizations and systems change the way they
relate to one another will enhance coordination at all levels. In the complex dynamic

environment where no single entity is in charge, the coordination and process of C2 will

prevail vice individual commanders. Through this coordination agility will increase

providing clarity in uncertain and dynamic situations; this agility will provide enhanced
communication. By establishing an effective formal and informal communication

network that is agile, interoperable, trusted and shared the necessary communication

between and among national, regional and local agencies would have decrease issues
experienced during Hurricane Katrina.


       Subsequently, the C2 configuration representing the best “fit” to these operational
requirements must be identified and executed to improve performance during the

employment at the local, state and federal level. Regardless of the mission, successful C2

systems must have agility with clear communication in order exercise unity of command,

which ultimately provide the optimal fit. There is no single approach, no best system
design or configuration, no best process for all situations and circumstances (NATO NEC

C2 Maturity Model, SAS-0651, 14).
























                                          142
                                      References
Striedl, P., Crosson, J., & Farr, L. (2006) Association of Contingency Planners
       Observations of Hurricane Katrina Lessons Learned. Albany, NY. ACP

Meeds, H. K. (2006). Communication Challenges During Incidents of National

       Significance: A Lesson from Hurricane Katrina. (Dissertation). U.S. Army War
       College. Carlisle Barracks. PA.

Dourandish, R., Zumel, N., Manno, M. (2007). Adapting C2 to the 21st Century: 2007
       Command and Control Research & Technology Symposium. Newport, RI.

Nissen M. E., Burton R. M. (2011). Designing Organizations for Dynamic Fit: System

Stability, Maneuverability, and Opportunity Loss, IEEE, VOL. 41, 3.

Alberts, D. S., Hayes, R. E. (203). Power to the Edge, DOD Command and Control


Research Program: CCRP Publication Series.

Creveld, M. V. (1985). Command in War. Cambridge, Massachusetts: Harvard
              University Press.

Teague M. J., Col, U.S. Army. The Domestic Coalition:  The C2 Relationship Between
              Active Component And National Guard Forces In Defense Support Of
       Civil  Authorities Operations, http://www.dtic.mil/dtic/tr/fulltext/u2/a476794.pdf

U.S. House of Representatives. (2006). Preparation for and Response to Hurricane

              Katrina, , http://www.c-span.org/pdf/katrinareport.pdf

Alberts and Hayes (1995), Command Arrangements for Peace Operations. Washington
              DC: CCRP, 1995 pp 5- 6; Alberts and Hayes, Understanding Command
       and    Control. Chapter 4; NATO Glossary:
       http://www.nato.int/docu/glossary/eng/15-   main.pdf &
       http://www.dtic.mil/doctrine/jel/other_pubs/aap_15_04rev1.pdf ;  BiSC C2 Plan:

       Bi Strategic Commands (NATO), the coordinated position of the    two Strategic
       Commands: Allied Command Europe (ACE) and Allied Command
              Atlantic (ACLANT).

Moffat J. & Alberts D. (2006), Maturity Levels for NATO NEC Command Dstl
              Unpublished Report

Townsend, F. (2006). The Federal Response to Hurricane Katrina, Lessons Learned.
       Department of Homeland Security, Washington, DC.






                                          143
THIS PAGE INTENTIONALLY LEFT BLANK





























                  144
APPENDIX B.        RAPID INFORMATION AND COMMUNICATION

  TECHNOLOGY ENTERPRISE ARCHITECTURE ASSESSMENT


      Appendix B is a paper written by the author for an enterprise architecture strategy

class at the Naval Postgraduate School (Beeson, 2013). This paper was instrumental in
the development of the RTAT “As Is” and “To Be” models tested with Savvion and was

instrumental in understanding the RTAT organization. Ideas, figures and quotes were
taken from this paper.

        Rapid Information and Communication Technology


                     Assessment Team (RTAT)



                       Enterprise Assessment (EA)




                          By: Major R. Travis Beeson
                     Instructor: Professor Kishore Sengupta




                       Naval Postgraduate School
                       CC 4250—Enterprise Architecture
                                Summer 2013



      Cataclysmic events such as the Indonesian tsunami in 2003 and hurricane Katrina

in 2005 leave a wide and devastating wake of destruction. The horrific human suffering
in these types of events is exacerbated by the inability of relief organizations to

collaborate efforts, that is collectively assess the situation, prioritize efforts and
effectively allocate scarce resources. This is due in large part to an ineffective or missing

overarching collaboration organization, and further compounded by severely damaged
host nation communication infrastructure. Most organizations “don what they

                                    145
don’t know” when they show up and as a result incorrectly equip themselves for the

information and communication technology (ICT) environment. Several organizations
assess various aspects of the ICT infrastructure, but none collate the information into a

complete understanding. To fill the gaps the Rapid ICT Assessment Team (RTAT) was

created to conduct a holistic assessment of the ICT environment and share this

information with other responding organizations. However, the problem of efficiently and
effectively collecting the data, creating a common ICT operational picture and getting

this information into the right hands has yet to be solved. This assessment is being

conducted to help RTAT solve this problem. RTAT is a startup organization that
currently has no documented Enterprise Architecture (EA) strategy. This assessment will

be the foundation of that strategy and the building block for its eventual Enterprise

Resource Planning solution.

































                                         146
                            ASSESS THE ENTERPRISE


       Sengupta (2013) stated, “The architecture must be congruent with the
organization of the enterprise, technology must be aligned with the “business”

requirements and “the architecture must be robust and durable.” For the purposes of this

paper the RTAT “Enterprise” is defined as the people, equipment and processes

associated with the collection, storage and promulgation of pre and post disaster ICT data
collection.

What does RTAT do?


       The mission of RTAT: Conduct and promulgate baseline and post-disaster

Information Communication Technology (ICT) infrastructure assessments, to facilitate
host nation and international disaster relief efforts (Steckler, 2012). “Facilitate” includes

the management and dissemination of a shared common operational picture and ICT

recovery prioritization recommendations.

       Currently RTAT is conceptually organized into “rapidly deployable small,

nimble, multi-organizational, multi-national integrated assessment teams of specialists in
key ICT areas such as wireless data communications, voice communications, radio

technologies, power, information sharing, social networking, etc.” (Steckler, 2012).

These teams would be led by team leader and a “national affected state member (such as

National Disaster Management Agency, Ministry of Communications or equivalent
affiliated organization)” (Steckler, 2012). RTAT is currently made up of international

founding member organizations/volunteers, but the aforementioned cadre of standardized

prepositioned teams and equipment is the future model.

       There are several stakeholder organizations associated with RTAT and include:

The UN, Association of Southeast Asian Nations (ASEAN), Pacific Disaster Center
(PDC), U.S. Pacific Command, and the Naval Postgraduate School, as well as several

other governmental and Non-Governmental Organizations (NGO) (Steckler, 2012).

How is the work done?


       Core processes are outlined in the use cases of attachment 1 and include:

                                         147
        1.     Conduct of baseline, update and post-disaster ICT and power assessment.

        2.     Process assessments into a Common Operational Picture (COP) or
               understanding.
        3.     Make recovery ICT priority recommendations.

        4.     Distribute the ICT assessments, COP and priority recommendations to
               host nation and the international relief community.

        Lines of reporting and responsibilities are still being developed and are a source

for friction for the organization.

        Current proposed RTAT team roles and responsibility model (based on Steckler,
RTAT Executive Summary 15 November 2012)

Team Leader:


        1.     Prioritizes the team’s efforts accounting for member strengths and
               environmental requirements/limitations.
        2.     Determine the team’s make up and skill sets for the mission.

        3.     Coordinates with host nation and other international relief organizations to
               prevent duplication of efforts and add value to the relief efforts.

        4.     Receives input from host nation ICT counterpart and team members, to
               make a prioritized ICT repair recommendation list.

        5.     Disseminate assessments and recommendations to host nation and
               participating international relief organizations.
        6.     Make recommendations/decisions regarding follow on RTAT efforts. I.e.

               Extend the stay for current RTAT, turnover to a relief RTAT, or conclude
               RTAT efforts.
        7.     Be prepared to fulfill tasks as a Team Member.

        Team Member:

        1.     Maintain the requisite skills in the area of expertise.

        2.     Be prepared to deploy within 12–24 hours to the disaster area.

        3.     Maintain self-sufficiency within the disaster area for up to 2 weeks.
        4.     Conduct assessments and “push” assessments to the server when and
               where an Internet connection is available.







                                            148
                       ASSESS THE OPERATING MODEL

Assess the Operating Environment


       As repeatedly demonstrated, large scale disasters like hurricane Katrina and the

Indonesian Tsunami create highly chaotic and turbulent operating environments. Getting
into and out of the affected country is difficult, physically moving about within the

disaster zone can be nearly impossible. Many disaster prone countries lack adequate

logistical and ICT infrastructure, this is further exacerbated by the by the wake of

destruction these disasters leave. This is a highly uncertain environment with high levels
of equivocality. While no one knows when and where the next disaster will strike, there

is certainty that there will be a disaster in certain regions of the earth on an annual basis.

To increase response agility, “RTAT teams would be stationed at key locations around

the world, perhaps modeled after the UN Disaster Assessment and Coordination Teams
program, or possibly as associate members of NetHope, the UN Emergency

Telecommunications Cluster (ETC Cluster) or other similar teams. These teams could be

called on by the host nation, UN agencies such as OCHA, WPF, or a regional entity such
as ASEAN” (Steckler 2012). While no one knows for certain what specific ICT

capabilities will be effected or what the operating environment will be like, there are

many trends and lessons that can be gleaned from previous disasters. This historical

perspective equates to high levels of uncertainty but low levels of turbulence for the
operating environment.


















                                         149
Organizational Agility Response


       Decision to respond: RTAT is designed to be a niche organization whose
services will not be needed for every disaster. RTAT services can be requested by the

host nation, regional authorities (ASEAN), global organizations (UN, NetHope, Red

Cross), but RTATs can self-deploy based on the team leaders decision.

       Range of RTAT responses: RTAT may deploy an assessment team, share

baseline assessment information, act as liaison to host nation, or once Lighthouse is on

online host/facilitate assessments conducted by locally trained responders. This niche
ICT assessment range does include power, wireless (TV, radio, Wi-Fi/WIMAX, cellular),

terrestrial (copper, cable, T-1, fiber optic), and satellite communications (Steckler, 2012).

       Team deployment response: RTAT members/teams will be highly trained on

both ICT assessment and personal sustainment skills (food, water, shelter, hygiene and

personal security/safety) to use within the environment (Steckler, 2012). Additionally,
teams will carry satellite communication and alternative power assets to facilitate mission

accomplishment within the disaster zone. Finally, baseline ICT assessments need to be

conducted for disaster prone areas to facilitate faster post disaster assessments (Steckler,

2012).

       Correct uncertainty organizational response: Forward located, ready to deploy

teams (within 12–24 hours), capable of self-sustainment (personal survival, satellite
communication and sustainable electrical power) and armed with baseline ICT

assessments are a must level of agility to successfully operate in the post-disaster

environment. Figure 1 shows the improvement in Agility Response with the “to-be” IT
changes made.












                                          150
                      Range Agility






                                                    High
                         X




Time Agility
              X




                                                    Low





             Low                    High



       Figure   1 Agility Improvement





















































                   151
                      Assess the Current State of IT in the Enterprise



Specify “As-Is” models of operation


        Currently RTAT uses a Microsoft Office host of programs, Skype as well as

various other communication programs to accomplish its mission. A Microsoft excel

spreadsheet is used to conduct the assessments and it relies upon assessors professional

judgment to gauge non-standardized markings within the spreadsheet. These spreadsheets

are then emailed, via various webmail services, to various points of contacts in the

disaster recovery effort to disseminate their findings. The current excel assessment

spreadsheet does not support export into a query-able database that can then be rapidly

promulgated. There is no centralized COP. No standardized RTAT training, team

makeup, or “go kit” equipment list. Current capabilities are very ad hoc and vary based

on responder’s personal assets, skills, and experience. Further compounding these issues

is the lack of baseline assessments conducted within disaster prone countries (Steckler,

2012). The current IT model does not meet RTAT mission requirements.

                                             Business Process Standardization



                                                                       High





                                    Business Process Integration       Low
                                              X

                                             Low             High


                                                 Figure 2


                                    Current RTAT Operating Model

                            Chart from Ross, Weill, and Robertson (2006)



        Currently, RTAT is operating as a start-up/fledgling “Diversification” operating

model, with few standardized or integrated processes (Ross et al., 2006). This is an

incorrect logic. While one could argue that the plethora of NGO’s and countries dictate a

                                                152
lack of “shared customers” with “independent transactions” and further argue that the

disjointedness is par for the industry and or a byproduct of the very chaotic post disaster
working environment. However, standards within the organization are a must for

consistency in the assessments, COP, and IT Solution. Minimally, there must standards

set regionally with a select few nations and NGOs as stakeholder customers. See

Enclosure 1 for As-Is IT use cases.
Specify “To-Be” Models of Operation


       RTAT is experimenting with a Lighthouse enabled Open Data Kit based mobile

data collection tool that will help facilitate core process requirements. This Lighthouse

program is the future core method for data collection and dissemination. Lighthouse has
some COP features but they currently do not meet all of the expected needs of the

international relief community. Lighthouse does have an exportable XML file feature that

can be imported by other relief organization. As part of the IT strategy, RTAT is
collaborating with the Pacific Disaster Center (PDC) to add ICT related information to

their disaster COP (see Figure 3 for screen shot). ICT Assessments would be linked under

the “more information” link. This will meet the intent for a common operational picture,

but the technical issues, permissions, and letters of agreements must be solved and
finalized.






















                                          153
        Figure 3, Pacific Disaster Screen Shot (atlas.pdc.org/atlas, retrieved 9/17/13)


         Finalizing the RTAT mobile data collection tool would greatly standardize their

core process of ICT assessment and dissemination integrating and linking RTAT

information to the end user customer. Importing the findings into the PDC Atlas would

standardize the COP and better integrate RTAT to the affected nation and responding


NGOs. Overall RTAT would shift from their current “Diversification” operating model to

a more process standardized and integrated “Replication” operating model with

teams/services being interchangeable but with some specialization, see Figure 4 from

Ross et al. (2006).

                                         Business Process Standardization

                                     Coordinated            Unification


                                                                             High




                                     Diversification        Replication

                                                                X

                           Business Process Integration                      Low

                                         X


                                        Low                    High


 Figure 4 Shift from Diversification to Replication Operating Model Chart (After Ross et
                                            al. 2006, p.39)





















                                                  154
                            BUSINESS ARCHITECTURE


       RTAT is currently using an iterative/spiral software development plan. Versions
of the RTAT assessment tool were tested in Thailand in July 2013. The tool underwent

refinement with follow on tests, including “beta” baseline assessments, scheduled in the

Republic of the Philippines 23–27 September 2013. Stakeholders at Naval Postgraduate

School, PDC, ASEAN and local/regional governments in the Republic of the Philippines
are very interested in and supportive of the endeavor to the point of hosting/funding the

initial baseline site surveys 23–27 September. Figure 5 shows the Actor Role Matrix “to-

be” with RTAT assessment tool and PDC COP operational.
                                         ActorRole Matrix

 R: Responsible A: Accountable
 C: Consulted I: Informed          RTATActors                         3rd Party Actors
 Activity                   RTATTeam      RTATLeader         PDC        Host Nation       NGOs

 Baseline Assessments           C             A, R                          C, I            I
 Assess ICT                     A              R                             I              I
 Assess Power                   A              R                             I              I

 Make Priority
                                C             A, R                          C, I            C
 Recommendations
 COP                                           C             A, C            I              I

                                       Figure 5





















                                         155
FORMULATE PRIORITIES, PLANS

       Currently there is no standard way of conducting post disaster ICT assessments,
therefore standardizing/ refining the assessment forms in a query-able format has been

the priority (Steckler, 2012). Running in parallel is the project with PDC to push the

information to their disaster Atlas. Team training, and standardization of “go-kits” is the
lowest priority, but some training has been conducted to test the various versions of the

RTAT assessment tool.


                                       CONCLUSION


       Responding efficiently and effectively to large-scale disasters is difficult in the

U.S. and near impossible in third world countries. Compounding relief efforts is an
inability to effectively collaborate due to the nonexistence of or damage to host nation

ICT infrastructure. International relief organization respondents don’t know the status of

the ICT infrastructure nor what would normally be available. RTAT may not be able to
answer the entire uncertainty question of post disaster recovery operation, but it was

created to answer the simple questions, “What kind of comms can I expect upon arrival”

and what should the respondents repair first to get the most communication “bang for the

buck”?

       RTAT is a fledgling organization still developing its IT plan. RTAT recognizes

that for it to be a viable/ useful part of the international help community it must be able to
effectively and efficiently communicate their ICT assessments. The RTAT/ Lighthouse

assessment tool combined with the Pacific Disaster Center Atlas integration is not the

“silver bullet” Enterprise Resource Planning system, but it is a step in the right direction.

The proposed “to-be” IT solution gives RTAT and its end customer an intuitive
standardized data collection tool that is useable on intermittent/ low bandwidth/ ad hoc

networks. The problem of relief effort collaboration in a post-disaster chaotic

environment is a complex problem that requires multiple solutions. RTAT and its new
assessment tool and PDC linkage is one such solution.





                                         156
157
158
159
                                     Bibliography

Ross, J., Weill, P., Robertson, D. (2006) Enterprise Architecture as Strategy. Harvard
       Business School Press. Boston, Massachusetts.


Sengupta, K. (2013) Enterprise Architecture—A Perspective. Brief given 8/1/2013.

Steckler, B. (2012) Rapid Technology Assessment Team (RTAT) Executive Summary 15
       November 2012. Naval Postgraduate School: Monterey, CA.

Galbraithe, J. (2014). The Star Model. Retrieved from
       http://www.jaygalbraith.com/images/pdfs/StarModel.pdf. Accessed 6/10/14.








































                                          160
       APPENDIX C.         RTAT SAVVION BUSINESS PROCESS
                                 MODELING



       Appendix C is an unpublished paper written by three students at the Naval
Postgraduate School to meet the final project requirements of a business process

improvement class (Beeson et al., 2014). RTAT was chosen as the organization of

interest for this project. This appendix contains all of the supporting work mentioned in
the Savvion model experiments. Tables, figures, quotes and ideas were taken directly

from this source. This document could not be found by any other means other than by

inclusion with the thesis.





          Rapid Information Communication Technology Assessment Team

                        Savvion Business Process Modeling

                                  Travis Beeson

                                Jennifer Gladdem


                                  Jose Gonzalez

                            Naval Postgraduate School



                              ProfessorGlenn Cook




                                  Introduction


       Cataclysmic events such as hurricane Katrina in 2005, and Typhoon Yolanda in
2013, leave a wide and devastating wake of destruction. The horrific suffering caused by

these types of events is exacerbated by the inability of relief organizations to collaborate

their efforts and collectively assess the situation, prioritize efforts and effectively allocate

                                      161
scarce resources. This is due in large part to an ineffective or missing overarching

collaboration organization, and further compounded by severely damaged host nation
communication infrastructure. Though several organizations assess various aspects of

ICT infrastructure, none collate or distribute the information.

       To fill the gaps the Rapid ICT Assessment Team (RTAT) was created to conduct

a holistic assessment of the ICT environment and share this information with other

responding organizations. However, the problem of efficiently and effectively collecting
the data, creating a common ICT operational picture and getting this information into the

right hands can still be challenging.

        We analyzed the current “As Is” RTAT method using Savvion to determine weak

points within the process. This analysis led to multiple recommendations for

improvement, which were then added to a second “To Be” model of the process. By

modeling, analyzing, and re-designing the RTAT process we hope to significantly reduce
ICT recovery time and enable HADR in the future.

 “As-Is” Model of Operation


       Currently RTAT is conceptually organized into “rapidly deployable, small,

nimble, multi-organizational, multi-national integrated assessment teams of specialists in
key ICT areas such as wireless data communications, voice communications, radio

technologies, power, information sharing, social networking, etc.” (Steckler, 2012).

“As Is” Scheme of Maneuver (further details are in the “As Is” assumptions section)

Disaster Strikes

             NGO request IT assessments from UN

             RTAT assessors dispatched to conduct assessments via UN

                     RTAT Responsible for all travel arrangements

             Assessors conduct assessments
                     Forward assessments to UN

             UN employees process assessments

                     Forward to Mapper (Confirms & Plots location)
             Mapper forwards back to UN

                                         162
             UN confirms info & forwards to NGO

             NGO reviews the info for relevance and forwards applicable info to end
              users

“As-Is” Assumptions

       Key assumptions were made based on the cumulative Fighting Hellfish

experience with RTAT deployment, these assumptions include:

             Salary is based on the 2013 GS Salary table.

                     (13) UN Employees all GS 9 Step 6 ($23.23)
                     (3003) NGO Employees all GS 4 Step 6 ($13.71)

                     (3) Mapper Employees all GS 5 Step 1 ($13.14)

                     (3) Assessor Employees all GS 12 Step 5 ($32.73)
             All assessors are paid hourly with assessors’ pay status beginning when

              they accept the assessment mission and terminating at the end of the
              assessment mission.
             All requests originate from the NGO group and are forwarded to the UN

              who then decided whether to accept the request or deny the request. There
              is a 50/50 chance that the UN will accept a given request for assessment.
              All UN accepted requests are forwarded to an Assessor.

             The Assessor evaluates the request and either accepts or denies the
              request. Once the Assessor accepts the request they begin to establish a
              plan for the assessment, which will entail travel time. Travel time is
              broken down into three possible time frames (3, 12 and 24 hours). This is
              contingent on the Assessors current location and the location of the
              assessment as well as the mode of travel (Commercial Air, Military Air,

              Vehicle etc.).
             The Assessor is allocated “life support” time which would include
              hygiene, food and rest. Combined in the “life support” time is the task of

              populating an excel document with the completed assessments for the day.
              Depending on the Assessors connectivity they will forward this
              information to the UN department. If connectivity cannot be established
              the document is delayed and an attempt will be tried at a later time. Once
              connectivity is established the document is forwarded to the UN.

             The limitations of Savvion (or our ability to use it) makes it impossible to
              easily replicate the disaster model which assumes the assessor will
              complete as many assessments as possible (unlimited assessments) during
              limited time, instead of completing a set number of assessments (limited
              assessments) with unlimited time like a typical workload model. IE RTAT


                                         163
               will assess as much as possible during the limited timeline of disaster
               recovery instead of assuming a national disaster will create an exact
               amount of damage. As such a 100 percent utilization of the assessor is not
               infeasible since in our model this utilization indicates assessor workload

               over assessor workload vice the typical utilization of workload over time.
              The UN receives the report and begins processing the form and decides if
               the information is beneficial. Depending on the information gathered by

               the assessor and the requirements of the UN the report may be discarded.
               All forms that are deemed beneficial will be formatted and forwarded on
               through the UN chain. Any form not correctly formatted will be reworked
               within the UN personnel. Once the form meets the specification of the UN
               it will be forwarded to the Mapper.

              The Mapper will receive the report and confirm the format. If there is a
               discrepancy with the format it will be returned to the UN for clarification.
               Once the form is deemed format compliant it will be plotted on the map.
               The Mapper will then notify the UN via the net. The UN will validate the
               plotted location on the map. If the location plotted is accurate the UN will

               send a mass communication to all NGOs for action.
              NGOs will review the report and begin their process of the report. NGOs
               finding the report applicable to them will disseminate the report to other

               users within their organization for action. For the NGOs that do not find it
               applicable they will discard the information and await further report.
RTAT “   As-Is” Process via Savvion


       The assumed “As Is” RTAT process was modeled and analyzed using Savvion in

an effort to identify bottleneck areas and determine possible improvements to the RTAT

process for future HADR operations. This process, shown in Figure 1, utilizes four key
performing groups: NGO (Customer), United Nations (UN) (Facilitator), Mapper

(Facilitator) and Assessor (Provider).















                                          164
                           Figure 1 Savvion Process Model


       Based on the above assessment and process model, a Savvion simulation of 100

assessments was conducted. The results are depicted in Figures 2 and 3.




















                      Figure2 Savvion Process Simulation Results




                                         165
                             Figure 3 Costs and Bottlenecks


 “As Is” Savvion Results


       The overall path of a single assessment is completed using multiple sub-processes
within the UN. The assessment is also (potentially) re-worked and re-checked twice,

which significantly slows down work flow and adds to labor costs. Very little automation

is used and significant delays appear due to connectivity issues (delaying the uploading
of data) and travel time to damaged areas. Bottlenecks were also created in the process

under the Assessors’ travel time section. With the current process bottlenecks are

unavoidable within the Assessor’s realm. Traveling is a time consuming task, especially

within a disaster area. The Assessor is the linchpin of RTAT and is responsible for a very
high workload as multiple runs incurred assessor utilizations of over 90 percent. In some

instances this may raise concerns; however, as noted previously in the RTAT scenario of

a large scale disaster the Assessor is exclusively focused on the RTAT process and will
complete as many assessments as possible during a limited time, therefore whether an

assessment team completes 1 assessment per day or 20, Assessors’ is still completely


                                           166
occupied by the assessment process. Since making assessments is their primary function

their utilization in this process is expectedly high. Due to this high tempo (100
assessments) and short duration (approximately 13 days) workload the assessor is well

compensated, earning $28,442.37 for their participation in this assessment mission. Of

note this compensation includes pay for a 24-hour workday that incorporates reserved

time for food, lodging and transportation while in the disaster area.
“As Is” Recommendations


       Some delaying factors such as travel time, breadth and scope of the disaster that

affect assessment time, are environmental and not subject to RTAT control. Other factors

such as connectivity are caused by the environment but can be mitigated with additional
gear or resources (in this case the addition of portable Broadband Global Area Network

(BGAN) terminals). However, many factors, such as staffing, process management,

automation, and tool functionality are controllable inputs to success.

       Within the controllable factors multiple areas can be significantly improved. The

largest of which are manpower and automation. As noted in the results section, the “As
Is” RTAT process includes a sub-process detour within the UN to process and check

assessment data before posting followed by a second review. By eliminating this step,

through the replacement of the excel document with a more user friendly Android based,

scalable mobile device application, the overall RTAT process was significantly
improved.

       This app which effectively walks novice assessors through a multi-step process

that identifies key infrastructure, potential damages, and utilizes available GPS to auto

update positions beneficially affects RTAT two fold. In addition to eliminating the excess

sub-process at the UN, the app opens the assessor category to a potential influx of
volunteers. By using a train the trainer model, skilled RTAT assessors can conduct an

inventory RTAT class to novice volunteers who will then be able to conduct independent

assessments, which greatly increases the assessor’s overall output without increasing

cost.
“As Is” to “To Be”


                                          167
       Based on the recommendations made from the “As-Is” process analysis, the

RTAT process was re-designed into a “To Be” model. Goals for the “To Be” model
included: increasing the assessment capacity to 200 and reducing costs by 25 percent,

wait time by 75 percent, and assessor utilization rate to <= 70 percent.

RTAT “To Be” Model of Operation


       The RTAT “To Be” model follows the “As Is” model of “rapidly deployable,
small, nimble, multi-organizational, multi-national integrated assessment teams of

specialists in key ICT” (Steckler, 2012), but it also incorporates training capability and

utilizes host nation volunteers to [potentially] increase manpower available for

assessments. Key to the success of this duplicative manpower model is the function of
RTAT as an assessment team concept, capable of gathering and relaying information, to

entities capable of repairing said infrastructure. RTAT does not seek to fix the situation

but rather to provide an accurate picture of the current situation. As such, detailed
knowledge of the ICT infrastructure is beneficial but unnecessary for the RTAT assessor.

An assessor in the “To Be” model merely relays the status of an ICT node to those

capable of taking further action. This makes it possible for a lay volunteer to follow the

simple Android application instructions and still provide valuable data that is the basis
and beginning of the newly automated “To Be” process.

RTAT “To Be” Scheme of Maneuver

(BOOM!!!!!) Disaster Strikes


             NGO request IT assessments from UN
             RTAT assessors dispatched to conduct assessments via UN

                    RTAT Responsible for coordinating all travel arrangements

             Assessors conduct training of volunteers qualifying them as assessors

             Assessors conduct assessments utilizing Android application
             Upload assessments to Naval Postgraduate School (NPS) servers remotely

             Assessment data is shared with the Pacific Disaster Center (PDC) servers

             Assessments are posted to an interactive PDC website that NGO’s and
              Governments can visit to obtain the latest disaster information

Assumptions
                                         168
       The following are some assumptions, derived from the Fighting Hellfish’

experience and resident knowledge of the RTAT process for this new model.
             Salary is based on the 2013 GS Salary Table.

             (10) UN Employees all at a GS 9 Step 6 level ($23.23)

             (3000) NGO volunteers who receive no pay.

             (3) Assessor/Trainer RTAT members all at a $40/hour (roughly GS 12
              Step 5 level)
             (17) Volunteer assessors who do not receive monetary compensation.

             All volunteer assessors will be met at the area of operation and will
              receive an initial 12-hours of training by the Assessor/Trainer team prior
              to any assessment mission.

             Travel to assessment area will be simultaneously coordinated during the
              training time.

             200 assessments will be conducted, with assessment requests arriving in
              10-minute intervals.

             All Assessors (non-volunteers) are paid exclusively for time spent making,
              processing, traveling or training volunteers from RTAT mission
              acceptance until mission completion.

             Assessors do not receive pay for hours of non-assessment activities (i.e.
              rest and refit).

             For the safety of Assessors, they are authorized to work only 12 hours in
              one 24-hour period.

RTAT “To Be” Process via Savvion

       The RTAT “To Be” process incorporates 4 key performer groups including:

NGO/ UN (Customer), Assessor and Volunteers (Facilitator), NPS Server (Facilitator)

and PDC Server (Provider). Assessors and Volunteers are broken up into 9 teams

identified by 9 parallel process routes (outlined in Figure 4). Each team will have a
BGAN assigned for data transmittal.


       All requests originate from the UN/NGO group and are forwarded to the Assessor
electronically. These requests are broken equally into three parallel tracks and further

broken down into three travel times, netting a total of 9 parallel processes that represent

the aforementioned 9 teams. Travel time is broken down into three possible time frames
(1, 3 and 9 hours with standard deviations of 15, 30 & 60 min respectively). This is

                                          169
contingent on the Assessors’ current location and the location of the assessment as well

as the mode of travel (Commercial Air, Military Air, Vehicle etc.). Upon completion of
the assessment, BGANS are utilized to ensure the forms (.XML format) with GPS data

are transmitted back to the NPS servers.

       The NPS servers receive, aggregate, store, and forward the forms to the PDC

servers. The PDC servers utilizes the information and GPS enabled fields in the form to

display the information to the correct region thereby disseminate the information via their
website to interested NGOs, UN personnel, and host nation officials.


       In order to compensate for the addition of (unpaid) volunteers the number of
assessors was increased to 20 while the pay per hour in Savvion was simultaneously

decreased (3 @$40/hour =$120/hour = 20 @6/hour). The assessors remain compensated

for a high workload short duration process and are expected to have significant off time

between disasters.
“To Be” Savvion Results

       The Savvion analysis (Figures 4 and 5) below was run with the 200 and 500

assessment goal.


















                      Figure 4 Savvion Process Simulation Results






                                          170
FinalSavvio nRTAT

Scenario                       (default)
Instances                      200


                                                              Time                  38.333 hrs



                                                                                                                                                                                    Fired
                                                                                         Waiting Time        Time To Complete
            Activity                    Performer                     Occur                                                       Total Time (Time)         Work Time             (occurs)            AWT
                                                                                             Time                  Time                                                              /Hr




Assess 1                       Any member of Assessor                             51          289:13:00               12:45:00              301:58:00          12.75                1.33              0.25
Assess 2                       Any member of Assessor                             13           75:14:00                 3:15:00             78:29:00           3.25                 0.34              0.25

Assess 3                       Any member of Assessor                              4           23:12:00                 1:00:00             24:12:00             1                  0.10              0.25

Assess 4                       Any member of Assessor                             44          239:53:00               88:00:00             327:53:00            88                  1.15              2.00

Assess 5                       Any member of Assessor                             12           68:28:00               24:00:00              92:28:00            24                  0.31              2.00

Assess 6                       Any member of Assessor                             10           57:47:00               20:00:00              77:47:00            20                  0.26              2.00
Assess 7                       Any member of Assessor                             49          269:37:00               98:00:00             367:37:00            98                  1.28              2.00

Assess 8                       Any member of Assessor                             13           74:45:00               26:00:00             100:45:00            26                  0.34              2.00

Assess 9                       Any member of Assessor                              4           22:13:00                 8:00:00             30:13:00             8                  0.10              2.00

                                                                                                                                                                281                 5.22             12.75

Travel 1                       Any member of Assessor                             51          214:35:00               52:37:00             267:12:00           52.62                1.33              1.03

Travel 2                       Any member of Assessor                             13           50:22:00               26:01:00              76:23:00           26.02                0.34              2.00
Travel 3                       Any member of Assessor                              4           15:14:00               37:59:00              53:13:00           37.98                0.10              9.50

Travel 4                       Any member of Assessor                             44          180:34:00               22:44:00             203:18:00           22.73                1.15              0.52

Travel 5                       Any member of Assessor                             12           49:43:00               24:18:00              74:01:00           24.3                 0.31              2.03

Travel 6                       Any member of Assessor                             10           43:23:00               92:29:00             135:52:00           92.48                0.26              9.25

Travel 7                       Any member of Assessor                             49          205:15:00               25:37:00             230:52:00           25.62                1.28              0.52
Travel 8                       Any member of Assessor                             13           52:01:00               26:01:00              78:02:00           26.02                0.34              2.00

Travel 9                       Any member of Assessor                              4           16:06:00               37:59:00              54:05:00           37.98                0.10              9.50

                                                                                                                                                             345.75                 5.22             36.34

UpLoad 2                       Any member of Assessor                             13           67:52:00                 3:16:00             71:08:00           3.27                 0.34              0.25

UpLoad 3                       Any member of Assessor                              4           17:04:00                 1:21:00             18:25:00           1.35                 0.10              0.34

UpLoad 4                       Any member of Assessor                             44          214:16:00               11:45:00             226:01:00           11.75                1.15              0.27
UpLoad 5                       Any member of Assessor                             12           57:19:00                 3:14:00             60:33:00           3.23                 0.31              0.27

UpLoad 6                       Any member of Assessor                             10           35:06:00                 2:56:00             38:02:00           2.93                 0.26              0.29

UpLoad 7                       Any member of Assessor                             49          235:16:00               13:13:00             248:29:00           13.22                1.28              0.27

UpLoad 8                       Any member of Assessor                             13           63:58:00                 3:16:00             67:14:00           3.27                 0.34              0.25

UpLoad 9                       Any member of Assessor                              4           13:23:00                 1:21:00             14:44:00           1.35                 0.10              0.34
UplLoad 1                      Any member of Assessor                             51          256:08:00               13:55:00             270:03:00           13.92                1.33              0.27

                                                                                                                                                               54.29                5.22              2.55

                                                                                                                                                             681.04                 15.65            51.64

Delay                          Any member of NPS Server                          200             0:00:00                6:46:00              6:46:00           6.77                 5.22              0.03

Forward Assessment             Any member of NPS Server                          200             0:00:00                6:40:00              6:40:00           6.67                 5.22              0.03

ReceiveAssessment              Any member of NPS Server                          200             0:00:00                6:40:00              6:40:00           6.67                 5.22              0.03

                                                                                                                                                               20.11                15.65             0.10

Display                        Any member of PDC Server                          200             0:00:00                6:40:00              6:40:00           6.67                 5.22              0.03
Receive                        Any member of PDC Server                          200             0:00:00                6:40:00              6:40:00           6.67                 5.22              0.03

                                                                                                                                                               13.34                10.43             0.07

Request                        Any member of UN/NGO                              200             0:00:00              33:20:00              33:20:00           33.33                5.22              0.17

                                                                                                                                                               33.33                5.22              0.17



           Resource                          Unit                   Cost/Unit            Threshold                Usage                   Cost           Total Converted         # People          Utilization
                                                                                                                                                         Work time (hr)

Any member of NPS Server       Hour                                              0.2                     0                    20                 $4.00                 20.11                  50         1.05%



                                                                                                     171
Any member of PDC Server     Hour                                            0.2                   0                    13                $2.60               13.34                 50        0.70%

Any member of Assessor       Hour                                             6                    0                   680          $4,080.00              681.04                   20       88.83%

Any member of UN/NGO         Hour                                          13.71                   0                    33            $452.43                 33.33                  1       86.95%

                                                                                                                                                                                    H/(K*$D$3)
                                                                                                                                                                                    worktime/(people*duration)

       Performers Queue Length and Utilization





                        Name                                  Average                           Min                       Max                 Utilized(%)                 Idle(%)


       Any member of NPS Server                                                     0                         0                         0                       1.05                      98.95


       Any member of PDC Server                                                     0                         0                         0                         0.7                     99.3



       Value of ‘Creator’                                                           0                         0                         0                          0                       100


       Any member of Assessor                                                   75.85                         0                       137                      88.82                      11.18


       Generic                                                                      0                         0                         0                          0                       100


       Any member of UN/NGO                                                         0                         0                         0                         8.7                     91.3













                                                                        Figure      5 Costs and Bottlenecks





















































                                                                                               172
       The completion time required for the 200 assessments by 20 assessors/volunteers

was 38.33 hours. With irreparable bottlenecks due to the Assessors’ travel time. Despite

the refinement of the assessment definition (exclude food/ refit/ rest etc.) the assessor

remained heavily utilized at 89%; however of note the assessor’s utilization was reduced
by 7.4%. The unlimited assessments needed with limited time available model remains in

effect and the assessor remains solely focused on assessments which accounts for the

high utilization, however, the slowed assessment input value (10 min vice 2 min) may
account for the reduction of 7.4%. Assessors are still compensated for their high

workload but pay is now restricted to a 12-hour workday which includes transportation

but excludes life support while in the disaster area.

“As Is” “To Be” Comparison

       Despite doubling the number of assessments to 200, the “To Be” model expenses

dropped 87%, wait time decreased by 81% and it took only 1/5 the time to complete.

Unfortunately, Assessor utilization remains high (89%), but is unlikely to be significantly

reduced due to the nature of the disaster scenario. Assessors can rest during the allotted
travel times between assessments. When stretched further, the “To Be” model shows over

500 assessments are possible in approximately 1/3 the time required in the original “As

Is” model, while still reducing costs by nearly 70%. Critical changes made proved to be

the addition of portable SATCOMs via 9 BGAN terminals, and the train the trainer
model, which both greatly improved speed and efficiency. The addition of the user-

friendly app also eliminated a redundant internal cycle and enabled additional manpower

all of which drastically improved the RTAT capability that will hopefully significantly
improve HADR in the future.

CONCLUSION


       Responding efficiently and effectively to large-scale disasters is difficult in the

U.S. and near impossible in third world countries. Compounding the difficulty to relief
efforts is an inability to effectively collaborate due to the nonexistence of or damage to

host nation ICT infrastructure. International relief organization respondents often don’t

know the status of the ICT infrastructure nor what would normally be available. RTAT

                                          173
may not be able to completely eliminate the uncertainty of post disaster recovery

operation, but it can possibly provide answers to some questions such as, “What kind of
communication infrastructure can I expect upon arrival” and what should the respondents

repair first to get the most communication “bang for the buck”?

       For RTAT to effectively benefit the international relief community it must

effectively and efficiently communicate accurate ICT assessments. The problem of relief

effort collaboration in a chaotic post-disaster environment is complex and requires an
intricate solution. A requirement to have boots on the ground is essential at this point to

gather information on an areas infrastructure; however, by automating as much of the

RTAT process as possible costs are dramatically reduced and the process is significantly

sped up providing an effective and efficient way to provide vital IT infrastructure status
to the proper officials. Additional improvements such as the inclusion of remotely piloted

or autonomous vehicles could benefit RTAT in the future, however, in the near term the

improved “To Be” RTAT process has proven to be of valuable benefit in both cost and

time savings in HADR.



























                                           174
         APPENDIX D.      TERRESTRIAL SYSTEMS FORM


      Appendices E–H represent the starting point for the data collection form efforts.

Questions programed into the mobile data collection tool were based on these
appendices.










































                                 175
176
APPENDIX E.    CELLULAR WIRELESS FORM














































                    177
178
179
180
APPENDIX F.    SATELLITE SYSTEMS FORM














































                    181
182
183
184
185
186
APPENDIX G.   RADIO WITH POWER FORM














































                   187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
THIS PAGE INTENTIONALLY LEFT BLANK





























                  202
        APPENDIX H.        RTAT V6 TRAINING POWERPOINT


      Appendix H was created by the author to help explain and teach how to use the

RTAT mobile data collection tool. A similar document was used to train volunteers in the

Philippines and for JIFX 2014–04.

      This appendix is included to give the reader a better understanding of how the
RTAT mobile data collection tool works and to give a better understanding of how

training was conducted for crowdsourced volunteers.





































                                     203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
     APPENDIX I.      RTAT QUICK LOOK REPUBLIC OF THE
                PHILIPPINES SEPTEMBER 2013 V2


      Appendix I is a report done by Marine Forces Pacific and is not readily available

(Chang, 2013). Figures, photos, supporting after action/experimental results are contained
within this document.








































                                  241
      RAPID INFORMATION AND
COMMUNICATION TECHNOLOGY (ICT)
   ASSESSMENT TEAMS (RTAT) V2


        QUICK LOOK REPORT





















   REPUBLIC OF THE PHILIPPINES

          SEPTEMBER 2013



                 242
                                                                           Form Approved
        REPORT DOCUMENTATION PAGE                                       OMB No. 0704-0188
  Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching
  existing data sources, gathering and maintaining the data needed, and completing and reviewing this collection of information. Send comments regarding this
  burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden to Department of Defense, Washington
  Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to any penalty for failing to comply with a collection of
  information if it does not display a currently valid OMB control number. PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS.
  1. REPORT DATE (DD-MM-      2. REPORT TYPE                        3. DATES COVERED (From - To)

  YYYY)                       Quicklook                              21—27 September 2013
  4. TITLE AND SUBTITLE                                             5a. CONTRACT NUMBER
  Rapid Information and Communication Technology (ICT) Assessment

  Teams (RTAT)
  Quicklook Report                                                  5b. GRANT NUMBER
   Quicklook Report
                                                                    5c. PROGRAM ELEMENT
                                                                    NUMBER
  6. AUTHOR(S)                                                      5d. PROJECT NUMBER

  Brian Steckler                                                    5e. TASK NUMBER

                                                                    5f. WORK UNIT NUMBER


  7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)                8. PERFORMING ORGANIZATION
                                                                    REPORT
  AND ADDRESS(ES)ps Forces,                                          NUMBER
  Pacific Experimentation Center

  (Attn: MEC)
  Box 64105

  Camp H. M. Smith, HI 96861

  9. SPONSORING / MONITORING AGENCY NAME(S) AND ADDRESS(ES)         10. SPONSOR/MONITOR’S
                                                                    ACRONYM(S)

                                                                    DEIC
  Defense Environmental
  International Cooperation                                         11. SPONSOR/MONITOR’S
                                                                    REPORT
                                                                     NUMBER(S)


  12. DISTRIBUTION / AVAILABILITY STATEMENT
  DISTRIBUTION STATEMENT A





  13. SUPPLEMENTARY NOTES


          14. ABSTRACT

          This Quicklook Report provides results of the RTAT baseline site surveys/assessments, RTAT

  mobile data collection tool refinement and validation, as well as briefs conducted various entities and
  government officials from 21—27 September at various locations in and around the city of Legazpi in the
  Republic of the Philippines. The RTAT team included participation by the MEC, Naval Postgraduate

  School, Bicol University, University of the Philippines, USAID, World Food Programme, NetHope, the
  Armed Forces of the Philippines and the Philippine National Police. Site surveys included providing hands
  on training to local Barangay Captains and council members and experimentation with various Command

  and Control techniques in the Legazpi .rea
  15. SUBJECT TERMS

This Quicklook Report describes the Rapid Information and Communication (ICT)
Assessment Teams (RTAT) project’s activities in Legazpi city Philippines from 21–            27
  16. SECURITY CLASSIFICATION OF:             17.            18.       19a. NAME OF RESPONSIBLE
  UNCLASSIFIED                                243ITATION     NUMBER    PERSON
                                              OF             OF
  a. REPORT     b. ABSTRACT    c. THIS PAGE   ABSTRACT       PAGES     19b. TELEPHONE NUMBER
                                                                       (include area code)


                                                                          Standard Form 298 (Rev.
                                                                          8-98)
                                                                          Prescribed by ANSI Std. Z39.18
SEP 2013. The RTAT project is a partnership between United States Pacific Command
(U.S. PACOM), United States Marine Corps Forces, Pacific (MARFORPAC)
Experimentation Center (MEC), the Naval Postgraduate School, the national and regional
government early responder community of the Philippines, and the Armed Forces of the

Philippines (AFP). This document provides feedback and observations gathered by the
MEC and Naval Postgraduate School and does not necessarily represent the official
position of the Marine Corps or the Department of the Navy.
This report is approved for public release, distribution unlimited. The use of trade names
in this document does not constitute an official endorsement, approval, or the use of such
commercial hardware or software. This document may not be cited for purposes of
advertisement.

The MEC especially wishes to recognize the collaboration, support, guidance and force
protection provided by the AFP, in particular, AFP Navy Intelligence and Navy
Intelligence Security Forces.





                                                  Shujie Chang, P.E.
                                                  Director, MEC
                                                  October 2013





























                                         244
                             LIST OF FIGURES
Figure 1: Major Beeson briefing local Barangay leadership on RTAT.....................................
Figure 2: Barangay Captain using RTAT Android App............................................................8
Figure 3: Bicol University Welcoming Party............................................................................9
Figure4: Lighthouse Android RTAT Data Collection Application...........................................9
Figure 5: Mr Steckler (NPS) presents RTAT to local community at RTAT Kickoff
Meeting.....10        Figure          6:         RTAT            Hotwash          26

September......................................................................11






















                                         245
                             EXECUTIVE SUMMARY

The U.S. Marine Corps Forces, Pacific Experimentation Center (MEC) conducted the
Rapid ICT Assessment Teams (RTAT) activities with support from the Naval
Postgraduate School (NPS) to engage the Armed Forces of the Philippines (AFP) and
selected members of the Philippines government disaster early responder community.
The focus of this RTAT event was to test and demonstrate the use and effectiveness of
the RTAT hardware, software and web based tools, techniques, and practices (TTPs) in a
real-world environment partnering with real-world disaster early responders.

Climate Change in recent years has been a significant contributor to a huge increase in
the number and severity of natural disasters around the world. Stronger and more
frequent major weather related events (Typhoons, hurricanes, tropical storms, rain-caused

flooding and landslides, tornados, etc.) have challenged the global early responder
community as well as the donor base for Non-Government Organizations (NGOs),
United Nations, and other non-profit international organizations that focus on
humanitarian assistance/disaster relief (HA/DR). One key aspect of providing adequate
response to these disasters is the status of the Information and Communication
Technology (ICT) infrastructure in the affected communities. One glaring gap in the tool
sets of the global early responder community is that they do not know what the status of

the ICT infrastructure is immediately after a disaster. They do not know if or how they
will be able to communication internally or externally once they arrive in a disaster zone.
The purpose of RTAT is to conduct fast, thorough assessments of the ICT infrastructure
immediately after the disaster and on an ongoing basis in the first weeks or months post-
disaster. These RTAT ICT assessments are made available to the global early responder
community as soon as they are conducted. RTAT assessment teams are made up of ICT

subject matter experts who come from academia, industry, UN, NGO, foreign
government/military, and affected nation government/military. RTAT assessments
include reporting status of the disaster zone’s copper/fiber landline systems, cellular
networks, Internet Service Providers (ISPs), UHF/VHF radio infrastructure, RF broadcast
networks (AM and FM for example), and the power grid.
The U.S. RTAT facilitation team’s advance party (four people from the Naval
Postgraduate School (NPS)) spent 21–22 Sept 2013 in Legazpi City, Philippines

conducting RTAT target site surveys and briefing various regional/local Philippines
government and military personnel. 23 Sept was spent with personnel from various
sectors transiting to Legazpi City Philippines - the most disaster-prone city in the entire
country - and obtaining RTAT tools familiarization and training by the NPS team. The
complete RTAT team included: NPS, MEC, AFP, national, regional and local
government personnel, local academia and the RTAT event host Bicol University. After
training, the entire RTAT team conducted actual RTAT assessments from 24–   26 Sept in

Legazpi City Philippines. These RTAT personnel divided into 6 teams of 7–8 personnel
each and drove vans to the various pre-designated RTAT locations and conducted the
RTAT surveys for those three days. During these RTAT tests and training sessions, the
team also interacted with numerous local government leaders including the Albay
Province Governor’s office and the Legazpi City Mayoral office.



                                         246
                                  INTRODUCTION
RTAT Purpose
The first hours and days after the onset of major global disasters are typically fraught
with chaos and lack of situational awareness. While there are existing disaster assessment

teams from major organizations that deploy to such events, these teams primarily focus
on sector specialty areas other than Information and Communications Technology (ICT)
and Information Sharing. The ICT sector is critically important as it enables and supports
all others.
The arrival of the global response community usually brings a welcome and powerful
ICT capacity resource, but sometimes their arrival and the accompanying ICT equipment
and capabilities do not link effectively with the host nation ICT and power suppliers. This

can mean that the effectiveness of the combined available resources are not maximized,
leading to gaps and duplication when there may be enough technical solutions present to
accommodate all requirements.
Recent disasters have shown the reluctance of the affected host country to request
international assistance, this trend is expected to continue. These disasters were managed
internally with only very specific host nation assistance requests being made.
Unfortunately these disasters revealed that the needs outpaced the host nation’s resources

and the affected nation was unable to provide an accurate assessment of ICT and power
needs to responding international efforts. As stated, this information is critical to enable
collaborative relief efforts that lead to focused and timely support to the affected
population. This vital information will directly reduce suffering and the overall recovery
time for the affected nation.
      Specific problems include:


          In the immediate aftermath of a major disaster there is often a gap in the
           knowledge of ICT infrastructure status and a lack of communication between
           the International Humanitarian Community (IHC) arriving on scene and the
           affected state’s national infrastructure stakeholders.
          We do not know how to get the overall ICT infrastructure Common Operating

           Picture in the hands of the effected state and the IHC as well as to the
           ISP/GSM/Telecom Ministers, etc.
          We do not know how to discover the methods and resources being used in a
           disaster for sharing information up and down the chain between the national,
           government, and infrastructure providers.
          There is no coordinated approach today of establishing a common situational

           overview of this ICT infrastructure
          Current assessment methods are limited as no single agency has the resources
           to perform a comprehensive assessment of the ICT situation

What Exists Now:
There are teams that currently perform some very basic ICT assessment functions

including ( i) the United Nations Disaster Assessment and Coordination (UNDAC)
international emergency response system whose core mandates are assessment,
coordination and information management to assist the UN and governments in an

                                          247
emergency; (ii) the Emergency Telecommunications Cluster (ETC); ( ii) the International
Federation of Red Cross/Red Crescent (IFRC) First Assessment and Coordination Teams
(FACT); and iv) ICT based NGO’s such as NetHope and TSF have some assessment
responsibilities. Some of these teams are on standby to deploy rapidly (in 12–24 hours) as

required.
The Requirement:
A proposed solution is to create/ fund RTAT teams - creating the ability to rapidly deploy
ICT Assessment Teams—small, nimble, multi-organizational, multi-national integrated
teams of specialists in key ICT areas (wireless data communications, voice
communications, radio technologies, power, information sharing, social networking,
etc.). The real niche this program represents is that the teams can be made up of experts

from a variety of different organizations –industry, UN, NGO, academia, IO, affected
nation government and military, and international governments/militaries.
Once a comprehensive overview of the ICT situation has been established, a priority list
of ICT needs can then be drawn up in coordination with the host nation.
The RTAT team will also be requested to provide specific ICT disaster assessments in the
event that full International assistance has not been requested by the host Nation.
RTAT Teams Provide:

Field data containing both Host nation and IHC information and communications
technology and power needs and capabilities.
Quality assessment of this information by experts and the distribution of reliable, trusted
information.
This Initiative does not seek to duplicate any existing process but to reinforce and enable
the existing internationally accepted processes by meeting a need that is recognized but

that is not currently effectively being met. Concentrating on human interfaces and not
technology, the team of highly trained inter-organizational personnel will identify and
find answers to specific questions, compile a common operations plot and link with the
host nation and the IHC enabling fast early recovery.
Specific requirements or capabilities include:
        Having the ability to quickly deploy (within 24 hours)

       Having direct links to local industry and government
       The ability to stay in the disaster zone 1–2 weeks, then reassess need to remain
         longer or to rotate in new teams
       The team having access to ICT expertise across the functional spectrum (ISP,
         cellular, data networks, power, etc.) with both the international technical
         community  as well as local/national citizen experts

       To have a keen understanding of the need to work in close collaboration with
         existing teams on the ground

Who the RTAT teams are made up of:
Ideally these small teams of experts are composed of 1–2 representatives from each of the
following organization types: UN, NGOs, IGOs, academia, industry, military and

government agencies from around the world. The formal/legal/business organizational
makeup of the overall program and teams themselves would be determined by the
founding member organizations.

                                          248
The leadership of the teams should be:
             Team Leader (from either the global or regional technical community)

             National (affected state) Member (National Disaster Management Agency
              or Ministry of Communications or equivalent organizations affiliated for
              example)

What the RTAT teams readiness status should be:
Small teams of qualified/trained experts from across the ICT spectrum on 24x7 stand-by
to deploy as soon as possible but likely for 1–2 weeks in shifts.

We believe that before being ready and able to deploy to a specific disaster zone there
should be a BASELINE ICT/Info Sharing assessment capability in place. These
assessments should be accomplished well ahead of time in each country prone to regular
disasters at a minimum. Such assessments could be done by RTAT supporting entities
such as industry and academia. The benefits for such assessments, which would be
provided to the host nation government, would go well beyond the RTAT concept and be
able to point out potential general ICT vulnerabilities and resilience gaps to all concerned

parties.
Where the RTAT team members should be located:
RTAT teams should be stationed at key locations around the world, perhaps modeled
after the UN Disaster Assessment and Coordination Teams program, or possibly as
associate members of NetHope, the UN UNDAC, ETC or other similar teams. These
teams should be called on by the host nation, UN/OCHA, or a regional entity such as
ASEAN.





Legazpi City 24–26 September RTAT Assessment Concept of Operations (CONOP):

WHO: Led by the Naval Postgraduate School’s Hastily Formed Networks (HFN) Center,
this initiative to conduct a baseline Information and Communication Technology (ICT)
assessment in Legazpi City, Albay Province, Philippines between 24 and 26 SEP 2013,

included a number of people from the Philippine government and military communities.
The invited participants included people from Manila and locally in the Albay Province /
Legazpi City area from NDRRMC, DOST, OCD, Manila Observatory, Philippines
National Police, University of the Philippines, Bicol University (project hosts), the NGO
consortium NetHope, the Japanese Civil Response community, U.S. military and
academia, and others.
WHAT: The group divided into up to 6 teams of approximately 8 members each. Each

team was equipped with vans, drivers, security and the teams featured mixed subject
matter expertise to conduct these ICT baseline assessments in the most disaster prone
region of the Philippines (Legazpi City and Albay Province region). The SME’s
conducted baseline assessments of cellular, UHF/ VHF, land lines such as copper and
fiber, Internet service provider provided access, satellite communications, meshed Wi-Fi,
wireless bridges with WiMAX or LTE, and alternate power systems to better prepare the

                                         249
community for the next catastrophic event. Having a completed baseline ICT assessment
will smooth host nation and international responses during the next real world disaster.
The local/regional early responder community will already have critical information on
the communications infrastructure and will not have to try to dig this information up

while in the chaos of a real-time disaster.
WHEN: The baseline ICT assessment was conducted in Legazpi City between 24 and 26
September 2013.
WHERE: In the vicinity of Legazpi City, Albay Province, Philippines with Bicol
University as the base of operations. Bicol University has also volunteered to be our host
and to provide classrooms for training sessions and meetings as well as their contact
network to help determine exactly where in the area we will be conducting the baseline

ICT assessments.
WHY: Since RTAT has never before been used to conduct actual pre- or post-disaster
ICT assessments, this opportunity to do a real-world, real country assessment is very
important to the overall RTAT development process. The RTAT ICT assessments
conducted this week also coincides with one of the agreed upon projects of the year-old
Kabalikat Science and Technology Innovation Initiative (STI2) that involves the same
organizations listed above (NDRRMC, DOST, OCD, Manila Observatory, AFP, PNP,

UP, Bicol University, etc.).
HOW: The RTAT effort will utilize a U.S. Naval Postgraduate School (NPS) created
hardware/software application system called Lighthouse to do the data collection and
dissemination. This Lighthouse application is Android based, and heavily leverages
standard Android smart phones and tablet computing devices during the week to
document the ICT status and information collected. The data was sent in real time (via

Wi-Fi or cellular connections) up to a data server housed at the Naval Postgraduate
School in Monterey, California, U.S.A.
Timeline of RTAT activities in the Philippines in September 2013:

21 September, Legazpi City, Bicol Province, Philippines
The U.S. RTAT Advance Team (Naval Postgraduate School personnel including faculty
member Brian Steckler and students, Major Travis Beeson, Capt Jennifer Gladem and LT
Jason Chamberlain) transited from Metro Manila to Legazpi City, Albay Province,

Philippines to conduct initial site surveys of the six locations pre-determined to be
assessed using the RTAT process later in the week. The NPS RTAT Advance Team was
met at the Legazpi Airport by the Bicol University personnel who were to be our hosts
for the RTAT week. After checking into the hotel, the team and commenced the site
surveys at three of the six pre-determined RTAT locations, briefing Barangay Captains
(local communities and the senior elected official in each community) on what RTAT is
and what we would be doing later in the week with the full RTAT assessment effort with

about 50 RTAT assessors in 6 teams of 7– 8 people.







                                          250
                     Figure 1 - Bicol University Welcoming Party
















      Figure 2—RTAT Team receives a local briefing by the Oro Barangay Captain


22 September, Legazpi City, Bicol Province, Philippines

The U.S. NPS RTAT Advance Team continued site surveys at the final three Barangays,
briefing Barangay Captains on RTAT concepts, and training Bicol University and
Barangay personnel on the operation of RTAT applications.















                                         251
         Figure 3 - Major Beeson briefing local Barangay leadership on RTAT






















                Figure 4 - Barangay Captain using RTAT Android App





23 September, Legazpi City, Bicol Province, Philippines
Full RTAT participation group from Manila transit to Legazpi City from Metro Manila

and are greeted by our Bicol University hosts. The RTAT participants transiting to
Legazpi City from Metro Manila included personnel from the Armed Forces of the
Philippines, Manila Observatory, University of the Philippines, Department of Science
and Technology, Office of Civil Defense, National Disaster Risk Reduction Management

                                         252
Center, and Philippine National Police. After checking everyone into the hotel (The

Oriental) there was an all hands orientation and training session on the use of the RTAT

application (combination of NPS’s Lighthouse data collection app and android phones).


                         5. Type of Information         6. How will you input location 6.a. Device GPS: select
                         Communication Technology (ICT) ‘Device’ requires imbedded GPS ‘record location’ when accuracy
                         Fill out separate forms for multiple                          is displayed.
                         services at the same location




                                                       a
                                                       b














              Figure 5 - NPS Lighthouse Android RTAT Data Collection Application

























































                                                                 253
24 September, Legazpi City, Bicol Province, Philippines

The full contingent of RTAT assessors (U.S., Filipino, other international volunteers) was
now in place in Legazpi City and ready to commence the actual RTAT assessments.

The 24th started with an RTAT kickoff session at Bicol University in which the Albay
Province representative made opening comments. The Legazpi City Mayor welcomed
everyone to town and thanked everyone for conducting the RTAT assessments in his city
and the Bicol University President welcomed everyone. Then the NPS faculty lead, and
RTAT overall coordinator, Brian Steckler, provided a RTAT presentation and the week’s
concept of operation plan. Major R. Travis Beeson (USMC) finished the meeting by
providing detailed RTAT application training to all attendees. The participation at this

initial RTAT meeting was very impressive and very well attended and included 81
participants from 42 organizations.















   Figure 6 - Mr Steckler (NPS) presents RTAT to local community at RTAT Kickoff
                                       Meeting



After the opening meeting, participants were divided into 6 teams of 7–8 people. Each
team received assignments of which Barangay to go to for their RTAT assessments.
EACH team van had working cell phones, and cellular and or on-the-move satellite
Internet connectivity. Command & control between the lead van (Brian Steckler) and the
team van leadership was enacted primarily through Skype chat, with cell phone and text
as back up.This enabled real time coordination for all team movements.
The evening of the 24th featured a Welcome Dinner hosted by the Legazpi City Mayor.











                                          254
25–26 September, Legazpi City, Bicol Province, Philippines
The teams rotated through the different Barangay sites on the 25th and 26th to conduct
their baseline RTAT assessments and to brief the local Barangay leadership. The teams
got together nightly for next day planning and hot wash. Lasting up to two hours, the

teams went over all of the lessons learned, command and control issues, site assessment
discoveries, and identified several RTAT application areas for improvement.

















                        Figure 7 -RTAT Hot wash 26 September


The evening of the 26th was a farewell dinner hosted by the Governor’s office (he was
unable to attend as he was unable to make it back to the area from Manila).
























                                          255
                                     SUMMARY

The RTAT baseline ICT assessment effort conducted from 24–26 September was very
successful. We had the full support (and participation) of national, regional and local

leadership from the very beginning. The U.S. RTAT leadership from NPS and the MEC
were well received and welcomed throughout the week by all levels of leadership and
everyone understood how this RTAT effort could significantly enhance their disaster
preparedness and improve their key capabilities in disaster risk reduction and resilience.
The event was very well attended with as many as 81 participants coming from 42
different entities including those from academia, industry, UN, NGO, U.S. government,
military, and law enforcement as well as Philippines national, regional and local

leadership and other government agencies. U.S. participants made up about 20 percent of
the people for the overall event. Defense related entities (U.S. and Philippines) made up
about 15 percent of the total entities.

The RTAT effort successfully began a much longer and far reaching process of base-
lining ICT infrastructure in this community and provided experience and understanding
of the RTAT process to key leaders up and down myriad national and local government

organizations. The RTAT baseline assessment process conducted this week also enhances
the overall RTAT program by providing many lessons learned and identifying many
tweaks to the Lighthouse RTAT data collection application that will help with all future
RTAT missions around the globe.




























                                          256
   APPENDIX J. TYPHOON HAIYAN AFTER ACTION REPORTS


      Appendix K is an internal RTAT after action report written by both RTAT waves

during Typhoon Haiyan (Steckler, 2013). This supporting information is not available by

any other means. Figures, photos, supporting after action/experimental results are
contained within this document.










































                                     257
                    After Action and Lessons Learned Report:


RTAT Lighthouse Application Deployment in Support of Typhoon Haiyan (Yolanda)


                           Republic of the Philippines


                             28-29 November 2014





                                R. Travis Beeson


                                11 February 2014


















                                      258
       On 8 November 2013 Typhoon Haiyan (Yolanda) made land fall in the Republic

of the Philippines (ROP). “Typhoon Haiyan (Yolanda) was one of the strongest typhoons
(cyclones) to strike land on record. Over a 16 hour period, the ‘super typhoon,’ with a

force equivalent to a Category 5 hurricane and sustained winds of up to 195 mph, directly

swept through six provinces (Lum & Margesson, 2013). This storm affected over 16

million people displacing 4.1 million people through the destruction of 1.1 million homes
and resulting in over 6,200 deaths (United States Agency for International Development,

2014).

Background


       Prior to Typhoon Haiyan, a team from Naval Postgraduate School (NPS) traveled

to the Legazpi City, ROP in order to introduce, test and validate the Rapid Information
and Communication Technology (ICT) Assessment Team (RTAT) concept and a

developed mobile data collection form (more below) for ICT assessment. Bicol

University, as well as, numerous other volunteers from academia, government officials,
representatives from the Armed Forces of the Philippines (AFP) and experts from the

disaster response community helped refine the RTAT concept and the ICT assessment

form.

       The resultant mission of RTAT became “To conduct and distribute baseline and

post-disaster ICT infrastructure assessments, in order to facilitate host nation and

international disaster relief efforts,” (Steckler, 2012). To ‘facilitate’ now included the
management and dissemination of a shared common operational picture and ICT

recovery prioritization recommendations. To facilitate the common operational picture

aspect, data collected from Lighthouse on NPS servers would be shared with the Pacific
Disaster Center (PDC) to be displayed on their Emergency Operations DisasterAware

website.

       The RTAT Concept of Operations was refined to be:

       Deploy ICT experts to the disaster zone

       Obtain local support/volunteers
       Assess the ICT with Lighthouse


                                         259
       Aggregate the data on servers back at NPS

       Share (manually/automated) this data with Pacific Disaster Center for display on
               their disaster website
       Disaster responders could then access this website to obtain the latest ICT status

               information to aid in disaster recovery efforts.
       The aforementioned mobile data collection form is based on the Open Data Kit

suite of tools. “Open Data Kit (ODK) is a free and open-source set of tools which help

organizations author, field, and manage mobile data collection solutions. ODK provides
an out-of-the-box solution for users to:” Build data collection forms (surveys), collect the

results from mobile platform to a central server, and aggregate/transform the data into

other useful format (http://opendatakit.org/, accessed 2/13/14). ODK is facilitated through

the NPS developed Lighthouse application and is compatible with any Android based
device. The RTAT concept and Lighthouse was refined during/after the September 2013

Legazpi City test in preparation for a future disaster deployment. The ODK developed

form residing on the Android device and facilitated through the Lighthouse application
will be referred commonly in the rest of this report as ‘Lighthouse’ or ‘Lighthouse

application’.

       Overarching concept of operations is: Deploy a team of competent IT experts and

obtain local support/volunteers. Assess ICT infrastructure utilizing Lighthouse.

Typhoon Relief Efforts


       Prior to typhoon Haiyan making land fall on 8 November, US forces were already

responding in anticipation of the damage resulting from the extremely high wind and
storm surge. Within days of the response, a team from Naval Postgraduate School was

requested to aid in the US DOD relief efforts then known as Operation Damayan.
                                                        rd
Providing initial satellite communication support to 3 Marine Expeditionary Brigade,

the team was re-tasked to support Joint Task Force JTF-505 on approximately 25
November and assign to assist the Armed Forces of the Philippines. Upon transitioning to

the JTF, the NPS team was joined by a group from the Roddenberry Foundation and

faculty from Bicol University, who participated in the September RTAT event, in order
plan and execute the first real world deployment of RTAT and the developed Lighthouse


                                           260
application. On 26 November the entire team moved to Mactan Air Base, near Cebu City,

ROP in order to be better positioned for operations in the Tacloban City disaster area. In
Mactan, RTAT operationally supported Brigadier General (BGen) Santiago (AFP). Not

coincidently BGen Santiago participated in the September Legazpi City event and was a

proponent of the RTAT concept and was ready to get RTAT into the disaster zone to help

with assessments. Connections with BGen Santiago and a local volunteer group known as
‘Team Patola’ netted over 40 volunteers that showed up for training on the 27th. Training

consisted of approximately three hours of classroom instruction and two hours of

practical application utilizing the Lighthouse tool. Classroom training included: What is
RTAT and the Roddenberry Foundation, how to visually identify ICT and Power

infrastructure and conduct ICT assessments, what to expect in a disaster zone, life

support and safety and how to utilize the Lighthouse tool. Two hours of practical

application paired five (+-) students with a trained person and walked around the local
area practicing the collection and transmission of assessments. Lessons learned from the

training:

       Local Audio/Video and power compatibility was an issue with the presentation.

Brings lots of power plug and HDMI to XX adapters.

       Volunteers for the training were acquired through a Team Patola Facebook post.

24-48 hour notice would be optimal, but the 12am post still netted more volunteers than

we could handle at the 12pm training (15 + volunteers).

       Many Volunteers had no idea how to identify various ICT equipment and

technical background/expertise highly varied amongst the volunteers. As a result an ad
hoc class was added to the curriculum and given after the practical application training. A

handbook with pictures of antennae and power infrastructure should be developed for use

by inexperienced volunteers. One weatherproofed handbook per team should suffice.

       Education certificates are a big deal in the ROP and certificates should be printed,

signed and given out to those participating in the training.

       On 28 November the 14 RTAT personnel were transported on a South Korean C-

130 into Tacloban City. This Military Air (MILAIR) flight was arranged through BGen

                                           261
Santiago. The personnel were organized into 2 main teams with one team having the

option to further split into two teams. Two main teams were led by the Roddenberry
Foundation and the optional team was led by Bicol University. The teams started out

visiting the local government officials, the Philippine National Police (PNP) local

headquarters, AFP contingent, and the United Nations (UN) Non-Governmental

Organization (NGO) coordination center. Each of visited locations netting critical local
conditions, security and needs information. Minimally courtesy visits should be paid to

each of the above before operating in the area. The UN updated local ICT conditions and

requested assessments be made in the local area and to the south of Tacloban City along
Maharlika/Pan-Philippine Highway to Abuyog, see Figure 1.


















Figure 4 Screen shot from PDC EMOPS with RTAT Assessments accessed 2/13/14
       Transportation and shelter in an available gym was arranged through Team

Patola. Security was a layered approach-A loose courtesy over watch was arranged

through the PNP with AFP personnel organic to each team. Each van had a local guide

and the numerous local volunteers provided the needed language interpretation. There
was one team member that remained behind in Mactan to coordinate team evacuation

should the need arise. Lessons learned.

       Cultural differences in regards to diet and expectation led to a couple of faux pas

that could be avoided. Tell the driver to ‘pack lunch’ and he’ll bring his own food. You

are expected to stop for lunch, not eat on the way between locations. Local diet consists


                                          262
mainly of rice with some vegetable and maybe a little meat. The RTAT carried

powerbars and heavier meat rations were not as well received as thought.

       Water was a significant issue. Teams carried water purification bags and a couple

of liters each into Tacloban, but there were no fresh water procurement sources and the
teams didn’t have 8 hours in a secured sunlit area to use while the bags purified the water

if fresh water was available. Unfortunately the RTAT had to rely on another NGO for

water.

       Communication was an issue as the Broadband Global Area network (BGAN)

satellite data system was left in Mactan and the local cellular data collection was
intermittent. Data from the Android devices could not be uploaded to the NPS servers

until they returned to Mactan hotel on 29 November.

       Tracking the team was difficult via intermittent text and voice. However, the Spot

GPS tracker was used as a tertiary means of tracking the teams and worked quite well.












                                 Figure 5 Spot GPS tracking of Team 1

       Data transfer to PDC was thwarted by form changes and the lack of server Letters

of Agreements to share data in an automated fashion. PDC was able to post data to their

website Late in December after the data from two separate forms were joined to a single
submission. The process of putting a data set onto the website takes minimally 3 hours

and adding a new form requiring a new website layer could take up to 3 weeks with the

current PDC work load. Bottom line the form needs to be finalized before PDC can

commit to creating yet another RTAT layer on the DisasterAware website and this
process needs to be automated to meet end user expectation.


       Lighthouse application feedback.
                                           263
        Users were a little frustrated with the order of questions. Assessors could get the

GPS location early in the form but then have to remain at the location, often in the rain,
until the end of the form to snap a photo before completing the form.


        Assessments could’ve been completed when enroute to the next location if the
form was more efficiently structured. While interviewing the local expert or caretaker for

the ICT site there were long breaks between questions that needed their input. This

resulted in awkward pauses requiring another team member to carry the conversation, to
keep the person from leaving, while the assessor answered other form questions.


        Query structure was such that only one type of ICT infrastructure could be
assessed per form. This lead to lengthy pauses at one location when for example a

cellular tower also held a VHF radio antenna and was linked via fiber optic cable. This

example would require three separate assessments for the physical structure.

        Weather was a significant factor. One phone (Lighthouse application device) was

lost due to rain. The devices need to be waterproof. The Otterbox Defender cases worked

well to ruggedize the cases, but were not water proof.

        Power to charge the phones was an issue. CrisisSignal.apk was installed on the

phones to aid in another post disaster assessment project. The application would send the
cell signal strength and send data to an aggregate server on a regular basis. The settings

should have been modified to lower the power usage and only one phone per team should

have been running CrisisSignal in order to conserve battery strength. Additionally, the
teams didn’t think until late on day one to keep all the phones off except the one in use to

elongate their ability to assess. Power to recharge was available on a limited basis, but

would require a team member to stand ‘gear guard’ while the phones charged at a public

recharging point (see Figure 3).










                                           264
                                Figure 6 Public Charging Station

        Specific recommendations for improving the Lighthouse application and the

utilized Android devices were:

        GPS should start sensing as soon as the application opens, not when the user

selects the ‘GPS’ option on ‘How will you record the location’ question. This led to
significant delays.


        Place the Picture and GPS (location) at the beginning of the form.

        Restructure the form to be able to assess more than one type of ICT infrastructure

on each form.

        Form needs to be rapid and more focused to the point. Where are you, what’s

wrong, what do you need to fix it.

        Remove redundant questions. For example there is no need for the address if you

have the GPS coordinates, and no need to assess, primary, secondary and tertiary power
if none of them work.

        Waterproof/ruggedize the device. Continue to use the Otterbox defender phone

case to protect the phone, but switch the device to the Samsung S4 Active (waterproof

Android based phone), or utilize a clear touchscreen compatible waterproof bag for

optimal protection in wet conditions.

        Phone should have AM/FM broadcast radio receiver capability to assess where

the broadcast are able to reach.

                                           265
       Extra batteries with a means to charge them off grid needs to be procured before

the next mission. Vehicle cigarette lighter adapter with enough cell phone charge cables
(10+ USB outlets) should also be procured.

















































                                          266
                                     Bibliography


USAID (2014). PHILIPPINES—TYPHOON YOLANDA/HAIYAN. Fact sheet #20

       Fiscal Year 2014, dated 24 January, 2014. Department of State. Washington, DC.


       http://www.usaid.gov/sites/default/files/documents/1866/01.24.14_USAID-

       DCHATyphoonYolandaHaiyan_FactSheet20.pdf, accessed 2/11/14


Lum, T., and Margessen, R. (2013). Typhoon Haiyan (Yolanda): U.S. and International


       Response to Philippines Disaster.gressional Research Service. Washington,

       DC.

       http://www.fas.org/sgp/crs/row/R43309.pdf, accessed 2/11/14.



                         CONCEPT OF OPERATIONS

   RAPID INFORMATION AND COMMUNICATION TECHNOLOGY (ICT)
                        ASSESSMENT TEAMS (RTAT)
FOLLOW-ON ASSESSMENT          - CEBU, TACLABON AND LEYTE PHILIPPINES
                  RTAT IN THE PHILIPPINES 8-11 DEC 2013:
                               Mr. Brian Steckler

WHO: Led by the Naval Postgraduate School’s Hastily Formed Networks (HFN) Center,

this initiative, which will take place from Dec 8th to Dec 11th, and is designed to conduct
continued assessment of the Information and Communication Technology (ICT) from
Tacloban to Borongan in the aftermath of Typhoon Haiyan (Yolanda). The assessment
teams will consist of US DoD personnel, a number of civilian personnel from Manila,
local government agencies from Tacloban, Armed Forces of the Philippines (AFP) and
Philippine National Police (PNP) entities.

WHAT: The group will divide into 2 teams of 5-6 people with vans/drivers and the
teams will feature mixed Subject Matter Expertise (SME) to conduct the ICT assessments
in the areas still suffering communication gaps from Typhoon Haiyan (Yolanda). The
SMEs will conduct assessments of the primary nodes which are significantly degraded or
none operational to include cellular, UHF/ VHF, land lines such as copper and fiber,
Internet service provider provided access, satellite communications, meshed WiFi,
wireless bridges with WiMAX or LTE, and alternate power systems to better prepare the

community for the next catastrophic event. Assessing the ICT within this area will help

                                      267
evaluate the network and provide vital information for future responses during real world
disaster(s). The local/regional government will have critical information on the
degradation of the communications infrastructure as well as an assessment of where to
focus resources to restore communications and provide valuable information on what

infrastructure needs to be hardened in the event of future disasters.
WHEN: The ICT assessment will be conducted in vicinity of Tacloban up to and may
include Borongan between 8 and 11 December 2013.
WHERE: The Tacloban airport will be the base of operations for the assessment teams.
The assessment team will arrive at Cebu on 8 December and conduct training at the
Marriot in Cebu from 1830 to 2130. On 9 December the assessment team will depart
Cebu/Mactan Airbase and travel to Tacloban city. Due to DoD restrictions prohibiting

DoD personnel traveling on foreign military aircraft all DoD personnel will depart Cebu
utilizing commercial aircraft. Civilian assessment team members will depart utilizing
military aircraft. They will all meet up at the Tacloban airport. The assessment team will
billet at Tacloban airport on December 9th utilizing established tents on premises. On
December 10 the assessment team will depart Tacloban airport and proceed north on Pan-
Philippine Highway (AH26) by vehicle to Guiuan via Basey, Marabut, Balangiga,
Gilorlos, Quinapondan turning southeast after Quinapindan towards Salcedo to Guiuan.

From Guiuan the teams will proceed back north to Salcedo to General MacArthur,
Llorente Balangkayan, Maydolong to Borongan City. All teams will travel east to Guiuan
and then proceed north towards Borongan. At the conclusion of assessments on
December 10 all teams will retrograde to a designated location along the route and
billeting. All members will be billeted in tents with security provided by PNP personnel.
The location of the billeting area will be dependent on assessment teams progress.

WHY: RTAT has limited exposure to post-disaster ICT assessments in this area, this
opportunity to do a real world, real area assessment is very important to the overall
RTAT development process. The RTAT ICT assessments to be conducted also coincided
with one of the agreed upon projects of the year-old Kabalikat Science and Technology
Innovation Initiative (STI2) that involves the following organizations NDRRMC, DOST,
OCD, Manila Observatory, AFP, PNP, UP, Bicol University. For this specific assessment
the AFP has requested us to travel in and around the Tacloban area gathering information

in order to understand the vulnerabilities present within their communication
infrastructure in order to properly prepare for future disasters
HOW: The RTAT effort will utilize a US Naval Postgraduate School created
hardware/software application system called Lighthouse to do the data collection and
dissemination. This Lighthouse application is Android based, and we will heavily
leverage standard Android smart phones and tablet computing devices during the week to
document the ICT status and information collected. The data will be sent in real time or

near real time (via WiFi or cellular connections) up to the CORE Lab’s data server
housed at the Naval Postgraduate School in Monterey, California, USA. The CORE lab
will then process the data and forward all data to Pacific Disaster Center (PDC) who in
turn will populate all data on their website (www.pdc.org).s All data will then be
accessible for official use to registered users utilizing their user name and password.




                                          268
RTAT Background and Executive Summary
Overview.
Information and Communication Technology (ICT) and power sectors are critical to the
response after major disasters. Currently existing post disaster assessments focus on areas

other than ICT, power and Information Sharing. A rapid assessment of the ICT status will
enable the host nation and the International humanitarian community to provide a
targeted allocation of resources and result in a reduction of gaps and duplication of effort.
The Rapid Technology Assessment Teams (RTAT) concept seeks to provide a pool of
multi-disciplinary experts who will rapidly deploy to the disaster zone to provide this
information. The RTAT concept is supported by many organization and individuals
within the ICT disaster response community and is in the process of obtaining further

funding. A crucial part of the development of the initiative is to gather support for the
adoption of the concept by key disaster prone countries. Their involvement will enable
RTAT to tailor responses based on specific country needs and to ensure that processes
and operations will be as effective as possible.
The Problem:
The first hours and days after the onset of major global disasters are typically fraught
with chaos and lack of situational awareness. While disaster assessment teams exist from

major organizations that deploy to such events, these teams primarily focus on sector
specialty areas other than ICT and Information Sharing. The ICT sector is critically
important as it enables and supports all other relief efforts.
Arrival of the global response community usually brings a welcome and powerful ICT
capacity, but sometimes their arrival and the accompanying ICT equipment and
capabilities do not link effectively with the host nations ICT or each other. This means

that the effectiveness of the combined available resources are not maximized, leading to
gaps and duplication.
Additionally, the host country often does not request international assistance after a
disaster. In this case, the disasters have often been managed internally with international
requests only made for specific assistance which can cause the host nation’s resources to
be stretched and unable to provide an accurate assessment of ICT and power needs thus
creating a significant gap in ICT assessments. Complete ICT information is critical to

obtaining targeted support that will enable the response, business recovery, and minimize
the effects of the disaster on the population.
      Specific problems include:

          In the immediate aftermath of a major disaster there is often a gap in the
           knowledge of ICT infrastructure and a lack of communication   between the

           International Humanitarian Community (IHC) and the host nation’s national
           infrastructure.
          We do not know how to get the overall ICT infrastructure Common Operating
           Picture in the hands of the affected state and the IHC as well as to the
           ISP/GSM/Telecom Ministers, etc.
          We do not know how to discover the methods and resources being used in a

           disaster for sharing information between the national, government, and
           infrastructure providers.

                                           269
          There is no coordinated approach today of establishing a Common Operating Picture
           (COP) of this ICT infrastructure.
          Current assessment methods are limited as no single agency has the resources to
           perform a comprehensive assessment of the ICT situation.

What Exists Now:

There are teams that currently perform some very basic ICT assessment functions. Some
of these teams are on standby to deploy rapidly in 12 -24 hours.
    1. The United Nations Disaster Assessment and Coordination (UNDAC)
       international emergency response system whose core mandates are assessment,
       coordination and information management to assist the UN and governments in
       an emergency.
    2. The Emergency Telecommunications Cluster (ETC)

    3. The International Federation of Red Cross/Red Crescent (IFRC) First Assessment
       and Coordination Teams (FACT)
    4. ICT based NGO’s such as NetHope and TSF have some assessment
       responsibilities.

The Requirement:

The proposed solution would create the ability to rapidly deploy small, nimble, multi-
organizational, multi-national integrated assessment teams of specialists in key ICT areas
such as wireless data communications, voice communications, radio technologies, power,
information sharing, social networking, etc. The real niche this program represents is that
the teams can be made up of experts from a variety of different organizations such as
industry,  UN,    NGO,     academia,    International  Organizations,   affected   nation
government/military, and international governments/militaries.

Once a comprehensive overview of the ICT situation has been established, a priority list
of ICT needs can be drawn up in coordination with the host nation.
The RTAT teams can also be requested to provide specific ICT disaster assessments in
the event that full international assistance has been requested by the host nation.
The Teams Will Provide:
Field data containing both host nation and IHC information as well as communications
technology and power needs and capabilities.

Quality assessment of this information by experts and the distribution of reliable, trusted
information.
This Initiative does not seek to duplicate any existing process but to reinforce and enable
the existing internationally accepted processes by meeting a need that is recognized but
that is not currently being effectively met. By concentrating on human interfaces and not
technology, the team of highly trained inter-organizational personnel will identify and
find answers to specific questions, compile a common operating picture and link with the

host nation and the IHC enabling fast early recovery.

Specific requirements or capabilities include:
        Having the ability to quickly deploy (within 24 hours)
       Having direct links to local industry and government


                                           270
       The ability to stay in the disaster zone 1-2 weeks, then reassess need to remain
         longer or to rotate in new teams
       The team having access to ICT expertise across the functional spectrum (ISP,
         cellular, data networks, power, etc.) with both the international technical

         community as well as local/national citizen experts
       Understanding the need to work in close collaboration with existing teams on the
         ground

Team Makeup:
Ideally these small teams of experts would be composed of 1-2 representatives from each

of the following organization types: UN, NGOs, International Government Organizations
IGOs, academia, industry, military and government agencies from around the world. The
formal/legal/business organizational makeup of the overall program and teams
themselves would be determined by the founding member organizations.
The leadership of the teams should be:
      Team Leader (from either the global or regional technical community)

      National affected state Member (such as National Disaster Management Agency,
       Ministry of Communications or equivalent affiliated organizations )

We still need to determine:
      Skill sets, qualifications and exact number of people to make up each team

      Current thinking is to have government and/or industry experts from the Internet
       Service Provider (ISP) industry, the GSM/other cellular/landline industry, the
       power infrastructure industry, the wireless broadband industry, and the satellite
       communications industry.

Teams Readiness Status:
Small teams of qualified/trained experts from across the ICT spectrum on 24 X 7 stand-

by to deploy as soon as possible but likely for 1-2 weeks in shifts.
 We believe that before deploying to a specific disaster zone there should be a
BASELINE ICT/Info Sharing assessment capability in place. These assessments should
be accomplished well ahead of time in each country prone to regular disasters. Such
assessments could be done by RTAT supporting entities such as industry and academia.
The benefits for such assessments, which would be provided to the host nation

government, would go well beyond the RTAT concept and be able to point out potential
general ICT vulnerabilities and resilience gaps to all concerned parties.

Team  Locations:
RTAT teams would be stationed at key locations around the world, perhaps modeled after
the UN Disaster Assessment and Coordination Teams program, or possibly as associate
members of NetHope, the UN Emergency Telecommunications Cluster (ETC Cluster) or

other similar teams. These teams could be called on by the host nation, UN agencies such
as OCHA, WPF , or a regional entity such as ASEAN.
Timeline of RTAT Concept Development: In late 2011 we began work on a process of
developing the concept, identifying founding member organizations, outlining team

                                          271
member qualifications forming the teams, training and exercising these teams, and
iteratively refining the program. We believe that if a real-world disaster event happens
any time in the near term future, and if the teams have been identified and the roles,
responsibilities and operating procedures are sufficiently advanced that an opportunity to

“jump start” the entire process by deploying to that real -world event is possible. Caution
of course would be needed to ensure this would not hinder but rather enhance the overall
response efforts.

Organizations That Have Helped Develop the RTAT Concept:

UN/NGO Community:

         UN (UN-OCHA)

         UN (UN-World Food Programme/ FITTEST)
         UN (Emergency Telecommunications Cluster (ETC))
         NetHope
         Demining NGO community
         Telecoms Sans Frontieres

         -New Zealand Red Cross
         InSTEDD
         CrisisMappers.Net
   Industry:
         Cisco Systems

         Microsoft
         Global VSAT Forum
         Delorme
         Inmarsat Government Services, US, Inc

         Oceus Networks
         -MEDWEB
   Academia:

         Naval Postgraduate School (US)
         University of Texas

         San Diego State University
         National Defense University (US)
   Government/Military Community:
         US Department of Defense
         Pacific Disaster Center

         -Japan Resiliency Initiative
         International Association of Emergency Managers (IAEM)






                                       272
Point of Contact:
Brian Steckler, US Naval Postgraduate School, Monterey CA USA
Cell: 831.402.1584 - Work: 831.656.3837 - steckler@nps.edu


















































                                     273
THIS PAGE INTENTIONALLY LEFT BLANK





























                  274
 APPENDIX K. JOINT INTER-AGENCY FIELD EXERCISE 2014–04
                         AFTER ACTION REPORT



       Appendix L contains an internal draft after action report written by the RTAT

team after JIFX 2014–04 written by Dr. Rebecca Goolsby and Mr. Brian Steckler (2014).
This supporting information is not available by any other means. Figures, photos,

supporting after action/experimental results are contained within this document.







                   JIFX After Action Report (AAR)
JIFX Experiment Number (X-00): B- 11
Experiment Title: Socio-Technical Information Operations (STIO) and Hastily Formed
Networks (HFNs) in Austere Environments
Organization:—Naval Postgraduate School(NPS), Office of Naval Research(ONR),

Arizona State University(ASU), and Pacific Disaster Center (PDC)
Experiment     Lead/Point    of   Contact:    Brian   Steckler   (+1.831.402.1584,
steckler@nps.edu); Dr. Rebecca Goolsby (+1.phone.number, preferred@email)
Quantitative Results : N/A
      208 synthetic tweets successfully merged into a secure twitter environment using
       CrisisTracker technology for deployment into the environment

      50 synthetic tweets and 20 “live” tweets were injected into the scenario itself
      1 alternative power demonstration (wind and solar) accomplished

      7 remote wireless networks established
      7 Rapid IT Assessments conducted using the RTAT mobile data collection tool
      Rapid, on-the-fly training in communications and power were accomplished for

       five new users who had no benefit of previous exposure to HFN or RTAT
       processes.

Qualitative Results (please be as descriptive and detailed as possible): The Socio-
Technical Information Operations and Hastily Formed Networks in Austere
Environments Project has three different areas of focus:risis Tracker—Led by ASU;
(2) Hastily Formed Networks (HFN)—Led by NPS; (3) Rapid IT Assessment Tool

(RTAT)—Led by NPS.
CrisisTracker Effort. This effort successfully trained 17 people in the use of a novel
technology for communications, information sharing and coordination. The technology
enabled the development and initiation of a novel exercise concept that integrated real-
time rapid IT assessment, alternative power team deployment, and alternative

                                       275
communications (mesh-networks) in an austere field setting. Crisis Tracker overcame
initial obstacles in connectivity in an austere environment; the technology was able to be
used to start, stop and coordinate team activities in the field over two days of
experimentation.

Hastily Formed Networks. This effort put into practice pre-event training on the
deployment of alternative power and communications in austere tactical or Humanitarian
Assistance/Disaster Relief (HA/DR) environments. A variety of communications
techniques were used for command and control, coordination, trouble-shooting, and
reach-back, including radios and CrisisTracker with mobile phones. Intense hands-on
field training for new users who lacked the pre-event training was successfully
accomplished on site, with five entirely new users to all of the technologies.

RTAT assessment. Experienced and novice users demonstrated the capability to perform
rapid assessments of the information and communication technology (ICT) environment
at 25 locations under austere conditions.
Scenario Development. A full, 208-Tweet scenario divided into 4 Master Scenario
Events List (MSEL)s (subdivided into multiple, flexible vignettes) was developed for this
effort. Since this was a highly novel integration with brand new technologies, it was not
expected that the full 208 tweet scenario would be played. Many vignettes within the

scenario were developed to accommodate from 10 to 100 participants. The 208 synthetic
tweets were available and demonstrated to be easily launched to assist in developing a
flexible scenario environment. Tweets were available for review; other participants could
have submitted synthetic tweets for this event if desired.

Note: generating synthetic tweets by hand is very difficult and requires quite a bit of

training and knowledge. Developing a full exercise, even as small as this, requires weeks
of effort. A full three to six months of lead time is ideal, depending on the size and scope
of the exercise. A year’s lead time for an advanced exercise would be recommended.

Monday 11 AUG 2014 Objectives: Set up of communications equipment, initial
deployment and tear down of alternative power equipment, preparation for follow-on
exercise events.

    1. Crisis Tracker
              Set up of mobile Crisis Tracker platform in the command station.
              Training of new users.

              Registration and testing of mobile phones (note: Crisis Tracker could be
               used by all mobile smart phones, regardless of brand. No download
               needed).

              Initial test uses and communications checks.
              Initial tests of information flows with the protected Twitter account.

              All synthetic tweets were made available within Crisis Tracker for pushing
               out to exercise participants as “injects.”
              Synthetic test tweets were initialized and pushed into the Crisis Tracker

               environment.

                                          276
Hot Wash Issues: The main problem for Crisis Tracker was connectivity. Local WiFi
capabilities were constantly being challenged by the number of users at Camp Roberts
McMillan Field NPS location and the load these users placed on the radio frequency

spectrum and WiFi channels. A number of minor bugs were discovered and fixed on the
fly, including problems in connecting people to chat rooms, problems in sending and
receiving Short Messaging System (SMS) messages, and general issues of dropping off
the WiFi. Later use of a Very Small Terminal Aperture Terminal (VSAT) satellite
broadband connection on the following day predominantly solved this issue.
2. Hastily Formed Networks (HFN)

             Alternative power sources were set up:RENEWS–a wind turbine, flexible
              solar panels, rigid solar panels, and fossil fuel generators.
             C OMMUNICATIONS    : VIA SATS / (HOW MANY) W      AVE MESH

              NETWORKS WERE ESTABLISHED
             Six working radios were tested.


Hot Wash issues: Difficulties in range of the radios—several times the range of the
radios was not adequate due to a lack of line of sight. In addition, a problem was
discovered in frequency assignments—2 of the 6 radios were on the wrong frequency.

   2. Rapid ICT Assessment Tool (RTAT)

             Set up beforehand,SIX ANDROID DEVICES WERE CHARGED AND READY

          Hot Wash issues: Android devices were not able to send/receive SMS due to
          lack of Subscriber Information Module (SIM) cards. Personal phones had to

          be used for Crisis Tracker. The VSAT brought with the team did not have built
          in WiFi capability bringing the SIM card issue to light.
Tuesday 12 AUG 2014Objective: Have all systems up and running for scenario injects/
deploy teams in scenarios
   1.  Crisis Tracker

             Thirty-five synthetic tweets were successfully sent out over CrisisTracker
              as injects
             Chat and sms were utilized to further push scenario events.

             Synthetic tweets also sent out to Protected Twitter account.
             CrisisTracker connectivity was intermittent.

             Crisis Tracker successfully sent out SMS messages, tweets, and chats.
             Skype Chat room established as per scenario requirements; very useful for
              trouble shooting.



   Hastily Formed Networks (HFN)
             Problems with Via Sat (brand) VSAT terminal; careful examination
              showed primary issues were cable connections.


                                        277
             Radios—intermittent due to the large number of other ongong projects
              during JIFX. These issues dealt with signal transmisstion, line of sight

              issues, and range deconfliction.
             TWO MULTI  PERSON TEAMS DEPLOYED   ,WERE ABLE TO SET UP TWO  W IFI

              “HOT SPOTS ”WITH V IAS AT AND ONE WITH 3G  CELLULAR ENHANCED   HFN
             Reassessed situation at operations meeting at noon—afternoon was

              executed more effectively. WO TEAMS WERE AGAIN DEPLOYED      ,
              ESTABLISHED THREE HOT SPOTS     ?


   2. Rapid ICT Assessment Tool (RTAT)

             Six RTAT assessments were made & successfully submitted

Hot Wash issues. Connectivity issues continued to be a problem for Crisis Tracker (a six
month old technology) but were ironed out by the end of the day by switching to a fast
VSAT Internet environment versus a much slower 3G cellular Internet connection. This

enabled the movement from the main site to the mobile site (Nemesis van) from which
Crisis Tracker was easily deployed and stable (using the ViaSat connection). Lesson
learned: Crisis Tracker will not always have stable connectivity; in the real world, Crisis
Tracker is just as likely to be deployed from a field station as from a more robust
communication or command center.

Novice users especially—but everyone needed to be reminded to check gear and cables.
Crisis Tracker successfully used to solicit assistance needed to overcome novice
problems. Injects were successful in directing activities and moving scenario forward.

Skype Chatroom meeting was especially useful in sorting out issues and problems. Crisis
Tracker and Skype overlapped but were not redundant; they handled different problems.
Crisis Tracker security (using SMS) enabled real-time communications but was limited
by connectivity issues in the morning events.


 Wednesday 13 AUG 2014Objectives: Execute Scenarios with Small, Novice Team; In-
Depth After Action Meeting (Since most of the NPS students were required to return to
NPS for class Wednesday, the exercise went forward with a single small team of six

people (total), incorporating untrained personnel (ASU students) who previously had no
experience. This day’s activities built on the performance of the previous day, with
remaining students now showing new proficiency and giving them the opportunity to
train others “on the fly” part of the exercise, just as they might do in a real disaster).
   1. Crisis Tracker

             Novice dispatchers had no trouble in sending injects and managing the
              smaller cadre of deployed users.

             Connectivity issues were largely solved.
             25 more injects were sent out using Crisis Tracker.
             Sms issues were largely solved.


                                        278
            Teams reported back using photographs posted on Protected Twitter.


   2. HFN
            HFN   SET UP OF MESH NETWORK WAS EASILY ACCOMPLISHED BY

             SMALLER TEAMS    ,EVEN WHEN INCORPORATING NOVICE ASSISTANTS
             AND TRAINING THEM AT THE SAME TIME
            S ET UP ONE MESH NETWORK    (WIFI HOTSPOT  ) UTILIZING THE

             CISCO RAPID RESPONSE KIT (RRK).


   3. RTAT

            O NE RTAT   ASSESSMENT COMPLETED WITH NOVICE TEAM MEMBERS
             ASSISTING .



In Depth After Action Meeting / Hot Wash with Collaborators
Crisis Tracker
            A “fly away kit” (FLAK) for Crisis Tracker is needed. FLAK should
             contain instructions for registration, use, trouble-shooting, and instructions

             for dispatchers as well.
            Crisis Tracker may have real utility in mobile command settings but needs

             to be able to deal with spotty connection problems more gracefully.
            Still uncertain whether the SMS capability as useful as the Internet-based
             communications capability; needs more testing. Users noted that options

             are good; data connection may not be available, but SMS may be more
             robust. SMS often was better than the radio—nice to have both in case one
             fails.

            Many people wanted a “proper app” rather than a web service.
            Notifications when one got an sms would be useful; perhaps this can be

             done if made into a “proper app.”
            Incorporation of Ops View application would be helpful, was suggested
             by a partner collaborator—to check on network capabilities and load.


   4. Hastily Formed Networks (HFN) (Lessons Learned)
            Collaborators needed more pre-exercise introduction.

                 i. Introduction of PEOPLE
                 ii. Introduction of GEAR and GEAR capabilities and limits
                iii. Introduction of the SCENARIO

                iv. Introduction of each team’s SPECIFIC and GENERAL
                    OBJECTIVES


                                       279
             ViaSat instructions needed to be updated.
             Pre-event preparation was adequate but would have been greatly improved

              with more time spent doing set ups and teardowns.
             More documentation for gear (serial number, weight, support information)
              should be included with the cases for each item. Current spreadsheet

              method good for in-lab monitoring but not sufficient for deployment
              monitoring of equipment.
             Advance labeling of fly away kits would be a distinct advantage.

             M ESH GEAR   SOP—   FLASH THEM BACK TO A BASIC CONFIGURATION
             Need a logistics package - including contact list for exercise participants
              AND for the collaborators; phone numbers, email lists needed to be
              distributed ahead of time to promote pre-exercise planning and
              coordination.

             Pre-event travel plan also needs to be developed and distributed pre-event.
             Two operations being prepped at the same time created challenges (one
              group was packing for Nepal and one was packing for JIFX). This
              accounts for some of thedisconnects in planning. In future, methods for
              coordinating multiple operations might be addressed.
             BGANs would be the recommended equipment for future exercises of this

              kind.
             Discussion on frequency management was substantive.
                  i. Recommendations: Use a WiFi analyzer to de-conflict. JIFX
                     participants were stepping on each other on WiFi channels.

                 ii. Communication frequency manager for JIFX should include pro-
                     active WiFi management.


   5. Observations & Comments:
       Collaborative member from CISCO provide the CISCO Rapid Response Travel
       Kit (RRK). It was very well received and added value to the event. Comment was
       made that there should be photos of the different levels to assist in repacking this
       gear in pelican case.


       Only two people signed up to participate in Crisis Tracker; every one was pretty
       busy with their own experiments. However, interest in the use of Protected
       Twitter was high. In the future, perhaps a white paper could be circulated to (1)
       explain Protected Twitter; (2) provide instructions on how to use it; and (3)
       consider use of Protected Twitter in a short demo prior to the event, to enable
       people to try it out for themselves during the weeks leading up to the JIFX event.


Additional Questions:
Did you receive constructive end-user feedback on technology?
      End users had many questions about the Crisis Tracker technologies and showed
       considerable interest in the Protected Twitter concept of operations.nd

                                        280
       RTAT teams were often in the field and thus, got less feedback. In the future,
       more outreach might be considered for explaining, demonstrating, and interacting
       with users.

Did you perform any on the fly development of your technology during the JIFX week?


      HFN and RTAT were able to collaborate with CISCO, ViaSAT, TrustComm,
       while the Joint Vulnerability Assessment Branch team (JVAB) did a wireless
       environment threat assessment of the systems used. These collaborative efforts
       assisted in the development of improved concepts of operations and greater
       understanding of how technologies could be used in tandem or in substitution for
       one another (such as when one system failed or had issues).

Were you provided with additional data necessary to conduct your experiment?

      No.
Were you provided with support services necessary to conduct your experiment?

      The tent provided was extremely useful. The tent provided a cool place for the
       computers and other devices we used and functioned as headquarters for our
       exercise.

Did you engage in ad-hoc experimentation or collaboration with other experimenters? If
so, include names of those experiments for purposes of identification.
      We collaborated with Rakesh Bharania from Cisco and were able to set up a

       network at the remote site.
      Progeny Systems and ASU developed a concept of operations for integration of
       their systems that will be pursued in future; actual code hacking was prevented
       due to firewall issues. If firewall issues had not presented themselves, those two
       systems would have had data flowing between them

Did members of the JVAB look at your experiment? If so, please describe the interaction.
      Yes, Joint Vulnerability Assessment Branch conducted a vulnerability assessment

       of the Rapid Response Kit (RRK). POC David Rohret do89261@jricp.osis.gov,
       drohet@csc.com, 210-925-4477.

What, if any, are the uniquely valuable aspects of this event?
      The ability to collaborate on the fly among many kinds of innovators (technical,
       software, and end-user innovators).

      The interaction with end users and discussions

      The ability to try out highly novel, likely-to-not-work-the-first-time, bleeding
       edge technologies in a realistic environment.
      JIFX is a significant boon to education, training, and innovation research with a
       practical, real-world domain and environment.

Photo/Graphics:


                                           281
                        Antennae Hill—RTAT Assessment























                            CrisisTracker Screen Shots
  CrisisTracker Screen Shots from an iPhone (above images). Pressing on an icon
 reveals information, such as the tweet message in the right image. This is from day
  3, Aug 13, 2014. This deployment used a Cisco Rapid Response Kit    101 satellite
device.It did not have as good of signal strength as using the Cisco Explorer 500 for
partly unknown reasons. Rakesh said that the Kit 101 had lower bandwidth service

 compared to the Explorer 500. The available AT&T 4G signal was used at times to
                                        282
confirm that the Kit 101 had a bad signal and not a problem with the Crisis Tracker
loading pages in this instance. This does show the functionality of the Crisis Tracker
  First Responder application that shows first responder locations, event locations,
         and tweets. Also available to show are medical centers in the area.





























   Deployment team in the afternoon of Aug 12, 2014 (above photo). Team leader
…Anibal… configured a correct Wave Rider wifi device for a mesh network (upper
right) while Rakesh from Cisco showed the ease of setting up the Cisco Explorer 500
  digital satellite antenna (next to the left tail light of the SUV). Also shown on the
ground in the middle is a flexible solar panel. This configuration had excellent Wi-Fi
                                      reception.














                                          283
  Day 3 Deployment, Aug. 13, 2014, Team leader Major Beeson on a radio device.
   Most of the Cisco Rapid Response Kit 101 parts are out of the travel cases and
ready to be connected. At this point a member of the team posted in a message that
 the team arrived at the assigned destination. In this simulation the destination is a

location in Pink Rhino City to give network connectivity to a medical team. The Kit


                                         284
  101 is later placed out of the shade in the background to try to improve signal
                                     strength.













































Day 3 deployment. A USB device is being connected to the Kit 101. The Kit 101 is
   then connected to the device on the batter using a standard Ethernet cable.




                                       285
Deployment of the Cisco Rapid Response Kit 101 on day 3, August 13, 2014. This
device allowed a phone for voice over IP to be connected. Also shown is Goal Zero
                                     battery.





                                       286
   Part of the deployment team of day 3, August 13, 2014, along with part of the
network vulnerability team working in the background. The Cisco Rapid Response

   Kit 101 on the Goal Zero battery can be seen in the background to the right.



                                       287
THIS PAGE INTENTIONALLY LEFT BLANK





























                  288
                            LIST OF REFERENCES



Akyildiz, I., Wang, X., &Wang, W. (2005). Wireless mesh networks: A survey.
       Computer Network, 45, 445–487.

Alberts, D. & Hayes, R. (1995). Command arrangements for peace operations.
       Washington DC: Department of Defense Command and Control Research
       Program.

Alberts, D. & Hayes, R.. (2002). Code of Best Practice for Experimentation.Washington
       DC: U.S. Department of Defense Command and Control Research Program.

Alberts, D. & Hayes, R. (2003). Power to the edge: Command and control in the

       information age. Washington, DC: U.S. Department of Defense Command and
       Control Research Project.

Alberts, D. & Hayes, R. (2005). Campaigns of experimentation: Pathways to Innovation
       and Transformation. Washington, DC: CCRP.

Aurea. (2014). Business process management: Savvion. Retrieved from
       http://www.aurea.com/technology-solutions/business-process-management/

Bharania, R. (2014). Rapid response kit. Retrieved from www.nps.edu/fx

Barangay. (2014). Wikipedia. Retrieved from http://en.wikipedia.org/wiki/Barangay.


Beeson, R. (2013). Rapid Information and Communication Technology Assessment
       Team (RTAT) enterprise assessment (EA) (unpublished). Naval Postgraduate
       School, Monterey, CA.

Beeson, R., Gladem, J., & Gonzalez, J. (2014). Rapid Information Communication
       Technology Assessment Team Savvion business process modeling (u    npublished).
       Naval Postgraduate School, Monterey, CA.

Cannon, C., Beeson, R., Mitchell, K., Spencer, P., & Liguori, M. (2012). Command &
       control case study of the response to Hurricane Katrina (uublished). Naval

       Postgraduate School, Monterey, CA.

Chang, S. (2013). RTAT quick look  Republic of the Philippines September 2013 v2
       (Unpublished). U.S. Marine Forces Pacific Experimentation Center, Camp Smith,
       HI.

Chopoorian J., Witherell R., Khalil O., Ahmed M. (2001). Mind your business by
       mining your data. Society of Advanced Management Journal,  66(2), 45–51.

Creveld, M. (1985). Command in war. Cambridge, MA: Harvard University Press.

                                         289
CyberTracker. (2014). Getting started. Retrieved from
       http://www.cybertracker.org/software/getting-started

Davenport, T., Harris, J., De Long, D., & Jacobson, A. (2000). Data to knowledge to
       results–building an analytic capability. San Jose, CA: Institute for Strategic

       Change. Accenture Consulting.

Denning, P. (2000). Hastily formed networks. Communication of the ACM. 49(4). 15–20.

Denning, P., & Hayes-Roth, R. (2006). Decision making in very large networks.
       Communication of the ACM 49(11), 19–23.

Department of Defense. (2005). Military support for stability, security, transition and
       reconstruction (SSTR) operations. (DOD Directive 3000.05). Washington, DC:
       U.S. Government Printing Office.


Department of Defense. (2006). Joint Communications System    (Joint Publication 6–0).
       Washington, DC: U.S. Government Printing Office.

Department of Defense. (2011). Department of Defense support to foreign disaster relief
       (handbook for JTF commanders and below). Washington, DC: U.S. Government
       Printing Office: James Schear.

Department of Defense.  (2014). Foreign humanitarian assistance (JP 3-29). Washington,
       DC: U.S. Government Printing Office: David Goldfein.


Department of Homeland Security. (n.d.). Wrapping up the exercise. Retrieved on
       September8, 2014, from
       http://emilms.fema.gov/IS921/921_Toolkit/downloads/NPPD_HotWash.pdf

Department of Homeland Security. (2011). National Preparedness System. Washington,
       DC: U.S. Government Printing Office.

Department of Homeland Security. (2013). National response framework (2nd ed.).

       Washington,   DC: U.S. Government Printing Office.

Dienes, Z. & Perner, J. (1999). A theory of implicit and explicit knowledge. Behavioral
       and Brain Sciences,  22, 735–808.

Donahue, A., & Tuohy, R. (2006). Lessons we don’t learn: A study of the lessons of
       disasters, why we repeat them, and how we can learn them. Homeland Security
       Affairs,I(2). Retrieved from http://www.hsaj.org/?article=2.2.4.

Dourandish, R., Zumel, N.,& Manno, M. (2007). Adapting C2 to the 21st century. 2007

       Command and Control Research & Technology Symposium. Newport, RI.



                                           290
Fulcrum. (n.d.). Plans. Retrieved on September 6, 2014, from
       http://fulcrumapp.com/plans/.

Goal Zero. (n.d.). Goal Zero Yeti 1250 Solar Generator. Retrieved on September 10,
       2014, from http://www.goalzero.com/p/140/goal-zero-yeti-1250-solar-generator/.


Goolsby, R., & Steckler, B. (2014). JIFX after action report (Unpublished). Naval
       Postgraduate School, Monterey, CA.

The Guardian. (2014). The NSA files. Retrieved from
       http://www.theguardian.com/world/the-nsa-files

Gupta, A., Thapar, J., Singh, A. Singh, P., Srinivasan, V., & Vardhan, V. (2013).
       Simplifying and improving mobile based data collection. Information and
       Communications Technologies and Development Vol. 2, 45–48.

       DOI=10.1145/2517899.2517929 http://doi.acm.org/10.1145/2517899.2517929

Hartung, C., Anokwa, Y., Brunette, W., Lerer, A., Tseng, C., & Borriello, G.2010).
       Open Data Kit: Tools to build information services for developing regions.
       Information and Communication Technologies and Development, 2010, article
       18, 12 pages. DOI=10.1145/2369220.2369236. Retrieved from
       http://opendatakit.org/wp-content/uploads/2010/10/ODK-Paper-ICTD-2010.pdf

Hawthorne S. & Lush, R. (2002). Clearing the confusion about spiral/evolutionary

       development.  CrossTalk, (08). Retrieved from
       http://www.crosstalkonline.org/storage/issue-archives/2002/200208/200208-
       Hawthorne.pdf

Humanitarian Data Toolkit. (2014). About the Humanitarian Data Toolkit. Retrieved
       from http://humanitariandatatoolkit.org/

Huang, J.,& Lien,Y. (2012). Challenges of emergency communication network for

       disaster response. Communication Systems (ICCS), 2012 IEEE International
       Conference on Disasters, 528–532. doi: 10.1109/ICCS.2012.6406204.
       http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6406204&url=http%3A%
       2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6406204

Headquarters, Department of the Army. (2012). Field  knowledge management operations
       (FM 6–  0.1). Washington, DC: author.

Headquarters, U.S. Marine Corps. (2002). Marine Corps Warfighting Publication

       Information Management (MCWP 3-40.2) Quantico, VA: U.S. Government
       Printing Office. Retrieved from
       http://www.marines.mil/Portals/59/Publications/MCWP%203-
       40.2%20Information%20Management.pdf

Humanity road. (2014). About. Retrieved from http://humanityroad.org/aboutus/

                                          291
Hyman, P. (2014). “Peace technologies” enable eyewitness reporting when disasters
       strike. ACM 57, 1, 27–29. DOI=10.1145/2555808
       http://doi.acm.org/10.1145/2555808

Inter-Agency Standing Committee. (2006). Guidance note on using the cluster approach

       to strengthen humanitarian response. New York: author
       https://www.humanitarianresponse.info/clusters/space/document/iasc-guidance-
       note-using-cluster-approach-strengthen-humanitarian-response.

Inter-Agency Standing Committee. (2011a). What is the IASC? Retrieved from
       http://www.humanitarianinfo.org/iasc/downloaddoc.aspx?docID=6050&type=pdf

Inter-Agency Standing Committee. (2011b). Operational guidance for coordianted
       assessments in humanitarian crises. New York: author. Retreived from

       http://www.who.int/hac/network/interagency/news/ocha_operational_guidance_c
       oordinated_assessments_v7.pdf

Inter-Agency Standing Committee. (2012a). Multi-cluster/sector initial rapidssessment
       (MIRA). New York: author. Retrieved from
       https://docs.unocha.org/sites/dms/CAP/mira_final_version2012.pdf

Inter-Agency Standing Committee. (2012   b). Reference module for cluster coordination at
       the country level. New York: author. Retrieved from

       http://www.humanitarianinfo.org/iasc/downloadDoc.aspx?docID=6461

Jung, J. (2011). Mobile Data Collection Systems A review of the current state of the field.
       Retrieved from http://humanitarian-nomad.org/wp-
       content/uploads/2013/03/NOMAD-MDC-Research.pdf

Kennerley, M. & Mason, S. (2008). Use of information in decision making literature
       review for the audit commission . Cranfield, England: Cranfield School of

       Management. Retrieved from http://archive.audit-
       commission.gov.uk/auditcommission/sitecollectiondocuments/AuditCommission
       Reports/NationalStudies/Cranfield_Information_use_review.pdf

Levine, B. (2014). Android hits 85 percent smartphone market share. Retrieved from
       http://www.toptechnews.com/article/index.php?story_id=0030003DMG66

Longley, C. (2010). The Field Information Support Tool (FIST). (Master’s thesis). Naval
       Postgraduate School, Monterey, CA.


MapAction. (2011). Field Guide to Humanitarian Mapping     (2nd ed.). Retrieved from
       http://www.mapaction.org/index.php?option=com_mapcat&id=2426&view=dow
       nload&fmt=pdf




                                          292
Meeds, H. K. (2006). Communication Challenges During Incidents of National
       Significance: A Lesson from Hurricane Katrina. (doctoral dissertation). U.S.
       Army War College. Carlisle Barracks. PA.

Merriam-Webster. (2014). Data. Retrieved from http://www.merriam-

       webster.com/dictionary/data

Microsoft. (2014). Excel. Retrieved from http://office.microsoft.com/en-us/excel/

MobileActive. (2010). Mobile Data Collection Tools–Comparison Matrix [Excel].
       Retrieved from
       https://docs.google.com/spreadsheet/ccc?key=0ArG7kkc9mE75dEdNNktocmVw
       T0hNbHVjTXl2ZU1VMXc&hl=en_US#gid=0

Moffat, J. (2008). The Response to Hurricane Katrina: A Case Study of Changing C2

       Maturity. Hampshire, United Kingdom: Crown. Retrieved from
       http://www.dodccrp.org/files/case_studies/Katrina_case_study.pdf

NetHope. (2014a). About us. Retrived from http://nethope.org/about/

National Geospatial Intelligence Agency. (2014). Geospatial intelligence (GEOINT)
       basic doctrine. Washington, DC.

Naval Postgraduate School (n.d.).Field experimentation: Background. Retrieved on
       September 9, 2014, from http://my.nps.edu/web/fx/what-is-jifx-


NetHope. (2014b). Emergency response. Retrived from
       http://nethope.org/programs/emergency-response/

Nissen, M. & Burton, R. (2011). Designing organizations for dynamic fit: System
       stability, maneuverability, and opportunity loss. Systems, Man and Cybernetics,
       Part A: Systems and Humans, IEEE Transactions on., vol.41, no.3, 418–433. doi:
       10.1109/TSMCA.2010.2084569


Humanitarian Operations Mobile Acquisition of Data. (n.d.-a). Online selection assistant
       Android. Retrieved onAugust 19, 2014, from http://humanitarian-
       no mad.org/online-selection-tool/#

Humanitarian Operations Mobile Acquisition of Data. (n.d.-b). Online selection assistant
       iPhone. Retrieved onAugust 19, 2014, from http://humanitarian-
       nomad.org/online-selection-tool/#

Humanitarian  Operations Mobile Acquisition of Data. (2012). The NOMAD project.

       Retrieved from http://humanitarian-nomad.org/wp-
       content/uploads/2013/03/Flyer_NomadServices_July2012.pdf



                                          293
Humanitarian Operations Mobile Acquisition of Data. (2014). Welcome. Retrieved from
       http://humanitarian-nomad.org/

Office for the Coordination of Humanitarian Affairs. (2011). OCHA on message: Inter-
       Agency Standing Committee. Retrieved from

       https://docs.unocha.org/sites/dms/Documents/120229_OOM-IASC_eng.pdf

Office for the Coordination of Humanitarian Affairs. (2013a). Humanitarianism in the
       network age. New York, New York: Christina Bennet. Retrieved from
       http://www.unocha.org/hina

Office for the Coordination of Humanitarian Affairs. (2013b). United Nations disaster
       assessment and coordination UNDAC field handbook (6th ed.). New York, New
       York. Retrieved from

       https://docs.unocha.org/sites/dms/Documents/UNDAC%20handbook%20-
       %20English.pdf

Office for the Coordination of Humanitarian Affairs. (2014a). Developing humanitarian
       data standards: an introduction and plan for 2014. Retrieved from
       http://docs.hdx.rwlabs.org/wp-content/uploads/HXL_Paper-forsite.pdf

Office for the Coordination of Humanitarian Affairs. (2014b). History of OCHA.
       Retrieved from http://www.unocha.org/about-us/who-we-are/history


Open Data Kit Collect. (2014). About. Retrieved from http://opendatakit.org/about/

Open Handset Alliance. (2014). Open Handset Alliance. Retrieved from
       http://www.openhandsetalliance.com/

One Platform Foundation. (2014). Appstores [infographic]. Retrieved from
       http://www.onepf.org/appstores/#opf-list

Oracle. (2014). A relational database overview.Retrieved from

       http://docs.oracle.com/javase/tutorial/jdbc/overview/database.html

Osmundson, J. (2014, April). 4.0 government & other standards [PowerPoint slides].
       Presented at Naval Postgraduate School, Monterey, CA.

Patnaik, S., Brunskill, E., & Thies, W. (2009). Evaluating the accuracy of data collection
       on mobile phones: a study of forms, sms, and voice. In Proceedings of the third
       international conference on Information and communication technologies and
       development (pp. 74 –84). Piscataway, NJ: IEEE Press. Retrieved from

       http://www.cs.cmu.edu/~ebrun/patnaik-ictd09.pdf

Pacific Disaster Center. (n.d.). DisasterAWARE. Retrieved on August 19, 2014,  from
       http://emops.pdc.org/emops.


                                          294
PreventionWeb. (n.d.). United States of America—Disaster statistics. Retrieved
       September 3, 2014, from
       http://www.preventionweb.net/english/countries/statistics/?cid=185

Reed, B. (2014). On BlackBerry 10’s 1st anniversary, BlackBerry’s U.S. market share

       hits 0%. Retrieved from http://bgr.com/2014/01/30/blackberry-us-market-share/

Red Cross. (n.d.). Who we are. Retrived September 3, 2014, from
       http://www.icrc.org/eng/who-we-are/index.jsp

Ross, J., Weill, P., & Robertson, D. (2006). Enterprise architecture as a strategy. Boston,
       MA: Harvard Business Press.

Satzinger, J., Jackson, R., & Burd, S. (2011). Systems analysis and design in a changing
       world (6th ed). Independence, KY: Cengage Learning


Savvion. (2006). Executive summary. Retrived from
       http://www.techrepublic.com/resource  -library/downloads/download-savvion-s-
       free-business-process-modeling-software/

Steckler, B. (2009). Hastily formed networks (HFNs) [PowerPoint slides]. Retrieved
       from
       http://www.cis.upenn.edu/~ngns/docs/Review_09/StecklerMURIReview.ppt.

Steckler, B. (2012). Rapid Technology Assessment Team (RTAT) Executive summary

       (unpublished). Naval Postgraduate School: Monterey, CA.

Steckler, B. (2013). Concept of operations Rapid Information and Communication
       Technology Assessment Teams (RTAT) follow on assessment-Cebu, Tacloban
       and Leyte Philippines (unpublished). Naval Postgraduate School, Monterey, CA.

Striedl, P., Crosson, J., & Farr, L. (2006). Association of Contingency Planners
       observations of Hurricane Katrina lessons learned. Albany, NY.   ACP.


Telvent. (2014). Relational database concepts for beginners. Retrieved from
       http://webs.wofford.edu/whisnantdm/courses/cs101/pdf/database/relational_datab
       ase_concepts.pdf.

Townsend, F. (2006).  The federal response to Hurricane Katrina, lessons learned.
       Department of Homeland Security, Washington, DC.

Telecoms Sans Frontieres. (2013). Annual report. Retrieved from
       http://www.tsfi.org/en/presentation/reports


Telecoms Sans Frontieres. (2014). Important dates in TSF’s history. Retrieved from
       http://www.tsfi.org/en/presentation/origin/46-les-dates-importantes-dans-lhistoire-
       de-tsf

                                          295
Tyszkiewicz, J. (2010). Spreadsheet as a relational database engine. In Proceedings of the
       2010 ACM SIGMOD International Conference on Management of data (pp. 195–
       206). New York, NY: ACM. DOI=10.1145/1807167.1807191. Retrieved from
       http://doi.acm.org/10.1145/1807167.1807191


United States Agency for International Development. (2005). Field operations guide for
       disaster assessment and response (v. 4.0). Washington, DC: U.S. Government
       Printing Office.

United States Agency for International Development. (2014a). Philippines. Retrieved
       from http://www.usaid.gov/crisis/philippines.

United States Agency for International Development. (2014b). Philippines-Typhoon
       Yolanda/Haiyan fact sheet #22 fiscal year 2014. Retrieved from

       http://www.usaid.gov/sites/default/files/documents/1866/philippines_ty_fs22_04-
       21 -2014.pdf

Verge Staff. (2014). iOS: A visual history. The verge, September 16, 2013. Retrieved
       from http://www.theverge.com/2011/12/13/2612736/ios-history-iphone-ipad

World Food Programme. (2002). World Food Programme emergency field operations
       pocketbook. Retrieved from http://www.unicef.org/emerg/files/WFP_manual.pdf

World Food Programme. (2011). EPIC Emergency Preparedness Integration Centre.

       Retrieved from
       http://documents.wfp.org/stellent/groups/public/documents/newsroom/wfp225858
       .pdf

World Food Programme. (2013a). FITTEST Fast IT & Telecommunications Emergency
       & Support Team. Retrieved from
       http://ictemergency.wfp.org/documents/10844/45c27de4-e823-40ae-8954-

       869582078f85

World Food Programme . (2013b). UN-ETC fact sheet. Retrieved from
       www.ictemergency.wfp.org

World Food Programme. (2013c). Emergency telecommunications cluster (ETC)
       Typhoon  Haiyan Operation, Philippines, 27 November 2013. Rerieved from
       http://ictemergency.wfp.org/web/ictepr/emergencies2013/philippines

Yank, K. (2009). Build your own database driven web site using PHP & MySQL     (4th

       ed.).Collingwood, VIC Australia: SitePoint Pty. Ltd.






                                          296
                            WORKS CONSULTED



Akyildiz, I. (2011). Sensor networks in challenged environments. Wireless Technologies
       for Humanitarian Relief, 3(3).doi=10.1145/2185216.2185220
       http://doi.acm.org/10.1145/2185216.2185220

Alberts, D. & Nissen, M. (2009). Toward harmonizing command and control with
       organization and management theory. The International C2 Journal, 3(2), 1–59.
       Retrieved from www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA508759

 Alberts D., Huber, R. & Moffat J. (2010). NATO NEC C2 maturity model. Washington
       DC: U.S. Department of Defense Command and Control Research Program.

       Retrieved from http://www.dodccrp.org/files/N2C2M2_web_optimized.pdf

Basu, D. (2006). A compendium of learnings from engagements in Afghanistan, Iraq,
       Liberia, Iran, Sudan, Guatemala, Indonesia, Sri Lanka, Pakistan, Lebanon.
       Fairfax, VA: NetHope. Retrived from
       http://nethope.org/images/uploads/casestudies/DisasterRelief.pdf

BelAir Networks. (2008). New public safety interoperable communications solution
       delivers high-speed wireless broadband for emergency management. Network

       Weekly News, 73. Retrieved from
       http://search.proquest.com/docview/211495609?accountid=12702

Guillén, K., Mendoza, U. & Santos, L. (2011). Crowdmap and Ushahidi: to obtain and
       visualize traffic congestion information in Mexico City. Computational
       Transportation Science, 24–27. DOI=10.1145/2068984.2068989
       http://doi.acm.org/10.1145/2068984.2068989

Headquarters, U.S. Marine Corps. (2007). Marine Corps Task List (MCTL) (Marine
       Corps Order 3500.26). Quantico, VA: U.S. Government Printing Office.


Hyman, P. (2014). ‘Peace technologies’ enable eyewitness reporting when disasters
       strike. ACM 57, 1 (January 2014),27–29. DOI=10.1145/2555808
       http://doi.acm.org/10.1145/2555808

Karaoglan, D., Levi, A., & Savas, E. (2010). A distributed key establishment scheme for
       wireless mesh networks using identity-based cryptography.
       DOI=10.1145/1868630.1868633 http://doi.acm.org/10.1145/1868630.1868633

Kasera, S. (2011). Wireless network security. Wireless Technologies for Humanitarian
       Relief. 2011, 32. DOI=10.1145/2185216.2185235

       http://doi.acm.org/10.1145/2185216.2185235



                                         297
Legendre, F. (2011). 30 years of ad hoc networking research: What about humanitarian
       and disaster relief solutions? What are we still missing? In Proceedings of the 1st
       International Conference on Wireless Technologies for Humanitarian Relief
       (ACWR ‘11) 217. New York. DOI=10.1145/2185216.2185279

       http://doi.acm.org/10.1145/2185216.2185279

Lancaster, D. (2005). Developing a fly-away-kit (FLAK) to support hastily formed
       networks (HFN) for humanitarian assistance and disaster relief. (Master’s thesis).
       Naval Postgraduate School, Monterey, CA.

Perfetti, M. (2012). An investigation of commercial off-the-shelf wireless in support of
       complex humanitarian disaster operations in the Argentine Army. (Unpublished
       master’s thesis). Naval Postgraduate School, Monterey, CA.


Ramachandran,   K., Sheriff, I., Belding, E., & Almeroth, K. (2008). A multi-radio 802.11
       mesh network architecture. Mobile Network. Application 13(1–2) 132–146.
       DOI=10.1007/s11036-008-0026-8 http://dx.doi.org/10.1007/s11036-008-0026-8

Rivera, W. (2012). Distributed agent-based networks in support of advanced marine
       corps command and control concept. (Unpublished master’s thesis). Naval
       Postgraduate School, Monterey, CA.

Roberts, N. (2001). Coping with wicked problems: The case of Afghanistan. In Research

       in Public Policy Analysis and Management, Vol.11 Part 2 (pp.353–375). Bingley,
       United Kingdom: Emerald Group Publishing Limited,. Retrieved from
       http://www.emeraldinsight.com/doi/abs/10.1016/S0732-1317%2801%2911006-7

Suri, S. (2011). Amateur radio in emergency communications advanced digital
       communication network. In Proceedings of the 1st International Conference on
       Wireless Technologies for Humanitarian Relief  (pp. 29–29). New York, NY:

       ACM 29–29. DOI=10.1145/2185216.2185233. Retrieved
       fromhttp://doi.acm.org/10.1145/2185216.2185233

Teague, M. (2007). The domestic coalition: The C2 relationship between active
       component and   National Guard Forces in defense support of civil authorities
       operations. Retrieved from http://www.dtic.mil/dtic/tr/fulltext/u2/a476794.pdf

Tuzhilin, A.(2011). Knowledge management revisited: Old dogs, New tricks. ACM
       Transactions on Management Information Systems, 2(3), Article 13 (October

       2011). DOI=10.1145/2019618.2019619. Retrived from
       http://doi.acm.org/10.1145/2019618.2019619

Tyler, B. K. (2008). A comparative analysis of fortress (ES520) and mesh dynamic’s
       (4000 SERIES) networking capabilities during COASTS 2007 field experiments.
       (Unpublished master’s thesis). Naval Postgraduate School, Monterey, CA.



                                          298
Wentz, L. (2010). Haiti information and communications observations trip report for
       visit 18 February to 1 March 2010. Washington, DC: Center for Technology and
       National Security Policy.

Wu, X., & Li, N. 2006. Achieving privacy in mesh networks. In Proceedings of the

       Fourth ACM workshop on Security of ad hoc and sensor networks (SASN ‘06)
       (pp. 13–22). New York, NY. DOI=10.1145/1180345.1180348
       http://doi.acm.org/10.1145/1180345.1180348

Zhu, X., Fang, Y., & Wang, Y. (2010). How to secure multi-domain wireless mesh
       networks. Wireless Network 16 (5), 1215–1222. DOI=10.1007/s11276-009-0198-
       6 http://dx.doi.org/10.1007/s11276-009-0198-6








































                                          299
THIS PAGE INTENTIONALLY LEFT BLANK





























                  300
                       INITIAL DISTRIBUTION LIST


1.     Defense Technical Information Center

       Ft. Belvoir, Virginia

2.     Dudley Knox Library
       Naval Postgraduate School
       Monterey, California









































                                        301