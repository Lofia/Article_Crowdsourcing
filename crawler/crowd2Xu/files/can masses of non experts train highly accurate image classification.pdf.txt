                      Can Masses of Non-Experts Train

                     Highly Accurate Image Classiﬁers?

                            AC  owdorigA ppochto

                Instrument Segmentation in Laparoscopic Images


                                 1,⋆,⋆⋆               1                     2
                  Lena Maier-Hein     ,SvenMersmann    , Daniel Kondermann ,
                  Sebastian Bodenstedt ,AlexandroSanchez  2,ChristianStock 4,
                                     5                   3                     3
               Hannes Gotz Kenngott , Mathias Eisenmann ,andStefanieSpeidel

                                1 Computer-assisted Interventions,
                         German Cancer Research Center (DKFZ), Germany
           2
            Heidelberg Colla3oratory for Image Processing, University of Heidelberg, Germany
                              Institute for Anthropomatics and Robotics,
                            Karlsruhe Institute of Technology, Germany
           4 Institute of Medical Biometry and Informatics, University of Heidelberg, Germany
                    5
                     Department of General, Visceral and Transplantation Surgery,
                                University of Heidelberg, Germany
                                l.maier-hein@dkfz-heidelberg.de


                Abstract. Machine learning algorithms are gaining increasing interest
                in the context of computer-assisted interventions. One of the bottlenecks
                so far, however, has been the availability of training data, typically gen-

                erated by medical experts with very limited resources. Crowdsourcing is
                anewtrendthatisbasedonoutsourcingcognitivetaskstomanyanony-
                mous untrained individuals from an online community. In this work, we
                investigate the potential of crowdsourcing for segmenting medical instru-

                ments in endoscopic image data. Our study suggests that (1) segmen-
                tations computed from annotations of multiple anonymous non-experts
                are comparable to those made by medical experts and (2) training data

                generated by the crowd is of the same quality as that annotated by med-
                ical experts. Given the speed of annotation, scalability and low costs,
                this implies that the scientiﬁc community might no longer need to rely
                on experts to generate reference or training data for certain applications.

                To trigger further research in endoscopic image processing, the data used
                in this study will be made publicly available.



noitcu ort1nI


          Computer-assisted minimally-invasive surgery (MIS) as well as computer-assisted
          surgical training is gaining increasing interest in the past years. One of the main

           ⋆ Correspondence author.
          ⋆⋆ This work was conducted within the setting of theSFB TRR 125: Cognition-guided

             surgery funded by the German Research Foundation (DFG). It was further spon-
             sored by the European Social Fund of the State Baden-Wuerttemberg and the Klaus
             Tschira Foundation.


          P. Golland et al. (Eds.): MICCAI 2014, Part II, LNCS 8674, pp. 438–445, 2014.
          ⃝ Springer International Publishing Switzerland 2014
                                  Crowdsourcing for Instrument Segmentation     439


   challenges in this context is the image-based tracking of medical instruments in
   the endoscopic images, which is a prerequisite for surgical navigation [1], skill
   assessment [2] and workﬂow analysis [3], for example. State-of-the-art methods

   apply machine learning techniques to learn the shape and appearance of dif-
   ferent objects from labeled training data [4]. However, the performance of the
   classiﬁers depends crucially on the availability of reference annotations, which

   are extremely expensive to obtain because they are typically made by medical
   experts with very limited resources. Asac nuc,edassdto
   train or validate a new method are typically small and thus not able to capture
   the wide range of anatomical/scene variance.

      Crowdsourcing is the process of outsourcing cognitive tasks to many anony-
   mous untrained individuals from an online community. In contrast to outsourc-
   ing, the work comes from an undeﬁned public rather than being commissioned

   from a speciﬁc, named group. Its advantages include speed of annotation, scal-
   ability and low cost. While the concept has already been applied to a variety
   of diﬀerent applications, its usage in the context of medical image processing

   is extremely limited. According to a very recent review article, the few medical
   applications can be classiﬁed into four main areas [5]: Problem solving, survey-
   ing, surveillance and data processing. Tasks related to the last category include

   shape-based classiﬁcation of polyps in computed tomography(CT) [6], skill as-
   sessment [7], and medical image classiﬁcation [8].
      The purpose of our work is to investigate whether crowdsourcing is an ap-
   propriate tool for training instrument tracking algorithms in the context of

   laparoscopic surgery. Using a set of endoscopic video images with reference in-
   strument segmentations we (1) quantify segmentation performance of the anony-
   mous crowd using the raw annotated data as well as segmentations obtained via

   majority voting and (2) determine the performance of a basic instrument seg-
   mentation algorithm on data sets labeled by experts, the crowd or both groups.

sdo2te


   2.1   Data Annotation Software
   Amazon Mechanical Turk (MTurk)9    []ianiternet-basiglt-wdsourcn

   form that allows requesters to distribute small computer-based tasks, referred to
   as human intelligence tasks (HITs), to a large number of untrained workers, re-
   ferred to as knowledge workers (KWs). The KWs can freelychoosetheHITsthey

   want to perform and receive a small monetray reward for each completed one from
   the requester (typically a couple of cents for a task of a few minutes). Our anno-
   tation user interface was integrated into MTurk by supplying a dynamic webpage

   (HTML5, JavaScript). In this study, each HIT refers to the segmentation of one
   medical instrument in a given endoscopic image. Figure 1 shows a screenshot of
   the annotation process. Based on a bounding box and a very rough contour speci-

   fying which of potentially multiple instruments in the image to segment, the KW
   needs to place a polygon around the object under investigation. For each HIT, our
   software records (1) the user ID, (2) the coordinates of the points as well as (3) the

   time needed for the completion of one HIT.
440     L. Maier-Hein et al.





























Fig.1. Screenshot of the Amazon Mechanical Turk[9] image annotation software de-
veloped for this study. In the header, the micro task to be performed is described to

the user. The user generates and moves the blue points from which a red contour is
generated.



2.2   Endoscopic Video Data


The training data was generated from a total of 6 surgical procedures, three from
laparoscopic adrenalectomies and three from laparoscopic pancreatic resections.

From each surgery, 20 images containing one or several medical instruments were
extracted, yielding 120 images in total.

   Half of the data from each surgical procedure was annotated by a medical
expert with experience in laparoscopic surgeries, as shown by means of example

in Fig. 2. The 6 · 10 images each contained 2.1 instruments on average, such
that 122 reference instrument segmentations (data set Y     I)and60bkground
                           B
segmentations (data set Y    ) were obtained for these images.
   All images (i.e. twice as many as those annotated by the experts) were further

annotated by 10 KWs each, yielding 2350 instrument segmentations in total
(data set X I).



2.3   Quality of Crowd Segmentations

The quality of the crowd segmentation was determined via the dice similarity
                                     I
coeﬃcient (DSC) as follows. Let Y      represent the set of binary images corre-
sponding to the reference segmentations of the instruments (|Y | =1I    2,ad
       REF
let X I    ⊂ X represent the set of crowd segmentations for which a reference
                              IREF        I                      I        IREF
annotation was available (|X       | = |Y |). The elements of Y    and X       are
                                 Crowdsourcing for Instrument Segmentation       441























Fig.2. Concept of endoscopic video annotation using crowdsourcing: For each instru-

ment in a given image, 10 annotations are acquired. Majority voting (MV) is applied
to remove outliers. Multiple instruments are then combined using the OR operator to
yield the ﬁnal result (blue) which is very similar to the reference segmentation (yellow).

In this example a Dice Similarity Coeﬃcient (DSC) of 0.95 was achieved


                                                                                REF
again sets (of pixels) representing a particular instrument. Let X     ik ∈ X  I
                                                                                  I
(i: instrument ID; k: KW id) further correspond to the same instrument as Y .     i
Then the DSC

                                           IREF      I
                            DSC     =  2|X ik    ∩ Yi|                           (1)
                                 ik       IREF       I
                                       |Xik    | + |i |

quantiﬁes the similarity of crowd segmentation k for instrument i with the cor-
responding reference segmentation.

   To investigate whether multiple segmentations for one object can be applied

to improve the segmentationREFf the crowd, the crowd segmentations for one
particular instrument X   I     was obtained by majority voting, i.e. a pixel was
                          i
classiﬁed as instrument, if and only if at least 5 KWs had marked it as in-
strument. The resulting DSC with the reference annotations was determined as

follows:


                                            REF
                                      2|X iI    ∩ Y i
                            DSC   i=     IREF       I                            (2)
                                      |X i    | + |Yi|

   The corresponding background annotations were deﬁned as the complement of

all (merged) instrument segmentations.T    oquatifystationperformance
by the crowd, a boxplot ofDSC        and DSC was generated.
                                  ik           i


2.4   Classiﬁer Performance–Experts vs Crowd

For this experiment, we developed a basic instrument classiﬁcation algorithm

based on random forests [10] that classiﬁes each pixel into instrument or back-
442     L. Maier-Hein et al.


ground. Based on a preliminary evaluation on diﬀerent data sets, we used the

following features: the B channel from RGB color space, the S channel from HSV
color space and o1 from opponent color space. The forests were trained with the

bagging approach [10] and consisted of ﬁve trees, each with a maximum depth
of ﬁve. Based on this algorithm and the data described in section 2.2 we trained

three classiﬁer types:

      EXP
  – C      (n): The classiﬁer was trained exclusively on expert data
    (n =10 ,20,..., 50 images)
  – C KW  (n): The classiﬁer was trained on images resulting from merging 10

    crowd annotations for each instrument via majority voting (cf. sec. 2.4 )
    (n =10 ,20,..., 100 images)
      EXP −KW
  – C           (n): A combination of 1. and 2.: The classiﬁer was trained on
    images from the experts and the crowd. For this purpose, half of the data

    was taken from the experts, half was taken from the crowd. In this process,
    each image was included at most once for each training (i.e., it was annotated

    either by an expertor by a KW)
    (n =10 ,20,..., 100 images)


   For a given n and a given classiﬁer type (1.-3.), each of the six video sequences
described in sec. 2.2 was used for testing in a leave-1-out approach. To ensure

comparability of results, the n training images and 10 testing images used were
identical for all methods for a given n. For each training set, 10 random forests

were trained leading to a total of 6·10·10 = 600 automatically annotated images
for testing.

   To investigate the inﬂuence of diﬀerent variables (group: crowd, expert, mixed;
surgery: OP1,...,OP6; number of training samples: n)o     nciinp-r

mance, we applied multiple linear regression modelling. Models of the following
form were ﬁtted for the three outcomes true positive (TP) rate, true negative

(TN) rate and precision:

  Y  = µ + α X     + α X    + β X    + ... + β X    + γ X    + ... + γ X    + e ,
    i         1  i1    2  i2    1  i1          4  i4    1  i1          5  i5    i

where Y iepresents the ith observation of the respective outcome and µ denotes

the common mean. Further, α     1 and α 2re regression coeﬃcients corresponding
to the dummy variables X and1X ,where2        X 11representsacrowdsegmen-
tation and X =1representsamixedsegmentation(             X · X =0).Th  us,they
              2                                            1    2
quantify the diﬀerence in the outcome to expert segmentations that serve as the
reference. In an analogous manner, β    to β are regression coeﬃcients pertaining
                                      1     4
to the number of training images (20, 30, 40, 50) with 10 training images being
the reference, and γ  to γ similarly adjust for diﬀerences between surgeries. The
                    1     5
eidenote normally distributed random errors with mean zero. We considered p
values <0.05 of the regression coeﬃcients to indicate statistical signiﬁcance.

   For n> 50, we only had data from the crowd and the mixed group. In this
case, we used an analogous model with X     2 omitted.
                                    Crowdsourcing for Instrument Segmentation     443






















     Fig.3. Box plot (median, ﬁrst and third quartiles, minimum and maximum) of the Dice
     Similarity Coeﬃcient (DSC) for all crowd segmentations for which reference data was
     available (60 images; n = 1200) as well as for the annotations obtained with majority

     voting (n = 120).



st luseR


     The mean time required for obtaining one segmentation for each tool in 20
     images (i.e. for one surgery) was 39 ± 11 min averaged over 10 requests (i.e.

     uploads of HITs). Hence, all 2350 annotations were available in less than 24
     hours. The mean DSC for the crowd was 0.89 ± 0.13 (n = 1200     1)averagedover

     all instruments in all images. This could be increased to 0.93 ± 0.05 (n = 120)
     using the concept of majority voting. Figure 3 shows boxplots of the DSC for

     the individual KWs as well as for the majority voting.


     Table 1.Selection of model coeﬃcients from multiple linear regression models of true

     positive (TP) rate, true negative (TN) rate and precision for the experiment with
     n ≤ 50.


                        Variable       TP rate TN rate Precision

                        (Intercept)     0.519    0.970     0.769

                        Experts         (ref.)   (ref.)    (ref.)
                        Crowd           0.012   −0.002∗   −0.007
                        Mixed           0.005   −0.001    −0.004

                        ∗p< 0.05


        With respect to the type of segmentation (by crowd, experts or mixed) the

     only statistically signiﬁcant impact on any of the three outcomes (TP rate, TN
     rate, precision) was observed for the TN rate in the case of the experiment

      1
       2 · 10 data sets had to be excluded due to a misplaced bounding box in two images.
         444    L. Maier-Hein et al.


         with ≤ 50 training images (p =0 .04). However, in this case, the diﬀerence
         in performance was only -0.2%. According to the model applied, the surgical
         data set used for testing explained by far the most variation in segmentation

         performance and was highly statistically signiﬁcantly associated with TP rate,
         TN rate and precision (all p< 0.001). A selection of regression coeﬃcients for
         the experiment with n ≤ 50 are shown in Tab. 1.



noissucs 4iD


         To our knowledge, we are the ﬁrst to apply the concept of crowdsourcing for
         training classiﬁcation algorithms in the context of computer-assisted MIS. In
         this study, we showed that the segmentations of medical tools generated by

         anonymous untrained workers are comparable to those made by medical ex-
         perts. Outliers can be removed with high reliability when using the concept of
         majority voting. The number of annotations required to reliably remove outliers,

         however, remains to be investigated. An important result of our study is that
         the performance of an endoscopic object classiﬁer was not statistically diﬀerent
         when trained with crowd data compared to expert data. Given the speed of an-
         notation, scalability and low costs, this implies that the community might no

         longer need to rely on experts to generate reference or training data.
           Amiinfurdyol enniheththem tin
         tool only allowed for drawing a polygon, and hence, tools with holes could not

         be segmented with maximal speciﬁcity. As the experts used diﬀerent software
         to annotate the images, a dice coeﬃcient of 100% could thus not be achieved.
         Furthermore, to distinguish multiple instruments in a single endoscopic image

         from each other, we manually positioned a contour with bounding box in the
         images. Future work should investigate computing automatic bounding boxes
         such that manual pre-processing is not necessary at all.

           It is worth mentioning that the classiﬁer performance achieved may not appear
         to be very high. One explanation is that we used endoscopic images with very
         high variability (6 diﬀerent surgeries) in order to capture a maximum of scene
         variance. However, the focus of this study was to investigate the quality of the

         crowd annotations rather than the performance of a speciﬁc algorithm.
           Although crowdsourcing platforms are becoming increasingly popular, user
         performance varies greatly. To improve performance, researchers have explored

         developing games to motivate workers, using qualiﬁcation tests to eliminate un-
         qualiﬁed workers, incorporating veriﬁcation tasks to conﬁrm that workers are
         paying attention, applying the concept of priming and - like us in this study

         - duplicating eﬀort across many workers. Although majority voting improved
         the DSC by only 5% in this study, the minimum could be improved from 0 to
         > 0.7. Based on a parallel study on correspondence establishment [11] and the

         results of this study, we believe that outlier removal is critical to fully exploit
         the potential of the crowd in the context of MIS.
           We showed that non-experts are able to generate high quality image segmen-

         tations, which implies that the physicians’ expert knowledge and experience is
                                 Crowdsourcing for Instrument Segmentation         445


not necessary for this particular task. Future studies should explicitly aim to

identify further key applications but also limitations of crowdsourcing in the
context of medical image computing and computer-assisted interventions.



References


 1. Maier-Hein, L., Mountney, P., Bartoli, A., Elhawary, H., Elson, D., Groch, A.,
    Kolb, A., Rodrigues, M., Sorger, J., Speidel, S., Stoyanov, D.: Optical techniques

    for 3d surface reconstruction in computer-assisted laparoscopic surgery. Med. Image
    Anal. 17, 974–996 (2013)
 2. Ahmidi, N., Gao, Y., Be  ´jar, B., Vedula, S.S., Khudanpur, S., Vidal, R., Hager,

    G.D.: String motif-based description of tool motion for detecting skill and gestures
    in robotic surgery. In: Mori, K., Sakuma, I., Sato, Y., Barillot, C., Navab, N. (eds.)
    MICCAI 2013, Part I. LNCS, vol. 8149, pp. 26–33. Springer, Heidelberg (2013)

 3. Katic, D., Wekerle, A.L., Go  ¨rtler, J., Spengler, P., Bodenstedt, S., Ro ¨hl, S.,
    Suwelack, S., Kenngott, H.G., Wagner, M., M¨ uller-Stich, B.P., Dillmann, R., Spei-
    del, S.: Context-aware augmented reality in laparoscopic surgery. Comp. Med.

    Imag. and Graph. 37(2), 174–182 (2013)
 4. Allan, M., Ourselin, S., Thompson, S., Hawkes, D., Kelly, J., Stoyanov, D.: Toward
    detection and localization of instruments in minimally invasive surgery. IEEE T.

    Bio-med. Eng. 60(4), 1050–1058 (2013)
 5. Ranard, B., Ha, Y., Meisel, Z., Asch, D., Hill, S., Becker, L., Seymour, A., Mer-
    chant, R.: Crowdsourcing - harnessing the masses to advance health and medicine,

    asystematicreview.J.Gen.Intern.Med.29(1),187–203(2014)
 6. Nguyen, T.B., Wang, S., Anugu, V., Rose, N., McKenna, M., Petrick, N., Burns,
    J.E., Summers, R.M.: Distributed human intelligence for colonic polyp classiﬁca-

    tion in computer-aided detection for CT colonography. Radiology 262(3), 824–833
    (2012)
 7. Chen, C., White, L., Kowalewski, T., Aggarwal, R., Lintott, C., Comstock, B.,

    Kuksenok, K., Aragon, C., Holst, D., Lendvay, T.: Crowd-sourced assessment of
    technical skills: a novel method to evaluate surgical performance. J. Surg. Res. 187,
    65–71 (2014)

 8. Foncubierta Rodr´ ıguez, A., M¨ller, H.: Ground truth generation in medical imag-
    ing: A crowdsourcing-based iterative approach. In: Proceedings of the ACM Mul-
    timedia 2012 Workshop on Crowdsourcing for Multimedia, CrowdMM 2012, pp.

    9–14. ACM, New York (2012)
 9. Chen, J.J., Menezes, N.J., Bradley, A.D., North, T.: Opportunities for crowdsourc-
    ing research on amazon mechanical turk. Interfaces 5(3) (2011)

10. Breiman, L.: Random forests. Machine Learning 45(1), 5–32 (2001)
11. Maier-Hein, L., et al.: Crowdsourcing for reference correspondence generation in
    endoscopic images. In: Golland, P., Hata, N., Barillot, C., Hornegger, J., Howe, R.

    (eds.) MICCAI 2014. LNCS, vol. 8674, pp. 345–352. Springer, Heidelberg (2014)